{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba5b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archticture:  Deep_Emotion(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "===================================Start Training===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4277: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4215: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.01435783 \tValidation Loss 0.01479289 \tTraining Acuuarcy 23.961% \tValidation Acuuarcy 22.848%\n",
      "Epoch: 2 \tTraining Loss: 0.01425000 \tValidation Loss 0.01471436 \tTraining Acuuarcy 24.579% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 3 \tTraining Loss: 0.01423477 \tValidation Loss 0.01471043 \tTraining Acuuarcy 25.025% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 4 \tTraining Loss: 0.01423052 \tValidation Loss 0.01470090 \tTraining Acuuarcy 24.947% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 5 \tTraining Loss: 0.01420315 \tValidation Loss 0.01472893 \tTraining Acuuarcy 25.075% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 6 \tTraining Loss: 0.01418517 \tValidation Loss 0.01470394 \tTraining Acuuarcy 25.103% \tValidation Acuuarcy 24.659%\n",
      "Epoch: 7 \tTraining Loss: 0.01419298 \tValidation Loss 0.01480615 \tTraining Acuuarcy 24.964% \tValidation Acuuarcy 25.104%\n",
      "Epoch: 8 \tTraining Loss: 0.01418384 \tValidation Loss 0.01485690 \tTraining Acuuarcy 25.070% \tValidation Acuuarcy 25.021%\n",
      "Epoch: 9 \tTraining Loss: 0.01417035 \tValidation Loss 0.01476907 \tTraining Acuuarcy 24.958% \tValidation Acuuarcy 24.965%\n",
      "Epoch: 10 \tTraining Loss: 0.01415667 \tValidation Loss 0.01480610 \tTraining Acuuarcy 25.125% \tValidation Acuuarcy 24.965%\n",
      "Epoch: 11 \tTraining Loss: 0.01415463 \tValidation Loss 0.01476423 \tTraining Acuuarcy 25.142% \tValidation Acuuarcy 24.213%\n",
      "Epoch: 12 \tTraining Loss: 0.01414504 \tValidation Loss 0.01474022 \tTraining Acuuarcy 25.092% \tValidation Acuuarcy 24.993%\n",
      "Epoch: 13 \tTraining Loss: 0.01414596 \tValidation Loss 0.01490992 \tTraining Acuuarcy 25.131% \tValidation Acuuarcy 24.603%\n",
      "Epoch: 14 \tTraining Loss: 0.01411815 \tValidation Loss 0.01499228 \tTraining Acuuarcy 25.242% \tValidation Acuuarcy 24.575%\n",
      "Epoch: 15 \tTraining Loss: 0.01409894 \tValidation Loss 0.01489622 \tTraining Acuuarcy 25.326% \tValidation Acuuarcy 23.823%\n",
      "Epoch: 16 \tTraining Loss: 0.01409410 \tValidation Loss 0.01486074 \tTraining Acuuarcy 25.426% \tValidation Acuuarcy 24.519%\n",
      "Epoch: 17 \tTraining Loss: 0.01407242 \tValidation Loss 0.01486967 \tTraining Acuuarcy 25.359% \tValidation Acuuarcy 24.101%\n",
      "Epoch: 18 \tTraining Loss: 0.01405289 \tValidation Loss 0.01511387 \tTraining Acuuarcy 25.477% \tValidation Acuuarcy 24.603%\n",
      "Epoch: 19 \tTraining Loss: 0.01403484 \tValidation Loss 0.01487360 \tTraining Acuuarcy 25.772% \tValidation Acuuarcy 24.687%\n",
      "Epoch: 20 \tTraining Loss: 0.01402670 \tValidation Loss 0.01524328 \tTraining Acuuarcy 25.560% \tValidation Acuuarcy 23.405%\n",
      "Epoch: 21 \tTraining Loss: 0.01399657 \tValidation Loss 0.01499194 \tTraining Acuuarcy 25.984% \tValidation Acuuarcy 23.739%\n",
      "Epoch: 22 \tTraining Loss: 0.01399210 \tValidation Loss 0.01502711 \tTraining Acuuarcy 26.067% \tValidation Acuuarcy 24.408%\n",
      "Epoch: 23 \tTraining Loss: 0.01394163 \tValidation Loss 0.01501054 \tTraining Acuuarcy 26.246% \tValidation Acuuarcy 22.569%\n",
      "Epoch: 24 \tTraining Loss: 0.01392769 \tValidation Loss 0.01502140 \tTraining Acuuarcy 26.123% \tValidation Acuuarcy 22.959%\n",
      "Epoch: 25 \tTraining Loss: 0.01387634 \tValidation Loss 0.01496325 \tTraining Acuuarcy 26.753% \tValidation Acuuarcy 22.262%\n",
      "Epoch: 26 \tTraining Loss: 0.01385113 \tValidation Loss 0.01505292 \tTraining Acuuarcy 26.970% \tValidation Acuuarcy 22.541%\n",
      "Epoch: 27 \tTraining Loss: 0.01385606 \tValidation Loss 0.01516165 \tTraining Acuuarcy 26.658% \tValidation Acuuarcy 22.959%\n",
      "Epoch: 28 \tTraining Loss: 0.01379066 \tValidation Loss 0.01518019 \tTraining Acuuarcy 27.344% \tValidation Acuuarcy 22.541%\n",
      "Epoch: 29 \tTraining Loss: 0.01379160 \tValidation Loss 0.01517713 \tTraining Acuuarcy 27.054% \tValidation Acuuarcy 23.154%\n",
      "Epoch: 30 \tTraining Loss: 0.01374131 \tValidation Loss 0.01525950 \tTraining Acuuarcy 27.806% \tValidation Acuuarcy 22.875%\n",
      "Epoch: 31 \tTraining Loss: 0.01371097 \tValidation Loss 0.01532202 \tTraining Acuuarcy 28.068% \tValidation Acuuarcy 22.987%\n",
      "Epoch: 32 \tTraining Loss: 0.01363065 \tValidation Loss 0.01524902 \tTraining Acuuarcy 28.280% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 33 \tTraining Loss: 0.01363465 \tValidation Loss 0.01536534 \tTraining Acuuarcy 28.481% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 34 \tTraining Loss: 0.01359536 \tValidation Loss 0.01534820 \tTraining Acuuarcy 28.653% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 35 \tTraining Loss: 0.01355740 \tValidation Loss 0.01557227 \tTraining Acuuarcy 29.244% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 36 \tTraining Loss: 0.01351605 \tValidation Loss 0.01548950 \tTraining Acuuarcy 29.612% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 37 \tTraining Loss: 0.01348380 \tValidation Loss 0.01565404 \tTraining Acuuarcy 29.668% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 38 \tTraining Loss: 0.01346053 \tValidation Loss 0.01543692 \tTraining Acuuarcy 29.974% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 39 \tTraining Loss: 0.01340283 \tValidation Loss 0.01545814 \tTraining Acuuarcy 30.504% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 40 \tTraining Loss: 0.01336070 \tValidation Loss 0.01548467 \tTraining Acuuarcy 30.749% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 41 \tTraining Loss: 0.01327511 \tValidation Loss 0.01576292 \tTraining Acuuarcy 31.412% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 42 \tTraining Loss: 0.01327114 \tValidation Loss 0.01576179 \tTraining Acuuarcy 31.306% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 43 \tTraining Loss: 0.01325182 \tValidation Loss 0.01591129 \tTraining Acuuarcy 31.345% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 44 \tTraining Loss: 0.01319736 \tValidation Loss 0.01593581 \tTraining Acuuarcy 31.830% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 45 \tTraining Loss: 0.01314082 \tValidation Loss 0.01615842 \tTraining Acuuarcy 32.076% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 46 \tTraining Loss: 0.01312379 \tValidation Loss 0.01583236 \tTraining Acuuarcy 32.165% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 47 \tTraining Loss: 0.01307339 \tValidation Loss 0.01600997 \tTraining Acuuarcy 32.538% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 48 \tTraining Loss: 0.01304779 \tValidation Loss 0.01597049 \tTraining Acuuarcy 32.544% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 49 \tTraining Loss: 0.01295599 \tValidation Loss 0.01627587 \tTraining Acuuarcy 33.536% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 50 \tTraining Loss: 0.01293228 \tValidation Loss 0.01606522 \tTraining Acuuarcy 33.235% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 51 \tTraining Loss: 0.01288766 \tValidation Loss 0.01626156 \tTraining Acuuarcy 34.032% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 52 \tTraining Loss: 0.01285145 \tValidation Loss 0.01631030 \tTraining Acuuarcy 33.803% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 53 \tTraining Loss: 0.01280354 \tValidation Loss 0.01650539 \tTraining Acuuarcy 34.717% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 54 \tTraining Loss: 0.01276978 \tValidation Loss 0.01641662 \tTraining Acuuarcy 34.500% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 55 \tTraining Loss: 0.01272696 \tValidation Loss 0.01636121 \tTraining Acuuarcy 35.191% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 56 \tTraining Loss: 0.01273902 \tValidation Loss 0.01672310 \tTraining Acuuarcy 34.823% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 57 \tTraining Loss: 0.01268782 \tValidation Loss 0.01656086 \tTraining Acuuarcy 35.007% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 58 \tTraining Loss: 0.01261651 \tValidation Loss 0.01677091 \tTraining Acuuarcy 35.849% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 59 \tTraining Loss: 0.01260164 \tValidation Loss 0.01674702 \tTraining Acuuarcy 35.576% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 60 \tTraining Loss: 0.01256235 \tValidation Loss 0.01676224 \tTraining Acuuarcy 36.284% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 61 \tTraining Loss: 0.01250576 \tValidation Loss 0.01658282 \tTraining Acuuarcy 36.741% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 62 \tTraining Loss: 0.01249399 \tValidation Loss 0.01691525 \tTraining Acuuarcy 36.317% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 63 \tTraining Loss: 0.01243418 \tValidation Loss 0.01679340 \tTraining Acuuarcy 36.690% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 64 \tTraining Loss: 0.01245148 \tValidation Loss 0.01692304 \tTraining Acuuarcy 36.802% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 65 \tTraining Loss: 0.01236355 \tValidation Loss 0.01682741 \tTraining Acuuarcy 37.226% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 66 \tTraining Loss: 0.01237154 \tValidation Loss 0.01673300 \tTraining Acuuarcy 37.270% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 67 \tTraining Loss: 0.01228058 \tValidation Loss 0.01710453 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 68 \tTraining Loss: 0.01226741 \tValidation Loss 0.01732945 \tTraining Acuuarcy 37.894% \tValidation Acuuarcy 19.699%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 \tTraining Loss: 0.01226030 \tValidation Loss 0.01678302 \tTraining Acuuarcy 37.683% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 70 \tTraining Loss: 0.01220158 \tValidation Loss 0.01705059 \tTraining Acuuarcy 38.452% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 71 \tTraining Loss: 0.01217680 \tValidation Loss 0.01721565 \tTraining Acuuarcy 38.268% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 72 \tTraining Loss: 0.01219527 \tValidation Loss 0.01726237 \tTraining Acuuarcy 38.323% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 73 \tTraining Loss: 0.01214383 \tValidation Loss 0.01737178 \tTraining Acuuarcy 38.552% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 74 \tTraining Loss: 0.01210750 \tValidation Loss 0.01719566 \tTraining Acuuarcy 38.964% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 75 \tTraining Loss: 0.01205507 \tValidation Loss 0.01765435 \tTraining Acuuarcy 39.260% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 76 \tTraining Loss: 0.01207949 \tValidation Loss 0.01781172 \tTraining Acuuarcy 39.600% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 77 \tTraining Loss: 0.01200781 \tValidation Loss 0.01736598 \tTraining Acuuarcy 39.427% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 78 \tTraining Loss: 0.01201924 \tValidation Loss 0.01755856 \tTraining Acuuarcy 39.176% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 79 \tTraining Loss: 0.01190308 \tValidation Loss 0.01741441 \tTraining Acuuarcy 40.469% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 80 \tTraining Loss: 0.01189916 \tValidation Loss 0.01749637 \tTraining Acuuarcy 39.828% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 81 \tTraining Loss: 0.01187520 \tValidation Loss 0.01803946 \tTraining Acuuarcy 40.308% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 82 \tTraining Loss: 0.01183965 \tValidation Loss 0.01762267 \tTraining Acuuarcy 40.865% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 83 \tTraining Loss: 0.01191956 \tValidation Loss 0.01768807 \tTraining Acuuarcy 40.140% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 84 \tTraining Loss: 0.01185523 \tValidation Loss 0.01770826 \tTraining Acuuarcy 40.369% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 85 \tTraining Loss: 0.01176676 \tValidation Loss 0.01818035 \tTraining Acuuarcy 40.943% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 86 \tTraining Loss: 0.01177919 \tValidation Loss 0.01778880 \tTraining Acuuarcy 41.015% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 87 \tTraining Loss: 0.01179984 \tValidation Loss 0.01818800 \tTraining Acuuarcy 40.458% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 88 \tTraining Loss: 0.01167122 \tValidation Loss 0.01843044 \tTraining Acuuarcy 41.528% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 89 \tTraining Loss: 0.01167074 \tValidation Loss 0.01814032 \tTraining Acuuarcy 41.534% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 90 \tTraining Loss: 0.01159326 \tValidation Loss 0.01799659 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 91 \tTraining Loss: 0.01160527 \tValidation Loss 0.01830420 \tTraining Acuuarcy 41.668% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 92 \tTraining Loss: 0.01167026 \tValidation Loss 0.01790540 \tTraining Acuuarcy 41.651% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 93 \tTraining Loss: 0.01168155 \tValidation Loss 0.01813898 \tTraining Acuuarcy 41.422% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 94 \tTraining Loss: 0.01152954 \tValidation Loss 0.01795216 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 95 \tTraining Loss: 0.01158016 \tValidation Loss 0.01842983 \tTraining Acuuarcy 42.125% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 96 \tTraining Loss: 0.01154223 \tValidation Loss 0.01805620 \tTraining Acuuarcy 41.996% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 97 \tTraining Loss: 0.01147257 \tValidation Loss 0.01813664 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 98 \tTraining Loss: 0.01148198 \tValidation Loss 0.01853304 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 99 \tTraining Loss: 0.01138515 \tValidation Loss 0.01850873 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 100 \tTraining Loss: 0.01147898 \tValidation Loss 0.01834403 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 101 \tTraining Loss: 0.01146652 \tValidation Loss 0.01845622 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 102 \tTraining Loss: 0.01141875 \tValidation Loss 0.01839024 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 103 \tTraining Loss: 0.01142585 \tValidation Loss 0.01844686 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 104 \tTraining Loss: 0.01135352 \tValidation Loss 0.01848049 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 105 \tTraining Loss: 0.01135466 \tValidation Loss 0.01884888 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 106 \tTraining Loss: 0.01127881 \tValidation Loss 0.01835286 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 107 \tTraining Loss: 0.01134763 \tValidation Loss 0.01863757 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 108 \tTraining Loss: 0.01129285 \tValidation Loss 0.01930295 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 109 \tTraining Loss: 0.01130068 \tValidation Loss 0.01874869 \tTraining Acuuarcy 44.254% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 110 \tTraining Loss: 0.01136107 \tValidation Loss 0.01855356 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 111 \tTraining Loss: 0.01128415 \tValidation Loss 0.01851214 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 112 \tTraining Loss: 0.01126405 \tValidation Loss 0.01875979 \tTraining Acuuarcy 44.137% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 113 \tTraining Loss: 0.01119626 \tValidation Loss 0.01868303 \tTraining Acuuarcy 44.293% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 114 \tTraining Loss: 0.01126812 \tValidation Loss 0.01864126 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 115 \tTraining Loss: 0.01110651 \tValidation Loss 0.01870197 \tTraining Acuuarcy 44.917% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 116 \tTraining Loss: 0.01117305 \tValidation Loss 0.01912405 \tTraining Acuuarcy 45.056% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 117 \tTraining Loss: 0.01123257 \tValidation Loss 0.01866991 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 118 \tTraining Loss: 0.01115719 \tValidation Loss 0.01888436 \tTraining Acuuarcy 45.090% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 119 \tTraining Loss: 0.01118944 \tValidation Loss 0.01897877 \tTraining Acuuarcy 44.421% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 120 \tTraining Loss: 0.01118051 \tValidation Loss 0.01908693 \tTraining Acuuarcy 44.750% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 121 \tTraining Loss: 0.01118620 \tValidation Loss 0.01898299 \tTraining Acuuarcy 44.605% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 122 \tTraining Loss: 0.01113665 \tValidation Loss 0.01875506 \tTraining Acuuarcy 44.884% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 123 \tTraining Loss: 0.01110079 \tValidation Loss 0.01881088 \tTraining Acuuarcy 45.034% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 124 \tTraining Loss: 0.01097396 \tValidation Loss 0.01885438 \tTraining Acuuarcy 45.497% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 125 \tTraining Loss: 0.01106626 \tValidation Loss 0.01934261 \tTraining Acuuarcy 45.413% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 126 \tTraining Loss: 0.01100081 \tValidation Loss 0.01930868 \tTraining Acuuarcy 45.424% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 127 \tTraining Loss: 0.01098335 \tValidation Loss 0.01929087 \tTraining Acuuarcy 45.268% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 128 \tTraining Loss: 0.01100283 \tValidation Loss 0.01891992 \tTraining Acuuarcy 45.853% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 129 \tTraining Loss: 0.01098515 \tValidation Loss 0.01909839 \tTraining Acuuarcy 45.446% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 130 \tTraining Loss: 0.01100245 \tValidation Loss 0.01950423 \tTraining Acuuarcy 45.491% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 131 \tTraining Loss: 0.01095006 \tValidation Loss 0.01948629 \tTraining Acuuarcy 45.742% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 132 \tTraining Loss: 0.01101473 \tValidation Loss 0.01937778 \tTraining Acuuarcy 45.809% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 133 \tTraining Loss: 0.01099480 \tValidation Loss 0.01921736 \tTraining Acuuarcy 46.065% \tValidation Acuuarcy 16.272%\n",
      "Epoch: 134 \tTraining Loss: 0.01091994 \tValidation Loss 0.01945446 \tTraining Acuuarcy 45.602% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 135 \tTraining Loss: 0.01091200 \tValidation Loss 0.01924735 \tTraining Acuuarcy 46.087% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 136 \tTraining Loss: 0.01091025 \tValidation Loss 0.01959389 \tTraining Acuuarcy 45.981% \tValidation Acuuarcy 18.222%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137 \tTraining Loss: 0.01084616 \tValidation Loss 0.01917907 \tTraining Acuuarcy 46.221% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 138 \tTraining Loss: 0.01084935 \tValidation Loss 0.01936367 \tTraining Acuuarcy 46.333% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 139 \tTraining Loss: 0.01084608 \tValidation Loss 0.01940234 \tTraining Acuuarcy 46.394% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 140 \tTraining Loss: 0.01088749 \tValidation Loss 0.01943430 \tTraining Acuuarcy 46.628% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 141 \tTraining Loss: 0.01084783 \tValidation Loss 0.01922976 \tTraining Acuuarcy 46.667% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 142 \tTraining Loss: 0.01086073 \tValidation Loss 0.01955004 \tTraining Acuuarcy 46.238% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 143 \tTraining Loss: 0.01081877 \tValidation Loss 0.01917986 \tTraining Acuuarcy 46.650% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 144 \tTraining Loss: 0.01090278 \tValidation Loss 0.01921402 \tTraining Acuuarcy 46.416% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 145 \tTraining Loss: 0.01079606 \tValidation Loss 0.01984145 \tTraining Acuuarcy 47.057% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 146 \tTraining Loss: 0.01089823 \tValidation Loss 0.01927453 \tTraining Acuuarcy 46.310% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 147 \tTraining Loss: 0.01079434 \tValidation Loss 0.01955833 \tTraining Acuuarcy 47.375% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 148 \tTraining Loss: 0.01077827 \tValidation Loss 0.01938774 \tTraining Acuuarcy 46.288% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 149 \tTraining Loss: 0.01073905 \tValidation Loss 0.01957310 \tTraining Acuuarcy 47.057% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 150 \tTraining Loss: 0.01075447 \tValidation Loss 0.01959988 \tTraining Acuuarcy 46.661% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 151 \tTraining Loss: 0.01076870 \tValidation Loss 0.01982887 \tTraining Acuuarcy 46.617% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 152 \tTraining Loss: 0.01076703 \tValidation Loss 0.01949672 \tTraining Acuuarcy 46.912% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 153 \tTraining Loss: 0.01075540 \tValidation Loss 0.01974589 \tTraining Acuuarcy 47.247% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 154 \tTraining Loss: 0.01070515 \tValidation Loss 0.01989936 \tTraining Acuuarcy 47.202% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 155 \tTraining Loss: 0.01073191 \tValidation Loss 0.01972786 \tTraining Acuuarcy 47.252% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 156 \tTraining Loss: 0.01067941 \tValidation Loss 0.01984043 \tTraining Acuuarcy 47.531% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 157 \tTraining Loss: 0.01070036 \tValidation Loss 0.01992083 \tTraining Acuuarcy 47.470% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 158 \tTraining Loss: 0.01064784 \tValidation Loss 0.01983053 \tTraining Acuuarcy 47.932% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 159 \tTraining Loss: 0.01068909 \tValidation Loss 0.01942571 \tTraining Acuuarcy 47.341% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 160 \tTraining Loss: 0.01061821 \tValidation Loss 0.02000442 \tTraining Acuuarcy 47.648% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 161 \tTraining Loss: 0.01063684 \tValidation Loss 0.01993996 \tTraining Acuuarcy 47.720% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 162 \tTraining Loss: 0.01060749 \tValidation Loss 0.01967264 \tTraining Acuuarcy 47.776% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 163 \tTraining Loss: 0.01057893 \tValidation Loss 0.02005250 \tTraining Acuuarcy 47.843% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 164 \tTraining Loss: 0.01058298 \tValidation Loss 0.01957059 \tTraining Acuuarcy 47.726% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 165 \tTraining Loss: 0.01057653 \tValidation Loss 0.01988084 \tTraining Acuuarcy 48.027% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 166 \tTraining Loss: 0.01052900 \tValidation Loss 0.01986536 \tTraining Acuuarcy 47.932% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 167 \tTraining Loss: 0.01054903 \tValidation Loss 0.02040544 \tTraining Acuuarcy 47.832% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 168 \tTraining Loss: 0.01062753 \tValidation Loss 0.01991303 \tTraining Acuuarcy 47.704% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 169 \tTraining Loss: 0.01050923 \tValidation Loss 0.01993114 \tTraining Acuuarcy 48.618% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 170 \tTraining Loss: 0.01055938 \tValidation Loss 0.01981124 \tTraining Acuuarcy 48.150% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 171 \tTraining Loss: 0.01062421 \tValidation Loss 0.01977319 \tTraining Acuuarcy 48.150% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 172 \tTraining Loss: 0.01053421 \tValidation Loss 0.02023677 \tTraining Acuuarcy 48.517% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 173 \tTraining Loss: 0.01048060 \tValidation Loss 0.01983392 \tTraining Acuuarcy 48.657% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 174 \tTraining Loss: 0.01043607 \tValidation Loss 0.01984293 \tTraining Acuuarcy 48.395% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 175 \tTraining Loss: 0.01045596 \tValidation Loss 0.02030148 \tTraining Acuuarcy 49.186% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 176 \tTraining Loss: 0.01047652 \tValidation Loss 0.02017087 \tTraining Acuuarcy 48.367% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 177 \tTraining Loss: 0.01055579 \tValidation Loss 0.02026753 \tTraining Acuuarcy 47.771% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 178 \tTraining Loss: 0.01047965 \tValidation Loss 0.02025595 \tTraining Acuuarcy 48.484% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 179 \tTraining Loss: 0.01041211 \tValidation Loss 0.01994411 \tTraining Acuuarcy 49.008% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 180 \tTraining Loss: 0.01037612 \tValidation Loss 0.02063829 \tTraining Acuuarcy 49.348% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 181 \tTraining Loss: 0.01045129 \tValidation Loss 0.02036353 \tTraining Acuuarcy 48.707% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 182 \tTraining Loss: 0.01043351 \tValidation Loss 0.02046736 \tTraining Acuuarcy 48.534% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 183 \tTraining Loss: 0.01042584 \tValidation Loss 0.02080142 \tTraining Acuuarcy 48.885% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 184 \tTraining Loss: 0.01046828 \tValidation Loss 0.02061591 \tTraining Acuuarcy 48.523% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 185 \tTraining Loss: 0.01035384 \tValidation Loss 0.02049034 \tTraining Acuuarcy 48.757% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 186 \tTraining Loss: 0.01037363 \tValidation Loss 0.02020896 \tTraining Acuuarcy 49.008% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 187 \tTraining Loss: 0.01036062 \tValidation Loss 0.01993132 \tTraining Acuuarcy 49.510% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 188 \tTraining Loss: 0.01041938 \tValidation Loss 0.01985890 \tTraining Acuuarcy 49.225% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 189 \tTraining Loss: 0.01048289 \tValidation Loss 0.02070067 \tTraining Acuuarcy 48.356% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 190 \tTraining Loss: 0.01028497 \tValidation Loss 0.01994500 \tTraining Acuuarcy 49.682% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 191 \tTraining Loss: 0.01035549 \tValidation Loss 0.02028904 \tTraining Acuuarcy 48.835% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 192 \tTraining Loss: 0.01025727 \tValidation Loss 0.02079512 \tTraining Acuuarcy 49.994% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 193 \tTraining Loss: 0.01028222 \tValidation Loss 0.02100825 \tTraining Acuuarcy 49.627% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 194 \tTraining Loss: 0.01027432 \tValidation Loss 0.02099180 \tTraining Acuuarcy 49.944% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 195 \tTraining Loss: 0.01033953 \tValidation Loss 0.02076439 \tTraining Acuuarcy 49.922% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 196 \tTraining Loss: 0.01038403 \tValidation Loss 0.02041859 \tTraining Acuuarcy 48.935% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 197 \tTraining Loss: 0.01034030 \tValidation Loss 0.02037993 \tTraining Acuuarcy 49.571% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 198 \tTraining Loss: 0.01029228 \tValidation Loss 0.02050678 \tTraining Acuuarcy 49.922% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 199 \tTraining Loss: 0.01033193 \tValidation Loss 0.02046462 \tTraining Acuuarcy 49.236% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 200 \tTraining Loss: 0.01026770 \tValidation Loss 0.02031553 \tTraining Acuuarcy 49.732% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 201 \tTraining Loss: 0.01033146 \tValidation Loss 0.02096254 \tTraining Acuuarcy 49.314% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 202 \tTraining Loss: 0.01033862 \tValidation Loss 0.02065546 \tTraining Acuuarcy 49.080% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 203 \tTraining Loss: 0.01019045 \tValidation Loss 0.02037279 \tTraining Acuuarcy 50.262% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 204 \tTraining Loss: 0.01020531 \tValidation Loss 0.02080535 \tTraining Acuuarcy 50.212% \tValidation Acuuarcy 18.445%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205 \tTraining Loss: 0.01017120 \tValidation Loss 0.02170177 \tTraining Acuuarcy 50.150% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 206 \tTraining Loss: 0.01020303 \tValidation Loss 0.02119471 \tTraining Acuuarcy 49.916% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 207 \tTraining Loss: 0.01013306 \tValidation Loss 0.02041289 \tTraining Acuuarcy 50.162% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 208 \tTraining Loss: 0.01027721 \tValidation Loss 0.02053412 \tTraining Acuuarcy 50.033% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 209 \tTraining Loss: 0.01025340 \tValidation Loss 0.02106524 \tTraining Acuuarcy 49.693% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 210 \tTraining Loss: 0.01017477 \tValidation Loss 0.02016896 \tTraining Acuuarcy 50.145% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 211 \tTraining Loss: 0.01023832 \tValidation Loss 0.02062268 \tTraining Acuuarcy 49.298% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 212 \tTraining Loss: 0.01024718 \tValidation Loss 0.02090547 \tTraining Acuuarcy 49.432% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 213 \tTraining Loss: 0.01022982 \tValidation Loss 0.02128426 \tTraining Acuuarcy 49.989% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 214 \tTraining Loss: 0.01021832 \tValidation Loss 0.02051598 \tTraining Acuuarcy 50.507% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 215 \tTraining Loss: 0.01018635 \tValidation Loss 0.02124264 \tTraining Acuuarcy 50.245% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 216 \tTraining Loss: 0.01011296 \tValidation Loss 0.02156739 \tTraining Acuuarcy 50.401% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 217 \tTraining Loss: 0.01011703 \tValidation Loss 0.02069064 \tTraining Acuuarcy 50.072% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 218 \tTraining Loss: 0.01021184 \tValidation Loss 0.02068877 \tTraining Acuuarcy 50.061% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 219 \tTraining Loss: 0.01009369 \tValidation Loss 0.02167210 \tTraining Acuuarcy 50.947% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 220 \tTraining Loss: 0.01021186 \tValidation Loss 0.02107008 \tTraining Acuuarcy 50.022% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 221 \tTraining Loss: 0.01006435 \tValidation Loss 0.02099883 \tTraining Acuuarcy 50.608% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 222 \tTraining Loss: 0.01013111 \tValidation Loss 0.02085674 \tTraining Acuuarcy 50.474% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 223 \tTraining Loss: 0.01008508 \tValidation Loss 0.02091567 \tTraining Acuuarcy 50.619% \tValidation Acuuarcy 16.662%\n",
      "Epoch: 224 \tTraining Loss: 0.01010313 \tValidation Loss 0.02082604 \tTraining Acuuarcy 50.892% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 225 \tTraining Loss: 0.01002542 \tValidation Loss 0.02105095 \tTraining Acuuarcy 51.159% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 226 \tTraining Loss: 0.01009486 \tValidation Loss 0.02066461 \tTraining Acuuarcy 50.474% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 227 \tTraining Loss: 0.01007829 \tValidation Loss 0.02121918 \tTraining Acuuarcy 50.903% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 228 \tTraining Loss: 0.01004153 \tValidation Loss 0.02186417 \tTraining Acuuarcy 50.987% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 229 \tTraining Loss: 0.01004589 \tValidation Loss 0.02109924 \tTraining Acuuarcy 51.572% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 230 \tTraining Loss: 0.01010819 \tValidation Loss 0.02105956 \tTraining Acuuarcy 50.446% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 231 \tTraining Loss: 0.01017397 \tValidation Loss 0.02088038 \tTraining Acuuarcy 49.894% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 232 \tTraining Loss: 0.01013761 \tValidation Loss 0.02108471 \tTraining Acuuarcy 51.109% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 233 \tTraining Loss: 0.01010558 \tValidation Loss 0.02148612 \tTraining Acuuarcy 50.474% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 234 \tTraining Loss: 0.01006667 \tValidation Loss 0.02102581 \tTraining Acuuarcy 50.936% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 235 \tTraining Loss: 0.00999329 \tValidation Loss 0.02108289 \tTraining Acuuarcy 51.221% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 236 \tTraining Loss: 0.01004361 \tValidation Loss 0.02130853 \tTraining Acuuarcy 50.630% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 237 \tTraining Loss: 0.00999080 \tValidation Loss 0.02089084 \tTraining Acuuarcy 51.416% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 238 \tTraining Loss: 0.01000828 \tValidation Loss 0.02117167 \tTraining Acuuarcy 51.471% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 239 \tTraining Loss: 0.01002569 \tValidation Loss 0.02162641 \tTraining Acuuarcy 50.847% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 240 \tTraining Loss: 0.01003098 \tValidation Loss 0.02129474 \tTraining Acuuarcy 51.193% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 241 \tTraining Loss: 0.00998261 \tValidation Loss 0.02087896 \tTraining Acuuarcy 51.421% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 242 \tTraining Loss: 0.00996070 \tValidation Loss 0.02138760 \tTraining Acuuarcy 51.594% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 243 \tTraining Loss: 0.00998036 \tValidation Loss 0.02147955 \tTraining Acuuarcy 51.466% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 244 \tTraining Loss: 0.01003942 \tValidation Loss 0.02044779 \tTraining Acuuarcy 50.964% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 245 \tTraining Loss: 0.00997134 \tValidation Loss 0.02144302 \tTraining Acuuarcy 51.221% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 246 \tTraining Loss: 0.00992769 \tValidation Loss 0.02107883 \tTraining Acuuarcy 51.862% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 247 \tTraining Loss: 0.00994424 \tValidation Loss 0.02086288 \tTraining Acuuarcy 52.006% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 248 \tTraining Loss: 0.01000771 \tValidation Loss 0.02126855 \tTraining Acuuarcy 51.622% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 249 \tTraining Loss: 0.00997881 \tValidation Loss 0.02131276 \tTraining Acuuarcy 51.505% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 250 \tTraining Loss: 0.01004818 \tValidation Loss 0.02141178 \tTraining Acuuarcy 51.248% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 251 \tTraining Loss: 0.00996521 \tValidation Loss 0.02180786 \tTraining Acuuarcy 51.862% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 252 \tTraining Loss: 0.00999488 \tValidation Loss 0.02161374 \tTraining Acuuarcy 51.020% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 253 \tTraining Loss: 0.00995338 \tValidation Loss 0.02122136 \tTraining Acuuarcy 51.655% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 254 \tTraining Loss: 0.00981215 \tValidation Loss 0.02226516 \tTraining Acuuarcy 52.397% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 255 \tTraining Loss: 0.00990325 \tValidation Loss 0.02147795 \tTraining Acuuarcy 51.951% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 256 \tTraining Loss: 0.00991938 \tValidation Loss 0.02149323 \tTraining Acuuarcy 51.906% \tValidation Acuuarcy 16.439%\n",
      "Epoch: 257 \tTraining Loss: 0.00989551 \tValidation Loss 0.02180173 \tTraining Acuuarcy 51.901% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 258 \tTraining Loss: 0.00997912 \tValidation Loss 0.02118842 \tTraining Acuuarcy 51.566% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 259 \tTraining Loss: 0.00982981 \tValidation Loss 0.02156088 \tTraining Acuuarcy 52.101% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 260 \tTraining Loss: 0.00979287 \tValidation Loss 0.02192524 \tTraining Acuuarcy 52.581% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 261 \tTraining Loss: 0.00999973 \tValidation Loss 0.02215892 \tTraining Acuuarcy 51.065% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 262 \tTraining Loss: 0.00985363 \tValidation Loss 0.02130859 \tTraining Acuuarcy 52.034% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 263 \tTraining Loss: 0.00989401 \tValidation Loss 0.02147727 \tTraining Acuuarcy 51.973% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 264 \tTraining Loss: 0.00986146 \tValidation Loss 0.02218049 \tTraining Acuuarcy 52.006% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 265 \tTraining Loss: 0.00990728 \tValidation Loss 0.02161263 \tTraining Acuuarcy 51.862% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 266 \tTraining Loss: 0.00986610 \tValidation Loss 0.02122772 \tTraining Acuuarcy 52.034% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 267 \tTraining Loss: 0.00985595 \tValidation Loss 0.02166595 \tTraining Acuuarcy 52.397% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 268 \tTraining Loss: 0.00989171 \tValidation Loss 0.02156612 \tTraining Acuuarcy 51.934% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 269 \tTraining Loss: 0.00988019 \tValidation Loss 0.02164267 \tTraining Acuuarcy 51.789% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 270 \tTraining Loss: 0.00982391 \tValidation Loss 0.02215792 \tTraining Acuuarcy 52.018% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 271 \tTraining Loss: 0.00974503 \tValidation Loss 0.02171786 \tTraining Acuuarcy 53.272% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 272 \tTraining Loss: 0.00989261 \tValidation Loss 0.02123666 \tTraining Acuuarcy 51.661% \tValidation Acuuarcy 18.167%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273 \tTraining Loss: 0.00978771 \tValidation Loss 0.02102651 \tTraining Acuuarcy 52.558% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 274 \tTraining Loss: 0.00976006 \tValidation Loss 0.02174200 \tTraining Acuuarcy 52.330% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 275 \tTraining Loss: 0.00972532 \tValidation Loss 0.02147027 \tTraining Acuuarcy 52.436% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 276 \tTraining Loss: 0.00977807 \tValidation Loss 0.02177500 \tTraining Acuuarcy 52.597% \tValidation Acuuarcy 16.662%\n",
      "Epoch: 277 \tTraining Loss: 0.00980604 \tValidation Loss 0.02218093 \tTraining Acuuarcy 51.956% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 278 \tTraining Loss: 0.00970362 \tValidation Loss 0.02203602 \tTraining Acuuarcy 53.249% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 279 \tTraining Loss: 0.00969997 \tValidation Loss 0.02292074 \tTraining Acuuarcy 52.865% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 280 \tTraining Loss: 0.00976577 \tValidation Loss 0.02205750 \tTraining Acuuarcy 52.341% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 281 \tTraining Loss: 0.00977130 \tValidation Loss 0.02195141 \tTraining Acuuarcy 52.597% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 282 \tTraining Loss: 0.00971577 \tValidation Loss 0.02178316 \tTraining Acuuarcy 52.681% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 283 \tTraining Loss: 0.00975233 \tValidation Loss 0.02162831 \tTraining Acuuarcy 52.608% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 284 \tTraining Loss: 0.00975197 \tValidation Loss 0.02145954 \tTraining Acuuarcy 53.116% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 285 \tTraining Loss: 0.00981119 \tValidation Loss 0.02179976 \tTraining Acuuarcy 52.742% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 286 \tTraining Loss: 0.00977578 \tValidation Loss 0.02129498 \tTraining Acuuarcy 52.335% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 287 \tTraining Loss: 0.00974045 \tValidation Loss 0.02177548 \tTraining Acuuarcy 52.296% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 288 \tTraining Loss: 0.00971489 \tValidation Loss 0.02109159 \tTraining Acuuarcy 52.737% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 289 \tTraining Loss: 0.00977668 \tValidation Loss 0.02201428 \tTraining Acuuarcy 52.698% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 290 \tTraining Loss: 0.00968924 \tValidation Loss 0.02214538 \tTraining Acuuarcy 52.971% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 291 \tTraining Loss: 0.00976312 \tValidation Loss 0.02164179 \tTraining Acuuarcy 53.015% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 292 \tTraining Loss: 0.00964736 \tValidation Loss 0.02205760 \tTraining Acuuarcy 53.149% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 293 \tTraining Loss: 0.00961573 \tValidation Loss 0.02159755 \tTraining Acuuarcy 53.472% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 294 \tTraining Loss: 0.00972043 \tValidation Loss 0.02219065 \tTraining Acuuarcy 52.653% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 295 \tTraining Loss: 0.00972428 \tValidation Loss 0.02175829 \tTraining Acuuarcy 52.776% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 296 \tTraining Loss: 0.00964116 \tValidation Loss 0.02203489 \tTraining Acuuarcy 53.288% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 297 \tTraining Loss: 0.00971234 \tValidation Loss 0.02222554 \tTraining Acuuarcy 52.921% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 298 \tTraining Loss: 0.00967472 \tValidation Loss 0.02140357 \tTraining Acuuarcy 52.887% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 299 \tTraining Loss: 0.00970660 \tValidation Loss 0.02139095 \tTraining Acuuarcy 53.121% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 300 \tTraining Loss: 0.00964227 \tValidation Loss 0.02195172 \tTraining Acuuarcy 53.545% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 301 \tTraining Loss: 0.00963842 \tValidation Loss 0.02229582 \tTraining Acuuarcy 52.932% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 302 \tTraining Loss: 0.00963186 \tValidation Loss 0.02217004 \tTraining Acuuarcy 53.004% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 303 \tTraining Loss: 0.00976408 \tValidation Loss 0.02204846 \tTraining Acuuarcy 53.149% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 304 \tTraining Loss: 0.00965539 \tValidation Loss 0.02207135 \tTraining Acuuarcy 53.065% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 305 \tTraining Loss: 0.00958011 \tValidation Loss 0.02247228 \tTraining Acuuarcy 53.706% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 306 \tTraining Loss: 0.00964564 \tValidation Loss 0.02209768 \tTraining Acuuarcy 53.456% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 307 \tTraining Loss: 0.00960238 \tValidation Loss 0.02163941 \tTraining Acuuarcy 53.127% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 308 \tTraining Loss: 0.00959221 \tValidation Loss 0.02194408 \tTraining Acuuarcy 53.851% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 309 \tTraining Loss: 0.00959506 \tValidation Loss 0.02211050 \tTraining Acuuarcy 53.366% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 310 \tTraining Loss: 0.00963348 \tValidation Loss 0.02173678 \tTraining Acuuarcy 53.333% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 311 \tTraining Loss: 0.00955155 \tValidation Loss 0.02231152 \tTraining Acuuarcy 54.080% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 312 \tTraining Loss: 0.00961728 \tValidation Loss 0.02303634 \tTraining Acuuarcy 53.640% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 313 \tTraining Loss: 0.00951857 \tValidation Loss 0.02273997 \tTraining Acuuarcy 53.952% \tValidation Acuuarcy 16.300%\n",
      "Epoch: 314 \tTraining Loss: 0.00963890 \tValidation Loss 0.02186806 \tTraining Acuuarcy 53.389% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 315 \tTraining Loss: 0.00958314 \tValidation Loss 0.02193208 \tTraining Acuuarcy 53.439% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 316 \tTraining Loss: 0.00967509 \tValidation Loss 0.02234166 \tTraining Acuuarcy 53.500% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 317 \tTraining Loss: 0.00946156 \tValidation Loss 0.02226819 \tTraining Acuuarcy 53.946% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 318 \tTraining Loss: 0.00956863 \tValidation Loss 0.02154554 \tTraining Acuuarcy 54.085% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 319 \tTraining Loss: 0.00948816 \tValidation Loss 0.02211858 \tTraining Acuuarcy 54.152% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 320 \tTraining Loss: 0.00953953 \tValidation Loss 0.02219813 \tTraining Acuuarcy 53.940% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 321 \tTraining Loss: 0.00961291 \tValidation Loss 0.02194219 \tTraining Acuuarcy 53.500% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 322 \tTraining Loss: 0.00952733 \tValidation Loss 0.02256566 \tTraining Acuuarcy 54.308% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 323 \tTraining Loss: 0.00956526 \tValidation Loss 0.02214410 \tTraining Acuuarcy 53.840% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 324 \tTraining Loss: 0.00962751 \tValidation Loss 0.02275068 \tTraining Acuuarcy 53.205% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 325 \tTraining Loss: 0.00948463 \tValidation Loss 0.02280013 \tTraining Acuuarcy 53.985% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 326 \tTraining Loss: 0.00947776 \tValidation Loss 0.02172811 \tTraining Acuuarcy 53.835% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 327 \tTraining Loss: 0.00954584 \tValidation Loss 0.02300736 \tTraining Acuuarcy 54.074% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 328 \tTraining Loss: 0.00965416 \tValidation Loss 0.02264840 \tTraining Acuuarcy 53.205% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 329 \tTraining Loss: 0.00957718 \tValidation Loss 0.02225601 \tTraining Acuuarcy 53.366% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 330 \tTraining Loss: 0.00959181 \tValidation Loss 0.02147708 \tTraining Acuuarcy 53.467% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 331 \tTraining Loss: 0.00969039 \tValidation Loss 0.02240506 \tTraining Acuuarcy 52.787% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 332 \tTraining Loss: 0.00950328 \tValidation Loss 0.02210268 \tTraining Acuuarcy 54.169% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 333 \tTraining Loss: 0.00946341 \tValidation Loss 0.02150307 \tTraining Acuuarcy 54.437% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 334 \tTraining Loss: 0.00950960 \tValidation Loss 0.02304839 \tTraining Acuuarcy 53.913% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 335 \tTraining Loss: 0.00942630 \tValidation Loss 0.02217940 \tTraining Acuuarcy 54.570% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 336 \tTraining Loss: 0.00950914 \tValidation Loss 0.02231364 \tTraining Acuuarcy 54.030% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 337 \tTraining Loss: 0.00941821 \tValidation Loss 0.02244970 \tTraining Acuuarcy 54.921% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 338 \tTraining Loss: 0.00941817 \tValidation Loss 0.02382146 \tTraining Acuuarcy 54.481% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 339 \tTraining Loss: 0.00946381 \tValidation Loss 0.02328759 \tTraining Acuuarcy 54.431% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 340 \tTraining Loss: 0.00949768 \tValidation Loss 0.02245443 \tTraining Acuuarcy 54.292% \tValidation Acuuarcy 17.777%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 341 \tTraining Loss: 0.00943972 \tValidation Loss 0.02266742 \tTraining Acuuarcy 54.687% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 342 \tTraining Loss: 0.00946584 \tValidation Loss 0.02214224 \tTraining Acuuarcy 53.985% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 343 \tTraining Loss: 0.00942568 \tValidation Loss 0.02288485 \tTraining Acuuarcy 54.091% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 344 \tTraining Loss: 0.00943111 \tValidation Loss 0.02252045 \tTraining Acuuarcy 54.676% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 345 \tTraining Loss: 0.00941230 \tValidation Loss 0.02279138 \tTraining Acuuarcy 54.620% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 346 \tTraining Loss: 0.00941914 \tValidation Loss 0.02290111 \tTraining Acuuarcy 54.347% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 347 \tTraining Loss: 0.00950503 \tValidation Loss 0.02178920 \tTraining Acuuarcy 54.275% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 348 \tTraining Loss: 0.00939760 \tValidation Loss 0.02270400 \tTraining Acuuarcy 54.308% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 349 \tTraining Loss: 0.00946687 \tValidation Loss 0.02230712 \tTraining Acuuarcy 53.952% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 350 \tTraining Loss: 0.00947177 \tValidation Loss 0.02227594 \tTraining Acuuarcy 54.648% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 351 \tTraining Loss: 0.00945236 \tValidation Loss 0.02242040 \tTraining Acuuarcy 54.035% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 352 \tTraining Loss: 0.00956718 \tValidation Loss 0.02293254 \tTraining Acuuarcy 53.366% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 353 \tTraining Loss: 0.00945816 \tValidation Loss 0.02264636 \tTraining Acuuarcy 54.230% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 354 \tTraining Loss: 0.00948954 \tValidation Loss 0.02301861 \tTraining Acuuarcy 54.202% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 355 \tTraining Loss: 0.00947418 \tValidation Loss 0.02247352 \tTraining Acuuarcy 54.353% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 356 \tTraining Loss: 0.00944505 \tValidation Loss 0.02250029 \tTraining Acuuarcy 54.052% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 357 \tTraining Loss: 0.00951732 \tValidation Loss 0.02217639 \tTraining Acuuarcy 54.119% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 358 \tTraining Loss: 0.00945497 \tValidation Loss 0.02236897 \tTraining Acuuarcy 54.141% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 359 \tTraining Loss: 0.00950039 \tValidation Loss 0.02286474 \tTraining Acuuarcy 53.840% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 360 \tTraining Loss: 0.00947675 \tValidation Loss 0.02288323 \tTraining Acuuarcy 54.236% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 361 \tTraining Loss: 0.00932972 \tValidation Loss 0.02238738 \tTraining Acuuarcy 54.793% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 362 \tTraining Loss: 0.00929526 \tValidation Loss 0.02240189 \tTraining Acuuarcy 54.832% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 363 \tTraining Loss: 0.00935843 \tValidation Loss 0.02240250 \tTraining Acuuarcy 54.698% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 364 \tTraining Loss: 0.00936188 \tValidation Loss 0.02276607 \tTraining Acuuarcy 54.492% \tValidation Acuuarcy 17.024%\n",
      "Epoch: 365 \tTraining Loss: 0.00937413 \tValidation Loss 0.02237825 \tTraining Acuuarcy 54.983% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 366 \tTraining Loss: 0.00934855 \tValidation Loss 0.02238492 \tTraining Acuuarcy 55.105% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 367 \tTraining Loss: 0.00930958 \tValidation Loss 0.02275645 \tTraining Acuuarcy 55.089% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 368 \tTraining Loss: 0.00941271 \tValidation Loss 0.02267408 \tTraining Acuuarcy 54.492% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 369 \tTraining Loss: 0.00940756 \tValidation Loss 0.02259104 \tTraining Acuuarcy 54.598% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 370 \tTraining Loss: 0.00934750 \tValidation Loss 0.02246674 \tTraining Acuuarcy 54.459% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 371 \tTraining Loss: 0.00932301 \tValidation Loss 0.02313465 \tTraining Acuuarcy 55.094% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 372 \tTraining Loss: 0.00934476 \tValidation Loss 0.02226859 \tTraining Acuuarcy 55.044% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 373 \tTraining Loss: 0.00942811 \tValidation Loss 0.02254552 \tTraining Acuuarcy 54.559% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 374 \tTraining Loss: 0.00944337 \tValidation Loss 0.02226700 \tTraining Acuuarcy 54.515% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 375 \tTraining Loss: 0.00933972 \tValidation Loss 0.02274429 \tTraining Acuuarcy 54.821% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 376 \tTraining Loss: 0.00925670 \tValidation Loss 0.02383525 \tTraining Acuuarcy 55.507% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 377 \tTraining Loss: 0.00929057 \tValidation Loss 0.02240645 \tTraining Acuuarcy 55.429% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 378 \tTraining Loss: 0.00932774 \tValidation Loss 0.02284433 \tTraining Acuuarcy 54.743% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 379 \tTraining Loss: 0.00932780 \tValidation Loss 0.02348927 \tTraining Acuuarcy 55.050% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 380 \tTraining Loss: 0.00933950 \tValidation Loss 0.02280685 \tTraining Acuuarcy 54.726% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 381 \tTraining Loss: 0.00936651 \tValidation Loss 0.02223574 \tTraining Acuuarcy 55.083% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 382 \tTraining Loss: 0.00930043 \tValidation Loss 0.02297977 \tTraining Acuuarcy 55.139% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 383 \tTraining Loss: 0.00930887 \tValidation Loss 0.02218128 \tTraining Acuuarcy 54.576% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 384 \tTraining Loss: 0.00932418 \tValidation Loss 0.02371601 \tTraining Acuuarcy 54.855% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 385 \tTraining Loss: 0.00935040 \tValidation Loss 0.02283386 \tTraining Acuuarcy 54.871% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 386 \tTraining Loss: 0.00930262 \tValidation Loss 0.02223592 \tTraining Acuuarcy 54.966% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 387 \tTraining Loss: 0.00927526 \tValidation Loss 0.02273446 \tTraining Acuuarcy 54.960% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 388 \tTraining Loss: 0.00925688 \tValidation Loss 0.02277076 \tTraining Acuuarcy 55.345% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 389 \tTraining Loss: 0.00925319 \tValidation Loss 0.02256134 \tTraining Acuuarcy 55.228% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 390 \tTraining Loss: 0.00950750 \tValidation Loss 0.02276643 \tTraining Acuuarcy 54.314% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 391 \tTraining Loss: 0.00929639 \tValidation Loss 0.02170279 \tTraining Acuuarcy 55.128% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 392 \tTraining Loss: 0.00923783 \tValidation Loss 0.02293658 \tTraining Acuuarcy 55.696% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 393 \tTraining Loss: 0.00921642 \tValidation Loss 0.02310728 \tTraining Acuuarcy 55.802% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 394 \tTraining Loss: 0.00929904 \tValidation Loss 0.02281666 \tTraining Acuuarcy 55.027% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 395 \tTraining Loss: 0.00921400 \tValidation Loss 0.02297620 \tTraining Acuuarcy 55.908% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 396 \tTraining Loss: 0.00930266 \tValidation Loss 0.02324256 \tTraining Acuuarcy 55.183% \tValidation Acuuarcy 16.662%\n",
      "Epoch: 397 \tTraining Loss: 0.00928290 \tValidation Loss 0.02281032 \tTraining Acuuarcy 55.306% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 398 \tTraining Loss: 0.00935270 \tValidation Loss 0.02288145 \tTraining Acuuarcy 54.598% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 399 \tTraining Loss: 0.00938963 \tValidation Loss 0.02244169 \tTraining Acuuarcy 54.459% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 400 \tTraining Loss: 0.00927027 \tValidation Loss 0.02305002 \tTraining Acuuarcy 55.395% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 401 \tTraining Loss: 0.00909699 \tValidation Loss 0.02299560 \tTraining Acuuarcy 56.359% \tValidation Acuuarcy 16.941%\n",
      "Epoch: 402 \tTraining Loss: 0.00926298 \tValidation Loss 0.02243491 \tTraining Acuuarcy 55.234% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 403 \tTraining Loss: 0.00919861 \tValidation Loss 0.02325358 \tTraining Acuuarcy 55.796% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 404 \tTraining Loss: 0.00921200 \tValidation Loss 0.02303165 \tTraining Acuuarcy 55.562% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 405 \tTraining Loss: 0.00925155 \tValidation Loss 0.02367735 \tTraining Acuuarcy 55.167% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 406 \tTraining Loss: 0.00922670 \tValidation Loss 0.02304999 \tTraining Acuuarcy 55.880% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 407 \tTraining Loss: 0.00926995 \tValidation Loss 0.02285032 \tTraining Acuuarcy 54.949% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 408 \tTraining Loss: 0.00928049 \tValidation Loss 0.02296315 \tTraining Acuuarcy 55.445% \tValidation Acuuarcy 17.665%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 409 \tTraining Loss: 0.00925551 \tValidation Loss 0.02291258 \tTraining Acuuarcy 55.122% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 410 \tTraining Loss: 0.00922904 \tValidation Loss 0.02327182 \tTraining Acuuarcy 55.679% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 411 \tTraining Loss: 0.00922466 \tValidation Loss 0.02340051 \tTraining Acuuarcy 55.551% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 412 \tTraining Loss: 0.00935489 \tValidation Loss 0.02280873 \tTraining Acuuarcy 55.011% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 413 \tTraining Loss: 0.00924539 \tValidation Loss 0.02327018 \tTraining Acuuarcy 55.596% \tValidation Acuuarcy 16.690%\n",
      "Epoch: 414 \tTraining Loss: 0.00922914 \tValidation Loss 0.02279362 \tTraining Acuuarcy 55.652% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 415 \tTraining Loss: 0.00925876 \tValidation Loss 0.02228234 \tTraining Acuuarcy 55.412% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 416 \tTraining Loss: 0.00928850 \tValidation Loss 0.02334128 \tTraining Acuuarcy 55.317% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 417 \tTraining Loss: 0.00925023 \tValidation Loss 0.02302008 \tTraining Acuuarcy 55.295% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 418 \tTraining Loss: 0.00930194 \tValidation Loss 0.02369395 \tTraining Acuuarcy 54.960% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 419 \tTraining Loss: 0.00922938 \tValidation Loss 0.02238736 \tTraining Acuuarcy 55.295% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 420 \tTraining Loss: 0.00912907 \tValidation Loss 0.02334305 \tTraining Acuuarcy 55.652% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 421 \tTraining Loss: 0.00922600 \tValidation Loss 0.02318464 \tTraining Acuuarcy 55.217% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 422 \tTraining Loss: 0.00913512 \tValidation Loss 0.02405307 \tTraining Acuuarcy 55.835% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 423 \tTraining Loss: 0.00919735 \tValidation Loss 0.02308802 \tTraining Acuuarcy 55.523% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 424 \tTraining Loss: 0.00920042 \tValidation Loss 0.02302359 \tTraining Acuuarcy 55.913% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 425 \tTraining Loss: 0.00919763 \tValidation Loss 0.02373288 \tTraining Acuuarcy 55.780% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 426 \tTraining Loss: 0.00916207 \tValidation Loss 0.02267203 \tTraining Acuuarcy 55.908% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 427 \tTraining Loss: 0.00929853 \tValidation Loss 0.02265351 \tTraining Acuuarcy 55.646% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 428 \tTraining Loss: 0.00928744 \tValidation Loss 0.02208911 \tTraining Acuuarcy 55.089% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 429 \tTraining Loss: 0.00917281 \tValidation Loss 0.02282789 \tTraining Acuuarcy 55.763% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 430 \tTraining Loss: 0.00921461 \tValidation Loss 0.02402896 \tTraining Acuuarcy 55.484% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 431 \tTraining Loss: 0.00922457 \tValidation Loss 0.02252491 \tTraining Acuuarcy 56.064% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 432 \tTraining Loss: 0.00916831 \tValidation Loss 0.02290622 \tTraining Acuuarcy 55.897% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 433 \tTraining Loss: 0.00918071 \tValidation Loss 0.02364411 \tTraining Acuuarcy 56.148% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 434 \tTraining Loss: 0.00908514 \tValidation Loss 0.02279886 \tTraining Acuuarcy 56.449% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 435 \tTraining Loss: 0.00919562 \tValidation Loss 0.02388087 \tTraining Acuuarcy 55.902% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 436 \tTraining Loss: 0.00912911 \tValidation Loss 0.02258257 \tTraining Acuuarcy 55.936% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 437 \tTraining Loss: 0.00917110 \tValidation Loss 0.02237782 \tTraining Acuuarcy 56.120% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 438 \tTraining Loss: 0.00911914 \tValidation Loss 0.02342639 \tTraining Acuuarcy 56.120% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 439 \tTraining Loss: 0.00908446 \tValidation Loss 0.02338229 \tTraining Acuuarcy 56.404% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 440 \tTraining Loss: 0.00904427 \tValidation Loss 0.02384666 \tTraining Acuuarcy 56.432% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 441 \tTraining Loss: 0.00908873 \tValidation Loss 0.02259910 \tTraining Acuuarcy 56.097% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 442 \tTraining Loss: 0.00910157 \tValidation Loss 0.02376313 \tTraining Acuuarcy 56.410% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 443 \tTraining Loss: 0.00906585 \tValidation Loss 0.02306850 \tTraining Acuuarcy 56.984% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 444 \tTraining Loss: 0.00916645 \tValidation Loss 0.02312733 \tTraining Acuuarcy 55.869% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 445 \tTraining Loss: 0.00907859 \tValidation Loss 0.02404889 \tTraining Acuuarcy 56.265% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 446 \tTraining Loss: 0.00902650 \tValidation Loss 0.02364134 \tTraining Acuuarcy 56.577% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 447 \tTraining Loss: 0.00918736 \tValidation Loss 0.02342110 \tTraining Acuuarcy 55.975% \tValidation Acuuarcy 16.746%\n",
      "Epoch: 448 \tTraining Loss: 0.00924196 \tValidation Loss 0.02313638 \tTraining Acuuarcy 55.351% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 449 \tTraining Loss: 0.00910864 \tValidation Loss 0.02312854 \tTraining Acuuarcy 56.019% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 450 \tTraining Loss: 0.00907273 \tValidation Loss 0.02414478 \tTraining Acuuarcy 56.554% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 451 \tTraining Loss: 0.00911907 \tValidation Loss 0.02310308 \tTraining Acuuarcy 55.941% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 452 \tTraining Loss: 0.00913423 \tValidation Loss 0.02295939 \tTraining Acuuarcy 56.237% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 453 \tTraining Loss: 0.00907754 \tValidation Loss 0.02338516 \tTraining Acuuarcy 56.571% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 454 \tTraining Loss: 0.00900122 \tValidation Loss 0.02300220 \tTraining Acuuarcy 56.878% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 455 \tTraining Loss: 0.00913323 \tValidation Loss 0.02294689 \tTraining Acuuarcy 55.685% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 456 \tTraining Loss: 0.00909141 \tValidation Loss 0.02330620 \tTraining Acuuarcy 56.036% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 457 \tTraining Loss: 0.00919949 \tValidation Loss 0.02289030 \tTraining Acuuarcy 55.718% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 458 \tTraining Loss: 0.00915719 \tValidation Loss 0.02339410 \tTraining Acuuarcy 55.774% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 459 \tTraining Loss: 0.00905252 \tValidation Loss 0.02328590 \tTraining Acuuarcy 56.499% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 460 \tTraining Loss: 0.00913483 \tValidation Loss 0.02242621 \tTraining Acuuarcy 55.902% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 461 \tTraining Loss: 0.00913834 \tValidation Loss 0.02330113 \tTraining Acuuarcy 55.992% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 462 \tTraining Loss: 0.00915728 \tValidation Loss 0.02380883 \tTraining Acuuarcy 56.164% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 463 \tTraining Loss: 0.00913443 \tValidation Loss 0.02321592 \tTraining Acuuarcy 56.276% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 464 \tTraining Loss: 0.00910310 \tValidation Loss 0.02376666 \tTraining Acuuarcy 56.449% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 465 \tTraining Loss: 0.00912183 \tValidation Loss 0.02328690 \tTraining Acuuarcy 56.209% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 466 \tTraining Loss: 0.00905727 \tValidation Loss 0.02297121 \tTraining Acuuarcy 56.683% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 467 \tTraining Loss: 0.00912860 \tValidation Loss 0.02394475 \tTraining Acuuarcy 55.824% \tValidation Acuuarcy 16.801%\n",
      "Epoch: 468 \tTraining Loss: 0.00900466 \tValidation Loss 0.02336902 \tTraining Acuuarcy 56.833% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 469 \tTraining Loss: 0.00908137 \tValidation Loss 0.02310998 \tTraining Acuuarcy 56.170% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 470 \tTraining Loss: 0.00895653 \tValidation Loss 0.02323714 \tTraining Acuuarcy 56.454% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 471 \tTraining Loss: 0.00904539 \tValidation Loss 0.02400149 \tTraining Acuuarcy 56.382% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 472 \tTraining Loss: 0.00903394 \tValidation Loss 0.02337119 \tTraining Acuuarcy 56.644% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 473 \tTraining Loss: 0.00900717 \tValidation Loss 0.02400636 \tTraining Acuuarcy 56.945% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 474 \tTraining Loss: 0.00908091 \tValidation Loss 0.02419136 \tTraining Acuuarcy 56.253% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 475 \tTraining Loss: 0.00908381 \tValidation Loss 0.02374556 \tTraining Acuuarcy 55.992% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 476 \tTraining Loss: 0.00902564 \tValidation Loss 0.02321683 \tTraining Acuuarcy 56.359% \tValidation Acuuarcy 18.055%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477 \tTraining Loss: 0.00898137 \tValidation Loss 0.02340168 \tTraining Acuuarcy 56.488% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 478 \tTraining Loss: 0.00904100 \tValidation Loss 0.02281591 \tTraining Acuuarcy 56.789% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 479 \tTraining Loss: 0.00907486 \tValidation Loss 0.02417341 \tTraining Acuuarcy 56.666% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 480 \tTraining Loss: 0.00905500 \tValidation Loss 0.02379302 \tTraining Acuuarcy 56.170% \tValidation Acuuarcy 16.411%\n",
      "Epoch: 481 \tTraining Loss: 0.00899087 \tValidation Loss 0.02390971 \tTraining Acuuarcy 56.878% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 482 \tTraining Loss: 0.00899834 \tValidation Loss 0.02328001 \tTraining Acuuarcy 56.554% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 483 \tTraining Loss: 0.00900256 \tValidation Loss 0.02399502 \tTraining Acuuarcy 56.800% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 484 \tTraining Loss: 0.00908501 \tValidation Loss 0.02257904 \tTraining Acuuarcy 56.214% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 485 \tTraining Loss: 0.00896728 \tValidation Loss 0.02410957 \tTraining Acuuarcy 56.822% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 486 \tTraining Loss: 0.00908154 \tValidation Loss 0.02372994 \tTraining Acuuarcy 56.354% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 487 \tTraining Loss: 0.00901095 \tValidation Loss 0.02413758 \tTraining Acuuarcy 56.744% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 488 \tTraining Loss: 0.00894263 \tValidation Loss 0.02366700 \tTraining Acuuarcy 56.822% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 489 \tTraining Loss: 0.00902124 \tValidation Loss 0.02382266 \tTraining Acuuarcy 56.566% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 490 \tTraining Loss: 0.00890588 \tValidation Loss 0.02435025 \tTraining Acuuarcy 57.312% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 491 \tTraining Loss: 0.00901904 \tValidation Loss 0.02323752 \tTraining Acuuarcy 56.900% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 492 \tTraining Loss: 0.00894479 \tValidation Loss 0.02330827 \tTraining Acuuarcy 57.402% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 493 \tTraining Loss: 0.00894870 \tValidation Loss 0.02355082 \tTraining Acuuarcy 57.151% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 494 \tTraining Loss: 0.00899004 \tValidation Loss 0.02344588 \tTraining Acuuarcy 56.872% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 495 \tTraining Loss: 0.00895479 \tValidation Loss 0.02357969 \tTraining Acuuarcy 56.699% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 496 \tTraining Loss: 0.00896443 \tValidation Loss 0.02386714 \tTraining Acuuarcy 57.140% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 497 \tTraining Loss: 0.00894932 \tValidation Loss 0.02332946 \tTraining Acuuarcy 57.156% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 498 \tTraining Loss: 0.00896662 \tValidation Loss 0.02296096 \tTraining Acuuarcy 56.722% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 499 \tTraining Loss: 0.00892690 \tValidation Loss 0.02366204 \tTraining Acuuarcy 57.697% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 500 \tTraining Loss: 0.00890960 \tValidation Loss 0.02397093 \tTraining Acuuarcy 57.574% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 501 \tTraining Loss: 0.00894282 \tValidation Loss 0.02388446 \tTraining Acuuarcy 57.513% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 502 \tTraining Loss: 0.00902482 \tValidation Loss 0.02339094 \tTraining Acuuarcy 56.543% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 503 \tTraining Loss: 0.00882818 \tValidation Loss 0.02362252 \tTraining Acuuarcy 57.814% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 504 \tTraining Loss: 0.00891266 \tValidation Loss 0.02333413 \tTraining Acuuarcy 57.363% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 505 \tTraining Loss: 0.00899991 \tValidation Loss 0.02330524 \tTraining Acuuarcy 56.599% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 506 \tTraining Loss: 0.00887296 \tValidation Loss 0.02437180 \tTraining Acuuarcy 57.045% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 507 \tTraining Loss: 0.00895988 \tValidation Loss 0.02338944 \tTraining Acuuarcy 56.894% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 508 \tTraining Loss: 0.00903039 \tValidation Loss 0.02435286 \tTraining Acuuarcy 56.928% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 509 \tTraining Loss: 0.00885518 \tValidation Loss 0.02412230 \tTraining Acuuarcy 57.429% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 510 \tTraining Loss: 0.00890595 \tValidation Loss 0.02404861 \tTraining Acuuarcy 57.664% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 511 \tTraining Loss: 0.00896092 \tValidation Loss 0.02306690 \tTraining Acuuarcy 57.151% \tValidation Acuuarcy 16.606%\n",
      "Epoch: 512 \tTraining Loss: 0.00894703 \tValidation Loss 0.02352968 \tTraining Acuuarcy 56.711% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 513 \tTraining Loss: 0.00899648 \tValidation Loss 0.02356880 \tTraining Acuuarcy 56.789% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 514 \tTraining Loss: 0.00900048 \tValidation Loss 0.02313560 \tTraining Acuuarcy 57.140% \tValidation Acuuarcy 16.746%\n",
      "Epoch: 515 \tTraining Loss: 0.00891911 \tValidation Loss 0.02372899 \tTraining Acuuarcy 57.090% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 516 \tTraining Loss: 0.00909091 \tValidation Loss 0.02363848 \tTraining Acuuarcy 56.504% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 517 \tTraining Loss: 0.00898341 \tValidation Loss 0.02363549 \tTraining Acuuarcy 56.839% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 518 \tTraining Loss: 0.00884331 \tValidation Loss 0.02297552 \tTraining Acuuarcy 57.385% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 519 \tTraining Loss: 0.00886232 \tValidation Loss 0.02302488 \tTraining Acuuarcy 57.758% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 520 \tTraining Loss: 0.00895370 \tValidation Loss 0.02461341 \tTraining Acuuarcy 57.084% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 521 \tTraining Loss: 0.00897636 \tValidation Loss 0.02369323 \tTraining Acuuarcy 57.134% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 522 \tTraining Loss: 0.00918043 \tValidation Loss 0.02346423 \tTraining Acuuarcy 56.019% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 523 \tTraining Loss: 0.00900126 \tValidation Loss 0.02385405 \tTraining Acuuarcy 57.084% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 524 \tTraining Loss: 0.00894642 \tValidation Loss 0.02347612 \tTraining Acuuarcy 57.368% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 525 \tTraining Loss: 0.00894917 \tValidation Loss 0.02388465 \tTraining Acuuarcy 56.889% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 526 \tTraining Loss: 0.00896746 \tValidation Loss 0.02389901 \tTraining Acuuarcy 56.945% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 527 \tTraining Loss: 0.00888958 \tValidation Loss 0.02337953 \tTraining Acuuarcy 57.017% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 528 \tTraining Loss: 0.00904752 \tValidation Loss 0.02307147 \tTraining Acuuarcy 56.605% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 529 \tTraining Loss: 0.00896493 \tValidation Loss 0.02372192 \tTraining Acuuarcy 57.011% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 530 \tTraining Loss: 0.00889659 \tValidation Loss 0.02369510 \tTraining Acuuarcy 57.329% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 531 \tTraining Loss: 0.00895015 \tValidation Loss 0.02328183 \tTraining Acuuarcy 57.106% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 532 \tTraining Loss: 0.00896136 \tValidation Loss 0.02407141 \tTraining Acuuarcy 56.889% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 533 \tTraining Loss: 0.00881112 \tValidation Loss 0.02466840 \tTraining Acuuarcy 57.853% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 534 \tTraining Loss: 0.00875464 \tValidation Loss 0.02408773 \tTraining Acuuarcy 57.931% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 535 \tTraining Loss: 0.00887329 \tValidation Loss 0.02312789 \tTraining Acuuarcy 57.524% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 536 \tTraining Loss: 0.00890145 \tValidation Loss 0.02371802 \tTraining Acuuarcy 56.995% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 537 \tTraining Loss: 0.00882611 \tValidation Loss 0.02353440 \tTraining Acuuarcy 57.848% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 538 \tTraining Loss: 0.00881428 \tValidation Loss 0.02427381 \tTraining Acuuarcy 57.513% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 539 \tTraining Loss: 0.00890773 \tValidation Loss 0.02384132 \tTraining Acuuarcy 57.541% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 540 \tTraining Loss: 0.00886928 \tValidation Loss 0.02445124 \tTraining Acuuarcy 57.881% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 541 \tTraining Loss: 0.00891519 \tValidation Loss 0.02404417 \tTraining Acuuarcy 57.050% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 542 \tTraining Loss: 0.00890613 \tValidation Loss 0.02435035 \tTraining Acuuarcy 57.496% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 543 \tTraining Loss: 0.00891089 \tValidation Loss 0.02354275 \tTraining Acuuarcy 57.363% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 544 \tTraining Loss: 0.00896769 \tValidation Loss 0.02492118 \tTraining Acuuarcy 56.906% \tValidation Acuuarcy 16.718%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 545 \tTraining Loss: 0.00889784 \tValidation Loss 0.02323064 \tTraining Acuuarcy 57.781% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 546 \tTraining Loss: 0.00891689 \tValidation Loss 0.02420589 \tTraining Acuuarcy 57.429% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 547 \tTraining Loss: 0.00888840 \tValidation Loss 0.02434361 \tTraining Acuuarcy 57.402% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 548 \tTraining Loss: 0.00881428 \tValidation Loss 0.02337844 \tTraining Acuuarcy 57.931% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 549 \tTraining Loss: 0.00888342 \tValidation Loss 0.02410517 \tTraining Acuuarcy 57.647% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 550 \tTraining Loss: 0.00881090 \tValidation Loss 0.02401927 \tTraining Acuuarcy 57.864% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 551 \tTraining Loss: 0.00888939 \tValidation Loss 0.02357681 \tTraining Acuuarcy 57.357% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 552 \tTraining Loss: 0.00883682 \tValidation Loss 0.02421400 \tTraining Acuuarcy 57.758% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 553 \tTraining Loss: 0.00884587 \tValidation Loss 0.02438717 \tTraining Acuuarcy 57.463% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 554 \tTraining Loss: 0.00881475 \tValidation Loss 0.02353675 \tTraining Acuuarcy 57.808% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 555 \tTraining Loss: 0.00886814 \tValidation Loss 0.02446275 \tTraining Acuuarcy 57.463% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 556 \tTraining Loss: 0.00873711 \tValidation Loss 0.02414891 \tTraining Acuuarcy 57.937% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 557 \tTraining Loss: 0.00887168 \tValidation Loss 0.02423855 \tTraining Acuuarcy 57.168% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 558 \tTraining Loss: 0.00881694 \tValidation Loss 0.02363919 \tTraining Acuuarcy 57.914% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 559 \tTraining Loss: 0.00881892 \tValidation Loss 0.02481199 \tTraining Acuuarcy 57.764% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 560 \tTraining Loss: 0.00876275 \tValidation Loss 0.02502275 \tTraining Acuuarcy 58.438% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 561 \tTraining Loss: 0.00887651 \tValidation Loss 0.02369652 \tTraining Acuuarcy 57.524% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 562 \tTraining Loss: 0.00884830 \tValidation Loss 0.02439664 \tTraining Acuuarcy 57.457% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 563 \tTraining Loss: 0.00886454 \tValidation Loss 0.02420689 \tTraining Acuuarcy 57.296% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 564 \tTraining Loss: 0.00885924 \tValidation Loss 0.02378214 \tTraining Acuuarcy 57.781% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 565 \tTraining Loss: 0.00878181 \tValidation Loss 0.02391598 \tTraining Acuuarcy 58.204% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 566 \tTraining Loss: 0.00879570 \tValidation Loss 0.02333881 \tTraining Acuuarcy 57.831% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 567 \tTraining Loss: 0.00885642 \tValidation Loss 0.02398039 \tTraining Acuuarcy 57.379% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 568 \tTraining Loss: 0.00879313 \tValidation Loss 0.02464345 \tTraining Acuuarcy 57.981% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 569 \tTraining Loss: 0.00888485 \tValidation Loss 0.02318647 \tTraining Acuuarcy 57.474% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 570 \tTraining Loss: 0.00878456 \tValidation Loss 0.02469128 \tTraining Acuuarcy 57.831% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 571 \tTraining Loss: 0.00878609 \tValidation Loss 0.02416941 \tTraining Acuuarcy 58.316% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 572 \tTraining Loss: 0.00881170 \tValidation Loss 0.02447184 \tTraining Acuuarcy 57.898% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 573 \tTraining Loss: 0.00879671 \tValidation Loss 0.02408755 \tTraining Acuuarcy 57.797% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 574 \tTraining Loss: 0.00884516 \tValidation Loss 0.02390063 \tTraining Acuuarcy 57.413% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 575 \tTraining Loss: 0.00875931 \tValidation Loss 0.02372710 \tTraining Acuuarcy 58.043% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 576 \tTraining Loss: 0.00881791 \tValidation Loss 0.02460378 \tTraining Acuuarcy 57.658% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 577 \tTraining Loss: 0.00885460 \tValidation Loss 0.02424918 \tTraining Acuuarcy 57.641% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 578 \tTraining Loss: 0.00883400 \tValidation Loss 0.02386782 \tTraining Acuuarcy 57.797% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 579 \tTraining Loss: 0.00881561 \tValidation Loss 0.02379899 \tTraining Acuuarcy 57.909% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 580 \tTraining Loss: 0.00887494 \tValidation Loss 0.02393129 \tTraining Acuuarcy 57.608% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 581 \tTraining Loss: 0.00881219 \tValidation Loss 0.02424925 \tTraining Acuuarcy 57.853% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 582 \tTraining Loss: 0.00871429 \tValidation Loss 0.02419897 \tTraining Acuuarcy 58.472% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 583 \tTraining Loss: 0.00876767 \tValidation Loss 0.02437594 \tTraining Acuuarcy 58.059% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 584 \tTraining Loss: 0.00877096 \tValidation Loss 0.02495530 \tTraining Acuuarcy 57.870% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 585 \tTraining Loss: 0.00881948 \tValidation Loss 0.02364297 \tTraining Acuuarcy 57.524% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 586 \tTraining Loss: 0.00872543 \tValidation Loss 0.02402095 \tTraining Acuuarcy 58.566% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 587 \tTraining Loss: 0.00877417 \tValidation Loss 0.02386622 \tTraining Acuuarcy 58.227% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 588 \tTraining Loss: 0.00880747 \tValidation Loss 0.02351574 \tTraining Acuuarcy 57.959% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 589 \tTraining Loss: 0.00883584 \tValidation Loss 0.02358044 \tTraining Acuuarcy 57.524% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 590 \tTraining Loss: 0.00878680 \tValidation Loss 0.02394831 \tTraining Acuuarcy 58.221% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 591 \tTraining Loss: 0.00866135 \tValidation Loss 0.02487611 \tTraining Acuuarcy 58.583% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 592 \tTraining Loss: 0.00877978 \tValidation Loss 0.02469007 \tTraining Acuuarcy 57.937% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 593 \tTraining Loss: 0.00871824 \tValidation Loss 0.02430443 \tTraining Acuuarcy 58.031% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 594 \tTraining Loss: 0.00868295 \tValidation Loss 0.02367969 \tTraining Acuuarcy 58.160% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 595 \tTraining Loss: 0.00872263 \tValidation Loss 0.02397488 \tTraining Acuuarcy 58.238% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 596 \tTraining Loss: 0.00871956 \tValidation Loss 0.02366926 \tTraining Acuuarcy 58.065% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 597 \tTraining Loss: 0.00865717 \tValidation Loss 0.02462491 \tTraining Acuuarcy 58.483% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 598 \tTraining Loss: 0.00872304 \tValidation Loss 0.02437346 \tTraining Acuuarcy 57.926% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 599 \tTraining Loss: 0.00868505 \tValidation Loss 0.02416405 \tTraining Acuuarcy 58.249% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 600 \tTraining Loss: 0.00870734 \tValidation Loss 0.02379133 \tTraining Acuuarcy 58.109% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 601 \tTraining Loss: 0.00873043 \tValidation Loss 0.02424798 \tTraining Acuuarcy 58.171% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 602 \tTraining Loss: 0.00871607 \tValidation Loss 0.02405193 \tTraining Acuuarcy 58.611% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 603 \tTraining Loss: 0.00880231 \tValidation Loss 0.02381978 \tTraining Acuuarcy 57.903% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 604 \tTraining Loss: 0.00878837 \tValidation Loss 0.02424241 \tTraining Acuuarcy 58.104% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 605 \tTraining Loss: 0.00869332 \tValidation Loss 0.02459543 \tTraining Acuuarcy 58.405% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 606 \tTraining Loss: 0.00871866 \tValidation Loss 0.02392970 \tTraining Acuuarcy 58.862% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 607 \tTraining Loss: 0.00875479 \tValidation Loss 0.02381570 \tTraining Acuuarcy 57.914% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 608 \tTraining Loss: 0.00862408 \tValidation Loss 0.02462193 \tTraining Acuuarcy 58.410% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 609 \tTraining Loss: 0.00867123 \tValidation Loss 0.02387147 \tTraining Acuuarcy 59.018% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 610 \tTraining Loss: 0.00877661 \tValidation Loss 0.02430617 \tTraining Acuuarcy 57.836% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 611 \tTraining Loss: 0.00871791 \tValidation Loss 0.02418580 \tTraining Acuuarcy 58.472% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 612 \tTraining Loss: 0.00875121 \tValidation Loss 0.02458543 \tTraining Acuuarcy 58.249% \tValidation Acuuarcy 17.916%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 613 \tTraining Loss: 0.00873238 \tValidation Loss 0.02404100 \tTraining Acuuarcy 58.461% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 614 \tTraining Loss: 0.00866883 \tValidation Loss 0.02347508 \tTraining Acuuarcy 58.817% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 615 \tTraining Loss: 0.00882955 \tValidation Loss 0.02429872 \tTraining Acuuarcy 57.836% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 616 \tTraining Loss: 0.00874346 \tValidation Loss 0.02405037 \tTraining Acuuarcy 58.277% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 617 \tTraining Loss: 0.00866862 \tValidation Loss 0.02450882 \tTraining Acuuarcy 58.488% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 618 \tTraining Loss: 0.00868469 \tValidation Loss 0.02418890 \tTraining Acuuarcy 58.026% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 619 \tTraining Loss: 0.00861496 \tValidation Loss 0.02455063 \tTraining Acuuarcy 59.074% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 620 \tTraining Loss: 0.00867199 \tValidation Loss 0.02393667 \tTraining Acuuarcy 58.109% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 621 \tTraining Loss: 0.00857822 \tValidation Loss 0.02419995 \tTraining Acuuarcy 58.957% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 622 \tTraining Loss: 0.00863076 \tValidation Loss 0.02467772 \tTraining Acuuarcy 58.840% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 623 \tTraining Loss: 0.00868195 \tValidation Loss 0.02369519 \tTraining Acuuarcy 58.321% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 624 \tTraining Loss: 0.00855922 \tValidation Loss 0.02495753 \tTraining Acuuarcy 59.313% \tValidation Acuuarcy 16.718%\n",
      "Epoch: 625 \tTraining Loss: 0.00859299 \tValidation Loss 0.02438425 \tTraining Acuuarcy 58.879% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 626 \tTraining Loss: 0.00856832 \tValidation Loss 0.02373796 \tTraining Acuuarcy 59.068% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 627 \tTraining Loss: 0.00866665 \tValidation Loss 0.02389057 \tTraining Acuuarcy 58.477% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 628 \tTraining Loss: 0.00869109 \tValidation Loss 0.02429353 \tTraining Acuuarcy 58.243% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 629 \tTraining Loss: 0.00860637 \tValidation Loss 0.02465698 \tTraining Acuuarcy 58.856% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 630 \tTraining Loss: 0.00871497 \tValidation Loss 0.02415858 \tTraining Acuuarcy 58.193% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 631 \tTraining Loss: 0.00862766 \tValidation Loss 0.02415309 \tTraining Acuuarcy 58.801% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 632 \tTraining Loss: 0.00865278 \tValidation Loss 0.02485322 \tTraining Acuuarcy 58.461% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 633 \tTraining Loss: 0.00873816 \tValidation Loss 0.02390055 \tTraining Acuuarcy 58.410% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 634 \tTraining Loss: 0.00863042 \tValidation Loss 0.02457618 \tTraining Acuuarcy 58.617% \tValidation Acuuarcy 16.439%\n",
      "Epoch: 635 \tTraining Loss: 0.00865792 \tValidation Loss 0.02451010 \tTraining Acuuarcy 58.762% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 636 \tTraining Loss: 0.00872207 \tValidation Loss 0.02424131 \tTraining Acuuarcy 58.461% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 637 \tTraining Loss: 0.00866594 \tValidation Loss 0.02400174 \tTraining Acuuarcy 58.778% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 638 \tTraining Loss: 0.00867992 \tValidation Loss 0.02419462 \tTraining Acuuarcy 58.360% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 639 \tTraining Loss: 0.00863085 \tValidation Loss 0.02481791 \tTraining Acuuarcy 58.555% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 640 \tTraining Loss: 0.00867709 \tValidation Loss 0.02446368 \tTraining Acuuarcy 58.578% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 641 \tTraining Loss: 0.00867852 \tValidation Loss 0.02460849 \tTraining Acuuarcy 58.940% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 642 \tTraining Loss: 0.00868530 \tValidation Loss 0.02458316 \tTraining Acuuarcy 58.600% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 643 \tTraining Loss: 0.00863072 \tValidation Loss 0.02348958 \tTraining Acuuarcy 58.600% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 644 \tTraining Loss: 0.00872458 \tValidation Loss 0.02490202 \tTraining Acuuarcy 58.622% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 645 \tTraining Loss: 0.00865493 \tValidation Loss 0.02483425 \tTraining Acuuarcy 58.444% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 646 \tTraining Loss: 0.00857513 \tValidation Loss 0.02481590 \tTraining Acuuarcy 59.168% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 647 \tTraining Loss: 0.00860922 \tValidation Loss 0.02454355 \tTraining Acuuarcy 59.051% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 648 \tTraining Loss: 0.00859946 \tValidation Loss 0.02482390 \tTraining Acuuarcy 59.046% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 649 \tTraining Loss: 0.00846854 \tValidation Loss 0.02437347 \tTraining Acuuarcy 59.051% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 650 \tTraining Loss: 0.00864331 \tValidation Loss 0.02462233 \tTraining Acuuarcy 58.684% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 651 \tTraining Loss: 0.00850165 \tValidation Loss 0.02549270 \tTraining Acuuarcy 59.559% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 652 \tTraining Loss: 0.00875676 \tValidation Loss 0.02366402 \tTraining Acuuarcy 58.410% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 653 \tTraining Loss: 0.00860028 \tValidation Loss 0.02500916 \tTraining Acuuarcy 58.912% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 654 \tTraining Loss: 0.00859128 \tValidation Loss 0.02477030 \tTraining Acuuarcy 59.185% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 655 \tTraining Loss: 0.00854931 \tValidation Loss 0.02470304 \tTraining Acuuarcy 59.124% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 656 \tTraining Loss: 0.00854464 \tValidation Loss 0.02498564 \tTraining Acuuarcy 59.442% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 657 \tTraining Loss: 0.00868390 \tValidation Loss 0.02446251 \tTraining Acuuarcy 58.555% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 658 \tTraining Loss: 0.00865977 \tValidation Loss 0.02476767 \tTraining Acuuarcy 58.745% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 659 \tTraining Loss: 0.00866628 \tValidation Loss 0.02407891 \tTraining Acuuarcy 58.834% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 660 \tTraining Loss: 0.00862050 \tValidation Loss 0.02388670 \tTraining Acuuarcy 58.594% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 661 \tTraining Loss: 0.00858166 \tValidation Loss 0.02385439 \tTraining Acuuarcy 59.302% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 662 \tTraining Loss: 0.00856598 \tValidation Loss 0.02403067 \tTraining Acuuarcy 59.319% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 663 \tTraining Loss: 0.00855005 \tValidation Loss 0.02486946 \tTraining Acuuarcy 59.274% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 664 \tTraining Loss: 0.00852169 \tValidation Loss 0.02461681 \tTraining Acuuarcy 59.369% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 665 \tTraining Loss: 0.00857851 \tValidation Loss 0.02457750 \tTraining Acuuarcy 58.795% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 666 \tTraining Loss: 0.00866007 \tValidation Loss 0.02419328 \tTraining Acuuarcy 58.494% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 667 \tTraining Loss: 0.00863263 \tValidation Loss 0.02422599 \tTraining Acuuarcy 58.656% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 668 \tTraining Loss: 0.00860435 \tValidation Loss 0.02524482 \tTraining Acuuarcy 58.856% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 669 \tTraining Loss: 0.00855061 \tValidation Loss 0.02416591 \tTraining Acuuarcy 59.280% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 670 \tTraining Loss: 0.00853697 \tValidation Loss 0.02498863 \tTraining Acuuarcy 58.996% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 671 \tTraining Loss: 0.00871955 \tValidation Loss 0.02455716 \tTraining Acuuarcy 58.449% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 672 \tTraining Loss: 0.00853563 \tValidation Loss 0.02514482 \tTraining Acuuarcy 59.414% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 673 \tTraining Loss: 0.00862552 \tValidation Loss 0.02424046 \tTraining Acuuarcy 58.711% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 674 \tTraining Loss: 0.00857329 \tValidation Loss 0.02420022 \tTraining Acuuarcy 59.258% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 675 \tTraining Loss: 0.00857457 \tValidation Loss 0.02465372 \tTraining Acuuarcy 59.453% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 676 \tTraining Loss: 0.00859919 \tValidation Loss 0.02470818 \tTraining Acuuarcy 58.901% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 677 \tTraining Loss: 0.00844462 \tValidation Loss 0.02488319 \tTraining Acuuarcy 59.581% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 678 \tTraining Loss: 0.00853924 \tValidation Loss 0.02391247 \tTraining Acuuarcy 59.230% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 679 \tTraining Loss: 0.00866341 \tValidation Loss 0.02551412 \tTraining Acuuarcy 58.165% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 680 \tTraining Loss: 0.00850577 \tValidation Loss 0.02419054 \tTraining Acuuarcy 59.648% \tValidation Acuuarcy 19.003%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 681 \tTraining Loss: 0.00860479 \tValidation Loss 0.02486681 \tTraining Acuuarcy 58.745% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 682 \tTraining Loss: 0.00855989 \tValidation Loss 0.02533172 \tTraining Acuuarcy 59.168% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 683 \tTraining Loss: 0.00858746 \tValidation Loss 0.02454673 \tTraining Acuuarcy 58.979% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 684 \tTraining Loss: 0.00849328 \tValidation Loss 0.02446639 \tTraining Acuuarcy 59.481% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 685 \tTraining Loss: 0.00856902 \tValidation Loss 0.02598418 \tTraining Acuuarcy 59.492% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 686 \tTraining Loss: 0.00854340 \tValidation Loss 0.02453903 \tTraining Acuuarcy 59.191% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 687 \tTraining Loss: 0.00862013 \tValidation Loss 0.02365003 \tTraining Acuuarcy 58.906% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 688 \tTraining Loss: 0.00872728 \tValidation Loss 0.02474059 \tTraining Acuuarcy 58.672% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 689 \tTraining Loss: 0.00837184 \tValidation Loss 0.02450483 \tTraining Acuuarcy 60.043% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 690 \tTraining Loss: 0.00855435 \tValidation Loss 0.02456705 \tTraining Acuuarcy 59.040% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 691 \tTraining Loss: 0.00854386 \tValidation Loss 0.02443005 \tTraining Acuuarcy 58.728% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 692 \tTraining Loss: 0.00851174 \tValidation Loss 0.02454585 \tTraining Acuuarcy 59.118% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 693 \tTraining Loss: 0.00856965 \tValidation Loss 0.02423199 \tTraining Acuuarcy 59.124% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 694 \tTraining Loss: 0.00862086 \tValidation Loss 0.02477630 \tTraining Acuuarcy 58.901% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 695 \tTraining Loss: 0.00841868 \tValidation Loss 0.02496491 \tTraining Acuuarcy 59.737% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 696 \tTraining Loss: 0.00853210 \tValidation Loss 0.02594067 \tTraining Acuuarcy 59.436% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 697 \tTraining Loss: 0.00859431 \tValidation Loss 0.02496702 \tTraining Acuuarcy 59.230% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 698 \tTraining Loss: 0.00853783 \tValidation Loss 0.02447110 \tTraining Acuuarcy 59.313% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 699 \tTraining Loss: 0.00850678 \tValidation Loss 0.02423776 \tTraining Acuuarcy 59.297% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 700 \tTraining Loss: 0.00855417 \tValidation Loss 0.02496890 \tTraining Acuuarcy 59.375% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 701 \tTraining Loss: 0.00848847 \tValidation Loss 0.02414556 \tTraining Acuuarcy 59.224% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 702 \tTraining Loss: 0.00843634 \tValidation Loss 0.02552594 \tTraining Acuuarcy 59.993% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 703 \tTraining Loss: 0.00841707 \tValidation Loss 0.02558377 \tTraining Acuuarcy 59.547% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 704 \tTraining Loss: 0.00837387 \tValidation Loss 0.02573835 \tTraining Acuuarcy 59.681% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 705 \tTraining Loss: 0.00843897 \tValidation Loss 0.02567409 \tTraining Acuuarcy 60.010% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 706 \tTraining Loss: 0.00845614 \tValidation Loss 0.02492213 \tTraining Acuuarcy 59.559% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 707 \tTraining Loss: 0.00854896 \tValidation Loss 0.02430606 \tTraining Acuuarcy 59.079% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 708 \tTraining Loss: 0.00853351 \tValidation Loss 0.02439600 \tTraining Acuuarcy 59.414% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 709 \tTraining Loss: 0.00843885 \tValidation Loss 0.02471648 \tTraining Acuuarcy 59.703% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 710 \tTraining Loss: 0.00859667 \tValidation Loss 0.02508273 \tTraining Acuuarcy 59.090% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 711 \tTraining Loss: 0.00848509 \tValidation Loss 0.02516207 \tTraining Acuuarcy 59.676% \tValidation Acuuarcy 16.718%\n",
      "Epoch: 712 \tTraining Loss: 0.00853993 \tValidation Loss 0.02457791 \tTraining Acuuarcy 59.068% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 713 \tTraining Loss: 0.00854953 \tValidation Loss 0.02602564 \tTraining Acuuarcy 59.447% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 714 \tTraining Loss: 0.00849470 \tValidation Loss 0.02394980 \tTraining Acuuarcy 59.453% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 715 \tTraining Loss: 0.00854021 \tValidation Loss 0.02509242 \tTraining Acuuarcy 59.007% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 716 \tTraining Loss: 0.00851306 \tValidation Loss 0.02484338 \tTraining Acuuarcy 59.609% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 717 \tTraining Loss: 0.00857438 \tValidation Loss 0.02528126 \tTraining Acuuarcy 59.213% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 718 \tTraining Loss: 0.00847743 \tValidation Loss 0.02432346 \tTraining Acuuarcy 59.403% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 719 \tTraining Loss: 0.00848551 \tValidation Loss 0.02447779 \tTraining Acuuarcy 59.871% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 720 \tTraining Loss: 0.00850632 \tValidation Loss 0.02465495 \tTraining Acuuarcy 59.297% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 721 \tTraining Loss: 0.00854639 \tValidation Loss 0.02534219 \tTraining Acuuarcy 59.258% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 722 \tTraining Loss: 0.00858504 \tValidation Loss 0.02490736 \tTraining Acuuarcy 59.024% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 723 \tTraining Loss: 0.00856541 \tValidation Loss 0.02518798 \tTraining Acuuarcy 59.308% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 724 \tTraining Loss: 0.00862824 \tValidation Loss 0.02444972 \tTraining Acuuarcy 58.589% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 725 \tTraining Loss: 0.00837923 \tValidation Loss 0.02475910 \tTraining Acuuarcy 59.731% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 726 \tTraining Loss: 0.00853736 \tValidation Loss 0.02475202 \tTraining Acuuarcy 59.057% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 727 \tTraining Loss: 0.00855556 \tValidation Loss 0.02505608 \tTraining Acuuarcy 58.912% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 728 \tTraining Loss: 0.00843987 \tValidation Loss 0.02422121 \tTraining Acuuarcy 60.122% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 729 \tTraining Loss: 0.00848004 \tValidation Loss 0.02557037 \tTraining Acuuarcy 59.564% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 730 \tTraining Loss: 0.00847814 \tValidation Loss 0.02429824 \tTraining Acuuarcy 59.408% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 731 \tTraining Loss: 0.00850630 \tValidation Loss 0.02499867 \tTraining Acuuarcy 59.553% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 732 \tTraining Loss: 0.00837910 \tValidation Loss 0.02537361 \tTraining Acuuarcy 60.528% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 733 \tTraining Loss: 0.00850708 \tValidation Loss 0.02487202 \tTraining Acuuarcy 59.425% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 734 \tTraining Loss: 0.00850881 \tValidation Loss 0.02516943 \tTraining Acuuarcy 59.341% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 735 \tTraining Loss: 0.00842729 \tValidation Loss 0.02484587 \tTraining Acuuarcy 59.258% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 736 \tTraining Loss: 0.00848226 \tValidation Loss 0.02518002 \tTraining Acuuarcy 59.107% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 737 \tTraining Loss: 0.00843236 \tValidation Loss 0.02486844 \tTraining Acuuarcy 60.361% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 738 \tTraining Loss: 0.00853711 \tValidation Loss 0.02456730 \tTraining Acuuarcy 58.912% \tValidation Acuuarcy 16.746%\n",
      "Epoch: 739 \tTraining Loss: 0.00839935 \tValidation Loss 0.02511063 \tTraining Acuuarcy 60.395% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 740 \tTraining Loss: 0.00843091 \tValidation Loss 0.02535319 \tTraining Acuuarcy 59.637% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 741 \tTraining Loss: 0.00851914 \tValidation Loss 0.02510667 \tTraining Acuuarcy 59.698% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 742 \tTraining Loss: 0.00859287 \tValidation Loss 0.02437873 \tTraining Acuuarcy 59.113% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 743 \tTraining Loss: 0.00849864 \tValidation Loss 0.02507036 \tTraining Acuuarcy 59.559% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 744 \tTraining Loss: 0.00844967 \tValidation Loss 0.02418615 \tTraining Acuuarcy 59.547% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 745 \tTraining Loss: 0.00853617 \tValidation Loss 0.02489662 \tTraining Acuuarcy 59.269% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 746 \tTraining Loss: 0.00846093 \tValidation Loss 0.02544833 \tTraining Acuuarcy 59.520% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 747 \tTraining Loss: 0.00839485 \tValidation Loss 0.02449176 \tTraining Acuuarcy 60.094% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 748 \tTraining Loss: 0.00842302 \tValidation Loss 0.02519052 \tTraining Acuuarcy 59.954% \tValidation Acuuarcy 18.585%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 749 \tTraining Loss: 0.00850615 \tValidation Loss 0.02373100 \tTraining Acuuarcy 59.386% \tValidation Acuuarcy 16.885%\n",
      "Epoch: 750 \tTraining Loss: 0.00846317 \tValidation Loss 0.02439934 \tTraining Acuuarcy 59.497% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 751 \tTraining Loss: 0.00849012 \tValidation Loss 0.02498230 \tTraining Acuuarcy 59.520% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 752 \tTraining Loss: 0.00842219 \tValidation Loss 0.02490805 \tTraining Acuuarcy 60.211% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 753 \tTraining Loss: 0.00853142 \tValidation Loss 0.02472263 \tTraining Acuuarcy 59.364% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 754 \tTraining Loss: 0.00855069 \tValidation Loss 0.02507373 \tTraining Acuuarcy 59.492% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 755 \tTraining Loss: 0.00858704 \tValidation Loss 0.02481048 \tTraining Acuuarcy 59.001% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 756 \tTraining Loss: 0.00841105 \tValidation Loss 0.02438491 \tTraining Acuuarcy 60.161% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 757 \tTraining Loss: 0.00845866 \tValidation Loss 0.02463203 \tTraining Acuuarcy 59.336% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 758 \tTraining Loss: 0.00843040 \tValidation Loss 0.02502961 \tTraining Acuuarcy 59.770% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 759 \tTraining Loss: 0.00836312 \tValidation Loss 0.02445798 \tTraining Acuuarcy 60.473% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 760 \tTraining Loss: 0.00841978 \tValidation Loss 0.02528667 \tTraining Acuuarcy 59.949% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 761 \tTraining Loss: 0.00838145 \tValidation Loss 0.02511946 \tTraining Acuuarcy 59.559% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 762 \tTraining Loss: 0.00831027 \tValidation Loss 0.02503128 \tTraining Acuuarcy 60.606% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 763 \tTraining Loss: 0.00841018 \tValidation Loss 0.02493924 \tTraining Acuuarcy 60.311% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 764 \tTraining Loss: 0.00842898 \tValidation Loss 0.02497709 \tTraining Acuuarcy 59.698% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 765 \tTraining Loss: 0.00843901 \tValidation Loss 0.02513664 \tTraining Acuuarcy 59.926% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 766 \tTraining Loss: 0.00837485 \tValidation Loss 0.02554164 \tTraining Acuuarcy 60.110% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 767 \tTraining Loss: 0.00836023 \tValidation Loss 0.02588773 \tTraining Acuuarcy 60.183% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 768 \tTraining Loss: 0.00853087 \tValidation Loss 0.02510070 \tTraining Acuuarcy 59.135% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 769 \tTraining Loss: 0.00842667 \tValidation Loss 0.02526289 \tTraining Acuuarcy 59.743% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 770 \tTraining Loss: 0.00852424 \tValidation Loss 0.02562382 \tTraining Acuuarcy 59.141% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 771 \tTraining Loss: 0.00844793 \tValidation Loss 0.02563184 \tTraining Acuuarcy 59.681% \tValidation Acuuarcy 16.801%\n",
      "Epoch: 772 \tTraining Loss: 0.00844023 \tValidation Loss 0.02520113 \tTraining Acuuarcy 59.408% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 773 \tTraining Loss: 0.00835660 \tValidation Loss 0.02487643 \tTraining Acuuarcy 60.049% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 774 \tTraining Loss: 0.00845738 \tValidation Loss 0.02464709 \tTraining Acuuarcy 59.319% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 775 \tTraining Loss: 0.00837656 \tValidation Loss 0.02511227 \tTraining Acuuarcy 59.954% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 776 \tTraining Loss: 0.00839033 \tValidation Loss 0.02576262 \tTraining Acuuarcy 60.122% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 777 \tTraining Loss: 0.00836478 \tValidation Loss 0.02496137 \tTraining Acuuarcy 60.395% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 778 \tTraining Loss: 0.00842117 \tValidation Loss 0.02469014 \tTraining Acuuarcy 60.573% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 779 \tTraining Loss: 0.00832990 \tValidation Loss 0.02560479 \tTraining Acuuarcy 60.339% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 780 \tTraining Loss: 0.00832988 \tValidation Loss 0.02595947 \tTraining Acuuarcy 60.461% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 781 \tTraining Loss: 0.00832815 \tValidation Loss 0.02512626 \tTraining Acuuarcy 60.283% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 782 \tTraining Loss: 0.00838790 \tValidation Loss 0.02502701 \tTraining Acuuarcy 60.411% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 783 \tTraining Loss: 0.00844199 \tValidation Loss 0.02484464 \tTraining Acuuarcy 59.715% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 784 \tTraining Loss: 0.00835368 \tValidation Loss 0.02484780 \tTraining Acuuarcy 60.161% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 785 \tTraining Loss: 0.00835700 \tValidation Loss 0.02463706 \tTraining Acuuarcy 59.893% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 786 \tTraining Loss: 0.00832742 \tValidation Loss 0.02529703 \tTraining Acuuarcy 60.133% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 787 \tTraining Loss: 0.00841920 \tValidation Loss 0.02487525 \tTraining Acuuarcy 59.832% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 788 \tTraining Loss: 0.00838168 \tValidation Loss 0.02520814 \tTraining Acuuarcy 60.027% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 789 \tTraining Loss: 0.00846785 \tValidation Loss 0.02479909 \tTraining Acuuarcy 59.564% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 790 \tTraining Loss: 0.00836836 \tValidation Loss 0.02597172 \tTraining Acuuarcy 60.383% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 791 \tTraining Loss: 0.00838955 \tValidation Loss 0.02556698 \tTraining Acuuarcy 59.904% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 792 \tTraining Loss: 0.00841981 \tValidation Loss 0.02480635 \tTraining Acuuarcy 59.804% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 793 \tTraining Loss: 0.00844538 \tValidation Loss 0.02531075 \tTraining Acuuarcy 59.988% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 794 \tTraining Loss: 0.00842174 \tValidation Loss 0.02572449 \tTraining Acuuarcy 59.971% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 795 \tTraining Loss: 0.00836704 \tValidation Loss 0.02588270 \tTraining Acuuarcy 59.876% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 796 \tTraining Loss: 0.00829874 \tValidation Loss 0.02490383 \tTraining Acuuarcy 60.294% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 797 \tTraining Loss: 0.00826333 \tValidation Loss 0.02527963 \tTraining Acuuarcy 60.344% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 798 \tTraining Loss: 0.00829360 \tValidation Loss 0.02603984 \tTraining Acuuarcy 60.645% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 799 \tTraining Loss: 0.00835568 \tValidation Loss 0.02481834 \tTraining Acuuarcy 60.066% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 800 \tTraining Loss: 0.00831917 \tValidation Loss 0.02467222 \tTraining Acuuarcy 60.305% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 801 \tTraining Loss: 0.00839067 \tValidation Loss 0.02477314 \tTraining Acuuarcy 59.748% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 802 \tTraining Loss: 0.00836094 \tValidation Loss 0.02520431 \tTraining Acuuarcy 60.333% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 803 \tTraining Loss: 0.00831719 \tValidation Loss 0.02495118 \tTraining Acuuarcy 60.495% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 804 \tTraining Loss: 0.00830379 \tValidation Loss 0.02687552 \tTraining Acuuarcy 60.651% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 805 \tTraining Loss: 0.00835791 \tValidation Loss 0.02524110 \tTraining Acuuarcy 60.027% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 806 \tTraining Loss: 0.00845539 \tValidation Loss 0.02528649 \tTraining Acuuarcy 59.776% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 807 \tTraining Loss: 0.00845481 \tValidation Loss 0.02533432 \tTraining Acuuarcy 59.826% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 808 \tTraining Loss: 0.00843916 \tValidation Loss 0.02569450 \tTraining Acuuarcy 59.910% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 809 \tTraining Loss: 0.00839883 \tValidation Loss 0.02538376 \tTraining Acuuarcy 59.910% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 810 \tTraining Loss: 0.00839748 \tValidation Loss 0.02583770 \tTraining Acuuarcy 60.021% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 811 \tTraining Loss: 0.00834983 \tValidation Loss 0.02498358 \tTraining Acuuarcy 59.887% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 812 \tTraining Loss: 0.00832803 \tValidation Loss 0.02581295 \tTraining Acuuarcy 60.746% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 813 \tTraining Loss: 0.00835894 \tValidation Loss 0.02528422 \tTraining Acuuarcy 59.932% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 814 \tTraining Loss: 0.00847789 \tValidation Loss 0.02526341 \tTraining Acuuarcy 59.943% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 815 \tTraining Loss: 0.00841813 \tValidation Loss 0.02448848 \tTraining Acuuarcy 59.815% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 816 \tTraining Loss: 0.00829364 \tValidation Loss 0.02551331 \tTraining Acuuarcy 60.657% \tValidation Acuuarcy 17.359%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 817 \tTraining Loss: 0.00828929 \tValidation Loss 0.02534533 \tTraining Acuuarcy 60.668% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 818 \tTraining Loss: 0.00836158 \tValidation Loss 0.02507831 \tTraining Acuuarcy 60.155% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 819 \tTraining Loss: 0.00839019 \tValidation Loss 0.02550553 \tTraining Acuuarcy 60.144% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 820 \tTraining Loss: 0.00844388 \tValidation Loss 0.02483089 \tTraining Acuuarcy 59.258% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 821 \tTraining Loss: 0.00832129 \tValidation Loss 0.02516568 \tTraining Acuuarcy 60.383% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 822 \tTraining Loss: 0.00822881 \tValidation Loss 0.02526782 \tTraining Acuuarcy 61.097% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 823 \tTraining Loss: 0.00829432 \tValidation Loss 0.02583778 \tTraining Acuuarcy 60.244% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 824 \tTraining Loss: 0.00841102 \tValidation Loss 0.02548292 \tTraining Acuuarcy 59.787% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 825 \tTraining Loss: 0.00837614 \tValidation Loss 0.02517760 \tTraining Acuuarcy 59.949% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 826 \tTraining Loss: 0.00834770 \tValidation Loss 0.02595898 \tTraining Acuuarcy 60.177% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 827 \tTraining Loss: 0.00829096 \tValidation Loss 0.02575829 \tTraining Acuuarcy 60.657% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 828 \tTraining Loss: 0.00830386 \tValidation Loss 0.02671292 \tTraining Acuuarcy 60.774% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 829 \tTraining Loss: 0.00845711 \tValidation Loss 0.02527061 \tTraining Acuuarcy 59.481% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 830 \tTraining Loss: 0.00821779 \tValidation Loss 0.02487256 \tTraining Acuuarcy 60.606% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 831 \tTraining Loss: 0.00842684 \tValidation Loss 0.02579897 \tTraining Acuuarcy 60.556% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 832 \tTraining Loss: 0.00829047 \tValidation Loss 0.02585652 \tTraining Acuuarcy 60.350% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 833 \tTraining Loss: 0.00831334 \tValidation Loss 0.02483983 \tTraining Acuuarcy 60.690% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 834 \tTraining Loss: 0.00825675 \tValidation Loss 0.02516733 \tTraining Acuuarcy 60.445% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 835 \tTraining Loss: 0.00832124 \tValidation Loss 0.02552344 \tTraining Acuuarcy 60.595% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 836 \tTraining Loss: 0.00836506 \tValidation Loss 0.02569247 \tTraining Acuuarcy 60.116% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 837 \tTraining Loss: 0.00836616 \tValidation Loss 0.02560130 \tTraining Acuuarcy 60.088% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 838 \tTraining Loss: 0.00833761 \tValidation Loss 0.02507983 \tTraining Acuuarcy 60.395% \tValidation Acuuarcy 16.606%\n",
      "Epoch: 839 \tTraining Loss: 0.00831211 \tValidation Loss 0.02483271 \tTraining Acuuarcy 60.534% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 840 \tTraining Loss: 0.00831136 \tValidation Loss 0.02514069 \tTraining Acuuarcy 60.194% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 841 \tTraining Loss: 0.00835966 \tValidation Loss 0.02449059 \tTraining Acuuarcy 60.311% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 842 \tTraining Loss: 0.00833577 \tValidation Loss 0.02450321 \tTraining Acuuarcy 60.651% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 843 \tTraining Loss: 0.00832324 \tValidation Loss 0.02568946 \tTraining Acuuarcy 60.618% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 844 \tTraining Loss: 0.00821161 \tValidation Loss 0.02484336 \tTraining Acuuarcy 60.852% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 845 \tTraining Loss: 0.00839952 \tValidation Loss 0.02488282 \tTraining Acuuarcy 59.982% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 846 \tTraining Loss: 0.00842883 \tValidation Loss 0.02549679 \tTraining Acuuarcy 59.848% \tValidation Acuuarcy 16.746%\n",
      "Epoch: 847 \tTraining Loss: 0.00831959 \tValidation Loss 0.02589960 \tTraining Acuuarcy 60.183% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 848 \tTraining Loss: 0.00832560 \tValidation Loss 0.02522373 \tTraining Acuuarcy 60.489% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 849 \tTraining Loss: 0.00831711 \tValidation Loss 0.02508356 \tTraining Acuuarcy 60.422% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 850 \tTraining Loss: 0.00830505 \tValidation Loss 0.02532477 \tTraining Acuuarcy 60.907% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 851 \tTraining Loss: 0.00834988 \tValidation Loss 0.02602205 \tTraining Acuuarcy 60.629% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 852 \tTraining Loss: 0.00831508 \tValidation Loss 0.02520447 \tTraining Acuuarcy 60.099% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 853 \tTraining Loss: 0.00825099 \tValidation Loss 0.02572019 \tTraining Acuuarcy 60.774% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 854 \tTraining Loss: 0.00824443 \tValidation Loss 0.02530328 \tTraining Acuuarcy 60.785% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 855 \tTraining Loss: 0.00827525 \tValidation Loss 0.02447667 \tTraining Acuuarcy 60.985% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 856 \tTraining Loss: 0.00828727 \tValidation Loss 0.02552022 \tTraining Acuuarcy 60.779% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 857 \tTraining Loss: 0.00834087 \tValidation Loss 0.02445197 \tTraining Acuuarcy 60.634% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 858 \tTraining Loss: 0.00834023 \tValidation Loss 0.02537139 \tTraining Acuuarcy 60.127% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 859 \tTraining Loss: 0.00832027 \tValidation Loss 0.02512885 \tTraining Acuuarcy 59.999% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 860 \tTraining Loss: 0.00839284 \tValidation Loss 0.02614071 \tTraining Acuuarcy 60.272% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 861 \tTraining Loss: 0.00834693 \tValidation Loss 0.02511528 \tTraining Acuuarcy 60.735% \tValidation Acuuarcy 16.049%\n",
      "Epoch: 862 \tTraining Loss: 0.00829098 \tValidation Loss 0.02548194 \tTraining Acuuarcy 60.523% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 863 \tTraining Loss: 0.00822402 \tValidation Loss 0.02542424 \tTraining Acuuarcy 61.258% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 864 \tTraining Loss: 0.00830614 \tValidation Loss 0.02513493 \tTraining Acuuarcy 60.417% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 865 \tTraining Loss: 0.00831115 \tValidation Loss 0.02455777 \tTraining Acuuarcy 60.573% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 866 \tTraining Loss: 0.00828461 \tValidation Loss 0.02526649 \tTraining Acuuarcy 60.595% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 867 \tTraining Loss: 0.00827301 \tValidation Loss 0.02468847 \tTraining Acuuarcy 60.618% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 868 \tTraining Loss: 0.00839805 \tValidation Loss 0.02611750 \tTraining Acuuarcy 59.999% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 869 \tTraining Loss: 0.00829216 \tValidation Loss 0.02505255 \tTraining Acuuarcy 60.762% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 870 \tTraining Loss: 0.00838691 \tValidation Loss 0.02474772 \tTraining Acuuarcy 59.943% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 871 \tTraining Loss: 0.00834952 \tValidation Loss 0.02448812 \tTraining Acuuarcy 60.127% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 872 \tTraining Loss: 0.00850124 \tValidation Loss 0.02515742 \tTraining Acuuarcy 59.514% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 873 \tTraining Loss: 0.00829760 \tValidation Loss 0.02484011 \tTraining Acuuarcy 60.294% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 874 \tTraining Loss: 0.00827297 \tValidation Loss 0.02497155 \tTraining Acuuarcy 60.545% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 875 \tTraining Loss: 0.00841413 \tValidation Loss 0.02431560 \tTraining Acuuarcy 60.200% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 876 \tTraining Loss: 0.00828794 \tValidation Loss 0.02484696 \tTraining Acuuarcy 60.411% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 877 \tTraining Loss: 0.00825833 \tValidation Loss 0.02465265 \tTraining Acuuarcy 60.707% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 878 \tTraining Loss: 0.00829048 \tValidation Loss 0.02599199 \tTraining Acuuarcy 60.980% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 879 \tTraining Loss: 0.00828605 \tValidation Loss 0.02536970 \tTraining Acuuarcy 60.684% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 880 \tTraining Loss: 0.00829573 \tValidation Loss 0.02484792 \tTraining Acuuarcy 60.116% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 881 \tTraining Loss: 0.00818293 \tValidation Loss 0.02587110 \tTraining Acuuarcy 61.376% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 882 \tTraining Loss: 0.00819043 \tValidation Loss 0.02524652 \tTraining Acuuarcy 60.634% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 883 \tTraining Loss: 0.00816768 \tValidation Loss 0.02516489 \tTraining Acuuarcy 61.208% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 884 \tTraining Loss: 0.00833526 \tValidation Loss 0.02525424 \tTraining Acuuarcy 60.467% \tValidation Acuuarcy 17.888%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 885 \tTraining Loss: 0.00822970 \tValidation Loss 0.02590864 \tTraining Acuuarcy 60.718% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 886 \tTraining Loss: 0.00837570 \tValidation Loss 0.02594147 \tTraining Acuuarcy 60.166% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 887 \tTraining Loss: 0.00836849 \tValidation Loss 0.02537177 \tTraining Acuuarcy 60.244% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 888 \tTraining Loss: 0.00828402 \tValidation Loss 0.02613769 \tTraining Acuuarcy 60.528% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 889 \tTraining Loss: 0.00839489 \tValidation Loss 0.02605886 \tTraining Acuuarcy 59.837% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 890 \tTraining Loss: 0.00831446 \tValidation Loss 0.02531185 \tTraining Acuuarcy 60.484% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 891 \tTraining Loss: 0.00817625 \tValidation Loss 0.02500932 \tTraining Acuuarcy 61.147% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 892 \tTraining Loss: 0.00828078 \tValidation Loss 0.02556165 \tTraining Acuuarcy 60.790% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 893 \tTraining Loss: 0.00830347 \tValidation Loss 0.02620567 \tTraining Acuuarcy 60.278% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 894 \tTraining Loss: 0.00818040 \tValidation Loss 0.02533826 \tTraining Acuuarcy 61.002% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 895 \tTraining Loss: 0.00829011 \tValidation Loss 0.02623932 \tTraining Acuuarcy 60.032% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 896 \tTraining Loss: 0.00818295 \tValidation Loss 0.02576366 \tTraining Acuuarcy 61.387% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 897 \tTraining Loss: 0.00824151 \tValidation Loss 0.02535729 \tTraining Acuuarcy 60.729% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 898 \tTraining Loss: 0.00822414 \tValidation Loss 0.02494017 \tTraining Acuuarcy 60.796% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 899 \tTraining Loss: 0.00811007 \tValidation Loss 0.02499371 \tTraining Acuuarcy 61.504% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 900 \tTraining Loss: 0.00824575 \tValidation Loss 0.02560329 \tTraining Acuuarcy 60.807% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 901 \tTraining Loss: 0.00823513 \tValidation Loss 0.02547323 \tTraining Acuuarcy 60.361% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 902 \tTraining Loss: 0.00829455 \tValidation Loss 0.02555456 \tTraining Acuuarcy 60.707% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 903 \tTraining Loss: 0.00818966 \tValidation Loss 0.02482340 \tTraining Acuuarcy 60.974% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 904 \tTraining Loss: 0.00823168 \tValidation Loss 0.02543858 \tTraining Acuuarcy 60.562% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 905 \tTraining Loss: 0.00817933 \tValidation Loss 0.02641775 \tTraining Acuuarcy 61.008% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 906 \tTraining Loss: 0.00837933 \tValidation Loss 0.02534958 \tTraining Acuuarcy 60.161% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 907 \tTraining Loss: 0.00828025 \tValidation Loss 0.02638655 \tTraining Acuuarcy 60.595% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 908 \tTraining Loss: 0.00824279 \tValidation Loss 0.02566755 \tTraining Acuuarcy 60.762% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 909 \tTraining Loss: 0.00824702 \tValidation Loss 0.02593185 \tTraining Acuuarcy 61.130% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 910 \tTraining Loss: 0.00833018 \tValidation Loss 0.02523647 \tTraining Acuuarcy 60.567% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 911 \tTraining Loss: 0.00825238 \tValidation Loss 0.02523999 \tTraining Acuuarcy 60.718% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 912 \tTraining Loss: 0.00828205 \tValidation Loss 0.02572852 \tTraining Acuuarcy 60.478% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 913 \tTraining Loss: 0.00812695 \tValidation Loss 0.02525890 \tTraining Acuuarcy 61.214% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 914 \tTraining Loss: 0.00821825 \tValidation Loss 0.02520594 \tTraining Acuuarcy 61.309% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 915 \tTraining Loss: 0.00822923 \tValidation Loss 0.02624256 \tTraining Acuuarcy 61.337% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 916 \tTraining Loss: 0.00826972 \tValidation Loss 0.02548774 \tTraining Acuuarcy 60.829% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 917 \tTraining Loss: 0.00834558 \tValidation Loss 0.02579380 \tTraining Acuuarcy 60.161% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 918 \tTraining Loss: 0.00829703 \tValidation Loss 0.02587031 \tTraining Acuuarcy 60.400% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 919 \tTraining Loss: 0.00832029 \tValidation Loss 0.02524189 \tTraining Acuuarcy 60.919% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 920 \tTraining Loss: 0.00828496 \tValidation Loss 0.02583970 \tTraining Acuuarcy 60.439% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 921 \tTraining Loss: 0.00820874 \tValidation Loss 0.02584745 \tTraining Acuuarcy 61.353% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 922 \tTraining Loss: 0.00825532 \tValidation Loss 0.02610894 \tTraining Acuuarcy 60.801% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 923 \tTraining Loss: 0.00830435 \tValidation Loss 0.02574011 \tTraining Acuuarcy 60.618% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 924 \tTraining Loss: 0.00824981 \tValidation Loss 0.02491783 \tTraining Acuuarcy 60.963% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 925 \tTraining Loss: 0.00825298 \tValidation Loss 0.02580406 \tTraining Acuuarcy 61.047% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 926 \tTraining Loss: 0.00811623 \tValidation Loss 0.02525427 \tTraining Acuuarcy 61.409% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 927 \tTraining Loss: 0.00817357 \tValidation Loss 0.02558606 \tTraining Acuuarcy 61.225% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 928 \tTraining Loss: 0.00819232 \tValidation Loss 0.02590917 \tTraining Acuuarcy 61.047% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 929 \tTraining Loss: 0.00817807 \tValidation Loss 0.02555298 \tTraining Acuuarcy 61.353% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 930 \tTraining Loss: 0.00819843 \tValidation Loss 0.02533780 \tTraining Acuuarcy 61.058% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 931 \tTraining Loss: 0.00825822 \tValidation Loss 0.02605523 \tTraining Acuuarcy 61.008% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 932 \tTraining Loss: 0.00816368 \tValidation Loss 0.02531122 \tTraining Acuuarcy 61.019% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 933 \tTraining Loss: 0.00816867 \tValidation Loss 0.02553719 \tTraining Acuuarcy 61.158% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 934 \tTraining Loss: 0.00817495 \tValidation Loss 0.02495115 \tTraining Acuuarcy 61.626% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 935 \tTraining Loss: 0.00828582 \tValidation Loss 0.02530806 \tTraining Acuuarcy 60.584% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 936 \tTraining Loss: 0.00817378 \tValidation Loss 0.02529682 \tTraining Acuuarcy 61.102% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 937 \tTraining Loss: 0.00827726 \tValidation Loss 0.02497525 \tTraining Acuuarcy 60.640% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 938 \tTraining Loss: 0.00816893 \tValidation Loss 0.02504250 \tTraining Acuuarcy 61.058% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 939 \tTraining Loss: 0.00829712 \tValidation Loss 0.02500581 \tTraining Acuuarcy 60.283% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 940 \tTraining Loss: 0.00832651 \tValidation Loss 0.02584289 \tTraining Acuuarcy 60.473% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 941 \tTraining Loss: 0.00814177 \tValidation Loss 0.02537690 \tTraining Acuuarcy 61.309% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 942 \tTraining Loss: 0.00822210 \tValidation Loss 0.02488281 \tTraining Acuuarcy 61.431% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 943 \tTraining Loss: 0.00819772 \tValidation Loss 0.02656374 \tTraining Acuuarcy 61.013% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 944 \tTraining Loss: 0.00809368 \tValidation Loss 0.02568499 \tTraining Acuuarcy 61.470% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 945 \tTraining Loss: 0.00808902 \tValidation Loss 0.02595743 \tTraining Acuuarcy 61.376% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 946 \tTraining Loss: 0.00831238 \tValidation Loss 0.02541362 \tTraining Acuuarcy 60.278% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 947 \tTraining Loss: 0.00813643 \tValidation Loss 0.02509347 \tTraining Acuuarcy 61.119% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 948 \tTraining Loss: 0.00826494 \tValidation Loss 0.02569099 \tTraining Acuuarcy 60.595% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 949 \tTraining Loss: 0.00827393 \tValidation Loss 0.02576906 \tTraining Acuuarcy 60.751% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 950 \tTraining Loss: 0.00808447 \tValidation Loss 0.02582860 \tTraining Acuuarcy 61.403% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 951 \tTraining Loss: 0.00824345 \tValidation Loss 0.02549104 \tTraining Acuuarcy 60.679% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 952 \tTraining Loss: 0.00820933 \tValidation Loss 0.02577118 \tTraining Acuuarcy 61.303% \tValidation Acuuarcy 17.804%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 953 \tTraining Loss: 0.00824851 \tValidation Loss 0.02597879 \tTraining Acuuarcy 60.935% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 954 \tTraining Loss: 0.00803550 \tValidation Loss 0.02568539 \tTraining Acuuarcy 61.682% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 955 \tTraining Loss: 0.00814274 \tValidation Loss 0.02590349 \tTraining Acuuarcy 61.325% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 956 \tTraining Loss: 0.00817905 \tValidation Loss 0.02498513 \tTraining Acuuarcy 61.415% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 957 \tTraining Loss: 0.00809927 \tValidation Loss 0.02581934 \tTraining Acuuarcy 61.097% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 958 \tTraining Loss: 0.00820599 \tValidation Loss 0.02620935 \tTraining Acuuarcy 61.153% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 959 \tTraining Loss: 0.00813441 \tValidation Loss 0.02556894 \tTraining Acuuarcy 61.186% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 960 \tTraining Loss: 0.00812181 \tValidation Loss 0.02547850 \tTraining Acuuarcy 61.532% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 961 \tTraining Loss: 0.00825824 \tValidation Loss 0.02540402 \tTraining Acuuarcy 60.545% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 962 \tTraining Loss: 0.00815511 \tValidation Loss 0.02552826 \tTraining Acuuarcy 61.286% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 963 \tTraining Loss: 0.00818044 \tValidation Loss 0.02535050 \tTraining Acuuarcy 61.559% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 964 \tTraining Loss: 0.00815744 \tValidation Loss 0.02594467 \tTraining Acuuarcy 61.677% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 965 \tTraining Loss: 0.00820772 \tValidation Loss 0.02600080 \tTraining Acuuarcy 61.141% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 966 \tTraining Loss: 0.00814097 \tValidation Loss 0.02593051 \tTraining Acuuarcy 60.723% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 967 \tTraining Loss: 0.00829756 \tValidation Loss 0.02596827 \tTraining Acuuarcy 60.958% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 968 \tTraining Loss: 0.00811419 \tValidation Loss 0.02658413 \tTraining Acuuarcy 61.487% \tValidation Acuuarcy 16.578%\n",
      "Epoch: 969 \tTraining Loss: 0.00812042 \tValidation Loss 0.02632249 \tTraining Acuuarcy 61.314% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 970 \tTraining Loss: 0.00824654 \tValidation Loss 0.02555883 \tTraining Acuuarcy 60.757% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 971 \tTraining Loss: 0.00812179 \tValidation Loss 0.02616583 \tTraining Acuuarcy 61.509% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 972 \tTraining Loss: 0.00819170 \tValidation Loss 0.02619397 \tTraining Acuuarcy 60.813% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 973 \tTraining Loss: 0.00806645 \tValidation Loss 0.02706236 \tTraining Acuuarcy 61.732% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 974 \tTraining Loss: 0.00811234 \tValidation Loss 0.02627418 \tTraining Acuuarcy 61.738% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 975 \tTraining Loss: 0.00827070 \tValidation Loss 0.02550272 \tTraining Acuuarcy 60.149% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 976 \tTraining Loss: 0.00840564 \tValidation Loss 0.02503579 \tTraining Acuuarcy 60.239% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 977 \tTraining Loss: 0.00847221 \tValidation Loss 0.02560442 \tTraining Acuuarcy 60.027% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 978 \tTraining Loss: 0.00830398 \tValidation Loss 0.02504819 \tTraining Acuuarcy 59.960% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 979 \tTraining Loss: 0.00827969 \tValidation Loss 0.02572494 \tTraining Acuuarcy 60.545% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 980 \tTraining Loss: 0.00813374 \tValidation Loss 0.02654677 \tTraining Acuuarcy 61.721% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 981 \tTraining Loss: 0.00820600 \tValidation Loss 0.02552495 \tTraining Acuuarcy 61.270% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 982 \tTraining Loss: 0.00818967 \tValidation Loss 0.02544489 \tTraining Acuuarcy 61.024% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 983 \tTraining Loss: 0.00827253 \tValidation Loss 0.02600334 \tTraining Acuuarcy 60.612% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 984 \tTraining Loss: 0.00808532 \tValidation Loss 0.02606676 \tTraining Acuuarcy 61.654% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 985 \tTraining Loss: 0.00799309 \tValidation Loss 0.02699727 \tTraining Acuuarcy 62.251% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 986 \tTraining Loss: 0.00809904 \tValidation Loss 0.02619589 \tTraining Acuuarcy 61.660% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 987 \tTraining Loss: 0.00805613 \tValidation Loss 0.02608422 \tTraining Acuuarcy 62.044% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 988 \tTraining Loss: 0.00813550 \tValidation Loss 0.02597206 \tTraining Acuuarcy 61.559% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 989 \tTraining Loss: 0.00811905 \tValidation Loss 0.02619429 \tTraining Acuuarcy 61.844% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 990 \tTraining Loss: 0.00810470 \tValidation Loss 0.02660037 \tTraining Acuuarcy 61.292% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 991 \tTraining Loss: 0.00823371 \tValidation Loss 0.02647578 \tTraining Acuuarcy 61.086% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 992 \tTraining Loss: 0.00804417 \tValidation Loss 0.02640331 \tTraining Acuuarcy 62.039% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 993 \tTraining Loss: 0.00808305 \tValidation Loss 0.02542228 \tTraining Acuuarcy 61.286% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 994 \tTraining Loss: 0.00807348 \tValidation Loss 0.02584091 \tTraining Acuuarcy 61.821% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 995 \tTraining Loss: 0.00819397 \tValidation Loss 0.02609483 \tTraining Acuuarcy 61.024% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 996 \tTraining Loss: 0.00821051 \tValidation Loss 0.02629969 \tTraining Acuuarcy 60.902% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 997 \tTraining Loss: 0.00828390 \tValidation Loss 0.02576546 \tTraining Acuuarcy 60.690% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 998 \tTraining Loss: 0.00809311 \tValidation Loss 0.02498841 \tTraining Acuuarcy 61.192% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 999 \tTraining Loss: 0.00819145 \tValidation Loss 0.02655349 \tTraining Acuuarcy 61.314% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1000 \tTraining Loss: 0.00809946 \tValidation Loss 0.02630179 \tTraining Acuuarcy 61.298% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1001 \tTraining Loss: 0.00823184 \tValidation Loss 0.02555743 \tTraining Acuuarcy 61.281% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1002 \tTraining Loss: 0.00814067 \tValidation Loss 0.02612349 \tTraining Acuuarcy 61.052% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1003 \tTraining Loss: 0.00810792 \tValidation Loss 0.02570429 \tTraining Acuuarcy 61.821% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1004 \tTraining Loss: 0.00833977 \tValidation Loss 0.02573957 \tTraining Acuuarcy 60.255% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1005 \tTraining Loss: 0.00810929 \tValidation Loss 0.02575450 \tTraining Acuuarcy 61.470% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1006 \tTraining Loss: 0.00815525 \tValidation Loss 0.02625022 \tTraining Acuuarcy 61.353% \tValidation Acuuarcy 16.885%\n",
      "Epoch: 1007 \tTraining Loss: 0.00812693 \tValidation Loss 0.02552991 \tTraining Acuuarcy 61.860% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1008 \tTraining Loss: 0.00801807 \tValidation Loss 0.02618192 \tTraining Acuuarcy 61.899% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1009 \tTraining Loss: 0.00811084 \tValidation Loss 0.02524076 \tTraining Acuuarcy 61.654% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1010 \tTraining Loss: 0.00809953 \tValidation Loss 0.02583206 \tTraining Acuuarcy 61.710% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1011 \tTraining Loss: 0.00810272 \tValidation Loss 0.02675118 \tTraining Acuuarcy 61.169% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1012 \tTraining Loss: 0.00809208 \tValidation Loss 0.02606090 \tTraining Acuuarcy 61.448% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1013 \tTraining Loss: 0.00806654 \tValidation Loss 0.02587058 \tTraining Acuuarcy 61.532% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 1014 \tTraining Loss: 0.00809731 \tValidation Loss 0.02599808 \tTraining Acuuarcy 61.415% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1015 \tTraining Loss: 0.00811249 \tValidation Loss 0.02532599 \tTraining Acuuarcy 61.626% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1016 \tTraining Loss: 0.00813795 \tValidation Loss 0.02514671 \tTraining Acuuarcy 61.225% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1017 \tTraining Loss: 0.00818152 \tValidation Loss 0.02631580 \tTraining Acuuarcy 61.258% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 1018 \tTraining Loss: 0.00809395 \tValidation Loss 0.02575328 \tTraining Acuuarcy 61.348% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1019 \tTraining Loss: 0.00821093 \tValidation Loss 0.02595866 \tTraining Acuuarcy 60.707% \tValidation Acuuarcy 17.804%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1020 \tTraining Loss: 0.00805324 \tValidation Loss 0.02575791 \tTraining Acuuarcy 62.251% \tValidation Acuuarcy 16.885%\n",
      "Epoch: 1021 \tTraining Loss: 0.00805681 \tValidation Loss 0.02612061 \tTraining Acuuarcy 61.571% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 1022 \tTraining Loss: 0.00808079 \tValidation Loss 0.02510459 \tTraining Acuuarcy 60.902% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1023 \tTraining Loss: 0.00808554 \tValidation Loss 0.02540001 \tTraining Acuuarcy 61.437% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 1024 \tTraining Loss: 0.00800603 \tValidation Loss 0.02647909 \tTraining Acuuarcy 61.682% \tValidation Acuuarcy 16.801%\n",
      "Epoch: 1025 \tTraining Loss: 0.00801710 \tValidation Loss 0.02632863 \tTraining Acuuarcy 62.345% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1026 \tTraining Loss: 0.00818480 \tValidation Loss 0.02548393 \tTraining Acuuarcy 61.203% \tValidation Acuuarcy 16.941%\n",
      "Epoch: 1027 \tTraining Loss: 0.00812153 \tValidation Loss 0.02634883 \tTraining Acuuarcy 61.403% \tValidation Acuuarcy 16.829%\n",
      "Epoch: 1028 \tTraining Loss: 0.00816899 \tValidation Loss 0.02552222 \tTraining Acuuarcy 61.013% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1029 \tTraining Loss: 0.00824561 \tValidation Loss 0.02532447 \tTraining Acuuarcy 60.985% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1030 \tTraining Loss: 0.00821155 \tValidation Loss 0.02562580 \tTraining Acuuarcy 61.219% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1031 \tTraining Loss: 0.00813807 \tValidation Loss 0.02652559 \tTraining Acuuarcy 61.281% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1032 \tTraining Loss: 0.00817527 \tValidation Loss 0.02645762 \tTraining Acuuarcy 60.958% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1033 \tTraining Loss: 0.00797748 \tValidation Loss 0.02620702 \tTraining Acuuarcy 62.033% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1034 \tTraining Loss: 0.00809209 \tValidation Loss 0.02643310 \tTraining Acuuarcy 61.565% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1035 \tTraining Loss: 0.00818994 \tValidation Loss 0.02654655 \tTraining Acuuarcy 60.935% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1036 \tTraining Loss: 0.00807072 \tValidation Loss 0.02545790 \tTraining Acuuarcy 61.353% \tValidation Acuuarcy 16.188%\n",
      "Epoch: 1037 \tTraining Loss: 0.00816451 \tValidation Loss 0.02593726 \tTraining Acuuarcy 61.381% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 1038 \tTraining Loss: 0.00805594 \tValidation Loss 0.02594345 \tTraining Acuuarcy 61.805% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1039 \tTraining Loss: 0.00813599 \tValidation Loss 0.02550506 \tTraining Acuuarcy 61.203% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1040 \tTraining Loss: 0.00810185 \tValidation Loss 0.02598667 \tTraining Acuuarcy 61.337% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1041 \tTraining Loss: 0.00804957 \tValidation Loss 0.02522569 \tTraining Acuuarcy 62.111% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1042 \tTraining Loss: 0.00803583 \tValidation Loss 0.02647853 \tTraining Acuuarcy 62.161% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1043 \tTraining Loss: 0.00815953 \tValidation Loss 0.02638546 \tTraining Acuuarcy 60.946% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1044 \tTraining Loss: 0.00795841 \tValidation Loss 0.02614883 \tTraining Acuuarcy 62.189% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 1045 \tTraining Loss: 0.00813692 \tValidation Loss 0.02487703 \tTraining Acuuarcy 61.755% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1046 \tTraining Loss: 0.00806946 \tValidation Loss 0.02488853 \tTraining Acuuarcy 61.598% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1047 \tTraining Loss: 0.00800231 \tValidation Loss 0.02594767 \tTraining Acuuarcy 62.273% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1048 \tTraining Loss: 0.00813285 \tValidation Loss 0.02629949 \tTraining Acuuarcy 61.180% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1049 \tTraining Loss: 0.00812393 \tValidation Loss 0.02552201 \tTraining Acuuarcy 61.114% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1050 \tTraining Loss: 0.00813148 \tValidation Loss 0.02620450 \tTraining Acuuarcy 61.320% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 1051 \tTraining Loss: 0.00801326 \tValidation Loss 0.02554553 \tTraining Acuuarcy 62.100% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1052 \tTraining Loss: 0.00802920 \tValidation Loss 0.02581675 \tTraining Acuuarcy 62.078% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1053 \tTraining Loss: 0.00806545 \tValidation Loss 0.02642710 \tTraining Acuuarcy 61.587% \tValidation Acuuarcy 16.773%\n",
      "Epoch: 1054 \tTraining Loss: 0.00803791 \tValidation Loss 0.02594898 \tTraining Acuuarcy 61.448% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1055 \tTraining Loss: 0.00800539 \tValidation Loss 0.02611618 \tTraining Acuuarcy 61.977% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1056 \tTraining Loss: 0.00797314 \tValidation Loss 0.02622242 \tTraining Acuuarcy 62.329% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1057 \tTraining Loss: 0.00807396 \tValidation Loss 0.02604493 \tTraining Acuuarcy 61.833% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 1058 \tTraining Loss: 0.00799969 \tValidation Loss 0.02651005 \tTraining Acuuarcy 61.537% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1059 \tTraining Loss: 0.00810407 \tValidation Loss 0.02532664 \tTraining Acuuarcy 61.024% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1060 \tTraining Loss: 0.00805313 \tValidation Loss 0.02606551 \tTraining Acuuarcy 61.598% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1061 \tTraining Loss: 0.00804722 \tValidation Loss 0.02618913 \tTraining Acuuarcy 61.660% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1062 \tTraining Loss: 0.00804945 \tValidation Loss 0.02604184 \tTraining Acuuarcy 61.704% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1063 \tTraining Loss: 0.00808667 \tValidation Loss 0.02617761 \tTraining Acuuarcy 62.061% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1064 \tTraining Loss: 0.00805905 \tValidation Loss 0.02616783 \tTraining Acuuarcy 62.028% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1065 \tTraining Loss: 0.00804813 \tValidation Loss 0.02646070 \tTraining Acuuarcy 61.810% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1066 \tTraining Loss: 0.00801542 \tValidation Loss 0.02663764 \tTraining Acuuarcy 62.262% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1067 \tTraining Loss: 0.00805951 \tValidation Loss 0.02576145 \tTraining Acuuarcy 61.760% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1068 \tTraining Loss: 0.00802564 \tValidation Loss 0.02608570 \tTraining Acuuarcy 61.782% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1069 \tTraining Loss: 0.00813305 \tValidation Loss 0.02578544 \tTraining Acuuarcy 61.337% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1070 \tTraining Loss: 0.00814442 \tValidation Loss 0.02611991 \tTraining Acuuarcy 61.710% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 1071 \tTraining Loss: 0.00810855 \tValidation Loss 0.02639957 \tTraining Acuuarcy 61.966% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1072 \tTraining Loss: 0.00805802 \tValidation Loss 0.02563894 \tTraining Acuuarcy 61.348% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1073 \tTraining Loss: 0.00814058 \tValidation Loss 0.02560530 \tTraining Acuuarcy 61.665% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1074 \tTraining Loss: 0.00818482 \tValidation Loss 0.02631679 \tTraining Acuuarcy 60.852% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1075 \tTraining Loss: 0.00809436 \tValidation Loss 0.02617702 \tTraining Acuuarcy 61.799% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1076 \tTraining Loss: 0.00817445 \tValidation Loss 0.02555175 \tTraining Acuuarcy 61.565% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 1077 \tTraining Loss: 0.00799106 \tValidation Loss 0.02512217 \tTraining Acuuarcy 62.200% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1078 \tTraining Loss: 0.00805077 \tValidation Loss 0.02602063 \tTraining Acuuarcy 61.911% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1079 \tTraining Loss: 0.00814359 \tValidation Loss 0.02532833 \tTraining Acuuarcy 61.470% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1080 \tTraining Loss: 0.00811025 \tValidation Loss 0.02577881 \tTraining Acuuarcy 61.426% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1081 \tTraining Loss: 0.00803427 \tValidation Loss 0.02680799 \tTraining Acuuarcy 61.359% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1082 \tTraining Loss: 0.00804696 \tValidation Loss 0.02591440 \tTraining Acuuarcy 61.610% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1083 \tTraining Loss: 0.00803462 \tValidation Loss 0.02632908 \tTraining Acuuarcy 62.139% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1084 \tTraining Loss: 0.00805150 \tValidation Loss 0.02658227 \tTraining Acuuarcy 62.072% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1085 \tTraining Loss: 0.00804134 \tValidation Loss 0.02573620 \tTraining Acuuarcy 61.643% \tValidation Acuuarcy 17.024%\n",
      "Epoch: 1086 \tTraining Loss: 0.00804645 \tValidation Loss 0.02606558 \tTraining Acuuarcy 62.139% \tValidation Acuuarcy 16.885%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1087 \tTraining Loss: 0.00798221 \tValidation Loss 0.02617876 \tTraining Acuuarcy 62.217% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1088 \tTraining Loss: 0.00795119 \tValidation Loss 0.02669723 \tTraining Acuuarcy 62.134% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1089 \tTraining Loss: 0.00795886 \tValidation Loss 0.02576365 \tTraining Acuuarcy 61.721% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1090 \tTraining Loss: 0.00825717 \tValidation Loss 0.02665779 \tTraining Acuuarcy 60.679% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1091 \tTraining Loss: 0.00804423 \tValidation Loss 0.02544270 \tTraining Acuuarcy 62.089% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1092 \tTraining Loss: 0.00816869 \tValidation Loss 0.02588664 \tTraining Acuuarcy 61.298% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 1093 \tTraining Loss: 0.00804440 \tValidation Loss 0.02561890 \tTraining Acuuarcy 61.827% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1094 \tTraining Loss: 0.00797448 \tValidation Loss 0.02610163 \tTraining Acuuarcy 62.000% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1095 \tTraining Loss: 0.00798850 \tValidation Loss 0.02609637 \tTraining Acuuarcy 62.273% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1096 \tTraining Loss: 0.00804771 \tValidation Loss 0.02593364 \tTraining Acuuarcy 62.100% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1097 \tTraining Loss: 0.00798559 \tValidation Loss 0.02556643 \tTraining Acuuarcy 62.044% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1098 \tTraining Loss: 0.00794731 \tValidation Loss 0.02607195 \tTraining Acuuarcy 61.866% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1099 \tTraining Loss: 0.00786743 \tValidation Loss 0.02673169 \tTraining Acuuarcy 62.808% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1100 \tTraining Loss: 0.00801387 \tValidation Loss 0.02566492 \tTraining Acuuarcy 61.866% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1101 \tTraining Loss: 0.00803432 \tValidation Loss 0.02549258 \tTraining Acuuarcy 62.156% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 1102 \tTraining Loss: 0.00807197 \tValidation Loss 0.02567182 \tTraining Acuuarcy 61.810% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 1103 \tTraining Loss: 0.00805493 \tValidation Loss 0.02539575 \tTraining Acuuarcy 61.821% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1104 \tTraining Loss: 0.00807157 \tValidation Loss 0.02590008 \tTraining Acuuarcy 61.403% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 1105 \tTraining Loss: 0.00802780 \tValidation Loss 0.02581014 \tTraining Acuuarcy 61.526% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1106 \tTraining Loss: 0.00807516 \tValidation Loss 0.02682330 \tTraining Acuuarcy 61.833% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1107 \tTraining Loss: 0.00797899 \tValidation Loss 0.02607583 \tTraining Acuuarcy 62.290% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1108 \tTraining Loss: 0.00808671 \tValidation Loss 0.02655683 \tTraining Acuuarcy 61.526% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1109 \tTraining Loss: 0.00797113 \tValidation Loss 0.02583462 \tTraining Acuuarcy 62.384% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1110 \tTraining Loss: 0.00794026 \tValidation Loss 0.02562323 \tTraining Acuuarcy 62.401% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1111 \tTraining Loss: 0.00799434 \tValidation Loss 0.02500039 \tTraining Acuuarcy 61.782% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1112 \tTraining Loss: 0.00801155 \tValidation Loss 0.02598351 \tTraining Acuuarcy 62.435% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 1113 \tTraining Loss: 0.00792985 \tValidation Loss 0.02623445 \tTraining Acuuarcy 62.262% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1114 \tTraining Loss: 0.00796743 \tValidation Loss 0.02615067 \tTraining Acuuarcy 62.011% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 1115 \tTraining Loss: 0.00800259 \tValidation Loss 0.02661414 \tTraining Acuuarcy 62.245% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1116 \tTraining Loss: 0.00794287 \tValidation Loss 0.02615626 \tTraining Acuuarcy 62.262% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1117 \tTraining Loss: 0.00811552 \tValidation Loss 0.02613815 \tTraining Acuuarcy 61.130% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1118 \tTraining Loss: 0.00793392 \tValidation Loss 0.02677061 \tTraining Acuuarcy 62.373% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1119 \tTraining Loss: 0.00797876 \tValidation Loss 0.02637386 \tTraining Acuuarcy 61.955% \tValidation Acuuarcy 16.690%\n",
      "Epoch: 1120 \tTraining Loss: 0.00794518 \tValidation Loss 0.02629252 \tTraining Acuuarcy 62.368% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1121 \tTraining Loss: 0.00800634 \tValidation Loss 0.02611984 \tTraining Acuuarcy 61.821% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1122 \tTraining Loss: 0.00790051 \tValidation Loss 0.02658986 \tTraining Acuuarcy 62.814% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1123 \tTraining Loss: 0.00804332 \tValidation Loss 0.02646724 \tTraining Acuuarcy 61.905% \tValidation Acuuarcy 16.941%\n",
      "Epoch: 1124 \tTraining Loss: 0.00807839 \tValidation Loss 0.02639164 \tTraining Acuuarcy 61.376% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1125 \tTraining Loss: 0.00812789 \tValidation Loss 0.02593818 \tTraining Acuuarcy 61.264% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1126 \tTraining Loss: 0.00803836 \tValidation Loss 0.02698747 \tTraining Acuuarcy 61.933% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1127 \tTraining Loss: 0.00797333 \tValidation Loss 0.02629121 \tTraining Acuuarcy 61.983% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1128 \tTraining Loss: 0.00813499 \tValidation Loss 0.02580986 \tTraining Acuuarcy 61.849% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1129 \tTraining Loss: 0.00803492 \tValidation Loss 0.02551353 \tTraining Acuuarcy 62.178% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1130 \tTraining Loss: 0.00797453 \tValidation Loss 0.02639066 \tTraining Acuuarcy 61.933% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1131 \tTraining Loss: 0.00796016 \tValidation Loss 0.02660018 \tTraining Acuuarcy 61.961% \tValidation Acuuarcy 16.885%\n",
      "Epoch: 1132 \tTraining Loss: 0.00812493 \tValidation Loss 0.02646035 \tTraining Acuuarcy 61.710% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 1133 \tTraining Loss: 0.00806758 \tValidation Loss 0.02557738 \tTraining Acuuarcy 61.498% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1134 \tTraining Loss: 0.00803328 \tValidation Loss 0.02572916 \tTraining Acuuarcy 62.016% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1135 \tTraining Loss: 0.00794483 \tValidation Loss 0.02622247 \tTraining Acuuarcy 62.072% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1136 \tTraining Loss: 0.00801570 \tValidation Loss 0.02647724 \tTraining Acuuarcy 62.016% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1137 \tTraining Loss: 0.00797732 \tValidation Loss 0.02608879 \tTraining Acuuarcy 61.994% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1138 \tTraining Loss: 0.00802172 \tValidation Loss 0.02616703 \tTraining Acuuarcy 61.760% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1139 \tTraining Loss: 0.00795290 \tValidation Loss 0.02685412 \tTraining Acuuarcy 62.284% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1140 \tTraining Loss: 0.00803318 \tValidation Loss 0.02787104 \tTraining Acuuarcy 61.860% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1141 \tTraining Loss: 0.00804909 \tValidation Loss 0.02607757 \tTraining Acuuarcy 61.877% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1142 \tTraining Loss: 0.00807452 \tValidation Loss 0.02556127 \tTraining Acuuarcy 61.593% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1143 \tTraining Loss: 0.00807858 \tValidation Loss 0.02583906 \tTraining Acuuarcy 61.643% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1144 \tTraining Loss: 0.00806911 \tValidation Loss 0.02599592 \tTraining Acuuarcy 61.069% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1145 \tTraining Loss: 0.00799926 \tValidation Loss 0.02698316 \tTraining Acuuarcy 62.446% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1146 \tTraining Loss: 0.00802050 \tValidation Loss 0.02630876 \tTraining Acuuarcy 61.821% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1147 \tTraining Loss: 0.00799557 \tValidation Loss 0.02669008 \tTraining Acuuarcy 62.379% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1148 \tTraining Loss: 0.00793998 \tValidation Loss 0.02660775 \tTraining Acuuarcy 62.446% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1149 \tTraining Loss: 0.00793526 \tValidation Loss 0.02599254 \tTraining Acuuarcy 62.262% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1150 \tTraining Loss: 0.00814174 \tValidation Loss 0.02715844 \tTraining Acuuarcy 61.231% \tValidation Acuuarcy 16.885%\n",
      "Epoch: 1151 \tTraining Loss: 0.00805838 \tValidation Loss 0.02610727 \tTraining Acuuarcy 61.760% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1152 \tTraining Loss: 0.00802039 \tValidation Loss 0.02625625 \tTraining Acuuarcy 62.111% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1153 \tTraining Loss: 0.00799992 \tValidation Loss 0.02661037 \tTraining Acuuarcy 62.529% \tValidation Acuuarcy 17.749%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1154 \tTraining Loss: 0.00801965 \tValidation Loss 0.02626770 \tTraining Acuuarcy 62.005% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1155 \tTraining Loss: 0.00793190 \tValidation Loss 0.02639089 \tTraining Acuuarcy 62.529% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1156 \tTraining Loss: 0.00802153 \tValidation Loss 0.02670307 \tTraining Acuuarcy 61.777% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1157 \tTraining Loss: 0.00809961 \tValidation Loss 0.02569510 \tTraining Acuuarcy 61.604% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1158 \tTraining Loss: 0.00802707 \tValidation Loss 0.02597809 \tTraining Acuuarcy 61.877% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1159 \tTraining Loss: 0.00798401 \tValidation Loss 0.02621204 \tTraining Acuuarcy 62.156% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1160 \tTraining Loss: 0.00806735 \tValidation Loss 0.02556993 \tTraining Acuuarcy 62.306% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1161 \tTraining Loss: 0.00808123 \tValidation Loss 0.02594939 \tTraining Acuuarcy 61.810% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1162 \tTraining Loss: 0.00785150 \tValidation Loss 0.02657560 \tTraining Acuuarcy 62.646% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1163 \tTraining Loss: 0.00794237 \tValidation Loss 0.02596058 \tTraining Acuuarcy 62.596% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 1164 \tTraining Loss: 0.00789245 \tValidation Loss 0.02607897 \tTraining Acuuarcy 62.602% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 1165 \tTraining Loss: 0.00788321 \tValidation Loss 0.02646969 \tTraining Acuuarcy 63.053% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1166 \tTraining Loss: 0.00797506 \tValidation Loss 0.02650528 \tTraining Acuuarcy 62.440% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1167 \tTraining Loss: 0.00791322 \tValidation Loss 0.02719649 \tTraining Acuuarcy 62.384% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1168 \tTraining Loss: 0.00795073 \tValidation Loss 0.02562248 \tTraining Acuuarcy 62.574% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1169 \tTraining Loss: 0.00790567 \tValidation Loss 0.02608966 \tTraining Acuuarcy 62.273% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1170 \tTraining Loss: 0.00796526 \tValidation Loss 0.02719482 \tTraining Acuuarcy 62.217% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1171 \tTraining Loss: 0.00790926 \tValidation Loss 0.02620010 \tTraining Acuuarcy 62.446% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1172 \tTraining Loss: 0.00792908 \tValidation Loss 0.02663670 \tTraining Acuuarcy 62.395% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1173 \tTraining Loss: 0.00796219 \tValidation Loss 0.02615753 \tTraining Acuuarcy 61.905% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1174 \tTraining Loss: 0.00798493 \tValidation Loss 0.02604649 \tTraining Acuuarcy 61.911% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1175 \tTraining Loss: 0.00802574 \tValidation Loss 0.02594487 \tTraining Acuuarcy 61.961% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1176 \tTraining Loss: 0.00781920 \tValidation Loss 0.02688333 \tTraining Acuuarcy 62.875% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1177 \tTraining Loss: 0.00786501 \tValidation Loss 0.02751286 \tTraining Acuuarcy 62.802% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 1178 \tTraining Loss: 0.00796594 \tValidation Loss 0.02711991 \tTraining Acuuarcy 61.938% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1179 \tTraining Loss: 0.00799994 \tValidation Loss 0.02636237 \tTraining Acuuarcy 62.078% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1180 \tTraining Loss: 0.00802518 \tValidation Loss 0.02660682 \tTraining Acuuarcy 61.827% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1181 \tTraining Loss: 0.00806624 \tValidation Loss 0.02629664 \tTraining Acuuarcy 62.005% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1182 \tTraining Loss: 0.00794673 \tValidation Loss 0.02637436 \tTraining Acuuarcy 62.083% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1183 \tTraining Loss: 0.00796678 \tValidation Loss 0.02564878 \tTraining Acuuarcy 62.290% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 1184 \tTraining Loss: 0.00790636 \tValidation Loss 0.02642035 \tTraining Acuuarcy 62.741% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1185 \tTraining Loss: 0.00797540 \tValidation Loss 0.02593674 \tTraining Acuuarcy 62.200% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1186 \tTraining Loss: 0.00795533 \tValidation Loss 0.02610648 \tTraining Acuuarcy 62.122% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1187 \tTraining Loss: 0.00792065 \tValidation Loss 0.02678099 \tTraining Acuuarcy 62.708% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1188 \tTraining Loss: 0.00804293 \tValidation Loss 0.02575171 \tTraining Acuuarcy 61.788% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1189 \tTraining Loss: 0.00797360 \tValidation Loss 0.02654248 \tTraining Acuuarcy 62.122% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1190 \tTraining Loss: 0.00792311 \tValidation Loss 0.02599116 \tTraining Acuuarcy 62.295% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1191 \tTraining Loss: 0.00791572 \tValidation Loss 0.02707927 \tTraining Acuuarcy 62.134% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1192 \tTraining Loss: 0.00788241 \tValidation Loss 0.02686339 \tTraining Acuuarcy 62.574% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 1193 \tTraining Loss: 0.00788605 \tValidation Loss 0.02642013 \tTraining Acuuarcy 62.925% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 1194 \tTraining Loss: 0.00793583 \tValidation Loss 0.02635943 \tTraining Acuuarcy 62.440% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1195 \tTraining Loss: 0.00798191 \tValidation Loss 0.02726780 \tTraining Acuuarcy 62.379% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1196 \tTraining Loss: 0.00801158 \tValidation Loss 0.02666173 \tTraining Acuuarcy 61.883% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1197 \tTraining Loss: 0.00804973 \tValidation Loss 0.02623333 \tTraining Acuuarcy 61.849% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1198 \tTraining Loss: 0.00798930 \tValidation Loss 0.02616237 \tTraining Acuuarcy 61.989% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1199 \tTraining Loss: 0.00804108 \tValidation Loss 0.02587928 \tTraining Acuuarcy 61.977% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1200 \tTraining Loss: 0.00797501 \tValidation Loss 0.02702804 \tTraining Acuuarcy 61.838% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 1201 \tTraining Loss: 0.00783411 \tValidation Loss 0.02731107 \tTraining Acuuarcy 62.964% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1202 \tTraining Loss: 0.00798719 \tValidation Loss 0.02664945 \tTraining Acuuarcy 62.228% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1203 \tTraining Loss: 0.00793912 \tValidation Loss 0.02599709 \tTraining Acuuarcy 62.685% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1204 \tTraining Loss: 0.00789166 \tValidation Loss 0.02656421 \tTraining Acuuarcy 62.674% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1205 \tTraining Loss: 0.00792550 \tValidation Loss 0.02647629 \tTraining Acuuarcy 62.691% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1206 \tTraining Loss: 0.00802607 \tValidation Loss 0.02663052 \tTraining Acuuarcy 62.011% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1207 \tTraining Loss: 0.00803575 \tValidation Loss 0.02682680 \tTraining Acuuarcy 61.855% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1208 \tTraining Loss: 0.00802061 \tValidation Loss 0.02651707 \tTraining Acuuarcy 61.738% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 1209 \tTraining Loss: 0.00800467 \tValidation Loss 0.02585824 \tTraining Acuuarcy 62.117% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1210 \tTraining Loss: 0.00798795 \tValidation Loss 0.02678248 \tTraining Acuuarcy 62.591% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1211 \tTraining Loss: 0.00792977 \tValidation Loss 0.02597377 \tTraining Acuuarcy 62.574% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1212 \tTraining Loss: 0.00794394 \tValidation Loss 0.02629762 \tTraining Acuuarcy 62.234% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1213 \tTraining Loss: 0.00796964 \tValidation Loss 0.02626612 \tTraining Acuuarcy 62.245% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1214 \tTraining Loss: 0.00795266 \tValidation Loss 0.02628605 \tTraining Acuuarcy 62.596% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1215 \tTraining Loss: 0.00788737 \tValidation Loss 0.02671001 \tTraining Acuuarcy 62.000% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 1216 \tTraining Loss: 0.00785844 \tValidation Loss 0.02615529 \tTraining Acuuarcy 62.685% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1217 \tTraining Loss: 0.00793366 \tValidation Loss 0.02686712 \tTraining Acuuarcy 62.306% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1218 \tTraining Loss: 0.00785422 \tValidation Loss 0.02658260 \tTraining Acuuarcy 62.368% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1219 \tTraining Loss: 0.00791066 \tValidation Loss 0.02687503 \tTraining Acuuarcy 62.485% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 1220 \tTraining Loss: 0.00791497 \tValidation Loss 0.02603364 \tTraining Acuuarcy 62.875% \tValidation Acuuarcy 17.944%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1221 \tTraining Loss: 0.00797253 \tValidation Loss 0.02623710 \tTraining Acuuarcy 62.223% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1222 \tTraining Loss: 0.00800378 \tValidation Loss 0.02573677 \tTraining Acuuarcy 61.727% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1223 \tTraining Loss: 0.00784073 \tValidation Loss 0.02702135 \tTraining Acuuarcy 62.992% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1224 \tTraining Loss: 0.00791628 \tValidation Loss 0.02581484 \tTraining Acuuarcy 62.501% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1225 \tTraining Loss: 0.00793601 \tValidation Loss 0.02638950 \tTraining Acuuarcy 63.059% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 1226 \tTraining Loss: 0.00794645 \tValidation Loss 0.02675976 \tTraining Acuuarcy 62.485% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1227 \tTraining Loss: 0.00799161 \tValidation Loss 0.02599748 \tTraining Acuuarcy 62.239% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1228 \tTraining Loss: 0.00794558 \tValidation Loss 0.02593798 \tTraining Acuuarcy 62.864% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 1229 \tTraining Loss: 0.00791935 \tValidation Loss 0.02639961 \tTraining Acuuarcy 62.696% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 1230 \tTraining Loss: 0.00795546 \tValidation Loss 0.02595396 \tTraining Acuuarcy 62.234% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1231 \tTraining Loss: 0.00805283 \tValidation Loss 0.02614067 \tTraining Acuuarcy 62.145% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1232 \tTraining Loss: 0.00783909 \tValidation Loss 0.02743764 \tTraining Acuuarcy 63.031% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1233 \tTraining Loss: 0.00776577 \tValidation Loss 0.02719909 \tTraining Acuuarcy 63.248% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1234 \tTraining Loss: 0.00787493 \tValidation Loss 0.02585161 \tTraining Acuuarcy 62.635% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1235 \tTraining Loss: 0.00793417 \tValidation Loss 0.02625477 \tTraining Acuuarcy 62.368% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1236 \tTraining Loss: 0.00786025 \tValidation Loss 0.02634073 \tTraining Acuuarcy 62.869% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1237 \tTraining Loss: 0.00797906 \tValidation Loss 0.02762864 \tTraining Acuuarcy 62.390% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1238 \tTraining Loss: 0.00792780 \tValidation Loss 0.02627271 \tTraining Acuuarcy 62.200% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 1239 \tTraining Loss: 0.00799162 \tValidation Loss 0.02579460 \tTraining Acuuarcy 62.005% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 1240 \tTraining Loss: 0.00788492 \tValidation Loss 0.02644205 \tTraining Acuuarcy 62.758% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1241 \tTraining Loss: 0.00794009 \tValidation Loss 0.02570655 \tTraining Acuuarcy 62.496% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1242 \tTraining Loss: 0.00781899 \tValidation Loss 0.02605772 \tTraining Acuuarcy 62.646% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1243 \tTraining Loss: 0.00792441 \tValidation Loss 0.02599510 \tTraining Acuuarcy 62.440% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1244 \tTraining Loss: 0.00797471 \tValidation Loss 0.02642504 \tTraining Acuuarcy 62.050% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1245 \tTraining Loss: 0.00791941 \tValidation Loss 0.02648459 \tTraining Acuuarcy 62.802% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1246 \tTraining Loss: 0.00797001 \tValidation Loss 0.02554831 \tTraining Acuuarcy 62.100% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1247 \tTraining Loss: 0.00786201 \tValidation Loss 0.02775081 \tTraining Acuuarcy 62.585% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1248 \tTraining Loss: 0.00792073 \tValidation Loss 0.02640709 \tTraining Acuuarcy 62.635% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1249 \tTraining Loss: 0.00787786 \tValidation Loss 0.02676093 \tTraining Acuuarcy 62.602% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1250 \tTraining Loss: 0.00791124 \tValidation Loss 0.02698819 \tTraining Acuuarcy 62.228% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1251 \tTraining Loss: 0.00787245 \tValidation Loss 0.02605156 \tTraining Acuuarcy 62.769% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1252 \tTraining Loss: 0.00800048 \tValidation Loss 0.02620815 \tTraining Acuuarcy 62.507% \tValidation Acuuarcy 16.021%\n",
      "Epoch: 1253 \tTraining Loss: 0.00796959 \tValidation Loss 0.02591483 \tTraining Acuuarcy 62.044% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1254 \tTraining Loss: 0.00791081 \tValidation Loss 0.02586620 \tTraining Acuuarcy 62.769% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1255 \tTraining Loss: 0.00792902 \tValidation Loss 0.02724063 \tTraining Acuuarcy 62.808% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1256 \tTraining Loss: 0.00785750 \tValidation Loss 0.02665030 \tTraining Acuuarcy 62.780% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1257 \tTraining Loss: 0.00784282 \tValidation Loss 0.02655309 \tTraining Acuuarcy 62.797% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1258 \tTraining Loss: 0.00782963 \tValidation Loss 0.02632296 \tTraining Acuuarcy 62.886% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1259 \tTraining Loss: 0.00780863 \tValidation Loss 0.02681044 \tTraining Acuuarcy 62.691% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1260 \tTraining Loss: 0.00782847 \tValidation Loss 0.02623379 \tTraining Acuuarcy 62.641% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1261 \tTraining Loss: 0.00776663 \tValidation Loss 0.02607236 \tTraining Acuuarcy 63.388% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1262 \tTraining Loss: 0.00776406 \tValidation Loss 0.02721367 \tTraining Acuuarcy 62.791% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1263 \tTraining Loss: 0.00788546 \tValidation Loss 0.02682743 \tTraining Acuuarcy 62.808% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1264 \tTraining Loss: 0.00792222 \tValidation Loss 0.02697772 \tTraining Acuuarcy 62.200% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 1265 \tTraining Loss: 0.00771244 \tValidation Loss 0.02730441 \tTraining Acuuarcy 63.544% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1266 \tTraining Loss: 0.00780232 \tValidation Loss 0.02651765 \tTraining Acuuarcy 63.293% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1267 \tTraining Loss: 0.00791181 \tValidation Loss 0.02662375 \tTraining Acuuarcy 62.886% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 1268 \tTraining Loss: 0.00776092 \tValidation Loss 0.02729032 \tTraining Acuuarcy 63.276% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 1269 \tTraining Loss: 0.00784194 \tValidation Loss 0.02706635 \tTraining Acuuarcy 63.098% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1270 \tTraining Loss: 0.00787644 \tValidation Loss 0.02648035 \tTraining Acuuarcy 62.741% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1271 \tTraining Loss: 0.00788202 \tValidation Loss 0.02717891 \tTraining Acuuarcy 62.663% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 1272 \tTraining Loss: 0.00787529 \tValidation Loss 0.02626383 \tTraining Acuuarcy 62.423% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1273 \tTraining Loss: 0.00807495 \tValidation Loss 0.02651712 \tTraining Acuuarcy 61.872% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1274 \tTraining Loss: 0.00799985 \tValidation Loss 0.02663681 \tTraining Acuuarcy 62.200% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1275 \tTraining Loss: 0.00791611 \tValidation Loss 0.02679633 \tTraining Acuuarcy 62.607% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1276 \tTraining Loss: 0.00787122 \tValidation Loss 0.02687658 \tTraining Acuuarcy 63.153% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1277 \tTraining Loss: 0.00781583 \tValidation Loss 0.02686669 \tTraining Acuuarcy 62.892% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1278 \tTraining Loss: 0.00797967 \tValidation Loss 0.02567832 \tTraining Acuuarcy 62.128% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1279 \tTraining Loss: 0.00793608 \tValidation Loss 0.02661633 \tTraining Acuuarcy 62.429% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1280 \tTraining Loss: 0.00782950 \tValidation Loss 0.02712497 \tTraining Acuuarcy 63.343% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1281 \tTraining Loss: 0.00796935 \tValidation Loss 0.02650788 \tTraining Acuuarcy 62.234% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1282 \tTraining Loss: 0.00788294 \tValidation Loss 0.02685478 \tTraining Acuuarcy 62.496% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1283 \tTraining Loss: 0.00819713 \tValidation Loss 0.02569284 \tTraining Acuuarcy 60.974% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 1284 \tTraining Loss: 0.00774170 \tValidation Loss 0.02615510 \tTraining Acuuarcy 63.415% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1285 \tTraining Loss: 0.00792868 \tValidation Loss 0.02665403 \tTraining Acuuarcy 62.596% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1286 \tTraining Loss: 0.00793298 \tValidation Loss 0.02606866 \tTraining Acuuarcy 62.524% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1287 \tTraining Loss: 0.00792181 \tValidation Loss 0.02698475 \tTraining Acuuarcy 62.674% \tValidation Acuuarcy 16.523%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1288 \tTraining Loss: 0.00783714 \tValidation Loss 0.02664959 \tTraining Acuuarcy 62.942% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1289 \tTraining Loss: 0.00777365 \tValidation Loss 0.02713761 \tTraining Acuuarcy 63.064% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1290 \tTraining Loss: 0.00782031 \tValidation Loss 0.02644011 \tTraining Acuuarcy 62.908% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1291 \tTraining Loss: 0.00784396 \tValidation Loss 0.02654319 \tTraining Acuuarcy 63.148% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1292 \tTraining Loss: 0.00786317 \tValidation Loss 0.02810283 \tTraining Acuuarcy 62.942% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1293 \tTraining Loss: 0.00787855 \tValidation Loss 0.02746702 \tTraining Acuuarcy 62.624% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1294 \tTraining Loss: 0.00791539 \tValidation Loss 0.02614407 \tTraining Acuuarcy 62.457% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1295 \tTraining Loss: 0.00778531 \tValidation Loss 0.02633685 \tTraining Acuuarcy 63.053% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1296 \tTraining Loss: 0.00791613 \tValidation Loss 0.02692165 \tTraining Acuuarcy 62.407% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1297 \tTraining Loss: 0.00804723 \tValidation Loss 0.02691169 \tTraining Acuuarcy 61.849% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1298 \tTraining Loss: 0.00789022 \tValidation Loss 0.02638531 \tTraining Acuuarcy 62.329% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1299 \tTraining Loss: 0.00791081 \tValidation Loss 0.02675648 \tTraining Acuuarcy 62.992% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1300 \tTraining Loss: 0.00791982 \tValidation Loss 0.02679123 \tTraining Acuuarcy 62.596% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1301 \tTraining Loss: 0.00787558 \tValidation Loss 0.02668633 \tTraining Acuuarcy 62.585% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1302 \tTraining Loss: 0.00791429 \tValidation Loss 0.02652369 \tTraining Acuuarcy 62.552% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1303 \tTraining Loss: 0.00791223 \tValidation Loss 0.02630630 \tTraining Acuuarcy 62.496% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1304 \tTraining Loss: 0.00774736 \tValidation Loss 0.02628120 \tTraining Acuuarcy 62.908% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1305 \tTraining Loss: 0.00785211 \tValidation Loss 0.02736403 \tTraining Acuuarcy 62.529% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1306 \tTraining Loss: 0.00785408 \tValidation Loss 0.02596498 \tTraining Acuuarcy 62.674% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1307 \tTraining Loss: 0.00778973 \tValidation Loss 0.02792199 \tTraining Acuuarcy 63.075% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1308 \tTraining Loss: 0.00785981 \tValidation Loss 0.02694224 \tTraining Acuuarcy 62.886% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1309 \tTraining Loss: 0.00787451 \tValidation Loss 0.02642300 \tTraining Acuuarcy 62.635% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1310 \tTraining Loss: 0.00794887 \tValidation Loss 0.02650183 \tTraining Acuuarcy 62.708% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1311 \tTraining Loss: 0.00789009 \tValidation Loss 0.02651626 \tTraining Acuuarcy 62.892% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1312 \tTraining Loss: 0.00787613 \tValidation Loss 0.02729889 \tTraining Acuuarcy 62.802% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1313 \tTraining Loss: 0.00783165 \tValidation Loss 0.02671523 \tTraining Acuuarcy 63.103% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1314 \tTraining Loss: 0.00781625 \tValidation Loss 0.02739722 \tTraining Acuuarcy 63.209% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1315 \tTraining Loss: 0.00784453 \tValidation Loss 0.02565765 \tTraining Acuuarcy 63.276% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1316 \tTraining Loss: 0.00777450 \tValidation Loss 0.02654090 \tTraining Acuuarcy 62.925% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1317 \tTraining Loss: 0.00780660 \tValidation Loss 0.02654317 \tTraining Acuuarcy 63.209% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1318 \tTraining Loss: 0.00782287 \tValidation Loss 0.02719621 \tTraining Acuuarcy 62.836% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1319 \tTraining Loss: 0.00772849 \tValidation Loss 0.02643685 \tTraining Acuuarcy 63.689% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1320 \tTraining Loss: 0.00789549 \tValidation Loss 0.02682911 \tTraining Acuuarcy 62.669% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1321 \tTraining Loss: 0.00784900 \tValidation Loss 0.02726867 \tTraining Acuuarcy 62.936% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 1322 \tTraining Loss: 0.00799448 \tValidation Loss 0.02712697 \tTraining Acuuarcy 62.173% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1323 \tTraining Loss: 0.00789230 \tValidation Loss 0.02636773 \tTraining Acuuarcy 62.652% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1324 \tTraining Loss: 0.00780756 \tValidation Loss 0.02638263 \tTraining Acuuarcy 63.304% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 1325 \tTraining Loss: 0.00791293 \tValidation Loss 0.02693777 \tTraining Acuuarcy 62.440% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1326 \tTraining Loss: 0.00795841 \tValidation Loss 0.02658660 \tTraining Acuuarcy 62.273% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1327 \tTraining Loss: 0.00790685 \tValidation Loss 0.02629069 \tTraining Acuuarcy 62.763% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1328 \tTraining Loss: 0.00778305 \tValidation Loss 0.02658729 \tTraining Acuuarcy 63.382% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1329 \tTraining Loss: 0.00781432 \tValidation Loss 0.02686226 \tTraining Acuuarcy 62.657% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1330 \tTraining Loss: 0.00792573 \tValidation Loss 0.02763303 \tTraining Acuuarcy 62.440% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1331 \tTraining Loss: 0.00775812 \tValidation Loss 0.02736929 \tTraining Acuuarcy 63.243% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1332 \tTraining Loss: 0.00785698 \tValidation Loss 0.02599028 \tTraining Acuuarcy 62.708% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1333 \tTraining Loss: 0.00793745 \tValidation Loss 0.02589285 \tTraining Acuuarcy 62.474% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1334 \tTraining Loss: 0.00778810 \tValidation Loss 0.02657350 \tTraining Acuuarcy 63.014% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1335 \tTraining Loss: 0.00778753 \tValidation Loss 0.02638871 \tTraining Acuuarcy 62.730% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1336 \tTraining Loss: 0.00800194 \tValidation Loss 0.02617833 \tTraining Acuuarcy 62.212% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1337 \tTraining Loss: 0.00786458 \tValidation Loss 0.02633574 \tTraining Acuuarcy 62.646% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1338 \tTraining Loss: 0.00779544 \tValidation Loss 0.02727977 \tTraining Acuuarcy 63.298% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1339 \tTraining Loss: 0.00775603 \tValidation Loss 0.02661924 \tTraining Acuuarcy 63.555% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1340 \tTraining Loss: 0.00773009 \tValidation Loss 0.02734595 \tTraining Acuuarcy 63.683% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1341 \tTraining Loss: 0.00780065 \tValidation Loss 0.02622790 \tTraining Acuuarcy 63.064% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1342 \tTraining Loss: 0.00767002 \tValidation Loss 0.02678065 \tTraining Acuuarcy 64.391% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1343 \tTraining Loss: 0.00790615 \tValidation Loss 0.02780684 \tTraining Acuuarcy 62.317% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 1344 \tTraining Loss: 0.00775220 \tValidation Loss 0.02697832 \tTraining Acuuarcy 63.209% \tValidation Acuuarcy 17.024%\n",
      "Epoch: 1345 \tTraining Loss: 0.00782680 \tValidation Loss 0.02678231 \tTraining Acuuarcy 63.276% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1346 \tTraining Loss: 0.00785300 \tValidation Loss 0.02639865 \tTraining Acuuarcy 63.036% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1347 \tTraining Loss: 0.00785796 \tValidation Loss 0.02665024 \tTraining Acuuarcy 62.880% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1348 \tTraining Loss: 0.00795519 \tValidation Loss 0.02737883 \tTraining Acuuarcy 62.769% \tValidation Acuuarcy 16.885%\n",
      "Epoch: 1349 \tTraining Loss: 0.00779410 \tValidation Loss 0.02672010 \tTraining Acuuarcy 63.427% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1350 \tTraining Loss: 0.00780484 \tValidation Loss 0.02657976 \tTraining Acuuarcy 63.276% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1351 \tTraining Loss: 0.00778461 \tValidation Loss 0.02687613 \tTraining Acuuarcy 63.142% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1352 \tTraining Loss: 0.00780175 \tValidation Loss 0.02652385 \tTraining Acuuarcy 63.103% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1353 \tTraining Loss: 0.00779609 \tValidation Loss 0.02623827 \tTraining Acuuarcy 63.404% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1354 \tTraining Loss: 0.00784760 \tValidation Loss 0.02642679 \tTraining Acuuarcy 62.830% \tValidation Acuuarcy 18.111%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1355 \tTraining Loss: 0.00787039 \tValidation Loss 0.02551857 \tTraining Acuuarcy 62.886% \tValidation Acuuarcy 16.885%\n",
      "Epoch: 1356 \tTraining Loss: 0.00797453 \tValidation Loss 0.02633630 \tTraining Acuuarcy 62.479% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1357 \tTraining Loss: 0.00786262 \tValidation Loss 0.02700737 \tTraining Acuuarcy 62.317% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1358 \tTraining Loss: 0.00776331 \tValidation Loss 0.02653872 \tTraining Acuuarcy 62.897% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1359 \tTraining Loss: 0.00787913 \tValidation Loss 0.02675033 \tTraining Acuuarcy 62.591% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1360 \tTraining Loss: 0.00794311 \tValidation Loss 0.02626682 \tTraining Acuuarcy 62.596% \tValidation Acuuarcy 16.662%\n",
      "Epoch: 1361 \tTraining Loss: 0.00787595 \tValidation Loss 0.02719005 \tTraining Acuuarcy 62.986% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1362 \tTraining Loss: 0.00782107 \tValidation Loss 0.02710493 \tTraining Acuuarcy 63.622% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1363 \tTraining Loss: 0.00787750 \tValidation Loss 0.02685537 \tTraining Acuuarcy 62.696% \tValidation Acuuarcy 16.801%\n",
      "Epoch: 1364 \tTraining Loss: 0.00788127 \tValidation Loss 0.02757031 \tTraining Acuuarcy 62.719% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 1365 \tTraining Loss: 0.00767150 \tValidation Loss 0.02677609 \tTraining Acuuarcy 63.449% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 1366 \tTraining Loss: 0.00775269 \tValidation Loss 0.02684051 \tTraining Acuuarcy 63.137% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1367 \tTraining Loss: 0.00780188 \tValidation Loss 0.02740554 \tTraining Acuuarcy 63.304% \tValidation Acuuarcy 16.746%\n",
      "Epoch: 1368 \tTraining Loss: 0.00788077 \tValidation Loss 0.02701123 \tTraining Acuuarcy 62.239% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1369 \tTraining Loss: 0.00781512 \tValidation Loss 0.02670441 \tTraining Acuuarcy 63.059% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1370 \tTraining Loss: 0.00786311 \tValidation Loss 0.02608386 \tTraining Acuuarcy 62.786% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 1371 \tTraining Loss: 0.00783237 \tValidation Loss 0.02711968 \tTraining Acuuarcy 63.438% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1372 \tTraining Loss: 0.00775075 \tValidation Loss 0.02675676 \tTraining Acuuarcy 63.343% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 1373 \tTraining Loss: 0.00787354 \tValidation Loss 0.02708681 \tTraining Acuuarcy 62.869% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1374 \tTraining Loss: 0.00777427 \tValidation Loss 0.02751590 \tTraining Acuuarcy 63.204% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1375 \tTraining Loss: 0.00777082 \tValidation Loss 0.02655143 \tTraining Acuuarcy 63.048% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1376 \tTraining Loss: 0.00771389 \tValidation Loss 0.02629615 \tTraining Acuuarcy 63.588% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1377 \tTraining Loss: 0.00766621 \tValidation Loss 0.02731675 \tTraining Acuuarcy 63.343% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1378 \tTraining Loss: 0.00773589 \tValidation Loss 0.02731553 \tTraining Acuuarcy 63.427% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1379 \tTraining Loss: 0.00790052 \tValidation Loss 0.02660816 \tTraining Acuuarcy 62.557% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 1380 \tTraining Loss: 0.00783729 \tValidation Loss 0.02662181 \tTraining Acuuarcy 62.702% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1381 \tTraining Loss: 0.00775261 \tValidation Loss 0.02701050 \tTraining Acuuarcy 63.544% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1382 \tTraining Loss: 0.00782174 \tValidation Loss 0.02635149 \tTraining Acuuarcy 62.947% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1383 \tTraining Loss: 0.00773610 \tValidation Loss 0.02762455 \tTraining Acuuarcy 63.449% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1384 \tTraining Loss: 0.00779649 \tValidation Loss 0.02686961 \tTraining Acuuarcy 63.672% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1385 \tTraining Loss: 0.00783526 \tValidation Loss 0.02722220 \tTraining Acuuarcy 62.685% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1386 \tTraining Loss: 0.00784739 \tValidation Loss 0.02685736 \tTraining Acuuarcy 63.232% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 1387 \tTraining Loss: 0.00774929 \tValidation Loss 0.02640251 \tTraining Acuuarcy 63.287% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1388 \tTraining Loss: 0.00783708 \tValidation Loss 0.02615107 \tTraining Acuuarcy 63.075% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1389 \tTraining Loss: 0.00774430 \tValidation Loss 0.02696074 \tTraining Acuuarcy 63.521% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1390 \tTraining Loss: 0.00786786 \tValidation Loss 0.02656924 \tTraining Acuuarcy 62.674% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1391 \tTraining Loss: 0.00783467 \tValidation Loss 0.02583690 \tTraining Acuuarcy 62.847% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1392 \tTraining Loss: 0.00782972 \tValidation Loss 0.02720069 \tTraining Acuuarcy 63.031% \tValidation Acuuarcy 16.829%\n",
      "Epoch: 1393 \tTraining Loss: 0.00798578 \tValidation Loss 0.02692279 \tTraining Acuuarcy 62.173% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 1394 \tTraining Loss: 0.00776186 \tValidation Loss 0.02657672 \tTraining Acuuarcy 63.544% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1395 \tTraining Loss: 0.00775970 \tValidation Loss 0.02595823 \tTraining Acuuarcy 63.131% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1396 \tTraining Loss: 0.00785365 \tValidation Loss 0.02658755 \tTraining Acuuarcy 62.875% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1397 \tTraining Loss: 0.00780409 \tValidation Loss 0.02707553 \tTraining Acuuarcy 63.209% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 1398 \tTraining Loss: 0.00793780 \tValidation Loss 0.02611182 \tTraining Acuuarcy 62.786% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1399 \tTraining Loss: 0.00785416 \tValidation Loss 0.02678879 \tTraining Acuuarcy 62.830% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1400 \tTraining Loss: 0.00782399 \tValidation Loss 0.02624437 \tTraining Acuuarcy 63.271% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1401 \tTraining Loss: 0.00785094 \tValidation Loss 0.02685796 \tTraining Acuuarcy 62.970% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1402 \tTraining Loss: 0.00769903 \tValidation Loss 0.02649246 \tTraining Acuuarcy 63.806% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 1403 \tTraining Loss: 0.00777033 \tValidation Loss 0.02707689 \tTraining Acuuarcy 63.070% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1404 \tTraining Loss: 0.00773060 \tValidation Loss 0.02714708 \tTraining Acuuarcy 63.443% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1405 \tTraining Loss: 0.00786142 \tValidation Loss 0.02806447 \tTraining Acuuarcy 62.892% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1406 \tTraining Loss: 0.00775634 \tValidation Loss 0.02689616 \tTraining Acuuarcy 63.599% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1407 \tTraining Loss: 0.00783669 \tValidation Loss 0.02690532 \tTraining Acuuarcy 63.293% \tValidation Acuuarcy 16.829%\n",
      "Epoch: 1408 \tTraining Loss: 0.00779156 \tValidation Loss 0.02638296 \tTraining Acuuarcy 62.964% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1409 \tTraining Loss: 0.00772409 \tValidation Loss 0.02735180 \tTraining Acuuarcy 63.583% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1410 \tTraining Loss: 0.00787736 \tValidation Loss 0.02676112 \tTraining Acuuarcy 62.485% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1411 \tTraining Loss: 0.00786799 \tValidation Loss 0.02680719 \tTraining Acuuarcy 62.780% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1412 \tTraining Loss: 0.00783198 \tValidation Loss 0.02689456 \tTraining Acuuarcy 62.763% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1413 \tTraining Loss: 0.00767588 \tValidation Loss 0.02655515 \tTraining Acuuarcy 63.599% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1414 \tTraining Loss: 0.00782796 \tValidation Loss 0.02622037 \tTraining Acuuarcy 63.131% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1415 \tTraining Loss: 0.00774354 \tValidation Loss 0.02729311 \tTraining Acuuarcy 63.075% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 1416 \tTraining Loss: 0.00774404 \tValidation Loss 0.02639165 \tTraining Acuuarcy 63.310% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 1417 \tTraining Loss: 0.00792234 \tValidation Loss 0.02618421 \tTraining Acuuarcy 62.613% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1418 \tTraining Loss: 0.00787239 \tValidation Loss 0.02645362 \tTraining Acuuarcy 62.970% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1419 \tTraining Loss: 0.00787535 \tValidation Loss 0.02658547 \tTraining Acuuarcy 62.947% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1420 \tTraining Loss: 0.00786720 \tValidation Loss 0.02583456 \tTraining Acuuarcy 63.042% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1421 \tTraining Loss: 0.00782329 \tValidation Loss 0.02699434 \tTraining Acuuarcy 63.025% \tValidation Acuuarcy 16.913%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1422 \tTraining Loss: 0.00789697 \tValidation Loss 0.02663870 \tTraining Acuuarcy 62.802% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1423 \tTraining Loss: 0.00779661 \tValidation Loss 0.02732153 \tTraining Acuuarcy 62.880% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1424 \tTraining Loss: 0.00789093 \tValidation Loss 0.02608977 \tTraining Acuuarcy 62.267% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1425 \tTraining Loss: 0.00778392 \tValidation Loss 0.02735568 \tTraining Acuuarcy 63.404% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 1426 \tTraining Loss: 0.00786014 \tValidation Loss 0.02721573 \tTraining Acuuarcy 62.763% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1427 \tTraining Loss: 0.00774959 \tValidation Loss 0.02625793 \tTraining Acuuarcy 63.354% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1428 \tTraining Loss: 0.00760697 \tValidation Loss 0.02717941 \tTraining Acuuarcy 63.945% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1429 \tTraining Loss: 0.00779038 \tValidation Loss 0.02689402 \tTraining Acuuarcy 63.382% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1430 \tTraining Loss: 0.00768832 \tValidation Loss 0.02651198 \tTraining Acuuarcy 63.616% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1431 \tTraining Loss: 0.00767641 \tValidation Loss 0.02651278 \tTraining Acuuarcy 63.884% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1432 \tTraining Loss: 0.00774657 \tValidation Loss 0.02663098 \tTraining Acuuarcy 62.981% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1433 \tTraining Loss: 0.00773011 \tValidation Loss 0.02709556 \tTraining Acuuarcy 63.655% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1434 \tTraining Loss: 0.00785844 \tValidation Loss 0.02704040 \tTraining Acuuarcy 62.635% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1435 \tTraining Loss: 0.00774600 \tValidation Loss 0.02723190 \tTraining Acuuarcy 63.360% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1436 \tTraining Loss: 0.00783271 \tValidation Loss 0.02685520 \tTraining Acuuarcy 62.607% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1437 \tTraining Loss: 0.00778644 \tValidation Loss 0.02567937 \tTraining Acuuarcy 63.025% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1438 \tTraining Loss: 0.00779530 \tValidation Loss 0.02752018 \tTraining Acuuarcy 63.153% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1439 \tTraining Loss: 0.00777852 \tValidation Loss 0.02677154 \tTraining Acuuarcy 63.087% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1440 \tTraining Loss: 0.00775304 \tValidation Loss 0.02666346 \tTraining Acuuarcy 63.187% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 1441 \tTraining Loss: 0.00779805 \tValidation Loss 0.02708866 \tTraining Acuuarcy 63.293% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 1442 \tTraining Loss: 0.00772990 \tValidation Loss 0.02718751 \tTraining Acuuarcy 63.549% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 1443 \tTraining Loss: 0.00789847 \tValidation Loss 0.02686365 \tTraining Acuuarcy 62.669% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1444 \tTraining Loss: 0.00766512 \tValidation Loss 0.02768823 \tTraining Acuuarcy 64.123% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1445 \tTraining Loss: 0.00772991 \tValidation Loss 0.02650913 \tTraining Acuuarcy 63.627% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1446 \tTraining Loss: 0.00772136 \tValidation Loss 0.02702460 \tTraining Acuuarcy 63.655% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1447 \tTraining Loss: 0.00758245 \tValidation Loss 0.02778179 \tTraining Acuuarcy 64.006% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1448 \tTraining Loss: 0.00783862 \tValidation Loss 0.02694541 \tTraining Acuuarcy 62.585% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1449 \tTraining Loss: 0.00782318 \tValidation Loss 0.02734685 \tTraining Acuuarcy 62.501% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1450 \tTraining Loss: 0.00787458 \tValidation Loss 0.02747075 \tTraining Acuuarcy 62.251% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1451 \tTraining Loss: 0.00776823 \tValidation Loss 0.02644298 \tTraining Acuuarcy 63.566% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1452 \tTraining Loss: 0.00786112 \tValidation Loss 0.02693306 \tTraining Acuuarcy 62.691% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1453 \tTraining Loss: 0.00775106 \tValidation Loss 0.02702964 \tTraining Acuuarcy 63.633% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1454 \tTraining Loss: 0.00787987 \tValidation Loss 0.02651818 \tTraining Acuuarcy 62.641% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1455 \tTraining Loss: 0.00782062 \tValidation Loss 0.02756751 \tTraining Acuuarcy 62.964% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1456 \tTraining Loss: 0.00789637 \tValidation Loss 0.02759957 \tTraining Acuuarcy 62.234% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1457 \tTraining Loss: 0.00782854 \tValidation Loss 0.02646293 \tTraining Acuuarcy 63.070% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1458 \tTraining Loss: 0.00773923 \tValidation Loss 0.02780969 \tTraining Acuuarcy 63.611% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 1459 \tTraining Loss: 0.00765169 \tValidation Loss 0.02677814 \tTraining Acuuarcy 63.633% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1460 \tTraining Loss: 0.00781061 \tValidation Loss 0.02674671 \tTraining Acuuarcy 63.148% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1461 \tTraining Loss: 0.00778928 \tValidation Loss 0.02626597 \tTraining Acuuarcy 63.165% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1462 \tTraining Loss: 0.00781778 \tValidation Loss 0.02688681 \tTraining Acuuarcy 63.159% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1463 \tTraining Loss: 0.00780388 \tValidation Loss 0.02641516 \tTraining Acuuarcy 63.755% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1464 \tTraining Loss: 0.00783993 \tValidation Loss 0.02716764 \tTraining Acuuarcy 62.981% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1465 \tTraining Loss: 0.00770534 \tValidation Loss 0.02701561 \tTraining Acuuarcy 63.616% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1466 \tTraining Loss: 0.00773468 \tValidation Loss 0.02668789 \tTraining Acuuarcy 63.510% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 1467 \tTraining Loss: 0.00779480 \tValidation Loss 0.02684490 \tTraining Acuuarcy 63.142% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1468 \tTraining Loss: 0.00777461 \tValidation Loss 0.02809474 \tTraining Acuuarcy 63.521% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 1469 \tTraining Loss: 0.00770744 \tValidation Loss 0.02740107 \tTraining Acuuarcy 63.594% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1470 \tTraining Loss: 0.00792246 \tValidation Loss 0.02698689 \tTraining Acuuarcy 62.635% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1471 \tTraining Loss: 0.00783436 \tValidation Loss 0.02651534 \tTraining Acuuarcy 63.053% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1472 \tTraining Loss: 0.00771061 \tValidation Loss 0.02793090 \tTraining Acuuarcy 63.666% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1473 \tTraining Loss: 0.00773151 \tValidation Loss 0.02734003 \tTraining Acuuarcy 63.443% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 1474 \tTraining Loss: 0.00789975 \tValidation Loss 0.02626948 \tTraining Acuuarcy 62.892% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 1475 \tTraining Loss: 0.00776035 \tValidation Loss 0.02674630 \tTraining Acuuarcy 63.232% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1476 \tTraining Loss: 0.00769512 \tValidation Loss 0.02623211 \tTraining Acuuarcy 63.689% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1477 \tTraining Loss: 0.00780447 \tValidation Loss 0.02789846 \tTraining Acuuarcy 63.092% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1478 \tTraining Loss: 0.00772125 \tValidation Loss 0.02674671 \tTraining Acuuarcy 63.532% \tValidation Acuuarcy 16.773%\n",
      "Epoch: 1479 \tTraining Loss: 0.00775770 \tValidation Loss 0.02700650 \tTraining Acuuarcy 63.778% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1480 \tTraining Loss: 0.00784239 \tValidation Loss 0.02632262 \tTraining Acuuarcy 63.075% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1481 \tTraining Loss: 0.00783895 \tValidation Loss 0.02659226 \tTraining Acuuarcy 63.003% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1482 \tTraining Loss: 0.00771338 \tValidation Loss 0.02715480 \tTraining Acuuarcy 63.778% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1483 \tTraining Loss: 0.00781329 \tValidation Loss 0.02687132 \tTraining Acuuarcy 63.521% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1484 \tTraining Loss: 0.00772713 \tValidation Loss 0.02631445 \tTraining Acuuarcy 63.466% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1485 \tTraining Loss: 0.00775884 \tValidation Loss 0.02786150 \tTraining Acuuarcy 63.315% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1486 \tTraining Loss: 0.00772417 \tValidation Loss 0.02694262 \tTraining Acuuarcy 63.454% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1487 \tTraining Loss: 0.00769666 \tValidation Loss 0.02715149 \tTraining Acuuarcy 63.767% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1488 \tTraining Loss: 0.00772059 \tValidation Loss 0.02735762 \tTraining Acuuarcy 63.153% \tValidation Acuuarcy 17.219%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1489 \tTraining Loss: 0.00771047 \tValidation Loss 0.02753616 \tTraining Acuuarcy 63.454% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1490 \tTraining Loss: 0.00778327 \tValidation Loss 0.02720514 \tTraining Acuuarcy 63.599% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1491 \tTraining Loss: 0.00765519 \tValidation Loss 0.02653171 \tTraining Acuuarcy 63.516% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 1492 \tTraining Loss: 0.00768743 \tValidation Loss 0.02692652 \tTraining Acuuarcy 63.382% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1493 \tTraining Loss: 0.00769415 \tValidation Loss 0.02737658 \tTraining Acuuarcy 63.583% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 1494 \tTraining Loss: 0.00777316 \tValidation Loss 0.02653713 \tTraining Acuuarcy 63.527% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1495 \tTraining Loss: 0.00771297 \tValidation Loss 0.02655387 \tTraining Acuuarcy 63.415% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1496 \tTraining Loss: 0.00768579 \tValidation Loss 0.02764645 \tTraining Acuuarcy 63.560% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1497 \tTraining Loss: 0.00779324 \tValidation Loss 0.02691970 \tTraining Acuuarcy 63.075% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1498 \tTraining Loss: 0.00760516 \tValidation Loss 0.02669620 \tTraining Acuuarcy 63.934% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1499 \tTraining Loss: 0.00789952 \tValidation Loss 0.02687199 \tTraining Acuuarcy 62.423% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1500 \tTraining Loss: 0.00779902 \tValidation Loss 0.02699897 \tTraining Acuuarcy 63.109% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1501 \tTraining Loss: 0.00774534 \tValidation Loss 0.02731697 \tTraining Acuuarcy 63.410% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1502 \tTraining Loss: 0.00777415 \tValidation Loss 0.02684955 \tTraining Acuuarcy 63.243% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1503 \tTraining Loss: 0.00768763 \tValidation Loss 0.02637984 \tTraining Acuuarcy 63.254% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1504 \tTraining Loss: 0.00770603 \tValidation Loss 0.02670298 \tTraining Acuuarcy 63.694% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1505 \tTraining Loss: 0.00777812 \tValidation Loss 0.02740826 \tTraining Acuuarcy 63.661% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1506 \tTraining Loss: 0.00770419 \tValidation Loss 0.02676894 \tTraining Acuuarcy 63.527% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1507 \tTraining Loss: 0.00770843 \tValidation Loss 0.02674405 \tTraining Acuuarcy 63.432% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1508 \tTraining Loss: 0.00767689 \tValidation Loss 0.02646276 \tTraining Acuuarcy 63.778% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 1509 \tTraining Loss: 0.00764857 \tValidation Loss 0.02725350 \tTraining Acuuarcy 63.466% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1510 \tTraining Loss: 0.00776182 \tValidation Loss 0.02622929 \tTraining Acuuarcy 63.399% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1511 \tTraining Loss: 0.00765484 \tValidation Loss 0.02738646 \tTraining Acuuarcy 63.889% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1512 \tTraining Loss: 0.00769798 \tValidation Loss 0.02800711 \tTraining Acuuarcy 63.482% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1513 \tTraining Loss: 0.00768407 \tValidation Loss 0.02672779 \tTraining Acuuarcy 63.393% \tValidation Acuuarcy 16.551%\n",
      "Epoch: 1514 \tTraining Loss: 0.00779655 \tValidation Loss 0.02644852 \tTraining Acuuarcy 63.271% \tValidation Acuuarcy 16.188%\n",
      "Epoch: 1515 \tTraining Loss: 0.00784057 \tValidation Loss 0.02744405 \tTraining Acuuarcy 62.758% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1516 \tTraining Loss: 0.00776083 \tValidation Loss 0.02685294 \tTraining Acuuarcy 63.187% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 1517 \tTraining Loss: 0.00776678 \tValidation Loss 0.02839519 \tTraining Acuuarcy 63.209% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1518 \tTraining Loss: 0.00765769 \tValidation Loss 0.02705492 \tTraining Acuuarcy 63.683% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1519 \tTraining Loss: 0.00767439 \tValidation Loss 0.02783225 \tTraining Acuuarcy 63.794% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1520 \tTraining Loss: 0.00770250 \tValidation Loss 0.02710971 \tTraining Acuuarcy 63.845% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 1521 \tTraining Loss: 0.00758182 \tValidation Loss 0.02803475 \tTraining Acuuarcy 64.045% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1522 \tTraining Loss: 0.00772765 \tValidation Loss 0.02722056 \tTraining Acuuarcy 63.421% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1523 \tTraining Loss: 0.00778127 \tValidation Loss 0.02727283 \tTraining Acuuarcy 62.886% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 1524 \tTraining Loss: 0.00779461 \tValidation Loss 0.02647332 \tTraining Acuuarcy 63.360% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1525 \tTraining Loss: 0.00789667 \tValidation Loss 0.02660434 \tTraining Acuuarcy 62.446% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1526 \tTraining Loss: 0.00787533 \tValidation Loss 0.02743117 \tTraining Acuuarcy 62.217% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1527 \tTraining Loss: 0.00776795 \tValidation Loss 0.02598867 \tTraining Acuuarcy 63.165% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1528 \tTraining Loss: 0.00772771 \tValidation Loss 0.02736546 \tTraining Acuuarcy 63.605% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1529 \tTraining Loss: 0.00776486 \tValidation Loss 0.02688323 \tTraining Acuuarcy 63.354% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1530 \tTraining Loss: 0.00777921 \tValidation Loss 0.02855587 \tTraining Acuuarcy 62.970% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1531 \tTraining Loss: 0.00770433 \tValidation Loss 0.02743373 \tTraining Acuuarcy 63.421% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1532 \tTraining Loss: 0.00777999 \tValidation Loss 0.02648888 \tTraining Acuuarcy 63.070% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1533 \tTraining Loss: 0.00782901 \tValidation Loss 0.02686444 \tTraining Acuuarcy 62.591% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1534 \tTraining Loss: 0.00773626 \tValidation Loss 0.02734918 \tTraining Acuuarcy 63.114% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1535 \tTraining Loss: 0.00770395 \tValidation Loss 0.02701951 \tTraining Acuuarcy 63.114% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1536 \tTraining Loss: 0.00767536 \tValidation Loss 0.02704635 \tTraining Acuuarcy 64.040% \tValidation Acuuarcy 16.746%\n",
      "Epoch: 1537 \tTraining Loss: 0.00773169 \tValidation Loss 0.02691198 \tTraining Acuuarcy 63.544% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1538 \tTraining Loss: 0.00767214 \tValidation Loss 0.02726364 \tTraining Acuuarcy 63.577% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1539 \tTraining Loss: 0.00756622 \tValidation Loss 0.02742378 \tTraining Acuuarcy 64.424% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1540 \tTraining Loss: 0.00761135 \tValidation Loss 0.02694796 \tTraining Acuuarcy 64.246% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1541 \tTraining Loss: 0.00772110 \tValidation Loss 0.02695046 \tTraining Acuuarcy 63.488% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 1542 \tTraining Loss: 0.00774671 \tValidation Loss 0.02656959 \tTraining Acuuarcy 63.354% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1543 \tTraining Loss: 0.00775815 \tValidation Loss 0.02729933 \tTraining Acuuarcy 63.622% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1544 \tTraining Loss: 0.00777991 \tValidation Loss 0.02732213 \tTraining Acuuarcy 63.700% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1545 \tTraining Loss: 0.00767974 \tValidation Loss 0.02782367 \tTraining Acuuarcy 63.962% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1546 \tTraining Loss: 0.00780560 \tValidation Loss 0.02722318 \tTraining Acuuarcy 63.009% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1547 \tTraining Loss: 0.00773193 \tValidation Loss 0.02712483 \tTraining Acuuarcy 63.521% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1548 \tTraining Loss: 0.00771388 \tValidation Loss 0.02710925 \tTraining Acuuarcy 63.939% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1549 \tTraining Loss: 0.00764247 \tValidation Loss 0.02714297 \tTraining Acuuarcy 64.045% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1550 \tTraining Loss: 0.00756715 \tValidation Loss 0.02843408 \tTraining Acuuarcy 64.413% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 1551 \tTraining Loss: 0.00763684 \tValidation Loss 0.02734386 \tTraining Acuuarcy 64.251% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1552 \tTraining Loss: 0.00766163 \tValidation Loss 0.02773661 \tTraining Acuuarcy 63.694% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1553 \tTraining Loss: 0.00772590 \tValidation Loss 0.02704604 \tTraining Acuuarcy 63.421% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1554 \tTraining Loss: 0.00765913 \tValidation Loss 0.02718019 \tTraining Acuuarcy 64.246% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1555 \tTraining Loss: 0.00763453 \tValidation Loss 0.02711162 \tTraining Acuuarcy 63.967% \tValidation Acuuarcy 18.278%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1556 \tTraining Loss: 0.00768165 \tValidation Loss 0.02732347 \tTraining Acuuarcy 64.012% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1557 \tTraining Loss: 0.00765435 \tValidation Loss 0.02740997 \tTraining Acuuarcy 63.594% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1558 \tTraining Loss: 0.00765216 \tValidation Loss 0.02663885 \tTraining Acuuarcy 63.728% \tValidation Acuuarcy 17.024%\n",
      "Epoch: 1559 \tTraining Loss: 0.00765404 \tValidation Loss 0.02797986 \tTraining Acuuarcy 63.744% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1560 \tTraining Loss: 0.00758695 \tValidation Loss 0.02715551 \tTraining Acuuarcy 63.990% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1561 \tTraining Loss: 0.00773713 \tValidation Loss 0.02754413 \tTraining Acuuarcy 63.700% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1562 \tTraining Loss: 0.00766589 \tValidation Loss 0.02800991 \tTraining Acuuarcy 63.672% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1563 \tTraining Loss: 0.00775958 \tValidation Loss 0.02626764 \tTraining Acuuarcy 63.098% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1564 \tTraining Loss: 0.00753201 \tValidation Loss 0.02699706 \tTraining Acuuarcy 64.452% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1565 \tTraining Loss: 0.00776177 \tValidation Loss 0.02601309 \tTraining Acuuarcy 63.025% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1566 \tTraining Loss: 0.00758540 \tValidation Loss 0.02712748 \tTraining Acuuarcy 64.056% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1567 \tTraining Loss: 0.00762121 \tValidation Loss 0.02795842 \tTraining Acuuarcy 63.850% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1568 \tTraining Loss: 0.00761360 \tValidation Loss 0.02654102 \tTraining Acuuarcy 64.101% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1569 \tTraining Loss: 0.00768220 \tValidation Loss 0.02803951 \tTraining Acuuarcy 63.984% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1570 \tTraining Loss: 0.00761866 \tValidation Loss 0.02743543 \tTraining Acuuarcy 63.978% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1571 \tTraining Loss: 0.00778081 \tValidation Loss 0.02777719 \tTraining Acuuarcy 63.003% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1572 \tTraining Loss: 0.00773624 \tValidation Loss 0.02662019 \tTraining Acuuarcy 63.666% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1573 \tTraining Loss: 0.00778081 \tValidation Loss 0.02644863 \tTraining Acuuarcy 62.830% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1574 \tTraining Loss: 0.00772740 \tValidation Loss 0.02747988 \tTraining Acuuarcy 63.020% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1575 \tTraining Loss: 0.00769313 \tValidation Loss 0.02697573 \tTraining Acuuarcy 63.577% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1576 \tTraining Loss: 0.00771440 \tValidation Loss 0.02756888 \tTraining Acuuarcy 63.304% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1577 \tTraining Loss: 0.00765886 \tValidation Loss 0.02723926 \tTraining Acuuarcy 63.800% \tValidation Acuuarcy 16.634%\n",
      "Epoch: 1578 \tTraining Loss: 0.00772570 \tValidation Loss 0.02635324 \tTraining Acuuarcy 63.622% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1579 \tTraining Loss: 0.00765999 \tValidation Loss 0.02707176 \tTraining Acuuarcy 63.326% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1580 \tTraining Loss: 0.00771803 \tValidation Loss 0.02627464 \tTraining Acuuarcy 63.399% \tValidation Acuuarcy 16.746%\n",
      "Epoch: 1581 \tTraining Loss: 0.00765720 \tValidation Loss 0.02625478 \tTraining Acuuarcy 63.934% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1582 \tTraining Loss: 0.00764309 \tValidation Loss 0.02646490 \tTraining Acuuarcy 64.134% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1583 \tTraining Loss: 0.00775517 \tValidation Loss 0.02725903 \tTraining Acuuarcy 63.555% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1584 \tTraining Loss: 0.00776112 \tValidation Loss 0.02721158 \tTraining Acuuarcy 63.337% \tValidation Acuuarcy 16.829%\n",
      "Epoch: 1585 \tTraining Loss: 0.00774389 \tValidation Loss 0.02716059 \tTraining Acuuarcy 63.293% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1586 \tTraining Loss: 0.00773793 \tValidation Loss 0.02703293 \tTraining Acuuarcy 63.577% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1587 \tTraining Loss: 0.00763938 \tValidation Loss 0.02652206 \tTraining Acuuarcy 63.900% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 1588 \tTraining Loss: 0.00772438 \tValidation Loss 0.02739216 \tTraining Acuuarcy 63.661% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1589 \tTraining Loss: 0.00770744 \tValidation Loss 0.02623866 \tTraining Acuuarcy 63.700% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1590 \tTraining Loss: 0.00777139 \tValidation Loss 0.02704587 \tTraining Acuuarcy 63.232% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1591 \tTraining Loss: 0.00775134 \tValidation Loss 0.02666305 \tTraining Acuuarcy 63.856% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 1592 \tTraining Loss: 0.00769361 \tValidation Loss 0.02692640 \tTraining Acuuarcy 63.722% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1593 \tTraining Loss: 0.00765801 \tValidation Loss 0.02744577 \tTraining Acuuarcy 64.112% \tValidation Acuuarcy 16.773%\n",
      "Epoch: 1594 \tTraining Loss: 0.00765193 \tValidation Loss 0.02705652 \tTraining Acuuarcy 63.583% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1595 \tTraining Loss: 0.00768186 \tValidation Loss 0.02766835 \tTraining Acuuarcy 63.449% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 1596 \tTraining Loss: 0.00758997 \tValidation Loss 0.02612539 \tTraining Acuuarcy 63.722% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1597 \tTraining Loss: 0.00772553 \tValidation Loss 0.02777788 \tTraining Acuuarcy 63.588% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 1598 \tTraining Loss: 0.00781396 \tValidation Loss 0.02755989 \tTraining Acuuarcy 63.254% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1599 \tTraining Loss: 0.00762762 \tValidation Loss 0.02755229 \tTraining Acuuarcy 63.945% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1600 \tTraining Loss: 0.00772109 \tValidation Loss 0.02636030 \tTraining Acuuarcy 63.622% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1601 \tTraining Loss: 0.00776085 \tValidation Loss 0.02729053 \tTraining Acuuarcy 63.360% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1602 \tTraining Loss: 0.00753566 \tValidation Loss 0.02690656 \tTraining Acuuarcy 64.274% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1603 \tTraining Loss: 0.00760302 \tValidation Loss 0.02680307 \tTraining Acuuarcy 64.285% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 1604 \tTraining Loss: 0.00763656 \tValidation Loss 0.02773185 \tTraining Acuuarcy 63.622% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1605 \tTraining Loss: 0.00776549 \tValidation Loss 0.02701326 \tTraining Acuuarcy 63.014% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 1606 \tTraining Loss: 0.00766960 \tValidation Loss 0.02728177 \tTraining Acuuarcy 63.493% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1607 \tTraining Loss: 0.00772353 \tValidation Loss 0.02770002 \tTraining Acuuarcy 63.421% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1608 \tTraining Loss: 0.00770373 \tValidation Loss 0.02851291 \tTraining Acuuarcy 63.399% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1609 \tTraining Loss: 0.00779912 \tValidation Loss 0.02695569 \tTraining Acuuarcy 62.847% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1610 \tTraining Loss: 0.00762151 \tValidation Loss 0.02719109 \tTraining Acuuarcy 64.408% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1611 \tTraining Loss: 0.00774858 \tValidation Loss 0.02775730 \tTraining Acuuarcy 63.516% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 1612 \tTraining Loss: 0.00773179 \tValidation Loss 0.02677236 \tTraining Acuuarcy 63.254% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1613 \tTraining Loss: 0.00766614 \tValidation Loss 0.02802743 \tTraining Acuuarcy 64.173% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1614 \tTraining Loss: 0.00761934 \tValidation Loss 0.02764668 \tTraining Acuuarcy 63.789% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1615 \tTraining Loss: 0.00773315 \tValidation Loss 0.02745284 \tTraining Acuuarcy 63.549% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1616 \tTraining Loss: 0.00767364 \tValidation Loss 0.02750337 \tTraining Acuuarcy 63.677% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 1617 \tTraining Loss: 0.00765666 \tValidation Loss 0.02742128 \tTraining Acuuarcy 63.794% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1618 \tTraining Loss: 0.00781772 \tValidation Loss 0.02701284 \tTraining Acuuarcy 63.131% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 1619 \tTraining Loss: 0.00783137 \tValidation Loss 0.02710356 \tTraining Acuuarcy 62.919% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1620 \tTraining Loss: 0.00776250 \tValidation Loss 0.02711424 \tTraining Acuuarcy 63.460% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1621 \tTraining Loss: 0.00767896 \tValidation Loss 0.02656962 \tTraining Acuuarcy 63.505% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1622 \tTraining Loss: 0.00783521 \tValidation Loss 0.02673063 \tTraining Acuuarcy 63.109% \tValidation Acuuarcy 17.498%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1623 \tTraining Loss: 0.00771068 \tValidation Loss 0.02695880 \tTraining Acuuarcy 63.705% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1624 \tTraining Loss: 0.00770193 \tValidation Loss 0.02714512 \tTraining Acuuarcy 63.700% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1625 \tTraining Loss: 0.00780065 \tValidation Loss 0.02707267 \tTraining Acuuarcy 63.053% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1626 \tTraining Loss: 0.00759590 \tValidation Loss 0.02791806 \tTraining Acuuarcy 63.722% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 1627 \tTraining Loss: 0.00748607 \tValidation Loss 0.02772982 \tTraining Acuuarcy 64.469% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1628 \tTraining Loss: 0.00767918 \tValidation Loss 0.02716041 \tTraining Acuuarcy 63.644% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1629 \tTraining Loss: 0.00762366 \tValidation Loss 0.02822357 \tTraining Acuuarcy 64.279% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1630 \tTraining Loss: 0.00753340 \tValidation Loss 0.02731942 \tTraining Acuuarcy 64.268% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1631 \tTraining Loss: 0.00768663 \tValidation Loss 0.02685556 \tTraining Acuuarcy 63.644% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1632 \tTraining Loss: 0.00762215 \tValidation Loss 0.02619829 \tTraining Acuuarcy 63.577% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1633 \tTraining Loss: 0.00755912 \tValidation Loss 0.02650720 \tTraining Acuuarcy 64.006% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1634 \tTraining Loss: 0.00760237 \tValidation Loss 0.02719964 \tTraining Acuuarcy 64.525% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 1635 \tTraining Loss: 0.00761158 \tValidation Loss 0.02692520 \tTraining Acuuarcy 63.700% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1636 \tTraining Loss: 0.00774454 \tValidation Loss 0.02689521 \tTraining Acuuarcy 63.438% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1637 \tTraining Loss: 0.00757855 \tValidation Loss 0.02698216 \tTraining Acuuarcy 64.408% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1638 \tTraining Loss: 0.00761956 \tValidation Loss 0.02758320 \tTraining Acuuarcy 64.012% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1639 \tTraining Loss: 0.00767608 \tValidation Loss 0.02764850 \tTraining Acuuarcy 63.733% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1640 \tTraining Loss: 0.00770950 \tValidation Loss 0.02639873 \tTraining Acuuarcy 63.371% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1641 \tTraining Loss: 0.00773111 \tValidation Loss 0.02754799 \tTraining Acuuarcy 63.064% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1642 \tTraining Loss: 0.00763231 \tValidation Loss 0.02749336 \tTraining Acuuarcy 63.744% \tValidation Acuuarcy 16.746%\n",
      "Epoch: 1643 \tTraining Loss: 0.00768386 \tValidation Loss 0.02817361 \tTraining Acuuarcy 63.650% \tValidation Acuuarcy 16.829%\n",
      "Epoch: 1644 \tTraining Loss: 0.00758823 \tValidation Loss 0.02833894 \tTraining Acuuarcy 64.430% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1645 \tTraining Loss: 0.00763488 \tValidation Loss 0.02689051 \tTraining Acuuarcy 64.385% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1646 \tTraining Loss: 0.00764279 \tValidation Loss 0.02657522 \tTraining Acuuarcy 64.017% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1647 \tTraining Loss: 0.00756928 \tValidation Loss 0.02697559 \tTraining Acuuarcy 64.056% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1648 \tTraining Loss: 0.00767897 \tValidation Loss 0.02839146 \tTraining Acuuarcy 64.034% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1649 \tTraining Loss: 0.00764644 \tValidation Loss 0.02740884 \tTraining Acuuarcy 64.218% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1650 \tTraining Loss: 0.00755762 \tValidation Loss 0.02829067 \tTraining Acuuarcy 64.073% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1651 \tTraining Loss: 0.00763085 \tValidation Loss 0.02708945 \tTraining Acuuarcy 63.783% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1652 \tTraining Loss: 0.00769079 \tValidation Loss 0.02604204 \tTraining Acuuarcy 63.555% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1653 \tTraining Loss: 0.00780582 \tValidation Loss 0.02698536 \tTraining Acuuarcy 62.535% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1654 \tTraining Loss: 0.00771528 \tValidation Loss 0.02737615 \tTraining Acuuarcy 63.399% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1655 \tTraining Loss: 0.00767575 \tValidation Loss 0.02714128 \tTraining Acuuarcy 63.722% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1656 \tTraining Loss: 0.00778995 \tValidation Loss 0.02709573 \tTraining Acuuarcy 63.204% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1657 \tTraining Loss: 0.00771094 \tValidation Loss 0.02737293 \tTraining Acuuarcy 63.716% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1658 \tTraining Loss: 0.00770326 \tValidation Loss 0.02700417 \tTraining Acuuarcy 63.672% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1659 \tTraining Loss: 0.00760712 \tValidation Loss 0.02688494 \tTraining Acuuarcy 64.212% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1660 \tTraining Loss: 0.00769413 \tValidation Loss 0.02691470 \tTraining Acuuarcy 63.990% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1661 \tTraining Loss: 0.00793516 \tValidation Loss 0.02727490 \tTraining Acuuarcy 62.663% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1662 \tTraining Loss: 0.00769979 \tValidation Loss 0.02746164 \tTraining Acuuarcy 63.493% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1663 \tTraining Loss: 0.00773272 \tValidation Loss 0.02586948 \tTraining Acuuarcy 63.549% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1664 \tTraining Loss: 0.00775678 \tValidation Loss 0.02778880 \tTraining Acuuarcy 63.365% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1665 \tTraining Loss: 0.00770879 \tValidation Loss 0.02692198 \tTraining Acuuarcy 63.627% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1666 \tTraining Loss: 0.00767623 \tValidation Loss 0.02752547 \tTraining Acuuarcy 63.856% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1667 \tTraining Loss: 0.00759505 \tValidation Loss 0.02734646 \tTraining Acuuarcy 64.313% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1668 \tTraining Loss: 0.00769928 \tValidation Loss 0.02802045 \tTraining Acuuarcy 63.332% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1669 \tTraining Loss: 0.00787247 \tValidation Loss 0.02716028 \tTraining Acuuarcy 63.137% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1670 \tTraining Loss: 0.00764745 \tValidation Loss 0.02717156 \tTraining Acuuarcy 64.034% \tValidation Acuuarcy 16.300%\n",
      "Epoch: 1671 \tTraining Loss: 0.00778191 \tValidation Loss 0.02780897 \tTraining Acuuarcy 63.432% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1672 \tTraining Loss: 0.00766723 \tValidation Loss 0.02729309 \tTraining Acuuarcy 63.644% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1673 \tTraining Loss: 0.00758923 \tValidation Loss 0.02695509 \tTraining Acuuarcy 64.285% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 1674 \tTraining Loss: 0.00766445 \tValidation Loss 0.02672083 \tTraining Acuuarcy 63.755% \tValidation Acuuarcy 16.885%\n",
      "Epoch: 1675 \tTraining Loss: 0.00773023 \tValidation Loss 0.02699820 \tTraining Acuuarcy 63.432% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1676 \tTraining Loss: 0.00751726 \tValidation Loss 0.02818147 \tTraining Acuuarcy 64.157% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1677 \tTraining Loss: 0.00750070 \tValidation Loss 0.02776398 \tTraining Acuuarcy 64.463% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1678 \tTraining Loss: 0.00760373 \tValidation Loss 0.02736434 \tTraining Acuuarcy 63.689% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1679 \tTraining Loss: 0.00762701 \tValidation Loss 0.02732196 \tTraining Acuuarcy 63.951% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 1680 \tTraining Loss: 0.00759995 \tValidation Loss 0.02717036 \tTraining Acuuarcy 64.580% \tValidation Acuuarcy 16.801%\n",
      "Epoch: 1681 \tTraining Loss: 0.00763341 \tValidation Loss 0.02661598 \tTraining Acuuarcy 64.251% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1682 \tTraining Loss: 0.00762708 \tValidation Loss 0.02771684 \tTraining Acuuarcy 64.190% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1683 \tTraining Loss: 0.00765350 \tValidation Loss 0.02765941 \tTraining Acuuarcy 63.856% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1684 \tTraining Loss: 0.00759248 \tValidation Loss 0.02722290 \tTraining Acuuarcy 64.012% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1685 \tTraining Loss: 0.00765573 \tValidation Loss 0.02698081 \tTraining Acuuarcy 63.650% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1686 \tTraining Loss: 0.00760656 \tValidation Loss 0.02765484 \tTraining Acuuarcy 64.001% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1687 \tTraining Loss: 0.00759300 \tValidation Loss 0.02714259 \tTraining Acuuarcy 64.251% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1688 \tTraining Loss: 0.00767741 \tValidation Loss 0.02822874 \tTraining Acuuarcy 63.633% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 1689 \tTraining Loss: 0.00761749 \tValidation Loss 0.02700472 \tTraining Acuuarcy 63.627% \tValidation Acuuarcy 16.773%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1690 \tTraining Loss: 0.00757556 \tValidation Loss 0.02832273 \tTraining Acuuarcy 64.502% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 1691 \tTraining Loss: 0.00765846 \tValidation Loss 0.02890511 \tTraining Acuuarcy 63.454% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1692 \tTraining Loss: 0.00770177 \tValidation Loss 0.02789862 \tTraining Acuuarcy 63.711% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 1693 \tTraining Loss: 0.00770416 \tValidation Loss 0.02704367 \tTraining Acuuarcy 63.577% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1694 \tTraining Loss: 0.00772793 \tValidation Loss 0.02687767 \tTraining Acuuarcy 63.237% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1695 \tTraining Loss: 0.00763091 \tValidation Loss 0.02802577 \tTraining Acuuarcy 63.939% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1696 \tTraining Loss: 0.00759208 \tValidation Loss 0.02782804 \tTraining Acuuarcy 64.118% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1697 \tTraining Loss: 0.00760502 \tValidation Loss 0.02750381 \tTraining Acuuarcy 64.168% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1698 \tTraining Loss: 0.00766738 \tValidation Loss 0.02732119 \tTraining Acuuarcy 63.739% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1699 \tTraining Loss: 0.00768284 \tValidation Loss 0.02721075 \tTraining Acuuarcy 63.499% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1700 \tTraining Loss: 0.00773829 \tValidation Loss 0.02673402 \tTraining Acuuarcy 63.148% \tValidation Acuuarcy 16.913%\n",
      "Epoch: 1701 \tTraining Loss: 0.00763074 \tValidation Loss 0.02697089 \tTraining Acuuarcy 63.689% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1702 \tTraining Loss: 0.00782656 \tValidation Loss 0.02703337 \tTraining Acuuarcy 63.315% \tValidation Acuuarcy 16.801%\n",
      "Epoch: 1703 \tTraining Loss: 0.00750571 \tValidation Loss 0.02731109 \tTraining Acuuarcy 64.904% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1704 \tTraining Loss: 0.00770249 \tValidation Loss 0.02714451 \tTraining Acuuarcy 63.666% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 1705 \tTraining Loss: 0.00764835 \tValidation Loss 0.02646410 \tTraining Acuuarcy 63.978% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1706 \tTraining Loss: 0.00769640 \tValidation Loss 0.02678758 \tTraining Acuuarcy 63.778% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1707 \tTraining Loss: 0.00764344 \tValidation Loss 0.02719880 \tTraining Acuuarcy 64.313% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1708 \tTraining Loss: 0.00773266 \tValidation Loss 0.02649108 \tTraining Acuuarcy 63.488% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1709 \tTraining Loss: 0.00757399 \tValidation Loss 0.02734461 \tTraining Acuuarcy 64.207% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1710 \tTraining Loss: 0.00760668 \tValidation Loss 0.02716910 \tTraining Acuuarcy 64.185% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1711 \tTraining Loss: 0.00763884 \tValidation Loss 0.02781645 \tTraining Acuuarcy 63.755% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1712 \tTraining Loss: 0.00759824 \tValidation Loss 0.02788421 \tTraining Acuuarcy 63.850% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1713 \tTraining Loss: 0.00762478 \tValidation Loss 0.02779730 \tTraining Acuuarcy 63.666% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1714 \tTraining Loss: 0.00779474 \tValidation Loss 0.02777432 \tTraining Acuuarcy 63.081% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1715 \tTraining Loss: 0.00762314 \tValidation Loss 0.02664420 \tTraining Acuuarcy 63.990% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1716 \tTraining Loss: 0.00767425 \tValidation Loss 0.02724775 \tTraining Acuuarcy 63.794% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 1717 \tTraining Loss: 0.00776895 \tValidation Loss 0.02686937 \tTraining Acuuarcy 63.321% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 1718 \tTraining Loss: 0.00770841 \tValidation Loss 0.02715872 \tTraining Acuuarcy 63.761% \tValidation Acuuarcy 16.773%\n",
      "Epoch: 1719 \tTraining Loss: 0.00768673 \tValidation Loss 0.02779969 \tTraining Acuuarcy 63.510% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1720 \tTraining Loss: 0.00756570 \tValidation Loss 0.02759270 \tTraining Acuuarcy 64.034% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1721 \tTraining Loss: 0.00770586 \tValidation Loss 0.02812534 \tTraining Acuuarcy 63.878% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1722 \tTraining Loss: 0.00763418 \tValidation Loss 0.02641331 \tTraining Acuuarcy 63.962% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1723 \tTraining Loss: 0.00766340 \tValidation Loss 0.02779004 \tTraining Acuuarcy 63.800% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 1724 \tTraining Loss: 0.00754021 \tValidation Loss 0.02780581 \tTraining Acuuarcy 64.486% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1725 \tTraining Loss: 0.00768898 \tValidation Loss 0.02704589 \tTraining Acuuarcy 63.421% \tValidation Acuuarcy 16.606%\n",
      "Epoch: 1726 \tTraining Loss: 0.00759944 \tValidation Loss 0.02667172 \tTraining Acuuarcy 64.185% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1727 \tTraining Loss: 0.00769117 \tValidation Loss 0.02719507 \tTraining Acuuarcy 63.572% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1728 \tTraining Loss: 0.00755752 \tValidation Loss 0.02699868 \tTraining Acuuarcy 64.229% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 1729 \tTraining Loss: 0.00761722 \tValidation Loss 0.02731338 \tTraining Acuuarcy 64.263% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1730 \tTraining Loss: 0.00755969 \tValidation Loss 0.02891978 \tTraining Acuuarcy 64.212% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1731 \tTraining Loss: 0.00747774 \tValidation Loss 0.02698056 \tTraining Acuuarcy 64.502% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1732 \tTraining Loss: 0.00753858 \tValidation Loss 0.02810125 \tTraining Acuuarcy 63.867% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1733 \tTraining Loss: 0.00768041 \tValidation Loss 0.02724405 \tTraining Acuuarcy 63.928% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1734 \tTraining Loss: 0.00771082 \tValidation Loss 0.02751475 \tTraining Acuuarcy 63.755% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 1735 \tTraining Loss: 0.00755453 \tValidation Loss 0.02703689 \tTraining Acuuarcy 64.608% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1736 \tTraining Loss: 0.00769229 \tValidation Loss 0.02677239 \tTraining Acuuarcy 63.705% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1737 \tTraining Loss: 0.00777513 \tValidation Loss 0.02769703 \tTraining Acuuarcy 63.466% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1738 \tTraining Loss: 0.00770621 \tValidation Loss 0.02717887 \tTraining Acuuarcy 63.365% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1739 \tTraining Loss: 0.00765148 \tValidation Loss 0.02712132 \tTraining Acuuarcy 64.263% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1740 \tTraining Loss: 0.00756448 \tValidation Loss 0.02649025 \tTraining Acuuarcy 64.586% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1741 \tTraining Loss: 0.00760986 \tValidation Loss 0.02700211 \tTraining Acuuarcy 63.928% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1742 \tTraining Loss: 0.00748973 \tValidation Loss 0.02745356 \tTraining Acuuarcy 64.959% \tValidation Acuuarcy 16.690%\n",
      "Epoch: 1743 \tTraining Loss: 0.00752613 \tValidation Loss 0.02805590 \tTraining Acuuarcy 64.363% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 1744 \tTraining Loss: 0.00765800 \tValidation Loss 0.02616470 \tTraining Acuuarcy 63.650% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1745 \tTraining Loss: 0.00757979 \tValidation Loss 0.02632514 \tTraining Acuuarcy 64.029% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1746 \tTraining Loss: 0.00757425 \tValidation Loss 0.02783475 \tTraining Acuuarcy 64.224% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1747 \tTraining Loss: 0.00764533 \tValidation Loss 0.02727420 \tTraining Acuuarcy 63.928% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1748 \tTraining Loss: 0.00759411 \tValidation Loss 0.02720242 \tTraining Acuuarcy 64.591% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 1749 \tTraining Loss: 0.00772477 \tValidation Loss 0.02790349 \tTraining Acuuarcy 63.633% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1750 \tTraining Loss: 0.00766244 \tValidation Loss 0.02771454 \tTraining Acuuarcy 63.973% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 1751 \tTraining Loss: 0.00754856 \tValidation Loss 0.02674629 \tTraining Acuuarcy 64.435% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1752 \tTraining Loss: 0.00772771 \tValidation Loss 0.02696413 \tTraining Acuuarcy 63.644% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1753 \tTraining Loss: 0.00761413 \tValidation Loss 0.02717754 \tTraining Acuuarcy 64.463% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 1754 \tTraining Loss: 0.00753594 \tValidation Loss 0.02697658 \tTraining Acuuarcy 64.474% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1755 \tTraining Loss: 0.00766650 \tValidation Loss 0.02719066 \tTraining Acuuarcy 63.783% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1756 \tTraining Loss: 0.00758110 \tValidation Loss 0.02728800 \tTraining Acuuarcy 64.095% \tValidation Acuuarcy 18.194%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1757 \tTraining Loss: 0.00756877 \tValidation Loss 0.02808569 \tTraining Acuuarcy 64.313% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1758 \tTraining Loss: 0.00758024 \tValidation Loss 0.02793897 \tTraining Acuuarcy 64.179% \tValidation Acuuarcy 16.885%\n",
      "Epoch: 1759 \tTraining Loss: 0.00755172 \tValidation Loss 0.02711990 \tTraining Acuuarcy 64.324% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1760 \tTraining Loss: 0.00756549 \tValidation Loss 0.02734516 \tTraining Acuuarcy 64.669% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1761 \tTraining Loss: 0.00761703 \tValidation Loss 0.02773797 \tTraining Acuuarcy 64.040% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 1762 \tTraining Loss: 0.00758167 \tValidation Loss 0.02708884 \tTraining Acuuarcy 64.441% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1763 \tTraining Loss: 0.00757396 \tValidation Loss 0.02645159 \tTraining Acuuarcy 64.536% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1764 \tTraining Loss: 0.00747059 \tValidation Loss 0.02736207 \tTraining Acuuarcy 65.305% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1765 \tTraining Loss: 0.00761954 \tValidation Loss 0.02746114 \tTraining Acuuarcy 64.006% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1766 \tTraining Loss: 0.00759132 \tValidation Loss 0.02752706 \tTraining Acuuarcy 63.778% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1767 \tTraining Loss: 0.00764696 \tValidation Loss 0.02727644 \tTraining Acuuarcy 64.134% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1768 \tTraining Loss: 0.00759439 \tValidation Loss 0.02661230 \tTraining Acuuarcy 64.246% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1769 \tTraining Loss: 0.00757596 \tValidation Loss 0.02714010 \tTraining Acuuarcy 63.990% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1770 \tTraining Loss: 0.00762426 \tValidation Loss 0.02777129 \tTraining Acuuarcy 64.263% \tValidation Acuuarcy 17.164%\n",
      "Epoch: 1771 \tTraining Loss: 0.00749034 \tValidation Loss 0.02714992 \tTraining Acuuarcy 64.709% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 1772 \tTraining Loss: 0.00774418 \tValidation Loss 0.02689285 \tTraining Acuuarcy 62.825% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1773 \tTraining Loss: 0.00764975 \tValidation Loss 0.02714395 \tTraining Acuuarcy 64.012% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 1774 \tTraining Loss: 0.00758977 \tValidation Loss 0.02682332 \tTraining Acuuarcy 64.179% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 1775 \tTraining Loss: 0.00765417 \tValidation Loss 0.02693074 \tTraining Acuuarcy 63.895% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 1776 \tTraining Loss: 0.00773203 \tValidation Loss 0.02666540 \tTraining Acuuarcy 63.583% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1777 \tTraining Loss: 0.00754175 \tValidation Loss 0.02711050 \tTraining Acuuarcy 64.402% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1778 \tTraining Loss: 0.00752490 \tValidation Loss 0.02740821 \tTraining Acuuarcy 64.575% \tValidation Acuuarcy 17.052%\n",
      "Epoch: 1779 \tTraining Loss: 0.00760682 \tValidation Loss 0.02781017 \tTraining Acuuarcy 63.800% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1780 \tTraining Loss: 0.00752133 \tValidation Loss 0.02813298 \tTraining Acuuarcy 64.692% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1781 \tTraining Loss: 0.00771566 \tValidation Loss 0.02733912 \tTraining Acuuarcy 63.811% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 1782 \tTraining Loss: 0.00748855 \tValidation Loss 0.02757070 \tTraining Acuuarcy 64.530% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1783 \tTraining Loss: 0.00776087 \tValidation Loss 0.02696671 \tTraining Acuuarcy 63.349% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1784 \tTraining Loss: 0.00767134 \tValidation Loss 0.02733148 \tTraining Acuuarcy 63.917% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1785 \tTraining Loss: 0.00768268 \tValidation Loss 0.02753344 \tTraining Acuuarcy 63.733% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1786 \tTraining Loss: 0.00750599 \tValidation Loss 0.02849931 \tTraining Acuuarcy 64.681% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1787 \tTraining Loss: 0.00751599 \tValidation Loss 0.02831021 \tTraining Acuuarcy 64.619% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 1788 \tTraining Loss: 0.00767248 \tValidation Loss 0.02769630 \tTraining Acuuarcy 63.978% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1789 \tTraining Loss: 0.00762965 \tValidation Loss 0.02727456 \tTraining Acuuarcy 63.722% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1790 \tTraining Loss: 0.00756809 \tValidation Loss 0.02634608 \tTraining Acuuarcy 64.251% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1791 \tTraining Loss: 0.00765058 \tValidation Loss 0.02642296 \tTraining Acuuarcy 63.911% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1792 \tTraining Loss: 0.00743475 \tValidation Loss 0.02812355 \tTraining Acuuarcy 64.725% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1793 \tTraining Loss: 0.00754681 \tValidation Loss 0.02717297 \tTraining Acuuarcy 64.123% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 1794 \tTraining Loss: 0.00755894 \tValidation Loss 0.02722385 \tTraining Acuuarcy 64.357% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1795 \tTraining Loss: 0.00771274 \tValidation Loss 0.02765196 \tTraining Acuuarcy 63.716% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1796 \tTraining Loss: 0.00768492 \tValidation Loss 0.02654185 \tTraining Acuuarcy 63.638% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1797 \tTraining Loss: 0.00776102 \tValidation Loss 0.02717821 \tTraining Acuuarcy 63.638% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1798 \tTraining Loss: 0.00767244 \tValidation Loss 0.02705638 \tTraining Acuuarcy 63.995% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1799 \tTraining Loss: 0.00760129 \tValidation Loss 0.02793107 \tTraining Acuuarcy 63.867% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1800 \tTraining Loss: 0.00760021 \tValidation Loss 0.02665949 \tTraining Acuuarcy 64.073% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 1801 \tTraining Loss: 0.00759067 \tValidation Loss 0.02687559 \tTraining Acuuarcy 64.430% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1802 \tTraining Loss: 0.00759702 \tValidation Loss 0.02761277 \tTraining Acuuarcy 64.424% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1803 \tTraining Loss: 0.00775446 \tValidation Loss 0.02685392 \tTraining Acuuarcy 63.198% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1804 \tTraining Loss: 0.00757229 \tValidation Loss 0.02725932 \tTraining Acuuarcy 64.552% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1805 \tTraining Loss: 0.00767526 \tValidation Loss 0.02725514 \tTraining Acuuarcy 63.923% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 1806 \tTraining Loss: 0.00761914 \tValidation Loss 0.02749140 \tTraining Acuuarcy 64.212% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1807 \tTraining Loss: 0.00773559 \tValidation Loss 0.02654439 \tTraining Acuuarcy 63.516% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1808 \tTraining Loss: 0.00772682 \tValidation Loss 0.02691833 \tTraining Acuuarcy 63.209% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1809 \tTraining Loss: 0.00760394 \tValidation Loss 0.02730056 \tTraining Acuuarcy 64.441% \tValidation Acuuarcy 17.247%\n",
      "Epoch: 1810 \tTraining Loss: 0.00768144 \tValidation Loss 0.02794475 \tTraining Acuuarcy 64.029% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1811 \tTraining Loss: 0.00759677 \tValidation Loss 0.02719745 \tTraining Acuuarcy 64.419% \tValidation Acuuarcy 16.857%\n",
      "Epoch: 1812 \tTraining Loss: 0.00763160 \tValidation Loss 0.02685035 \tTraining Acuuarcy 64.134% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1813 \tTraining Loss: 0.00769643 \tValidation Loss 0.02744960 \tTraining Acuuarcy 63.884% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1814 \tTraining Loss: 0.00758950 \tValidation Loss 0.02667922 \tTraining Acuuarcy 64.190% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 1815 \tTraining Loss: 0.00759686 \tValidation Loss 0.02695993 \tTraining Acuuarcy 64.296% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 1816 \tTraining Loss: 0.00763289 \tValidation Loss 0.02654855 \tTraining Acuuarcy 63.884% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 1817 \tTraining Loss: 0.00752534 \tValidation Loss 0.02704234 \tTraining Acuuarcy 65.143% \tValidation Acuuarcy 17.024%\n",
      "Epoch: 1818 \tTraining Loss: 0.00753418 \tValidation Loss 0.02772509 \tTraining Acuuarcy 64.647% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1819 \tTraining Loss: 0.00741418 \tValidation Loss 0.02767411 \tTraining Acuuarcy 65.271% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1820 \tTraining Loss: 0.00755959 \tValidation Loss 0.02781479 \tTraining Acuuarcy 64.413% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1821 \tTraining Loss: 0.00758946 \tValidation Loss 0.02725941 \tTraining Acuuarcy 64.151% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1822 \tTraining Loss: 0.00768714 \tValidation Loss 0.02675627 \tTraining Acuuarcy 63.672% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1823 \tTraining Loss: 0.00758040 \tValidation Loss 0.02677399 \tTraining Acuuarcy 64.185% \tValidation Acuuarcy 18.222%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1824 \tTraining Loss: 0.00749183 \tValidation Loss 0.02745487 \tTraining Acuuarcy 64.725% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 1825 \tTraining Loss: 0.00759568 \tValidation Loss 0.02636139 \tTraining Acuuarcy 64.352% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1826 \tTraining Loss: 0.00765581 \tValidation Loss 0.02841733 \tTraining Acuuarcy 63.962% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1827 \tTraining Loss: 0.00755781 \tValidation Loss 0.02811404 \tTraining Acuuarcy 64.263% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1828 \tTraining Loss: 0.00764140 \tValidation Loss 0.02693446 \tTraining Acuuarcy 64.279% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1829 \tTraining Loss: 0.00765757 \tValidation Loss 0.02749006 \tTraining Acuuarcy 63.856% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1830 \tTraining Loss: 0.00754066 \tValidation Loss 0.02688520 \tTraining Acuuarcy 64.435% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1831 \tTraining Loss: 0.00763809 \tValidation Loss 0.02749654 \tTraining Acuuarcy 64.525% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 1832 \tTraining Loss: 0.00768575 \tValidation Loss 0.02703746 \tTraining Acuuarcy 63.226% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 1833 \tTraining Loss: 0.00743672 \tValidation Loss 0.02777505 \tTraining Acuuarcy 64.943% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1834 \tTraining Loss: 0.00770194 \tValidation Loss 0.02719593 \tTraining Acuuarcy 63.811% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1835 \tTraining Loss: 0.00763509 \tValidation Loss 0.02735727 \tTraining Acuuarcy 63.644% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1836 \tTraining Loss: 0.00767985 \tValidation Loss 0.02749233 \tTraining Acuuarcy 63.962% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1837 \tTraining Loss: 0.00759640 \tValidation Loss 0.02755323 \tTraining Acuuarcy 64.380% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1838 \tTraining Loss: 0.00760250 \tValidation Loss 0.02744962 \tTraining Acuuarcy 63.911% \tValidation Acuuarcy 16.996%\n",
      "Epoch: 1839 \tTraining Loss: 0.00749268 \tValidation Loss 0.02797835 \tTraining Acuuarcy 64.402% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1840 \tTraining Loss: 0.00755063 \tValidation Loss 0.02695116 \tTraining Acuuarcy 64.279% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 1841 \tTraining Loss: 0.00759457 \tValidation Loss 0.02714671 \tTraining Acuuarcy 64.363% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 1842 \tTraining Loss: 0.00758475 \tValidation Loss 0.02717276 \tTraining Acuuarcy 64.441% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1843 \tTraining Loss: 0.00761763 \tValidation Loss 0.02684874 \tTraining Acuuarcy 63.705% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 1844 \tTraining Loss: 0.00755655 \tValidation Loss 0.02723446 \tTraining Acuuarcy 64.491% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1845 \tTraining Loss: 0.00760552 \tValidation Loss 0.02769524 \tTraining Acuuarcy 64.357% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1846 \tTraining Loss: 0.00757224 \tValidation Loss 0.02685449 \tTraining Acuuarcy 64.474% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1847 \tTraining Loss: 0.00757052 \tValidation Loss 0.02671303 \tTraining Acuuarcy 64.346% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1848 \tTraining Loss: 0.00765120 \tValidation Loss 0.02638308 \tTraining Acuuarcy 64.157% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 1849 \tTraining Loss: 0.00758520 \tValidation Loss 0.02755253 \tTraining Acuuarcy 64.140% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 1850 \tTraining Loss: 0.00762807 \tValidation Loss 0.02717744 \tTraining Acuuarcy 63.984% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 1851 \tTraining Loss: 0.00763843 \tValidation Loss 0.02711873 \tTraining Acuuarcy 63.677% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 1852 \tTraining Loss: 0.00756705 \tValidation Loss 0.02720438 \tTraining Acuuarcy 64.134% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1853 \tTraining Loss: 0.00770073 \tValidation Loss 0.02659242 \tTraining Acuuarcy 63.833% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 1854 \tTraining Loss: 0.00756799 \tValidation Loss 0.02655869 \tTraining Acuuarcy 64.274% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1855 \tTraining Loss: 0.00742289 \tValidation Loss 0.02760279 \tTraining Acuuarcy 64.781% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1856 \tTraining Loss: 0.00760783 \tValidation Loss 0.02675114 \tTraining Acuuarcy 64.274% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 1857 \tTraining Loss: 0.00750947 \tValidation Loss 0.02689127 \tTraining Acuuarcy 64.586% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 1858 \tTraining Loss: 0.00761266 \tValidation Loss 0.02733833 \tTraining Acuuarcy 64.162% \tValidation Acuuarcy 16.439%\n",
      "Epoch: 1859 \tTraining Loss: 0.00752328 \tValidation Loss 0.02780812 \tTraining Acuuarcy 64.803% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1860 \tTraining Loss: 0.00746482 \tValidation Loss 0.02810187 \tTraining Acuuarcy 64.658% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1861 \tTraining Loss: 0.00776196 \tValidation Loss 0.02704125 \tTraining Acuuarcy 63.315% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 1862 \tTraining Loss: 0.00751984 \tValidation Loss 0.02739540 \tTraining Acuuarcy 64.474% \tValidation Acuuarcy 17.386%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"CUDA is not available. Make sure you have a CUDA-enabled GPU.\")\n",
    "\n",
    "def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        # Train the model  #\n",
    "        net.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optmizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        #validate the model#\n",
    "        net.eval()\n",
    "        for data,labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            val_outputs = net(data)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs,1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss =  validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
    "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
    "\n",
    "    torch.save(net.state_dict(),'deep_emotion-{}-{}-{}.pt'.format(epochs,batchsize,lr))\n",
    "    print(\"===================================Training Finished===================================\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"Configuration of setup and training process\")\n",
    "#     parser.add_argument('-s', '--setup', type=bool, help='setup the dataset for the first time')\n",
    "#     parser.add_argument('-d', '--data', type=str,required= True,\n",
    "#                                help='data folder that contains data files that downloaded from kaggle (train.csv and test.csv)')\n",
    "#     parser.add_argument('-hparams', '--hyperparams', type=bool,\n",
    "#                                help='True when changing the hyperparameters e.g (batch size, LR, num. of epochs)')\n",
    "#     parser.add_argument('-e', '--epochs', type= int, help= 'number of epochs')\n",
    "#     parser.add_argument('-lr', '--learning_rate', type= float, help= 'value of learning rate')\n",
    "#     parser.add_argument('-bs', '--batch_size', type= int, help= 'training/validation batch size')\n",
    "#     parser.add_argument('-t', '--train', type=bool, help='True when training')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if args.setup :\n",
    "#         generate_dataset = Generate_data(args.data)\n",
    "#         generate_dataset.split_test()\n",
    "#         generate_dataset.save_images('train')\n",
    "#         generate_dataset.save_images('test')\n",
    "#         generate_dataset.save_images('val')\n",
    "\n",
    "#     if args.hyperparams:\n",
    "#         epochs = args.epochs\n",
    "#         lr = args.learning_rate\n",
    "#         batchsize = args.batch_size\n",
    "#     else :\n",
    "epochs = 2000\n",
    "lr = 0.0006\n",
    "batchsize = 128\n",
    "\n",
    "#     if args.train:\n",
    "net = Deep_Emotion()\n",
    "net.to(device)\n",
    "print(\"Model archticture: \", net)\n",
    "traincsv_file = 'data'+'/'+'train.csv'\n",
    "validationcsv_file = 'data'+'/'+'val.csv'\n",
    "train_img_dir = 'data'+'/'+'train/'\n",
    "validation_img_dir = 'data'+'/'+'val/'\n",
    "\n",
    "transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
    "validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
    "train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optmizer= optim.Adam(net.parameters(),lr= lr)\n",
    "Train(epochs, train_loader, val_loader, criterion, optmizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90895e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'Speaktrum_by_SOVA_70.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a871c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep_Emotion(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
       "  (localization): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_loc): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net= Deep_Emotion()\n",
    "net.load_state_dict(torch.load('deep_emotion-2000-128-0.0009.pt'))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47aa49bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "Face not detected\n",
      "Face not detected\n",
      "tensor(0, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Define emotions and their corresponding colors\n",
    "emotions = {\n",
    "    0: (\"Angry, take a deep breath\", (0, 0, 255)),\n",
    "    2: (\"Fear, calm down\", (0, 0, 255)),\n",
    "    3: (\"Happy, you are good\", (0, 255, 0)),\n",
    "    4: (\"Sad, relax and meditate\", (255, 0, 0))\n",
    "}\n",
    "\n",
    "# Load the custom logo or watermark image\n",
    "logo = cv2.imread(\"D:\\Anaconda\\Project by me\\Deep-Emotion-master\\logo.png\", cv2.IMREAD_UNCHANGED)\n",
    "logo_height, logo_width = logo.shape[:2]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if prediction.item() in emotions:\n",
    "                emotion, color = emotions[prediction.item()]\n",
    "                status = emotion\n",
    "            else:\n",
    "                status = \"No emotion detected\"\n",
    "                color = (255, 255, 255)\n",
    "\n",
    "            # Draw background rectangle for the emotion text\n",
    "            bg_rect_height = 60\n",
    "            bg_rect_width = int(w * 1.6)  # Adjust the width of the rectangle\n",
    "            bg_rect_x = x - int((bg_rect_width - w) / 2)  # Adjust the x-coordinate of the rectangle\n",
    "            bg_rect_y = y - bg_rect_height\n",
    "            cv2.rectangle(frame, (bg_rect_x, bg_rect_y), (bg_rect_x + bg_rect_width, y), color, -1)\n",
    "\n",
    "\n",
    "            # Draw emotion text\n",
    "            text = f\"Emotion: {status}\"\n",
    "            text_size, _ = cv2.getTextSize(text, font, font_scale, 1)\n",
    "            text_x = x + int((w - text_size[0]) / 2)\n",
    "            text_y = y - bg_rect_height + int((bg_rect_height - text_size[1]) / 2)\n",
    "            cv2.putText(frame, text, (text_x, text_y), font, font_scale, (0, 0, 0),1, cv2.LINE_AA)\n",
    "           \n",
    "        \n",
    "    # Add frame to the whole screen\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    frame_with_frame = cv2.rectangle(frame, (0, 0), (frame_width, frame_height), (255, 255, 255), 40)\n",
    "    \n",
    "    # Add the custom logo or watermark to the frame (top left corner)\n",
    "    logo_resized = cv2.resize(logo, (int(frame_width / 4), int(frame_height / 7)))  # Resize the logo\n",
    "    logo_height, logo_width = logo_resized.shape[:2]\n",
    "    logo_x = 6  # Adjust the x-coordinate of the logo (distance from the left edge)\n",
    "    logo_y = 6  # Adjust the y-coordinate of the logo (distance from the top edge)\n",
    "    alpha_logo = logo_resized[:, :, 3] / 255.0  # Extract alpha channel\n",
    "    alpha_frame = 1.0 - alpha_logo\n",
    "    for c in range(3):\n",
    "        frame[logo_y:logo_y + logo_height, logo_x:logo_x + logo_width, c] = (\n",
    "                alpha_logo * logo_resized[:, :, c] + alpha_frame * frame[logo_y:logo_y + logo_height, logo_x:logo_x + logo_width, c]\n",
    "        )        \n",
    "            \n",
    "            \n",
    "    \n",
    "    # Add frame to the whole screen\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    frame_with_frame = cv2.rectangle(frame, (0, 0), (frame_width, frame_height), (0, 0, 0), 15)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Display the updated frame\n",
    "    cv2.imshow('Speaktrum', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887037d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
