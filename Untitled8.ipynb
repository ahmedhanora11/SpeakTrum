{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8985b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archticture:  Deep_Emotion(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "===================================Start Training===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4277: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4215: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.01434239 \tValidation Loss 0.01477167 \tTraining Acuuarcy 23.693% \tValidation Acuuarcy 24.854%\n",
      "Epoch: 2 \tTraining Loss: 0.01426444 \tValidation Loss 0.01477285 \tTraining Acuuarcy 24.813% \tValidation Acuuarcy 24.742%\n",
      "Epoch: 3 \tTraining Loss: 0.01423561 \tValidation Loss 0.01485570 \tTraining Acuuarcy 24.914% \tValidation Acuuarcy 24.909%\n",
      "Epoch: 4 \tTraining Loss: 0.01422313 \tValidation Loss 0.01484282 \tTraining Acuuarcy 24.964% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 5 \tTraining Loss: 0.01422000 \tValidation Loss 0.01476074 \tTraining Acuuarcy 24.914% \tValidation Acuuarcy 25.021%\n",
      "Epoch: 6 \tTraining Loss: 0.01420103 \tValidation Loss 0.01485341 \tTraining Acuuarcy 25.008% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 7 \tTraining Loss: 0.01419886 \tValidation Loss 0.01477782 \tTraining Acuuarcy 25.064% \tValidation Acuuarcy 24.854%\n",
      "Epoch: 8 \tTraining Loss: 0.01419717 \tValidation Loss 0.01482774 \tTraining Acuuarcy 25.075% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 9 \tTraining Loss: 0.01418847 \tValidation Loss 0.01481776 \tTraining Acuuarcy 25.031% \tValidation Acuuarcy 24.882%\n",
      "Epoch: 10 \tTraining Loss: 0.01417989 \tValidation Loss 0.01484589 \tTraining Acuuarcy 25.237% \tValidation Acuuarcy 24.687%\n",
      "Epoch: 11 \tTraining Loss: 0.01415567 \tValidation Loss 0.01499658 \tTraining Acuuarcy 25.142% \tValidation Acuuarcy 25.077%\n",
      "Epoch: 12 \tTraining Loss: 0.01415834 \tValidation Loss 0.01499885 \tTraining Acuuarcy 25.120% \tValidation Acuuarcy 23.238%\n",
      "Epoch: 13 \tTraining Loss: 0.01415988 \tValidation Loss 0.01492522 \tTraining Acuuarcy 25.209% \tValidation Acuuarcy 24.269%\n",
      "Epoch: 14 \tTraining Loss: 0.01414342 \tValidation Loss 0.01480796 \tTraining Acuuarcy 25.075% \tValidation Acuuarcy 24.714%\n",
      "Epoch: 15 \tTraining Loss: 0.01412477 \tValidation Loss 0.01490747 \tTraining Acuuarcy 24.986% \tValidation Acuuarcy 24.659%\n",
      "Epoch: 16 \tTraining Loss: 0.01412602 \tValidation Loss 0.01496411 \tTraining Acuuarcy 25.421% \tValidation Acuuarcy 23.321%\n",
      "Epoch: 17 \tTraining Loss: 0.01411859 \tValidation Loss 0.01480237 \tTraining Acuuarcy 25.209% \tValidation Acuuarcy 24.492%\n",
      "Epoch: 18 \tTraining Loss: 0.01411573 \tValidation Loss 0.01492034 \tTraining Acuuarcy 25.181% \tValidation Acuuarcy 23.962%\n",
      "Epoch: 19 \tTraining Loss: 0.01407706 \tValidation Loss 0.01502379 \tTraining Acuuarcy 25.226% \tValidation Acuuarcy 23.711%\n",
      "Epoch: 20 \tTraining Loss: 0.01407201 \tValidation Loss 0.01491634 \tTraining Acuuarcy 25.477% \tValidation Acuuarcy 22.541%\n",
      "Epoch: 21 \tTraining Loss: 0.01403299 \tValidation Loss 0.01493521 \tTraining Acuuarcy 25.610% \tValidation Acuuarcy 23.293%\n",
      "Epoch: 22 \tTraining Loss: 0.01402815 \tValidation Loss 0.01495313 \tTraining Acuuarcy 25.605% \tValidation Acuuarcy 23.461%\n",
      "Epoch: 23 \tTraining Loss: 0.01404436 \tValidation Loss 0.01500333 \tTraining Acuuarcy 25.906% \tValidation Acuuarcy 23.266%\n",
      "Epoch: 24 \tTraining Loss: 0.01401655 \tValidation Loss 0.01493704 \tTraining Acuuarcy 25.867% \tValidation Acuuarcy 22.708%\n",
      "Epoch: 25 \tTraining Loss: 0.01398420 \tValidation Loss 0.01492146 \tTraining Acuuarcy 25.861% \tValidation Acuuarcy 23.266%\n",
      "Epoch: 26 \tTraining Loss: 0.01400046 \tValidation Loss 0.01498474 \tTraining Acuuarcy 25.649% \tValidation Acuuarcy 24.129%\n",
      "Epoch: 27 \tTraining Loss: 0.01395910 \tValidation Loss 0.01502013 \tTraining Acuuarcy 26.352% \tValidation Acuuarcy 23.070%\n",
      "Epoch: 28 \tTraining Loss: 0.01395459 \tValidation Loss 0.01508686 \tTraining Acuuarcy 25.989% \tValidation Acuuarcy 23.739%\n",
      "Epoch: 29 \tTraining Loss: 0.01393091 \tValidation Loss 0.01503974 \tTraining Acuuarcy 26.067% \tValidation Acuuarcy 22.541%\n",
      "Epoch: 30 \tTraining Loss: 0.01392647 \tValidation Loss 0.01502168 \tTraining Acuuarcy 26.184% \tValidation Acuuarcy 23.043%\n",
      "Epoch: 31 \tTraining Loss: 0.01390310 \tValidation Loss 0.01516292 \tTraining Acuuarcy 26.452% \tValidation Acuuarcy 22.290%\n",
      "Epoch: 32 \tTraining Loss: 0.01387087 \tValidation Loss 0.01505709 \tTraining Acuuarcy 26.491% \tValidation Acuuarcy 23.851%\n",
      "Epoch: 33 \tTraining Loss: 0.01386807 \tValidation Loss 0.01506006 \tTraining Acuuarcy 26.502% \tValidation Acuuarcy 22.095%\n",
      "Epoch: 34 \tTraining Loss: 0.01382832 \tValidation Loss 0.01513501 \tTraining Acuuarcy 26.653% \tValidation Acuuarcy 22.485%\n",
      "Epoch: 35 \tTraining Loss: 0.01382229 \tValidation Loss 0.01513985 \tTraining Acuuarcy 27.210% \tValidation Acuuarcy 23.238%\n",
      "Epoch: 36 \tTraining Loss: 0.01381277 \tValidation Loss 0.01519319 \tTraining Acuuarcy 26.892% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 37 \tTraining Loss: 0.01376815 \tValidation Loss 0.01524320 \tTraining Acuuarcy 27.360% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 38 \tTraining Loss: 0.01377860 \tValidation Loss 0.01516205 \tTraining Acuuarcy 27.405% \tValidation Acuuarcy 21.928%\n",
      "Epoch: 39 \tTraining Loss: 0.01374616 \tValidation Loss 0.01528151 \tTraining Acuuarcy 27.176% \tValidation Acuuarcy 21.845%\n",
      "Epoch: 40 \tTraining Loss: 0.01370143 \tValidation Loss 0.01530949 \tTraining Acuuarcy 27.829% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 41 \tTraining Loss: 0.01366901 \tValidation Loss 0.01521225 \tTraining Acuuarcy 27.957% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 42 \tTraining Loss: 0.01367305 \tValidation Loss 0.01519086 \tTraining Acuuarcy 28.085% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 43 \tTraining Loss: 0.01363299 \tValidation Loss 0.01534943 \tTraining Acuuarcy 28.486% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 44 \tTraining Loss: 0.01361223 \tValidation Loss 0.01528636 \tTraining Acuuarcy 28.341% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 45 \tTraining Loss: 0.01357302 \tValidation Loss 0.01546116 \tTraining Acuuarcy 28.849% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 46 \tTraining Loss: 0.01355185 \tValidation Loss 0.01527768 \tTraining Acuuarcy 29.077% \tValidation Acuuarcy 22.346%\n",
      "Epoch: 47 \tTraining Loss: 0.01353764 \tValidation Loss 0.01535397 \tTraining Acuuarcy 29.311% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 48 \tTraining Loss: 0.01353178 \tValidation Loss 0.01539844 \tTraining Acuuarcy 28.993% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 49 \tTraining Loss: 0.01347256 \tValidation Loss 0.01539369 \tTraining Acuuarcy 29.423% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 50 \tTraining Loss: 0.01344737 \tValidation Loss 0.01559329 \tTraining Acuuarcy 29.701% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 51 \tTraining Loss: 0.01342641 \tValidation Loss 0.01571952 \tTraining Acuuarcy 29.824% \tValidation Acuuarcy 22.123%\n",
      "Epoch: 52 \tTraining Loss: 0.01332501 \tValidation Loss 0.01564265 \tTraining Acuuarcy 30.398% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 53 \tTraining Loss: 0.01336175 \tValidation Loss 0.01551959 \tTraining Acuuarcy 30.476% \tValidation Acuuarcy 22.067%\n",
      "Epoch: 54 \tTraining Loss: 0.01329019 \tValidation Loss 0.01574581 \tTraining Acuuarcy 31.050% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 55 \tTraining Loss: 0.01329537 \tValidation Loss 0.01558068 \tTraining Acuuarcy 30.922% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 56 \tTraining Loss: 0.01325019 \tValidation Loss 0.01572596 \tTraining Acuuarcy 31.111% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 57 \tTraining Loss: 0.01326627 \tValidation Loss 0.01575524 \tTraining Acuuarcy 31.223% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 58 \tTraining Loss: 0.01319513 \tValidation Loss 0.01642612 \tTraining Acuuarcy 31.139% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 59 \tTraining Loss: 0.01315486 \tValidation Loss 0.01571539 \tTraining Acuuarcy 31.708% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 60 \tTraining Loss: 0.01316912 \tValidation Loss 0.01593049 \tTraining Acuuarcy 31.613% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 61 \tTraining Loss: 0.01311971 \tValidation Loss 0.01585761 \tTraining Acuuarcy 31.864% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 62 \tTraining Loss: 0.01310123 \tValidation Loss 0.01602657 \tTraining Acuuarcy 32.293% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 63 \tTraining Loss: 0.01306099 \tValidation Loss 0.01581956 \tTraining Acuuarcy 32.611% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 64 \tTraining Loss: 0.01301897 \tValidation Loss 0.01621361 \tTraining Acuuarcy 32.867% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 65 \tTraining Loss: 0.01298655 \tValidation Loss 0.01590584 \tTraining Acuuarcy 33.547% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 66 \tTraining Loss: 0.01294865 \tValidation Loss 0.01615371 \tTraining Acuuarcy 33.101% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 67 \tTraining Loss: 0.01295450 \tValidation Loss 0.01605247 \tTraining Acuuarcy 33.486% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 68 \tTraining Loss: 0.01291049 \tValidation Loss 0.01599804 \tTraining Acuuarcy 33.625% \tValidation Acuuarcy 19.811%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 \tTraining Loss: 0.01288929 \tValidation Loss 0.01615039 \tTraining Acuuarcy 33.709% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 70 \tTraining Loss: 0.01284300 \tValidation Loss 0.01616588 \tTraining Acuuarcy 33.753% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 71 \tTraining Loss: 0.01283347 \tValidation Loss 0.01630144 \tTraining Acuuarcy 33.959% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 72 \tTraining Loss: 0.01283085 \tValidation Loss 0.01619857 \tTraining Acuuarcy 34.049% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 73 \tTraining Loss: 0.01278778 \tValidation Loss 0.01646622 \tTraining Acuuarcy 34.361% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 74 \tTraining Loss: 0.01275657 \tValidation Loss 0.01623159 \tTraining Acuuarcy 34.233% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 75 \tTraining Loss: 0.01278171 \tValidation Loss 0.01623350 \tTraining Acuuarcy 34.054% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 76 \tTraining Loss: 0.01277722 \tValidation Loss 0.01627964 \tTraining Acuuarcy 34.188% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 77 \tTraining Loss: 0.01274441 \tValidation Loss 0.01659414 \tTraining Acuuarcy 34.706% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 78 \tTraining Loss: 0.01267930 \tValidation Loss 0.01647915 \tTraining Acuuarcy 34.940% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 79 \tTraining Loss: 0.01267462 \tValidation Loss 0.01628538 \tTraining Acuuarcy 34.985% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 80 \tTraining Loss: 0.01267959 \tValidation Loss 0.01650854 \tTraining Acuuarcy 34.807% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 81 \tTraining Loss: 0.01260307 \tValidation Loss 0.01638551 \tTraining Acuuarcy 35.988% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 82 \tTraining Loss: 0.01259874 \tValidation Loss 0.01680994 \tTraining Acuuarcy 35.682% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 83 \tTraining Loss: 0.01258322 \tValidation Loss 0.01669809 \tTraining Acuuarcy 35.827% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 84 \tTraining Loss: 0.01263008 \tValidation Loss 0.01633809 \tTraining Acuuarcy 35.520% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 85 \tTraining Loss: 0.01258631 \tValidation Loss 0.01658818 \tTraining Acuuarcy 36.083% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 86 \tTraining Loss: 0.01252134 \tValidation Loss 0.01658490 \tTraining Acuuarcy 36.144% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 87 \tTraining Loss: 0.01249876 \tValidation Loss 0.01664281 \tTraining Acuuarcy 36.272% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 88 \tTraining Loss: 0.01245555 \tValidation Loss 0.01654261 \tTraining Acuuarcy 36.763% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 89 \tTraining Loss: 0.01245645 \tValidation Loss 0.01672641 \tTraining Acuuarcy 36.289% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 90 \tTraining Loss: 0.01241900 \tValidation Loss 0.01682272 \tTraining Acuuarcy 36.752% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 91 \tTraining Loss: 0.01246438 \tValidation Loss 0.01649116 \tTraining Acuuarcy 36.033% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 92 \tTraining Loss: 0.01244727 \tValidation Loss 0.01689514 \tTraining Acuuarcy 36.534% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 93 \tTraining Loss: 0.01239191 \tValidation Loss 0.01690328 \tTraining Acuuarcy 37.147% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 94 \tTraining Loss: 0.01241431 \tValidation Loss 0.01683081 \tTraining Acuuarcy 36.997% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 95 \tTraining Loss: 0.01242523 \tValidation Loss 0.01695097 \tTraining Acuuarcy 36.529% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 96 \tTraining Loss: 0.01239018 \tValidation Loss 0.01673156 \tTraining Acuuarcy 37.382% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 97 \tTraining Loss: 0.01234703 \tValidation Loss 0.01668913 \tTraining Acuuarcy 37.125% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 98 \tTraining Loss: 0.01233487 \tValidation Loss 0.01679282 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 99 \tTraining Loss: 0.01236917 \tValidation Loss 0.01675523 \tTraining Acuuarcy 37.209% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 100 \tTraining Loss: 0.01227581 \tValidation Loss 0.01742196 \tTraining Acuuarcy 37.671% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 101 \tTraining Loss: 0.01230455 \tValidation Loss 0.01703101 \tTraining Acuuarcy 37.716% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 102 \tTraining Loss: 0.01233631 \tValidation Loss 0.01691248 \tTraining Acuuarcy 37.448% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 103 \tTraining Loss: 0.01229152 \tValidation Loss 0.01681068 \tTraining Acuuarcy 37.738% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 104 \tTraining Loss: 0.01229461 \tValidation Loss 0.01702075 \tTraining Acuuarcy 37.727% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 105 \tTraining Loss: 0.01219758 \tValidation Loss 0.01717133 \tTraining Acuuarcy 38.262% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 106 \tTraining Loss: 0.01223855 \tValidation Loss 0.01713633 \tTraining Acuuarcy 37.744% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 107 \tTraining Loss: 0.01220418 \tValidation Loss 0.01710669 \tTraining Acuuarcy 38.396% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 108 \tTraining Loss: 0.01227221 \tValidation Loss 0.01727560 \tTraining Acuuarcy 37.967% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 109 \tTraining Loss: 0.01219758 \tValidation Loss 0.01737194 \tTraining Acuuarcy 38.151% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 110 \tTraining Loss: 0.01219934 \tValidation Loss 0.01728027 \tTraining Acuuarcy 38.714% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 111 \tTraining Loss: 0.01220361 \tValidation Loss 0.01687999 \tTraining Acuuarcy 38.089% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 112 \tTraining Loss: 0.01220527 \tValidation Loss 0.01727430 \tTraining Acuuarcy 38.245% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 113 \tTraining Loss: 0.01214359 \tValidation Loss 0.01725806 \tTraining Acuuarcy 38.318% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 114 \tTraining Loss: 0.01211861 \tValidation Loss 0.01725879 \tTraining Acuuarcy 38.530% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 115 \tTraining Loss: 0.01215793 \tValidation Loss 0.01739539 \tTraining Acuuarcy 38.708% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 116 \tTraining Loss: 0.01218999 \tValidation Loss 0.01724574 \tTraining Acuuarcy 38.742% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 117 \tTraining Loss: 0.01211721 \tValidation Loss 0.01713463 \tTraining Acuuarcy 38.747% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 118 \tTraining Loss: 0.01213720 \tValidation Loss 0.01728451 \tTraining Acuuarcy 38.541% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 119 \tTraining Loss: 0.01211388 \tValidation Loss 0.01727985 \tTraining Acuuarcy 38.881% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 120 \tTraining Loss: 0.01213039 \tValidation Loss 0.01743618 \tTraining Acuuarcy 38.346% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 121 \tTraining Loss: 0.01210470 \tValidation Loss 0.01739703 \tTraining Acuuarcy 38.742% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 122 \tTraining Loss: 0.01211071 \tValidation Loss 0.01718089 \tTraining Acuuarcy 38.663% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 123 \tTraining Loss: 0.01203904 \tValidation Loss 0.01734855 \tTraining Acuuarcy 39.148% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 124 \tTraining Loss: 0.01206534 \tValidation Loss 0.01750130 \tTraining Acuuarcy 38.680% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 125 \tTraining Loss: 0.01207201 \tValidation Loss 0.01717572 \tTraining Acuuarcy 39.494% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 126 \tTraining Loss: 0.01203085 \tValidation Loss 0.01757492 \tTraining Acuuarcy 39.689% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 127 \tTraining Loss: 0.01202236 \tValidation Loss 0.01761220 \tTraining Acuuarcy 39.483% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 128 \tTraining Loss: 0.01198909 \tValidation Loss 0.01747087 \tTraining Acuuarcy 39.466% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 129 \tTraining Loss: 0.01197881 \tValidation Loss 0.01734761 \tTraining Acuuarcy 39.996% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 130 \tTraining Loss: 0.01203652 \tValidation Loss 0.01749227 \tTraining Acuuarcy 39.845% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 131 \tTraining Loss: 0.01195708 \tValidation Loss 0.01733145 \tTraining Acuuarcy 40.380% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 132 \tTraining Loss: 0.01198618 \tValidation Loss 0.01716121 \tTraining Acuuarcy 39.700% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 133 \tTraining Loss: 0.01195541 \tValidation Loss 0.01755221 \tTraining Acuuarcy 39.477% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 134 \tTraining Loss: 0.01202090 \tValidation Loss 0.01771922 \tTraining Acuuarcy 39.589% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 135 \tTraining Loss: 0.01202315 \tValidation Loss 0.01734165 \tTraining Acuuarcy 39.617% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 136 \tTraining Loss: 0.01194232 \tValidation Loss 0.01751902 \tTraining Acuuarcy 40.375% \tValidation Acuuarcy 19.699%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137 \tTraining Loss: 0.01189171 \tValidation Loss 0.01777796 \tTraining Acuuarcy 40.241% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 138 \tTraining Loss: 0.01194806 \tValidation Loss 0.01760658 \tTraining Acuuarcy 39.851% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 139 \tTraining Loss: 0.01195893 \tValidation Loss 0.01770244 \tTraining Acuuarcy 39.410% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 140 \tTraining Loss: 0.01189453 \tValidation Loss 0.01755328 \tTraining Acuuarcy 40.269% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 141 \tTraining Loss: 0.01192066 \tValidation Loss 0.01743640 \tTraining Acuuarcy 40.007% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 142 \tTraining Loss: 0.01193663 \tValidation Loss 0.01788429 \tTraining Acuuarcy 40.018% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 143 \tTraining Loss: 0.01188673 \tValidation Loss 0.01768414 \tTraining Acuuarcy 40.670% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 144 \tTraining Loss: 0.01190130 \tValidation Loss 0.01765370 \tTraining Acuuarcy 40.046% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 145 \tTraining Loss: 0.01188908 \tValidation Loss 0.01776262 \tTraining Acuuarcy 40.336% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 146 \tTraining Loss: 0.01186183 \tValidation Loss 0.01800639 \tTraining Acuuarcy 40.664% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 147 \tTraining Loss: 0.01186535 \tValidation Loss 0.01751669 \tTraining Acuuarcy 40.274% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 148 \tTraining Loss: 0.01182257 \tValidation Loss 0.01762090 \tTraining Acuuarcy 40.402% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 149 \tTraining Loss: 0.01188536 \tValidation Loss 0.01747739 \tTraining Acuuarcy 40.324% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 150 \tTraining Loss: 0.01187666 \tValidation Loss 0.01783588 \tTraining Acuuarcy 40.213% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 151 \tTraining Loss: 0.01185483 \tValidation Loss 0.01793164 \tTraining Acuuarcy 40.263% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 152 \tTraining Loss: 0.01183990 \tValidation Loss 0.01780639 \tTraining Acuuarcy 40.235% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 153 \tTraining Loss: 0.01182479 \tValidation Loss 0.01761118 \tTraining Acuuarcy 40.480% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 154 \tTraining Loss: 0.01180503 \tValidation Loss 0.01771049 \tTraining Acuuarcy 40.854% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 155 \tTraining Loss: 0.01186897 \tValidation Loss 0.01759845 \tTraining Acuuarcy 40.241% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 156 \tTraining Loss: 0.01178368 \tValidation Loss 0.01801329 \tTraining Acuuarcy 41.417% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 157 \tTraining Loss: 0.01179646 \tValidation Loss 0.01786015 \tTraining Acuuarcy 40.770% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 158 \tTraining Loss: 0.01179856 \tValidation Loss 0.01765513 \tTraining Acuuarcy 40.804% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 159 \tTraining Loss: 0.01173915 \tValidation Loss 0.01791307 \tTraining Acuuarcy 40.999% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 160 \tTraining Loss: 0.01174117 \tValidation Loss 0.01778495 \tTraining Acuuarcy 41.584% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 161 \tTraining Loss: 0.01184924 \tValidation Loss 0.01772357 \tTraining Acuuarcy 40.692% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 162 \tTraining Loss: 0.01178292 \tValidation Loss 0.01791188 \tTraining Acuuarcy 40.731% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 163 \tTraining Loss: 0.01174049 \tValidation Loss 0.01798666 \tTraining Acuuarcy 40.898% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 164 \tTraining Loss: 0.01181652 \tValidation Loss 0.01764570 \tTraining Acuuarcy 40.726% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 165 \tTraining Loss: 0.01177341 \tValidation Loss 0.01760047 \tTraining Acuuarcy 40.804% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 166 \tTraining Loss: 0.01175046 \tValidation Loss 0.01765810 \tTraining Acuuarcy 41.105% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 167 \tTraining Loss: 0.01178100 \tValidation Loss 0.01765005 \tTraining Acuuarcy 40.854% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 168 \tTraining Loss: 0.01178017 \tValidation Loss 0.01788905 \tTraining Acuuarcy 41.004% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 169 \tTraining Loss: 0.01178697 \tValidation Loss 0.01777681 \tTraining Acuuarcy 41.277% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 170 \tTraining Loss: 0.01167710 \tValidation Loss 0.01810008 \tTraining Acuuarcy 41.294% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 171 \tTraining Loss: 0.01172422 \tValidation Loss 0.01802954 \tTraining Acuuarcy 41.110% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 172 \tTraining Loss: 0.01174830 \tValidation Loss 0.01779642 \tTraining Acuuarcy 41.172% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 173 \tTraining Loss: 0.01172262 \tValidation Loss 0.01813409 \tTraining Acuuarcy 41.690% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 174 \tTraining Loss: 0.01172396 \tValidation Loss 0.01801086 \tTraining Acuuarcy 41.629% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 175 \tTraining Loss: 0.01169600 \tValidation Loss 0.01750233 \tTraining Acuuarcy 41.394% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 176 \tTraining Loss: 0.01168954 \tValidation Loss 0.01777243 \tTraining Acuuarcy 41.673% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 177 \tTraining Loss: 0.01169829 \tValidation Loss 0.01780229 \tTraining Acuuarcy 41.796% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 178 \tTraining Loss: 0.01168476 \tValidation Loss 0.01781271 \tTraining Acuuarcy 41.450% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 179 \tTraining Loss: 0.01166447 \tValidation Loss 0.01783159 \tTraining Acuuarcy 41.500% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 180 \tTraining Loss: 0.01166155 \tValidation Loss 0.01814932 \tTraining Acuuarcy 41.478% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 181 \tTraining Loss: 0.01169987 \tValidation Loss 0.01793656 \tTraining Acuuarcy 41.333% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 182 \tTraining Loss: 0.01165043 \tValidation Loss 0.01777773 \tTraining Acuuarcy 41.556% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 183 \tTraining Loss: 0.01169328 \tValidation Loss 0.01837368 \tTraining Acuuarcy 41.718% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 184 \tTraining Loss: 0.01158505 \tValidation Loss 0.01782240 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 185 \tTraining Loss: 0.01164994 \tValidation Loss 0.01782303 \tTraining Acuuarcy 41.623% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 186 \tTraining Loss: 0.01164407 \tValidation Loss 0.01845796 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 187 \tTraining Loss: 0.01163687 \tValidation Loss 0.01777854 \tTraining Acuuarcy 42.236% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 188 \tTraining Loss: 0.01160941 \tValidation Loss 0.01796426 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 189 \tTraining Loss: 0.01167056 \tValidation Loss 0.01774143 \tTraining Acuuarcy 42.214% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 190 \tTraining Loss: 0.01158679 \tValidation Loss 0.01814610 \tTraining Acuuarcy 42.392% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 191 \tTraining Loss: 0.01159331 \tValidation Loss 0.01829436 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 192 \tTraining Loss: 0.01155474 \tValidation Loss 0.01805469 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 193 \tTraining Loss: 0.01162242 \tValidation Loss 0.01790103 \tTraining Acuuarcy 42.102% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 194 \tTraining Loss: 0.01160005 \tValidation Loss 0.01792040 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 195 \tTraining Loss: 0.01167075 \tValidation Loss 0.01787580 \tTraining Acuuarcy 41.863% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 196 \tTraining Loss: 0.01157608 \tValidation Loss 0.01795308 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 197 \tTraining Loss: 0.01153088 \tValidation Loss 0.01786608 \tTraining Acuuarcy 42.158% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 198 \tTraining Loss: 0.01158212 \tValidation Loss 0.01806937 \tTraining Acuuarcy 42.180% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 199 \tTraining Loss: 0.01156443 \tValidation Loss 0.01796598 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 200 \tTraining Loss: 0.01156138 \tValidation Loss 0.01811559 \tTraining Acuuarcy 42.476% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 201 \tTraining Loss: 0.01157917 \tValidation Loss 0.01830543 \tTraining Acuuarcy 42.476% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 202 \tTraining Loss: 0.01163765 \tValidation Loss 0.01856738 \tTraining Acuuarcy 41.695% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 203 \tTraining Loss: 0.01156921 \tValidation Loss 0.01797396 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 204 \tTraining Loss: 0.01152611 \tValidation Loss 0.01814898 \tTraining Acuuarcy 42.598% \tValidation Acuuarcy 18.612%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205 \tTraining Loss: 0.01157606 \tValidation Loss 0.01832162 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 206 \tTraining Loss: 0.01157484 \tValidation Loss 0.01790762 \tTraining Acuuarcy 42.403% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 207 \tTraining Loss: 0.01159391 \tValidation Loss 0.01799803 \tTraining Acuuarcy 41.924% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 208 \tTraining Loss: 0.01146103 \tValidation Loss 0.01790224 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 209 \tTraining Loss: 0.01153431 \tValidation Loss 0.01837702 \tTraining Acuuarcy 42.342% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 210 \tTraining Loss: 0.01156236 \tValidation Loss 0.01804131 \tTraining Acuuarcy 42.147% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 211 \tTraining Loss: 0.01143129 \tValidation Loss 0.01818089 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 212 \tTraining Loss: 0.01151163 \tValidation Loss 0.01784898 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 213 \tTraining Loss: 0.01150520 \tValidation Loss 0.01846259 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 214 \tTraining Loss: 0.01156457 \tValidation Loss 0.01851746 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 215 \tTraining Loss: 0.01150297 \tValidation Loss 0.01813636 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 216 \tTraining Loss: 0.01153000 \tValidation Loss 0.01830302 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 217 \tTraining Loss: 0.01146092 \tValidation Loss 0.01829856 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 218 \tTraining Loss: 0.01148887 \tValidation Loss 0.01839123 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 219 \tTraining Loss: 0.01151754 \tValidation Loss 0.01831965 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 220 \tTraining Loss: 0.01151793 \tValidation Loss 0.01835977 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 221 \tTraining Loss: 0.01146762 \tValidation Loss 0.01823427 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 222 \tTraining Loss: 0.01142441 \tValidation Loss 0.01848367 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 223 \tTraining Loss: 0.01150657 \tValidation Loss 0.01799341 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 224 \tTraining Loss: 0.01144150 \tValidation Loss 0.01813160 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 225 \tTraining Loss: 0.01149744 \tValidation Loss 0.01801256 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 226 \tTraining Loss: 0.01149054 \tValidation Loss 0.01833944 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 227 \tTraining Loss: 0.01141135 \tValidation Loss 0.01826171 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 228 \tTraining Loss: 0.01145348 \tValidation Loss 0.01829968 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 229 \tTraining Loss: 0.01154515 \tValidation Loss 0.01806374 \tTraining Acuuarcy 42.392% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 230 \tTraining Loss: 0.01141176 \tValidation Loss 0.01814714 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 231 \tTraining Loss: 0.01145754 \tValidation Loss 0.01809961 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 232 \tTraining Loss: 0.01146410 \tValidation Loss 0.01815972 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 233 \tTraining Loss: 0.01150126 \tValidation Loss 0.01820956 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 234 \tTraining Loss: 0.01141531 \tValidation Loss 0.01818424 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 235 \tTraining Loss: 0.01137567 \tValidation Loss 0.01840567 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 236 \tTraining Loss: 0.01140677 \tValidation Loss 0.01834975 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 237 \tTraining Loss: 0.01143638 \tValidation Loss 0.01857815 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 238 \tTraining Loss: 0.01141926 \tValidation Loss 0.01877202 \tTraining Acuuarcy 43.016% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 239 \tTraining Loss: 0.01137757 \tValidation Loss 0.01839829 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 240 \tTraining Loss: 0.01142734 \tValidation Loss 0.01863966 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 241 \tTraining Loss: 0.01138407 \tValidation Loss 0.01820549 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 242 \tTraining Loss: 0.01133970 \tValidation Loss 0.01858431 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 243 \tTraining Loss: 0.01147786 \tValidation Loss 0.01822987 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 244 \tTraining Loss: 0.01136494 \tValidation Loss 0.01835490 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 245 \tTraining Loss: 0.01137934 \tValidation Loss 0.01823098 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 246 \tTraining Loss: 0.01138735 \tValidation Loss 0.01861960 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 247 \tTraining Loss: 0.01128840 \tValidation Loss 0.01839287 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 248 \tTraining Loss: 0.01136075 \tValidation Loss 0.01819955 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 249 \tTraining Loss: 0.01138570 \tValidation Loss 0.01859056 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 250 \tTraining Loss: 0.01139653 \tValidation Loss 0.01853163 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 251 \tTraining Loss: 0.01135498 \tValidation Loss 0.01854914 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 252 \tTraining Loss: 0.01132429 \tValidation Loss 0.01825825 \tTraining Acuuarcy 43.908% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 253 \tTraining Loss: 0.01136139 \tValidation Loss 0.01815841 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 254 \tTraining Loss: 0.01133558 \tValidation Loss 0.01857849 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 255 \tTraining Loss: 0.01139526 \tValidation Loss 0.01800022 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 256 \tTraining Loss: 0.01140697 \tValidation Loss 0.01865626 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 257 \tTraining Loss: 0.01134890 \tValidation Loss 0.01824705 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 258 \tTraining Loss: 0.01140769 \tValidation Loss 0.01837138 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 259 \tTraining Loss: 0.01140825 \tValidation Loss 0.01850805 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 260 \tTraining Loss: 0.01135682 \tValidation Loss 0.01877267 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 261 \tTraining Loss: 0.01133691 \tValidation Loss 0.01863958 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 262 \tTraining Loss: 0.01131096 \tValidation Loss 0.01821894 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 263 \tTraining Loss: 0.01132048 \tValidation Loss 0.01846941 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 264 \tTraining Loss: 0.01133720 \tValidation Loss 0.01854523 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 265 \tTraining Loss: 0.01134787 \tValidation Loss 0.01855444 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 266 \tTraining Loss: 0.01130433 \tValidation Loss 0.01827677 \tTraining Acuuarcy 44.354% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 267 \tTraining Loss: 0.01130588 \tValidation Loss 0.01881866 \tTraining Acuuarcy 44.198% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 268 \tTraining Loss: 0.01122793 \tValidation Loss 0.01832267 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 269 \tTraining Loss: 0.01132787 \tValidation Loss 0.01856902 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 270 \tTraining Loss: 0.01133105 \tValidation Loss 0.01859603 \tTraining Acuuarcy 44.137% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 271 \tTraining Loss: 0.01135222 \tValidation Loss 0.01827482 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 272 \tTraining Loss: 0.01132108 \tValidation Loss 0.01880341 \tTraining Acuuarcy 43.992% \tValidation Acuuarcy 19.504%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273 \tTraining Loss: 0.01136142 \tValidation Loss 0.01823808 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 274 \tTraining Loss: 0.01126401 \tValidation Loss 0.01851093 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 275 \tTraining Loss: 0.01123280 \tValidation Loss 0.01900932 \tTraining Acuuarcy 44.605% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 276 \tTraining Loss: 0.01128146 \tValidation Loss 0.01865201 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 277 \tTraining Loss: 0.01127232 \tValidation Loss 0.01877111 \tTraining Acuuarcy 44.142% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 278 \tTraining Loss: 0.01130625 \tValidation Loss 0.01816634 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 279 \tTraining Loss: 0.01124650 \tValidation Loss 0.01865871 \tTraining Acuuarcy 44.287% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 280 \tTraining Loss: 0.01124773 \tValidation Loss 0.01872406 \tTraining Acuuarcy 44.471% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 281 \tTraining Loss: 0.01134029 \tValidation Loss 0.01823913 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 282 \tTraining Loss: 0.01129859 \tValidation Loss 0.01855018 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 283 \tTraining Loss: 0.01130644 \tValidation Loss 0.01831648 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 284 \tTraining Loss: 0.01123397 \tValidation Loss 0.01854094 \tTraining Acuuarcy 44.460% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 285 \tTraining Loss: 0.01130124 \tValidation Loss 0.01849286 \tTraining Acuuarcy 44.153% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 286 \tTraining Loss: 0.01125019 \tValidation Loss 0.01856087 \tTraining Acuuarcy 44.399% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 287 \tTraining Loss: 0.01121971 \tValidation Loss 0.01838778 \tTraining Acuuarcy 44.666% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 288 \tTraining Loss: 0.01127052 \tValidation Loss 0.01859477 \tTraining Acuuarcy 44.025% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 289 \tTraining Loss: 0.01128497 \tValidation Loss 0.01849286 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 290 \tTraining Loss: 0.01126703 \tValidation Loss 0.01829278 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 291 \tTraining Loss: 0.01121420 \tValidation Loss 0.01825176 \tTraining Acuuarcy 44.376% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 292 \tTraining Loss: 0.01129844 \tValidation Loss 0.01853375 \tTraining Acuuarcy 44.293% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 293 \tTraining Loss: 0.01124017 \tValidation Loss 0.01849971 \tTraining Acuuarcy 44.482% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 294 \tTraining Loss: 0.01122524 \tValidation Loss 0.01876219 \tTraining Acuuarcy 44.794% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 295 \tTraining Loss: 0.01122768 \tValidation Loss 0.01848060 \tTraining Acuuarcy 44.733% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 296 \tTraining Loss: 0.01124681 \tValidation Loss 0.01822311 \tTraining Acuuarcy 44.399% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 297 \tTraining Loss: 0.01120685 \tValidation Loss 0.01852918 \tTraining Acuuarcy 44.270% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 298 \tTraining Loss: 0.01121270 \tValidation Loss 0.01836185 \tTraining Acuuarcy 44.694% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 299 \tTraining Loss: 0.01126720 \tValidation Loss 0.01845623 \tTraining Acuuarcy 44.036% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 300 \tTraining Loss: 0.01124089 \tValidation Loss 0.01875233 \tTraining Acuuarcy 44.243% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 301 \tTraining Loss: 0.01129192 \tValidation Loss 0.01867892 \tTraining Acuuarcy 44.209% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 302 \tTraining Loss: 0.01120501 \tValidation Loss 0.01958935 \tTraining Acuuarcy 44.549% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 303 \tTraining Loss: 0.01121080 \tValidation Loss 0.01865021 \tTraining Acuuarcy 44.638% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 304 \tTraining Loss: 0.01121783 \tValidation Loss 0.01907021 \tTraining Acuuarcy 44.889% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 305 \tTraining Loss: 0.01123348 \tValidation Loss 0.01842715 \tTraining Acuuarcy 44.209% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 306 \tTraining Loss: 0.01112709 \tValidation Loss 0.01827034 \tTraining Acuuarcy 45.346% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 307 \tTraining Loss: 0.01123324 \tValidation Loss 0.01855307 \tTraining Acuuarcy 44.822% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 308 \tTraining Loss: 0.01122209 \tValidation Loss 0.01844389 \tTraining Acuuarcy 44.192% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 309 \tTraining Loss: 0.01126334 \tValidation Loss 0.01849830 \tTraining Acuuarcy 44.170% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 310 \tTraining Loss: 0.01117510 \tValidation Loss 0.01875914 \tTraining Acuuarcy 44.817% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 311 \tTraining Loss: 0.01112199 \tValidation Loss 0.01870283 \tTraining Acuuarcy 45.056% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 312 \tTraining Loss: 0.01117042 \tValidation Loss 0.01866450 \tTraining Acuuarcy 44.967% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 313 \tTraining Loss: 0.01118424 \tValidation Loss 0.01869559 \tTraining Acuuarcy 44.365% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 314 \tTraining Loss: 0.01118788 \tValidation Loss 0.01856081 \tTraining Acuuarcy 44.594% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 315 \tTraining Loss: 0.01121874 \tValidation Loss 0.01939814 \tTraining Acuuarcy 44.426% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 316 \tTraining Loss: 0.01121551 \tValidation Loss 0.01897052 \tTraining Acuuarcy 44.761% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 317 \tTraining Loss: 0.01117544 \tValidation Loss 0.01857566 \tTraining Acuuarcy 44.850% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 318 \tTraining Loss: 0.01120752 \tValidation Loss 0.01875632 \tTraining Acuuarcy 45.006% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 319 \tTraining Loss: 0.01117513 \tValidation Loss 0.01891957 \tTraining Acuuarcy 44.800% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 320 \tTraining Loss: 0.01120953 \tValidation Loss 0.01826233 \tTraining Acuuarcy 44.315% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 321 \tTraining Loss: 0.01121629 \tValidation Loss 0.01835988 \tTraining Acuuarcy 44.727% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 322 \tTraining Loss: 0.01117688 \tValidation Loss 0.01871643 \tTraining Acuuarcy 44.426% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 323 \tTraining Loss: 0.01117149 \tValidation Loss 0.01862442 \tTraining Acuuarcy 44.783% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 324 \tTraining Loss: 0.01124192 \tValidation Loss 0.01855963 \tTraining Acuuarcy 44.415% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 325 \tTraining Loss: 0.01119539 \tValidation Loss 0.01907839 \tTraining Acuuarcy 44.282% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 326 \tTraining Loss: 0.01111887 \tValidation Loss 0.01860740 \tTraining Acuuarcy 45.106% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 327 \tTraining Loss: 0.01115906 \tValidation Loss 0.01855638 \tTraining Acuuarcy 44.783% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 328 \tTraining Loss: 0.01114907 \tValidation Loss 0.01857055 \tTraining Acuuarcy 45.123% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 329 \tTraining Loss: 0.01112296 \tValidation Loss 0.01872328 \tTraining Acuuarcy 44.694% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 330 \tTraining Loss: 0.01110803 \tValidation Loss 0.01850036 \tTraining Acuuarcy 45.218% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 331 \tTraining Loss: 0.01116766 \tValidation Loss 0.01868816 \tTraining Acuuarcy 44.750% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 332 \tTraining Loss: 0.01115607 \tValidation Loss 0.01863414 \tTraining Acuuarcy 45.380% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 333 \tTraining Loss: 0.01114243 \tValidation Loss 0.01887689 \tTraining Acuuarcy 44.789% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 334 \tTraining Loss: 0.01117402 \tValidation Loss 0.01875728 \tTraining Acuuarcy 44.844% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 335 \tTraining Loss: 0.01109475 \tValidation Loss 0.01917661 \tTraining Acuuarcy 45.212% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 336 \tTraining Loss: 0.01116088 \tValidation Loss 0.01887852 \tTraining Acuuarcy 44.884% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 337 \tTraining Loss: 0.01114326 \tValidation Loss 0.01859977 \tTraining Acuuarcy 44.655% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 338 \tTraining Loss: 0.01115557 \tValidation Loss 0.01878280 \tTraining Acuuarcy 44.449% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 339 \tTraining Loss: 0.01113484 \tValidation Loss 0.01824895 \tTraining Acuuarcy 44.950% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 340 \tTraining Loss: 0.01110669 \tValidation Loss 0.01883040 \tTraining Acuuarcy 45.173% \tValidation Acuuarcy 19.476%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 341 \tTraining Loss: 0.01117300 \tValidation Loss 0.01867572 \tTraining Acuuarcy 45.179% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 342 \tTraining Loss: 0.01110235 \tValidation Loss 0.01878890 \tTraining Acuuarcy 45.357% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 343 \tTraining Loss: 0.01119114 \tValidation Loss 0.01849151 \tTraining Acuuarcy 44.789% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 344 \tTraining Loss: 0.01108677 \tValidation Loss 0.01835387 \tTraining Acuuarcy 45.140% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 345 \tTraining Loss: 0.01108973 \tValidation Loss 0.01908164 \tTraining Acuuarcy 45.285% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 346 \tTraining Loss: 0.01106288 \tValidation Loss 0.01900851 \tTraining Acuuarcy 45.853% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 347 \tTraining Loss: 0.01109544 \tValidation Loss 0.01897831 \tTraining Acuuarcy 44.727% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 348 \tTraining Loss: 0.01116050 \tValidation Loss 0.01918055 \tTraining Acuuarcy 44.555% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 349 \tTraining Loss: 0.01107148 \tValidation Loss 0.01870693 \tTraining Acuuarcy 45.491% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 350 \tTraining Loss: 0.01111463 \tValidation Loss 0.01908952 \tTraining Acuuarcy 44.872% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 351 \tTraining Loss: 0.01112383 \tValidation Loss 0.01886176 \tTraining Acuuarcy 45.196% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 352 \tTraining Loss: 0.01106501 \tValidation Loss 0.01856914 \tTraining Acuuarcy 44.878% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 353 \tTraining Loss: 0.01108128 \tValidation Loss 0.01893452 \tTraining Acuuarcy 45.519% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 354 \tTraining Loss: 0.01120807 \tValidation Loss 0.01888470 \tTraining Acuuarcy 45.040% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 355 \tTraining Loss: 0.01108324 \tValidation Loss 0.01854481 \tTraining Acuuarcy 45.424% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 356 \tTraining Loss: 0.01113878 \tValidation Loss 0.01874617 \tTraining Acuuarcy 44.588% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 357 \tTraining Loss: 0.01114340 \tValidation Loss 0.01903371 \tTraining Acuuarcy 45.329% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 358 \tTraining Loss: 0.01106781 \tValidation Loss 0.01892167 \tTraining Acuuarcy 44.995% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 359 \tTraining Loss: 0.01109232 \tValidation Loss 0.01852754 \tTraining Acuuarcy 45.051% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 360 \tTraining Loss: 0.01095942 \tValidation Loss 0.01952936 \tTraining Acuuarcy 45.452% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 361 \tTraining Loss: 0.01110426 \tValidation Loss 0.01840145 \tTraining Acuuarcy 44.811% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 362 \tTraining Loss: 0.01107499 \tValidation Loss 0.01876213 \tTraining Acuuarcy 45.168% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 363 \tTraining Loss: 0.01111522 \tValidation Loss 0.01860342 \tTraining Acuuarcy 45.151% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 364 \tTraining Loss: 0.01107782 \tValidation Loss 0.01889406 \tTraining Acuuarcy 45.541% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 365 \tTraining Loss: 0.01108972 \tValidation Loss 0.01868643 \tTraining Acuuarcy 45.508% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 366 \tTraining Loss: 0.01104289 \tValidation Loss 0.01899199 \tTraining Acuuarcy 45.569% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 367 \tTraining Loss: 0.01100586 \tValidation Loss 0.01941586 \tTraining Acuuarcy 45.786% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 368 \tTraining Loss: 0.01109249 \tValidation Loss 0.01919833 \tTraining Acuuarcy 45.274% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 369 \tTraining Loss: 0.01109911 \tValidation Loss 0.01909023 \tTraining Acuuarcy 45.095% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 370 \tTraining Loss: 0.01109917 \tValidation Loss 0.01893457 \tTraining Acuuarcy 44.566% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 371 \tTraining Loss: 0.01103610 \tValidation Loss 0.01885896 \tTraining Acuuarcy 45.296% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 372 \tTraining Loss: 0.01103566 \tValidation Loss 0.01898286 \tTraining Acuuarcy 45.842% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 373 \tTraining Loss: 0.01102789 \tValidation Loss 0.01905858 \tTraining Acuuarcy 45.686% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 374 \tTraining Loss: 0.01104493 \tValidation Loss 0.01895980 \tTraining Acuuarcy 45.296% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 375 \tTraining Loss: 0.01101220 \tValidation Loss 0.01841537 \tTraining Acuuarcy 45.837% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 376 \tTraining Loss: 0.01103508 \tValidation Loss 0.01872088 \tTraining Acuuarcy 45.263% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 377 \tTraining Loss: 0.01108398 \tValidation Loss 0.01903893 \tTraining Acuuarcy 44.900% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 378 \tTraining Loss: 0.01109564 \tValidation Loss 0.01888902 \tTraining Acuuarcy 44.817% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 379 \tTraining Loss: 0.01104277 \tValidation Loss 0.01951534 \tTraining Acuuarcy 45.720% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 380 \tTraining Loss: 0.01104218 \tValidation Loss 0.01923845 \tTraining Acuuarcy 44.895% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 381 \tTraining Loss: 0.01103966 \tValidation Loss 0.01896262 \tTraining Acuuarcy 45.391% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 382 \tTraining Loss: 0.01109788 \tValidation Loss 0.01861340 \tTraining Acuuarcy 45.329% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 383 \tTraining Loss: 0.01095978 \tValidation Loss 0.01891800 \tTraining Acuuarcy 46.048% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 384 \tTraining Loss: 0.01103519 \tValidation Loss 0.01892785 \tTraining Acuuarcy 45.541% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 385 \tTraining Loss: 0.01096780 \tValidation Loss 0.01900916 \tTraining Acuuarcy 46.021% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 386 \tTraining Loss: 0.01103360 \tValidation Loss 0.01906575 \tTraining Acuuarcy 45.697% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 387 \tTraining Loss: 0.01099290 \tValidation Loss 0.01909996 \tTraining Acuuarcy 45.686% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 388 \tTraining Loss: 0.01107438 \tValidation Loss 0.01917010 \tTraining Acuuarcy 45.948% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 389 \tTraining Loss: 0.01106539 \tValidation Loss 0.01875794 \tTraining Acuuarcy 45.341% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 390 \tTraining Loss: 0.01101510 \tValidation Loss 0.01900090 \tTraining Acuuarcy 45.502% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 391 \tTraining Loss: 0.01108503 \tValidation Loss 0.01853641 \tTraining Acuuarcy 45.380% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 392 \tTraining Loss: 0.01093414 \tValidation Loss 0.01844552 \tTraining Acuuarcy 46.478% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 393 \tTraining Loss: 0.01097677 \tValidation Loss 0.01934740 \tTraining Acuuarcy 45.597% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 394 \tTraining Loss: 0.01102083 \tValidation Loss 0.01901015 \tTraining Acuuarcy 45.569% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 395 \tTraining Loss: 0.01108119 \tValidation Loss 0.01864686 \tTraining Acuuarcy 45.173% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 396 \tTraining Loss: 0.01103451 \tValidation Loss 0.01923667 \tTraining Acuuarcy 44.917% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 397 \tTraining Loss: 0.01100534 \tValidation Loss 0.01880352 \tTraining Acuuarcy 45.993% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 398 \tTraining Loss: 0.01106676 \tValidation Loss 0.01851807 \tTraining Acuuarcy 45.385% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 399 \tTraining Loss: 0.01106125 \tValidation Loss 0.01901367 \tTraining Acuuarcy 45.424% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 400 \tTraining Loss: 0.01104294 \tValidation Loss 0.01859375 \tTraining Acuuarcy 45.602% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 401 \tTraining Loss: 0.01101126 \tValidation Loss 0.01914911 \tTraining Acuuarcy 45.876% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 402 \tTraining Loss: 0.01103392 \tValidation Loss 0.01882885 \tTraining Acuuarcy 45.837% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 403 \tTraining Loss: 0.01098954 \tValidation Loss 0.01929514 \tTraining Acuuarcy 45.831% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 404 \tTraining Loss: 0.01095700 \tValidation Loss 0.01850667 \tTraining Acuuarcy 45.809% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 405 \tTraining Loss: 0.01103626 \tValidation Loss 0.01891511 \tTraining Acuuarcy 45.591% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 406 \tTraining Loss: 0.01107227 \tValidation Loss 0.01849123 \tTraining Acuuarcy 45.329% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 407 \tTraining Loss: 0.01103218 \tValidation Loss 0.01867509 \tTraining Acuuarcy 45.446% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 408 \tTraining Loss: 0.01103283 \tValidation Loss 0.01873047 \tTraining Acuuarcy 45.547% \tValidation Acuuarcy 18.919%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 409 \tTraining Loss: 0.01099192 \tValidation Loss 0.01855077 \tTraining Acuuarcy 45.892% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 410 \tTraining Loss: 0.01096107 \tValidation Loss 0.01917923 \tTraining Acuuarcy 45.898% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 411 \tTraining Loss: 0.01095473 \tValidation Loss 0.01937273 \tTraining Acuuarcy 46.082% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 412 \tTraining Loss: 0.01096551 \tValidation Loss 0.01906049 \tTraining Acuuarcy 46.082% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 413 \tTraining Loss: 0.01096688 \tValidation Loss 0.01908977 \tTraining Acuuarcy 45.820% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 414 \tTraining Loss: 0.01091595 \tValidation Loss 0.01907256 \tTraining Acuuarcy 46.009% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 415 \tTraining Loss: 0.01100245 \tValidation Loss 0.01923163 \tTraining Acuuarcy 45.675% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 416 \tTraining Loss: 0.01099315 \tValidation Loss 0.01907301 \tTraining Acuuarcy 45.513% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 417 \tTraining Loss: 0.01095730 \tValidation Loss 0.01893963 \tTraining Acuuarcy 45.636% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 418 \tTraining Loss: 0.01101008 \tValidation Loss 0.01902865 \tTraining Acuuarcy 45.870% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 419 \tTraining Loss: 0.01097392 \tValidation Loss 0.01890164 \tTraining Acuuarcy 46.110% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 420 \tTraining Loss: 0.01090975 \tValidation Loss 0.01904846 \tTraining Acuuarcy 46.110% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 421 \tTraining Loss: 0.01101884 \tValidation Loss 0.01882994 \tTraining Acuuarcy 46.076% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 422 \tTraining Loss: 0.01095275 \tValidation Loss 0.01893618 \tTraining Acuuarcy 46.243% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 423 \tTraining Loss: 0.01095165 \tValidation Loss 0.01890058 \tTraining Acuuarcy 46.160% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 424 \tTraining Loss: 0.01091986 \tValidation Loss 0.01877264 \tTraining Acuuarcy 46.160% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 425 \tTraining Loss: 0.01097818 \tValidation Loss 0.01887542 \tTraining Acuuarcy 46.138% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 426 \tTraining Loss: 0.01088420 \tValidation Loss 0.01932216 \tTraining Acuuarcy 46.700% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 427 \tTraining Loss: 0.01101238 \tValidation Loss 0.01877396 \tTraining Acuuarcy 45.636% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 428 \tTraining Loss: 0.01101696 \tValidation Loss 0.01905834 \tTraining Acuuarcy 45.825% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 429 \tTraining Loss: 0.01103553 \tValidation Loss 0.01916971 \tTraining Acuuarcy 45.775% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 430 \tTraining Loss: 0.01094129 \tValidation Loss 0.01857180 \tTraining Acuuarcy 46.082% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 431 \tTraining Loss: 0.01101475 \tValidation Loss 0.01875223 \tTraining Acuuarcy 45.441% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 432 \tTraining Loss: 0.01100073 \tValidation Loss 0.01942408 \tTraining Acuuarcy 45.987% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 433 \tTraining Loss: 0.01100119 \tValidation Loss 0.01914649 \tTraining Acuuarcy 45.842% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 434 \tTraining Loss: 0.01098105 \tValidation Loss 0.01901637 \tTraining Acuuarcy 45.881% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 435 \tTraining Loss: 0.01096401 \tValidation Loss 0.01902496 \tTraining Acuuarcy 46.021% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 436 \tTraining Loss: 0.01096153 \tValidation Loss 0.01921614 \tTraining Acuuarcy 46.138% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 437 \tTraining Loss: 0.01095087 \tValidation Loss 0.01943926 \tTraining Acuuarcy 45.842% \tValidation Acuuarcy 16.662%\n",
      "Epoch: 438 \tTraining Loss: 0.01097615 \tValidation Loss 0.01867569 \tTraining Acuuarcy 45.697% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 439 \tTraining Loss: 0.01097881 \tValidation Loss 0.01866558 \tTraining Acuuarcy 45.759% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 440 \tTraining Loss: 0.01096865 \tValidation Loss 0.01869947 \tTraining Acuuarcy 46.556% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 441 \tTraining Loss: 0.01099069 \tValidation Loss 0.01860853 \tTraining Acuuarcy 46.255% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 442 \tTraining Loss: 0.01094275 \tValidation Loss 0.01858517 \tTraining Acuuarcy 46.015% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 443 \tTraining Loss: 0.01091441 \tValidation Loss 0.01892427 \tTraining Acuuarcy 46.110% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 444 \tTraining Loss: 0.01094636 \tValidation Loss 0.01886255 \tTraining Acuuarcy 45.948% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 445 \tTraining Loss: 0.01098515 \tValidation Loss 0.01922249 \tTraining Acuuarcy 46.238% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 446 \tTraining Loss: 0.01095205 \tValidation Loss 0.01902409 \tTraining Acuuarcy 45.903% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 447 \tTraining Loss: 0.01097046 \tValidation Loss 0.01931226 \tTraining Acuuarcy 46.004% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 448 \tTraining Loss: 0.01097320 \tValidation Loss 0.01862947 \tTraining Acuuarcy 46.015% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 449 \tTraining Loss: 0.01100734 \tValidation Loss 0.01900091 \tTraining Acuuarcy 45.898% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 450 \tTraining Loss: 0.01085325 \tValidation Loss 0.01926501 \tTraining Acuuarcy 46.857% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 451 \tTraining Loss: 0.01095512 \tValidation Loss 0.01919683 \tTraining Acuuarcy 46.650% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 452 \tTraining Loss: 0.01095310 \tValidation Loss 0.01918218 \tTraining Acuuarcy 46.065% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 453 \tTraining Loss: 0.01092546 \tValidation Loss 0.01901212 \tTraining Acuuarcy 46.004% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 454 \tTraining Loss: 0.01095640 \tValidation Loss 0.01910000 \tTraining Acuuarcy 45.987% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 455 \tTraining Loss: 0.01092537 \tValidation Loss 0.01908378 \tTraining Acuuarcy 46.433% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 456 \tTraining Loss: 0.01092356 \tValidation Loss 0.01899936 \tTraining Acuuarcy 46.076% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 457 \tTraining Loss: 0.01098181 \tValidation Loss 0.01901290 \tTraining Acuuarcy 45.552% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 458 \tTraining Loss: 0.01088627 \tValidation Loss 0.01928792 \tTraining Acuuarcy 46.383% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 459 \tTraining Loss: 0.01088462 \tValidation Loss 0.01982740 \tTraining Acuuarcy 46.177% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 460 \tTraining Loss: 0.01095376 \tValidation Loss 0.01911544 \tTraining Acuuarcy 45.859% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 461 \tTraining Loss: 0.01097464 \tValidation Loss 0.01943803 \tTraining Acuuarcy 46.093% \tValidation Acuuarcy 17.414%\n",
      "Epoch: 462 \tTraining Loss: 0.01085736 \tValidation Loss 0.01906502 \tTraining Acuuarcy 46.734% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 463 \tTraining Loss: 0.01090924 \tValidation Loss 0.01931805 \tTraining Acuuarcy 46.260% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 464 \tTraining Loss: 0.01092559 \tValidation Loss 0.01882946 \tTraining Acuuarcy 46.255% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 465 \tTraining Loss: 0.01096425 \tValidation Loss 0.01888369 \tTraining Acuuarcy 45.614% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 466 \tTraining Loss: 0.01097225 \tValidation Loss 0.01890628 \tTraining Acuuarcy 45.658% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 467 \tTraining Loss: 0.01091265 \tValidation Loss 0.01938144 \tTraining Acuuarcy 46.700% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 468 \tTraining Loss: 0.01094147 \tValidation Loss 0.01893240 \tTraining Acuuarcy 46.349% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 469 \tTraining Loss: 0.01080258 \tValidation Loss 0.01930811 \tTraining Acuuarcy 46.868% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 470 \tTraining Loss: 0.01093408 \tValidation Loss 0.01934286 \tTraining Acuuarcy 46.260% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 471 \tTraining Loss: 0.01087561 \tValidation Loss 0.01949954 \tTraining Acuuarcy 46.684% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 472 \tTraining Loss: 0.01088016 \tValidation Loss 0.01917121 \tTraining Acuuarcy 46.940% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 473 \tTraining Loss: 0.01093529 \tValidation Loss 0.01846504 \tTraining Acuuarcy 46.048% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 474 \tTraining Loss: 0.01091012 \tValidation Loss 0.01898028 \tTraining Acuuarcy 46.372% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 475 \tTraining Loss: 0.01085948 \tValidation Loss 0.01906603 \tTraining Acuuarcy 46.383% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 476 \tTraining Loss: 0.01090698 \tValidation Loss 0.01946086 \tTraining Acuuarcy 46.840% \tValidation Acuuarcy 18.306%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477 \tTraining Loss: 0.01087277 \tValidation Loss 0.01899285 \tTraining Acuuarcy 46.411% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 478 \tTraining Loss: 0.01088640 \tValidation Loss 0.01914006 \tTraining Acuuarcy 45.959% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 479 \tTraining Loss: 0.01087552 \tValidation Loss 0.01890095 \tTraining Acuuarcy 46.333% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 480 \tTraining Loss: 0.01091508 \tValidation Loss 0.01897337 \tTraining Acuuarcy 45.954% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 481 \tTraining Loss: 0.01087423 \tValidation Loss 0.01921058 \tTraining Acuuarcy 46.132% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 482 \tTraining Loss: 0.01087821 \tValidation Loss 0.01910938 \tTraining Acuuarcy 46.355% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 483 \tTraining Loss: 0.01083295 \tValidation Loss 0.01932383 \tTraining Acuuarcy 46.544% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 484 \tTraining Loss: 0.01087344 \tValidation Loss 0.01921573 \tTraining Acuuarcy 46.528% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 485 \tTraining Loss: 0.01083597 \tValidation Loss 0.01915006 \tTraining Acuuarcy 46.427% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 486 \tTraining Loss: 0.01083145 \tValidation Loss 0.01885438 \tTraining Acuuarcy 46.901% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 487 \tTraining Loss: 0.01086197 \tValidation Loss 0.01934551 \tTraining Acuuarcy 46.957% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 488 \tTraining Loss: 0.01094441 \tValidation Loss 0.01905130 \tTraining Acuuarcy 45.892% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 489 \tTraining Loss: 0.01087607 \tValidation Loss 0.01912324 \tTraining Acuuarcy 46.288% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 490 \tTraining Loss: 0.01092806 \tValidation Loss 0.01890207 \tTraining Acuuarcy 46.349% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 491 \tTraining Loss: 0.01094860 \tValidation Loss 0.01917587 \tTraining Acuuarcy 46.138% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 492 \tTraining Loss: 0.01092786 \tValidation Loss 0.01913858 \tTraining Acuuarcy 46.310% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 493 \tTraining Loss: 0.01086007 \tValidation Loss 0.01899717 \tTraining Acuuarcy 46.990% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 494 \tTraining Loss: 0.01091206 \tValidation Loss 0.01936274 \tTraining Acuuarcy 46.360% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 495 \tTraining Loss: 0.01085861 \tValidation Loss 0.01932238 \tTraining Acuuarcy 46.873% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 496 \tTraining Loss: 0.01086516 \tValidation Loss 0.01912417 \tTraining Acuuarcy 47.158% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 497 \tTraining Loss: 0.01082221 \tValidation Loss 0.01929254 \tTraining Acuuarcy 46.845% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 498 \tTraining Loss: 0.01092490 \tValidation Loss 0.01897664 \tTraining Acuuarcy 46.043% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 499 \tTraining Loss: 0.01086256 \tValidation Loss 0.01880224 \tTraining Acuuarcy 46.896% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 500 \tTraining Loss: 0.01080963 \tValidation Loss 0.01932019 \tTraining Acuuarcy 46.773% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 501 \tTraining Loss: 0.01081718 \tValidation Loss 0.01891009 \tTraining Acuuarcy 46.812% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 502 \tTraining Loss: 0.01091373 \tValidation Loss 0.01879694 \tTraining Acuuarcy 46.082% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 503 \tTraining Loss: 0.01089583 \tValidation Loss 0.01997108 \tTraining Acuuarcy 46.388% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 504 \tTraining Loss: 0.01079492 \tValidation Loss 0.01939361 \tTraining Acuuarcy 46.572% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 505 \tTraining Loss: 0.01082972 \tValidation Loss 0.01950520 \tTraining Acuuarcy 46.611% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 506 \tTraining Loss: 0.01084417 \tValidation Loss 0.01956646 \tTraining Acuuarcy 46.823% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 507 \tTraining Loss: 0.01079419 \tValidation Loss 0.01921934 \tTraining Acuuarcy 47.302% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 508 \tTraining Loss: 0.01087185 \tValidation Loss 0.01968110 \tTraining Acuuarcy 46.478% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 509 \tTraining Loss: 0.01086651 \tValidation Loss 0.01903018 \tTraining Acuuarcy 46.617% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 510 \tTraining Loss: 0.01078789 \tValidation Loss 0.01907893 \tTraining Acuuarcy 47.180% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 511 \tTraining Loss: 0.01097658 \tValidation Loss 0.01918786 \tTraining Acuuarcy 45.591% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 512 \tTraining Loss: 0.01081553 \tValidation Loss 0.01924601 \tTraining Acuuarcy 46.806% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 513 \tTraining Loss: 0.01079803 \tValidation Loss 0.01935909 \tTraining Acuuarcy 47.035% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 514 \tTraining Loss: 0.01087332 \tValidation Loss 0.01893758 \tTraining Acuuarcy 46.377% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 515 \tTraining Loss: 0.01088326 \tValidation Loss 0.01912728 \tTraining Acuuarcy 46.400% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 516 \tTraining Loss: 0.01086241 \tValidation Loss 0.01901097 \tTraining Acuuarcy 46.188% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 517 \tTraining Loss: 0.01080552 \tValidation Loss 0.01931219 \tTraining Acuuarcy 46.962% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 518 \tTraining Loss: 0.01079136 \tValidation Loss 0.01949889 \tTraining Acuuarcy 46.946% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 519 \tTraining Loss: 0.01084756 \tValidation Loss 0.01983206 \tTraining Acuuarcy 46.689% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 520 \tTraining Loss: 0.01087440 \tValidation Loss 0.01939924 \tTraining Acuuarcy 46.851% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 521 \tTraining Loss: 0.01083270 \tValidation Loss 0.01925111 \tTraining Acuuarcy 47.113% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 522 \tTraining Loss: 0.01084596 \tValidation Loss 0.01932875 \tTraining Acuuarcy 46.639% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 523 \tTraining Loss: 0.01087434 \tValidation Loss 0.01918153 \tTraining Acuuarcy 46.433% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 524 \tTraining Loss: 0.01082786 \tValidation Loss 0.01878575 \tTraining Acuuarcy 46.556% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 525 \tTraining Loss: 0.01095441 \tValidation Loss 0.01875281 \tTraining Acuuarcy 46.060% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 526 \tTraining Loss: 0.01083108 \tValidation Loss 0.01924424 \tTraining Acuuarcy 46.918% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 527 \tTraining Loss: 0.01081261 \tValidation Loss 0.01908669 \tTraining Acuuarcy 46.801% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 528 \tTraining Loss: 0.01085245 \tValidation Loss 0.01908786 \tTraining Acuuarcy 46.712% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 529 \tTraining Loss: 0.01091903 \tValidation Loss 0.01893284 \tTraining Acuuarcy 46.305% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 530 \tTraining Loss: 0.01085646 \tValidation Loss 0.01897178 \tTraining Acuuarcy 46.745% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 531 \tTraining Loss: 0.01084469 \tValidation Loss 0.01891956 \tTraining Acuuarcy 46.634% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 532 \tTraining Loss: 0.01088095 \tValidation Loss 0.01897906 \tTraining Acuuarcy 46.165% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 533 \tTraining Loss: 0.01076598 \tValidation Loss 0.01918412 \tTraining Acuuarcy 46.923% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 534 \tTraining Loss: 0.01083886 \tValidation Loss 0.01921645 \tTraining Acuuarcy 46.466% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 535 \tTraining Loss: 0.01082793 \tValidation Loss 0.01918115 \tTraining Acuuarcy 46.890% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 536 \tTraining Loss: 0.01077535 \tValidation Loss 0.01917373 \tTraining Acuuarcy 46.901% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 537 \tTraining Loss: 0.01082289 \tValidation Loss 0.01928725 \tTraining Acuuarcy 47.419% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 538 \tTraining Loss: 0.01089680 \tValidation Loss 0.01910091 \tTraining Acuuarcy 46.695% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 539 \tTraining Loss: 0.01077569 \tValidation Loss 0.01939865 \tTraining Acuuarcy 47.046% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 540 \tTraining Loss: 0.01081733 \tValidation Loss 0.01907552 \tTraining Acuuarcy 47.252% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 541 \tTraining Loss: 0.01082409 \tValidation Loss 0.01934334 \tTraining Acuuarcy 47.074% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 542 \tTraining Loss: 0.01088839 \tValidation Loss 0.01903690 \tTraining Acuuarcy 46.762% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 543 \tTraining Loss: 0.01084309 \tValidation Loss 0.01884406 \tTraining Acuuarcy 47.007% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 544 \tTraining Loss: 0.01076128 \tValidation Loss 0.01977472 \tTraining Acuuarcy 47.336% \tValidation Acuuarcy 17.498%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 545 \tTraining Loss: 0.01076961 \tValidation Loss 0.01955666 \tTraining Acuuarcy 47.174% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 546 \tTraining Loss: 0.01084788 \tValidation Loss 0.01912453 \tTraining Acuuarcy 46.884% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 547 \tTraining Loss: 0.01076193 \tValidation Loss 0.01941112 \tTraining Acuuarcy 47.503% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 548 \tTraining Loss: 0.01079662 \tValidation Loss 0.01932903 \tTraining Acuuarcy 47.297% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 549 \tTraining Loss: 0.01084393 \tValidation Loss 0.01921526 \tTraining Acuuarcy 46.784% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 550 \tTraining Loss: 0.01078325 \tValidation Loss 0.01899786 \tTraining Acuuarcy 46.940% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 551 \tTraining Loss: 0.01081010 \tValidation Loss 0.01933681 \tTraining Acuuarcy 46.717% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 552 \tTraining Loss: 0.01084779 \tValidation Loss 0.01938087 \tTraining Acuuarcy 46.974% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 553 \tTraining Loss: 0.01077961 \tValidation Loss 0.01932810 \tTraining Acuuarcy 46.728% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 554 \tTraining Loss: 0.01089464 \tValidation Loss 0.01934853 \tTraining Acuuarcy 46.232% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 555 \tTraining Loss: 0.01073620 \tValidation Loss 0.01973979 \tTraining Acuuarcy 47.325% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 556 \tTraining Loss: 0.01080086 \tValidation Loss 0.01912103 \tTraining Acuuarcy 47.029% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 557 \tTraining Loss: 0.01082318 \tValidation Loss 0.01931738 \tTraining Acuuarcy 46.617% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 558 \tTraining Loss: 0.01082905 \tValidation Loss 0.01920885 \tTraining Acuuarcy 46.550% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 559 \tTraining Loss: 0.01080567 \tValidation Loss 0.01928723 \tTraining Acuuarcy 46.461% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 560 \tTraining Loss: 0.01075440 \tValidation Loss 0.01932286 \tTraining Acuuarcy 47.035% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 561 \tTraining Loss: 0.01073900 \tValidation Loss 0.01897690 \tTraining Acuuarcy 47.375% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 562 \tTraining Loss: 0.01076918 \tValidation Loss 0.01935912 \tTraining Acuuarcy 46.985% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 563 \tTraining Loss: 0.01076095 \tValidation Loss 0.01937064 \tTraining Acuuarcy 47.314% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 564 \tTraining Loss: 0.01083282 \tValidation Loss 0.01920364 \tTraining Acuuarcy 46.439% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 565 \tTraining Loss: 0.01082162 \tValidation Loss 0.01938045 \tTraining Acuuarcy 47.001% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 566 \tTraining Loss: 0.01071903 \tValidation Loss 0.01944130 \tTraining Acuuarcy 47.481% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 567 \tTraining Loss: 0.01079636 \tValidation Loss 0.01921147 \tTraining Acuuarcy 47.386% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 568 \tTraining Loss: 0.01086359 \tValidation Loss 0.01940825 \tTraining Acuuarcy 46.611% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 569 \tTraining Loss: 0.01083797 \tValidation Loss 0.01945321 \tTraining Acuuarcy 46.834% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 570 \tTraining Loss: 0.01077809 \tValidation Loss 0.01918794 \tTraining Acuuarcy 47.224% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 571 \tTraining Loss: 0.01082632 \tValidation Loss 0.01939519 \tTraining Acuuarcy 46.873% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 572 \tTraining Loss: 0.01084207 \tValidation Loss 0.01946441 \tTraining Acuuarcy 46.739% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 573 \tTraining Loss: 0.01078988 \tValidation Loss 0.01984564 \tTraining Acuuarcy 47.414% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 574 \tTraining Loss: 0.01079123 \tValidation Loss 0.01935403 \tTraining Acuuarcy 46.829% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 575 \tTraining Loss: 0.01074042 \tValidation Loss 0.01914002 \tTraining Acuuarcy 47.107% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 576 \tTraining Loss: 0.01076080 \tValidation Loss 0.01916228 \tTraining Acuuarcy 47.341% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 577 \tTraining Loss: 0.01075161 \tValidation Loss 0.01947854 \tTraining Acuuarcy 47.431% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 578 \tTraining Loss: 0.01079005 \tValidation Loss 0.01906263 \tTraining Acuuarcy 47.358% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 579 \tTraining Loss: 0.01073314 \tValidation Loss 0.01917387 \tTraining Acuuarcy 47.297% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 580 \tTraining Loss: 0.01066998 \tValidation Loss 0.01911247 \tTraining Acuuarcy 47.375% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 581 \tTraining Loss: 0.01077863 \tValidation Loss 0.01940115 \tTraining Acuuarcy 46.818% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 582 \tTraining Loss: 0.01072326 \tValidation Loss 0.01948938 \tTraining Acuuarcy 47.208% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 583 \tTraining Loss: 0.01073811 \tValidation Loss 0.01932142 \tTraining Acuuarcy 47.035% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 584 \tTraining Loss: 0.01073006 \tValidation Loss 0.01949292 \tTraining Acuuarcy 47.325% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 585 \tTraining Loss: 0.01076037 \tValidation Loss 0.01939464 \tTraining Acuuarcy 47.102% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 586 \tTraining Loss: 0.01077433 \tValidation Loss 0.01873443 \tTraining Acuuarcy 46.801% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 587 \tTraining Loss: 0.01075128 \tValidation Loss 0.01943096 \tTraining Acuuarcy 47.347% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 588 \tTraining Loss: 0.01079102 \tValidation Loss 0.01951807 \tTraining Acuuarcy 46.611% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 589 \tTraining Loss: 0.01076322 \tValidation Loss 0.01946380 \tTraining Acuuarcy 47.325% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 590 \tTraining Loss: 0.01076601 \tValidation Loss 0.02015576 \tTraining Acuuarcy 46.990% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 591 \tTraining Loss: 0.01084112 \tValidation Loss 0.01931551 \tTraining Acuuarcy 46.773% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 592 \tTraining Loss: 0.01080928 \tValidation Loss 0.01918904 \tTraining Acuuarcy 46.773% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 593 \tTraining Loss: 0.01079837 \tValidation Loss 0.01905281 \tTraining Acuuarcy 46.862% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 594 \tTraining Loss: 0.01069751 \tValidation Loss 0.01910757 \tTraining Acuuarcy 47.141% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 595 \tTraining Loss: 0.01073296 \tValidation Loss 0.01949061 \tTraining Acuuarcy 47.431% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 596 \tTraining Loss: 0.01074363 \tValidation Loss 0.01926361 \tTraining Acuuarcy 47.236% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 597 \tTraining Loss: 0.01080361 \tValidation Loss 0.01927783 \tTraining Acuuarcy 47.046% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 598 \tTraining Loss: 0.01080719 \tValidation Loss 0.01905105 \tTraining Acuuarcy 46.862% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 599 \tTraining Loss: 0.01067923 \tValidation Loss 0.01928666 \tTraining Acuuarcy 47.704% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 600 \tTraining Loss: 0.01079567 \tValidation Loss 0.01950999 \tTraining Acuuarcy 47.130% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 601 \tTraining Loss: 0.01061499 \tValidation Loss 0.01946612 \tTraining Acuuarcy 47.821% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 602 \tTraining Loss: 0.01076256 \tValidation Loss 0.01972852 \tTraining Acuuarcy 47.146% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 603 \tTraining Loss: 0.01070552 \tValidation Loss 0.01919742 \tTraining Acuuarcy 47.275% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 604 \tTraining Loss: 0.01073082 \tValidation Loss 0.01940023 \tTraining Acuuarcy 47.130% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 605 \tTraining Loss: 0.01083181 \tValidation Loss 0.01915449 \tTraining Acuuarcy 46.706% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 606 \tTraining Loss: 0.01076558 \tValidation Loss 0.01931576 \tTraining Acuuarcy 47.358% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 607 \tTraining Loss: 0.01072783 \tValidation Loss 0.01941448 \tTraining Acuuarcy 47.620% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 608 \tTraining Loss: 0.01070161 \tValidation Loss 0.01933673 \tTraining Acuuarcy 47.960% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 609 \tTraining Loss: 0.01071056 \tValidation Loss 0.01951022 \tTraining Acuuarcy 47.537% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 610 \tTraining Loss: 0.01079697 \tValidation Loss 0.01903967 \tTraining Acuuarcy 46.990% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 611 \tTraining Loss: 0.01075888 \tValidation Loss 0.01930246 \tTraining Acuuarcy 46.879% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 612 \tTraining Loss: 0.01076497 \tValidation Loss 0.01931992 \tTraining Acuuarcy 47.297% \tValidation Acuuarcy 18.752%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 613 \tTraining Loss: 0.01076108 \tValidation Loss 0.01906734 \tTraining Acuuarcy 47.269% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 614 \tTraining Loss: 0.01069507 \tValidation Loss 0.01939937 \tTraining Acuuarcy 47.615% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 615 \tTraining Loss: 0.01074278 \tValidation Loss 0.01963635 \tTraining Acuuarcy 47.174% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 616 \tTraining Loss: 0.01077451 \tValidation Loss 0.01918268 \tTraining Acuuarcy 46.901% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 617 \tTraining Loss: 0.01069868 \tValidation Loss 0.01953503 \tTraining Acuuarcy 47.280% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 618 \tTraining Loss: 0.01079869 \tValidation Loss 0.01935827 \tTraining Acuuarcy 47.113% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 619 \tTraining Loss: 0.01076704 \tValidation Loss 0.01935539 \tTraining Acuuarcy 47.163% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 620 \tTraining Loss: 0.01071491 \tValidation Loss 0.01949786 \tTraining Acuuarcy 47.447% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 621 \tTraining Loss: 0.01075858 \tValidation Loss 0.01935556 \tTraining Acuuarcy 46.951% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 622 \tTraining Loss: 0.01067658 \tValidation Loss 0.01918172 \tTraining Acuuarcy 48.228% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 623 \tTraining Loss: 0.01067517 \tValidation Loss 0.01907386 \tTraining Acuuarcy 47.497% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 624 \tTraining Loss: 0.01073070 \tValidation Loss 0.01934869 \tTraining Acuuarcy 47.436% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 625 \tTraining Loss: 0.01078768 \tValidation Loss 0.01961987 \tTraining Acuuarcy 46.985% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 626 \tTraining Loss: 0.01080546 \tValidation Loss 0.01913810 \tTraining Acuuarcy 46.812% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 627 \tTraining Loss: 0.01068714 \tValidation Loss 0.01914470 \tTraining Acuuarcy 47.174% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 628 \tTraining Loss: 0.01073734 \tValidation Loss 0.01943190 \tTraining Acuuarcy 47.230% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 629 \tTraining Loss: 0.01070538 \tValidation Loss 0.01905711 \tTraining Acuuarcy 47.709% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 630 \tTraining Loss: 0.01074393 \tValidation Loss 0.01931897 \tTraining Acuuarcy 47.336% \tValidation Acuuarcy 17.136%\n",
      "Epoch: 631 \tTraining Loss: 0.01077543 \tValidation Loss 0.01929322 \tTraining Acuuarcy 46.907% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 632 \tTraining Loss: 0.01071117 \tValidation Loss 0.01948596 \tTraining Acuuarcy 47.302% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 633 \tTraining Loss: 0.01067338 \tValidation Loss 0.01966476 \tTraining Acuuarcy 47.949% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 634 \tTraining Loss: 0.01080354 \tValidation Loss 0.01985989 \tTraining Acuuarcy 47.057% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 635 \tTraining Loss: 0.01076777 \tValidation Loss 0.01920741 \tTraining Acuuarcy 47.124% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 636 \tTraining Loss: 0.01073125 \tValidation Loss 0.01959745 \tTraining Acuuarcy 47.576% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 637 \tTraining Loss: 0.01066076 \tValidation Loss 0.01982512 \tTraining Acuuarcy 47.888% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 638 \tTraining Loss: 0.01069398 \tValidation Loss 0.01958405 \tTraining Acuuarcy 47.837% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 639 \tTraining Loss: 0.01069647 \tValidation Loss 0.01985437 \tTraining Acuuarcy 47.414% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 640 \tTraining Loss: 0.01070734 \tValidation Loss 0.01937777 \tTraining Acuuarcy 47.063% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 641 \tTraining Loss: 0.01067029 \tValidation Loss 0.01941693 \tTraining Acuuarcy 47.364% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 642 \tTraining Loss: 0.01071057 \tValidation Loss 0.01925804 \tTraining Acuuarcy 47.358% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 643 \tTraining Loss: 0.01072110 \tValidation Loss 0.01921341 \tTraining Acuuarcy 47.185% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 644 \tTraining Loss: 0.01072863 \tValidation Loss 0.01879110 \tTraining Acuuarcy 47.191% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 645 \tTraining Loss: 0.01069550 \tValidation Loss 0.01976179 \tTraining Acuuarcy 47.620% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 646 \tTraining Loss: 0.01074259 \tValidation Loss 0.01936906 \tTraining Acuuarcy 47.174% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 647 \tTraining Loss: 0.01072257 \tValidation Loss 0.01936478 \tTraining Acuuarcy 47.085% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 648 \tTraining Loss: 0.01076088 \tValidation Loss 0.01911798 \tTraining Acuuarcy 47.425% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 649 \tTraining Loss: 0.01072176 \tValidation Loss 0.01938022 \tTraining Acuuarcy 47.754% \tValidation Acuuarcy 16.606%\n",
      "Epoch: 650 \tTraining Loss: 0.01070965 \tValidation Loss 0.01923877 \tTraining Acuuarcy 46.873% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 651 \tTraining Loss: 0.01075580 \tValidation Loss 0.01953139 \tTraining Acuuarcy 47.353% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 652 \tTraining Loss: 0.01068889 \tValidation Loss 0.01928108 \tTraining Acuuarcy 47.353% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 653 \tTraining Loss: 0.01079791 \tValidation Loss 0.01911875 \tTraining Acuuarcy 47.236% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 654 \tTraining Loss: 0.01070688 \tValidation Loss 0.01943171 \tTraining Acuuarcy 47.280% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 655 \tTraining Loss: 0.01066447 \tValidation Loss 0.01955960 \tTraining Acuuarcy 47.988% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 656 \tTraining Loss: 0.01069882 \tValidation Loss 0.01935517 \tTraining Acuuarcy 47.436% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 657 \tTraining Loss: 0.01068237 \tValidation Loss 0.02006115 \tTraining Acuuarcy 48.016% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 658 \tTraining Loss: 0.01065251 \tValidation Loss 0.01939780 \tTraining Acuuarcy 47.860% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 659 \tTraining Loss: 0.01070271 \tValidation Loss 0.01903765 \tTraining Acuuarcy 47.581% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 660 \tTraining Loss: 0.01073577 \tValidation Loss 0.01901681 \tTraining Acuuarcy 47.314% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 661 \tTraining Loss: 0.01067470 \tValidation Loss 0.01961301 \tTraining Acuuarcy 47.776% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 662 \tTraining Loss: 0.01070137 \tValidation Loss 0.01946105 \tTraining Acuuarcy 47.158% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 663 \tTraining Loss: 0.01071081 \tValidation Loss 0.01891641 \tTraining Acuuarcy 47.620% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 664 \tTraining Loss: 0.01071436 \tValidation Loss 0.01968711 \tTraining Acuuarcy 47.068% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 665 \tTraining Loss: 0.01072942 \tValidation Loss 0.02002092 \tTraining Acuuarcy 47.074% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 666 \tTraining Loss: 0.01066588 \tValidation Loss 0.01918041 \tTraining Acuuarcy 47.860% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 667 \tTraining Loss: 0.01074920 \tValidation Loss 0.01978317 \tTraining Acuuarcy 47.252% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 668 \tTraining Loss: 0.01068682 \tValidation Loss 0.01975614 \tTraining Acuuarcy 47.810% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 669 \tTraining Loss: 0.01069696 \tValidation Loss 0.01937666 \tTraining Acuuarcy 47.977% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 670 \tTraining Loss: 0.01067964 \tValidation Loss 0.01942086 \tTraining Acuuarcy 47.325% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 671 \tTraining Loss: 0.01072708 \tValidation Loss 0.01901000 \tTraining Acuuarcy 47.481% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 672 \tTraining Loss: 0.01061792 \tValidation Loss 0.01954097 \tTraining Acuuarcy 48.334% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 673 \tTraining Loss: 0.01065581 \tValidation Loss 0.01991449 \tTraining Acuuarcy 47.492% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 674 \tTraining Loss: 0.01070019 \tValidation Loss 0.01938761 \tTraining Acuuarcy 47.525% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 675 \tTraining Loss: 0.01074121 \tValidation Loss 0.01958380 \tTraining Acuuarcy 47.035% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 676 \tTraining Loss: 0.01070025 \tValidation Loss 0.01959468 \tTraining Acuuarcy 47.046% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 677 \tTraining Loss: 0.01070548 \tValidation Loss 0.01933812 \tTraining Acuuarcy 47.358% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 678 \tTraining Loss: 0.01069741 \tValidation Loss 0.01984336 \tTraining Acuuarcy 47.765% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 679 \tTraining Loss: 0.01067173 \tValidation Loss 0.01948068 \tTraining Acuuarcy 47.587% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 680 \tTraining Loss: 0.01066996 \tValidation Loss 0.01986079 \tTraining Acuuarcy 47.971% \tValidation Acuuarcy 18.139%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 681 \tTraining Loss: 0.01069572 \tValidation Loss 0.01916050 \tTraining Acuuarcy 47.185% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 682 \tTraining Loss: 0.01066612 \tValidation Loss 0.01950778 \tTraining Acuuarcy 47.620% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 683 \tTraining Loss: 0.01067473 \tValidation Loss 0.01949719 \tTraining Acuuarcy 47.709% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 684 \tTraining Loss: 0.01067209 \tValidation Loss 0.01973756 \tTraining Acuuarcy 48.300% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 685 \tTraining Loss: 0.01071301 \tValidation Loss 0.01981688 \tTraining Acuuarcy 47.269% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 686 \tTraining Loss: 0.01059332 \tValidation Loss 0.01934944 \tTraining Acuuarcy 48.222% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 687 \tTraining Loss: 0.01063787 \tValidation Loss 0.01963438 \tTraining Acuuarcy 48.066% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 688 \tTraining Loss: 0.01078285 \tValidation Loss 0.01941584 \tTraining Acuuarcy 46.533% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 689 \tTraining Loss: 0.01067903 \tValidation Loss 0.01933404 \tTraining Acuuarcy 47.520% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 690 \tTraining Loss: 0.01065764 \tValidation Loss 0.01930749 \tTraining Acuuarcy 47.626% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 691 \tTraining Loss: 0.01069061 \tValidation Loss 0.01921829 \tTraining Acuuarcy 47.681% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 692 \tTraining Loss: 0.01070060 \tValidation Loss 0.01903916 \tTraining Acuuarcy 47.693% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 693 \tTraining Loss: 0.01060261 \tValidation Loss 0.01949302 \tTraining Acuuarcy 47.882% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 694 \tTraining Loss: 0.01063707 \tValidation Loss 0.01938883 \tTraining Acuuarcy 47.921% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 695 \tTraining Loss: 0.01065624 \tValidation Loss 0.02023073 \tTraining Acuuarcy 47.654% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 696 \tTraining Loss: 0.01069324 \tValidation Loss 0.01938960 \tTraining Acuuarcy 47.720% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 697 \tTraining Loss: 0.01068059 \tValidation Loss 0.01935286 \tTraining Acuuarcy 47.336% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 698 \tTraining Loss: 0.01068937 \tValidation Loss 0.01974703 \tTraining Acuuarcy 47.492% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 699 \tTraining Loss: 0.01069518 \tValidation Loss 0.01921605 \tTraining Acuuarcy 47.408% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 700 \tTraining Loss: 0.01069600 \tValidation Loss 0.01930859 \tTraining Acuuarcy 47.609% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 701 \tTraining Loss: 0.01067305 \tValidation Loss 0.01944431 \tTraining Acuuarcy 47.815% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 702 \tTraining Loss: 0.01075113 \tValidation Loss 0.01960928 \tTraining Acuuarcy 47.113% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 703 \tTraining Loss: 0.01063947 \tValidation Loss 0.01986246 \tTraining Acuuarcy 47.681% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 704 \tTraining Loss: 0.01070366 \tValidation Loss 0.02022107 \tTraining Acuuarcy 47.676% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 705 \tTraining Loss: 0.01058055 \tValidation Loss 0.01978034 \tTraining Acuuarcy 48.083% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 706 \tTraining Loss: 0.01062910 \tValidation Loss 0.01921222 \tTraining Acuuarcy 47.475% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 707 \tTraining Loss: 0.01064269 \tValidation Loss 0.01920979 \tTraining Acuuarcy 47.681% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 708 \tTraining Loss: 0.01068249 \tValidation Loss 0.01955663 \tTraining Acuuarcy 47.386% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 709 \tTraining Loss: 0.01064043 \tValidation Loss 0.01946545 \tTraining Acuuarcy 48.334% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 710 \tTraining Loss: 0.01067795 \tValidation Loss 0.01954945 \tTraining Acuuarcy 48.021% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 711 \tTraining Loss: 0.01063719 \tValidation Loss 0.01936808 \tTraining Acuuarcy 47.810% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 712 \tTraining Loss: 0.01064718 \tValidation Loss 0.01988537 \tTraining Acuuarcy 48.300% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 713 \tTraining Loss: 0.01071737 \tValidation Loss 0.01972116 \tTraining Acuuarcy 47.158% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 714 \tTraining Loss: 0.01068016 \tValidation Loss 0.01915637 \tTraining Acuuarcy 47.765% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 715 \tTraining Loss: 0.01070006 \tValidation Loss 0.01934418 \tTraining Acuuarcy 47.514% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 716 \tTraining Loss: 0.01061596 \tValidation Loss 0.01965039 \tTraining Acuuarcy 48.088% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 717 \tTraining Loss: 0.01060351 \tValidation Loss 0.01967064 \tTraining Acuuarcy 47.642% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 718 \tTraining Loss: 0.01066641 \tValidation Loss 0.01963688 \tTraining Acuuarcy 47.759% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 719 \tTraining Loss: 0.01066679 \tValidation Loss 0.01901216 \tTraining Acuuarcy 47.888% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 720 \tTraining Loss: 0.01067467 \tValidation Loss 0.01916361 \tTraining Acuuarcy 47.994% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 721 \tTraining Loss: 0.01061945 \tValidation Loss 0.01958534 \tTraining Acuuarcy 47.447% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 722 \tTraining Loss: 0.01065257 \tValidation Loss 0.01938271 \tTraining Acuuarcy 48.049% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 723 \tTraining Loss: 0.01063076 \tValidation Loss 0.01954265 \tTraining Acuuarcy 47.637% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 724 \tTraining Loss: 0.01067611 \tValidation Loss 0.01929669 \tTraining Acuuarcy 47.358% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 725 \tTraining Loss: 0.01069600 \tValidation Loss 0.01927439 \tTraining Acuuarcy 47.631% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 726 \tTraining Loss: 0.01066918 \tValidation Loss 0.01933956 \tTraining Acuuarcy 47.759% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 727 \tTraining Loss: 0.01060366 \tValidation Loss 0.01948988 \tTraining Acuuarcy 48.456% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 728 \tTraining Loss: 0.01067296 \tValidation Loss 0.01949913 \tTraining Acuuarcy 47.960% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 729 \tTraining Loss: 0.01059898 \tValidation Loss 0.02002834 \tTraining Acuuarcy 48.233% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 730 \tTraining Loss: 0.01057596 \tValidation Loss 0.01925367 \tTraining Acuuarcy 48.261% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 731 \tTraining Loss: 0.01066664 \tValidation Loss 0.01929974 \tTraining Acuuarcy 47.497% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 732 \tTraining Loss: 0.01070236 \tValidation Loss 0.01954760 \tTraining Acuuarcy 47.559% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 733 \tTraining Loss: 0.01062083 \tValidation Loss 0.01950724 \tTraining Acuuarcy 47.737% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 734 \tTraining Loss: 0.01064206 \tValidation Loss 0.01971501 \tTraining Acuuarcy 47.882% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 735 \tTraining Loss: 0.01064388 \tValidation Loss 0.01940458 \tTraining Acuuarcy 47.955% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 736 \tTraining Loss: 0.01066836 \tValidation Loss 0.02001349 \tTraining Acuuarcy 47.687% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 737 \tTraining Loss: 0.01066393 \tValidation Loss 0.01945055 \tTraining Acuuarcy 47.392% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 738 \tTraining Loss: 0.01057687 \tValidation Loss 0.01958183 \tTraining Acuuarcy 48.250% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 739 \tTraining Loss: 0.01059909 \tValidation Loss 0.01933204 \tTraining Acuuarcy 48.021% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 740 \tTraining Loss: 0.01063282 \tValidation Loss 0.01950644 \tTraining Acuuarcy 48.077% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 741 \tTraining Loss: 0.01064930 \tValidation Loss 0.01936369 \tTraining Acuuarcy 47.804% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 742 \tTraining Loss: 0.01054341 \tValidation Loss 0.01946679 \tTraining Acuuarcy 48.361% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 743 \tTraining Loss: 0.01067828 \tValidation Loss 0.01905373 \tTraining Acuuarcy 47.977% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 744 \tTraining Loss: 0.01065436 \tValidation Loss 0.01975798 \tTraining Acuuarcy 47.999% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 745 \tTraining Loss: 0.01057206 \tValidation Loss 0.01979912 \tTraining Acuuarcy 47.971% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 746 \tTraining Loss: 0.01061457 \tValidation Loss 0.01982392 \tTraining Acuuarcy 47.927% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 747 \tTraining Loss: 0.01066349 \tValidation Loss 0.01979880 \tTraining Acuuarcy 47.810% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 748 \tTraining Loss: 0.01066507 \tValidation Loss 0.01945854 \tTraining Acuuarcy 47.559% \tValidation Acuuarcy 17.832%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 749 \tTraining Loss: 0.01062721 \tValidation Loss 0.01923768 \tTraining Acuuarcy 47.837% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 750 \tTraining Loss: 0.01056410 \tValidation Loss 0.01958048 \tTraining Acuuarcy 48.228% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 751 \tTraining Loss: 0.01068697 \tValidation Loss 0.01928557 \tTraining Acuuarcy 47.943% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 752 \tTraining Loss: 0.01056072 \tValidation Loss 0.02011394 \tTraining Acuuarcy 48.395% \tValidation Acuuarcy 17.080%\n",
      "Epoch: 753 \tTraining Loss: 0.01059985 \tValidation Loss 0.01930442 \tTraining Acuuarcy 47.893% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 754 \tTraining Loss: 0.01067402 \tValidation Loss 0.01965784 \tTraining Acuuarcy 47.726% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 755 \tTraining Loss: 0.01067223 \tValidation Loss 0.01940678 \tTraining Acuuarcy 47.654% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 756 \tTraining Loss: 0.01068248 \tValidation Loss 0.01956335 \tTraining Acuuarcy 47.369% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 757 \tTraining Loss: 0.01064441 \tValidation Loss 0.01962406 \tTraining Acuuarcy 48.033% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 758 \tTraining Loss: 0.01063037 \tValidation Loss 0.01966071 \tTraining Acuuarcy 47.893% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 759 \tTraining Loss: 0.01065694 \tValidation Loss 0.01898018 \tTraining Acuuarcy 47.475% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 760 \tTraining Loss: 0.01066501 \tValidation Loss 0.02024797 \tTraining Acuuarcy 47.921% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 761 \tTraining Loss: 0.01066470 \tValidation Loss 0.01935646 \tTraining Acuuarcy 47.787% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 762 \tTraining Loss: 0.01071737 \tValidation Loss 0.01906705 \tTraining Acuuarcy 47.431% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 763 \tTraining Loss: 0.01061369 \tValidation Loss 0.01964890 \tTraining Acuuarcy 47.815% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 764 \tTraining Loss: 0.01056739 \tValidation Loss 0.01974357 \tTraining Acuuarcy 48.322% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 765 \tTraining Loss: 0.01062557 \tValidation Loss 0.01926197 \tTraining Acuuarcy 47.737% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 766 \tTraining Loss: 0.01062988 \tValidation Loss 0.01971689 \tTraining Acuuarcy 48.172% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 767 \tTraining Loss: 0.01056848 \tValidation Loss 0.01991648 \tTraining Acuuarcy 48.573% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 768 \tTraining Loss: 0.01058913 \tValidation Loss 0.01960729 \tTraining Acuuarcy 48.534% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 769 \tTraining Loss: 0.01069426 \tValidation Loss 0.01980982 \tTraining Acuuarcy 47.559% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 770 \tTraining Loss: 0.01067015 \tValidation Loss 0.01958397 \tTraining Acuuarcy 47.408% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 771 \tTraining Loss: 0.01054072 \tValidation Loss 0.01965749 \tTraining Acuuarcy 48.629% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 772 \tTraining Loss: 0.01066955 \tValidation Loss 0.01975068 \tTraining Acuuarcy 47.670% \tValidation Acuuarcy 17.108%\n",
      "Epoch: 773 \tTraining Loss: 0.01056745 \tValidation Loss 0.02012134 \tTraining Acuuarcy 48.166% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 774 \tTraining Loss: 0.01062542 \tValidation Loss 0.01960987 \tTraining Acuuarcy 47.949% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 775 \tTraining Loss: 0.01063733 \tValidation Loss 0.01982568 \tTraining Acuuarcy 47.832% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 776 \tTraining Loss: 0.01063618 \tValidation Loss 0.01965225 \tTraining Acuuarcy 47.994% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 777 \tTraining Loss: 0.01055969 \tValidation Loss 0.01953288 \tTraining Acuuarcy 48.807% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 778 \tTraining Loss: 0.01064072 \tValidation Loss 0.01956973 \tTraining Acuuarcy 48.216% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 779 \tTraining Loss: 0.01070812 \tValidation Loss 0.01928133 \tTraining Acuuarcy 47.787% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 780 \tTraining Loss: 0.01055813 \tValidation Loss 0.01980353 \tTraining Acuuarcy 48.077% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 781 \tTraining Loss: 0.01058812 \tValidation Loss 0.01944024 \tTraining Acuuarcy 48.122% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 782 \tTraining Loss: 0.01059538 \tValidation Loss 0.01980067 \tTraining Acuuarcy 48.155% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 783 \tTraining Loss: 0.01061268 \tValidation Loss 0.01922124 \tTraining Acuuarcy 47.832% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 784 \tTraining Loss: 0.01058603 \tValidation Loss 0.01958641 \tTraining Acuuarcy 48.846% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 785 \tTraining Loss: 0.01062747 \tValidation Loss 0.01996102 \tTraining Acuuarcy 47.626% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 786 \tTraining Loss: 0.01051436 \tValidation Loss 0.01980090 \tTraining Acuuarcy 48.027% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 787 \tTraining Loss: 0.01063607 \tValidation Loss 0.01912034 \tTraining Acuuarcy 48.033% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 788 \tTraining Loss: 0.01063802 \tValidation Loss 0.01951519 \tTraining Acuuarcy 48.328% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 789 \tTraining Loss: 0.01056144 \tValidation Loss 0.01948786 \tTraining Acuuarcy 48.339% \tValidation Acuuarcy 17.191%\n",
      "Epoch: 790 \tTraining Loss: 0.01059169 \tValidation Loss 0.01953139 \tTraining Acuuarcy 48.010% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 791 \tTraining Loss: 0.01059490 \tValidation Loss 0.01954759 \tTraining Acuuarcy 48.161% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 792 \tTraining Loss: 0.01059717 \tValidation Loss 0.01962983 \tTraining Acuuarcy 48.127% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 793 \tTraining Loss: 0.01063595 \tValidation Loss 0.01965358 \tTraining Acuuarcy 48.105% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 794 \tTraining Loss: 0.01061037 \tValidation Loss 0.01930165 \tTraining Acuuarcy 48.545% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 795 \tTraining Loss: 0.01062188 \tValidation Loss 0.01929720 \tTraining Acuuarcy 47.949% \tValidation Acuuarcy 17.275%\n",
      "Epoch: 796 \tTraining Loss: 0.01064379 \tValidation Loss 0.01965748 \tTraining Acuuarcy 47.648% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 797 \tTraining Loss: 0.01062643 \tValidation Loss 0.01948796 \tTraining Acuuarcy 48.378% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 798 \tTraining Loss: 0.01055844 \tValidation Loss 0.01981142 \tTraining Acuuarcy 48.445% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 799 \tTraining Loss: 0.01057705 \tValidation Loss 0.01919600 \tTraining Acuuarcy 47.765% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 800 \tTraining Loss: 0.01059133 \tValidation Loss 0.01947996 \tTraining Acuuarcy 48.105% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 801 \tTraining Loss: 0.01066063 \tValidation Loss 0.01929095 \tTraining Acuuarcy 48.150% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 802 \tTraining Loss: 0.01058969 \tValidation Loss 0.01953313 \tTraining Acuuarcy 48.161% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 803 \tTraining Loss: 0.01057639 \tValidation Loss 0.01967297 \tTraining Acuuarcy 48.283% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 804 \tTraining Loss: 0.01058713 \tValidation Loss 0.01934361 \tTraining Acuuarcy 47.832% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 805 \tTraining Loss: 0.01062555 \tValidation Loss 0.01969906 \tTraining Acuuarcy 48.267% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 806 \tTraining Loss: 0.01064941 \tValidation Loss 0.01962671 \tTraining Acuuarcy 48.072% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 807 \tTraining Loss: 0.01065207 \tValidation Loss 0.01976664 \tTraining Acuuarcy 47.759% \tValidation Acuuarcy 17.359%\n",
      "Epoch: 808 \tTraining Loss: 0.01054501 \tValidation Loss 0.01943191 \tTraining Acuuarcy 48.651% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 809 \tTraining Loss: 0.01060317 \tValidation Loss 0.02003262 \tTraining Acuuarcy 48.713% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 810 \tTraining Loss: 0.01061201 \tValidation Loss 0.01971642 \tTraining Acuuarcy 47.810% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 811 \tTraining Loss: 0.01051863 \tValidation Loss 0.01991830 \tTraining Acuuarcy 48.467% \tValidation Acuuarcy 17.331%\n",
      "Epoch: 812 \tTraining Loss: 0.01053854 \tValidation Loss 0.01982022 \tTraining Acuuarcy 48.634% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 813 \tTraining Loss: 0.01058433 \tValidation Loss 0.01948217 \tTraining Acuuarcy 47.921% \tValidation Acuuarcy 16.969%\n",
      "Epoch: 814 \tTraining Loss: 0.01060444 \tValidation Loss 0.01959036 \tTraining Acuuarcy 48.044% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 815 \tTraining Loss: 0.01059200 \tValidation Loss 0.01976646 \tTraining Acuuarcy 48.222% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 816 \tTraining Loss: 0.01052907 \tValidation Loss 0.01979875 \tTraining Acuuarcy 48.278% \tValidation Acuuarcy 17.526%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 817 \tTraining Loss: 0.01052277 \tValidation Loss 0.01963200 \tTraining Acuuarcy 48.439% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 818 \tTraining Loss: 0.01060648 \tValidation Loss 0.01954505 \tTraining Acuuarcy 47.609% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 819 \tTraining Loss: 0.01059892 \tValidation Loss 0.01941986 \tTraining Acuuarcy 48.674% \tValidation Acuuarcy 17.916%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"CUDA is not available. Make sure you have a CUDA-enabled GPU.\")\n",
    "\n",
    "def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        # Train the model  #\n",
    "        net.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optmizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        #validate the model#\n",
    "        net.eval()\n",
    "        for data,labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            val_outputs = net(data)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs,1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss =  validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
    "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
    "\n",
    "    torch.save(net.state_dict(),'deep_emotion-{}-{}-{}.pt'.format(epochs,batchsize,lr))\n",
    "    print(\"===================================Training Finished===================================\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"Configuration of setup and training process\")\n",
    "#     parser.add_argument('-s', '--setup', type=bool, help='setup the dataset for the first time')\n",
    "#     parser.add_argument('-d', '--data', type=str,required= True,\n",
    "#                                help='data folder that contains data files that downloaded from kaggle (train.csv and test.csv)')\n",
    "#     parser.add_argument('-hparams', '--hyperparams', type=bool,\n",
    "#                                help='True when changing the hyperparameters e.g (batch size, LR, num. of epochs)')\n",
    "#     parser.add_argument('-e', '--epochs', type= int, help= 'number of epochs')\n",
    "#     parser.add_argument('-lr', '--learning_rate', type= float, help= 'value of learning rate')\n",
    "#     parser.add_argument('-bs', '--batch_size', type= int, help= 'training/validation batch size')\n",
    "#     parser.add_argument('-t', '--train', type=bool, help='True when training')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if args.setup :\n",
    "#         generate_dataset = Generate_data(args.data)\n",
    "#         generate_dataset.split_test()\n",
    "#         generate_dataset.save_images('train')\n",
    "#         generate_dataset.save_images('test')\n",
    "#         generate_dataset.save_images('val')\n",
    "\n",
    "#     if args.hyperparams:\n",
    "#         epochs = args.epochs\n",
    "#         lr = args.learning_rate\n",
    "#         batchsize = args.batch_size\n",
    "#     else :\n",
    "epochs = 3000\n",
    "lr = 0.001\n",
    "batchsize = 128\n",
    "\n",
    "#     if args.train:\n",
    "net = Deep_Emotion()\n",
    "net.to(device)\n",
    "print(\"Model archticture: \", net)\n",
    "traincsv_file = 'data'+'/'+'train.csv'\n",
    "validationcsv_file = 'data'+'/'+'val.csv'\n",
    "train_img_dir = 'data'+'/'+'train/'\n",
    "validation_img_dir = 'data'+'/'+'val/'\n",
    "\n",
    "transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
    "validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
    "train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optmizer= optim.Adam(net.parameters(),lr= lr)\n",
    "Train(epochs, train_loader, val_loader, criterion, optmizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60929613",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'Speaktrum_by_SOVA.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
