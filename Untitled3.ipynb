{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320d51d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archticture:  Deep_Emotion(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "===================================Start Training===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4277: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4215: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.01438240 \tValidation Loss 0.01483837 \tTraining Acuuarcy 24.222% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 2 \tTraining Loss: 0.01424402 \tValidation Loss 0.01487854 \tTraining Acuuarcy 24.958% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 3 \tTraining Loss: 0.01424051 \tValidation Loss 0.01468265 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 4 \tTraining Loss: 0.01424159 \tValidation Loss 0.01476370 \tTraining Acuuarcy 25.064% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 5 \tTraining Loss: 0.01422649 \tValidation Loss 0.01477962 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 24.659%\n",
      "Epoch: 6 \tTraining Loss: 0.01422700 \tValidation Loss 0.01474683 \tTraining Acuuarcy 24.969% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 7 \tTraining Loss: 0.01422443 \tValidation Loss 0.01479012 \tTraining Acuuarcy 25.025% \tValidation Acuuarcy 25.049%\n",
      "Epoch: 8 \tTraining Loss: 0.01420824 \tValidation Loss 0.01477472 \tTraining Acuuarcy 24.986% \tValidation Acuuarcy 25.104%\n",
      "Epoch: 9 \tTraining Loss: 0.01421388 \tValidation Loss 0.01471719 \tTraining Acuuarcy 25.070% \tValidation Acuuarcy 25.216%\n",
      "Epoch: 10 \tTraining Loss: 0.01420701 \tValidation Loss 0.01475564 \tTraining Acuuarcy 25.031% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 11 \tTraining Loss: 0.01421948 \tValidation Loss 0.01487769 \tTraining Acuuarcy 25.053% \tValidation Acuuarcy 25.077%\n",
      "Epoch: 12 \tTraining Loss: 0.01419702 \tValidation Loss 0.01478135 \tTraining Acuuarcy 25.153% \tValidation Acuuarcy 25.104%\n",
      "Epoch: 13 \tTraining Loss: 0.01420296 \tValidation Loss 0.01473837 \tTraining Acuuarcy 25.059% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 14 \tTraining Loss: 0.01419471 \tValidation Loss 0.01476054 \tTraining Acuuarcy 25.059% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 15 \tTraining Loss: 0.01418812 \tValidation Loss 0.01471453 \tTraining Acuuarcy 25.098% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 16 \tTraining Loss: 0.01417645 \tValidation Loss 0.01475715 \tTraining Acuuarcy 25.203% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 17 \tTraining Loss: 0.01418187 \tValidation Loss 0.01477000 \tTraining Acuuarcy 24.964% \tValidation Acuuarcy 25.077%\n",
      "Epoch: 18 \tTraining Loss: 0.01416983 \tValidation Loss 0.01470955 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 19 \tTraining Loss: 0.01416334 \tValidation Loss 0.01471833 \tTraining Acuuarcy 25.031% \tValidation Acuuarcy 25.606%\n",
      "Epoch: 20 \tTraining Loss: 0.01415293 \tValidation Loss 0.01492387 \tTraining Acuuarcy 25.198% \tValidation Acuuarcy 24.575%\n",
      "Epoch: 21 \tTraining Loss: 0.01416114 \tValidation Loss 0.01472381 \tTraining Acuuarcy 25.020% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 22 \tTraining Loss: 0.01414921 \tValidation Loss 0.01478105 \tTraining Acuuarcy 25.220% \tValidation Acuuarcy 24.798%\n",
      "Epoch: 23 \tTraining Loss: 0.01415759 \tValidation Loss 0.01474649 \tTraining Acuuarcy 25.164% \tValidation Acuuarcy 25.244%\n",
      "Epoch: 24 \tTraining Loss: 0.01414901 \tValidation Loss 0.01477913 \tTraining Acuuarcy 25.460% \tValidation Acuuarcy 24.770%\n",
      "Epoch: 25 \tTraining Loss: 0.01414459 \tValidation Loss 0.01478989 \tTraining Acuuarcy 25.181% \tValidation Acuuarcy 24.659%\n",
      "Epoch: 26 \tTraining Loss: 0.01413142 \tValidation Loss 0.01476571 \tTraining Acuuarcy 25.176% \tValidation Acuuarcy 24.659%\n",
      "Epoch: 27 \tTraining Loss: 0.01412109 \tValidation Loss 0.01484295 \tTraining Acuuarcy 25.516% \tValidation Acuuarcy 24.380%\n",
      "Epoch: 28 \tTraining Loss: 0.01412235 \tValidation Loss 0.01476829 \tTraining Acuuarcy 25.376% \tValidation Acuuarcy 24.714%\n",
      "Epoch: 29 \tTraining Loss: 0.01412262 \tValidation Loss 0.01482208 \tTraining Acuuarcy 25.426% \tValidation Acuuarcy 23.934%\n",
      "Epoch: 30 \tTraining Loss: 0.01409588 \tValidation Loss 0.01480933 \tTraining Acuuarcy 25.794% \tValidation Acuuarcy 23.210%\n",
      "Epoch: 31 \tTraining Loss: 0.01411710 \tValidation Loss 0.01476875 \tTraining Acuuarcy 25.683% \tValidation Acuuarcy 23.405%\n",
      "Epoch: 32 \tTraining Loss: 0.01409234 \tValidation Loss 0.01477612 \tTraining Acuuarcy 25.621% \tValidation Acuuarcy 24.046%\n",
      "Epoch: 33 \tTraining Loss: 0.01407203 \tValidation Loss 0.01484941 \tTraining Acuuarcy 26.017% \tValidation Acuuarcy 23.572%\n",
      "Epoch: 34 \tTraining Loss: 0.01404590 \tValidation Loss 0.01489035 \tTraining Acuuarcy 26.095% \tValidation Acuuarcy 24.157%\n",
      "Epoch: 35 \tTraining Loss: 0.01404680 \tValidation Loss 0.01484198 \tTraining Acuuarcy 26.251% \tValidation Acuuarcy 24.352%\n",
      "Epoch: 36 \tTraining Loss: 0.01402824 \tValidation Loss 0.01491439 \tTraining Acuuarcy 26.123% \tValidation Acuuarcy 22.820%\n",
      "Epoch: 37 \tTraining Loss: 0.01402701 \tValidation Loss 0.01487767 \tTraining Acuuarcy 25.889% \tValidation Acuuarcy 23.210%\n",
      "Epoch: 38 \tTraining Loss: 0.01402956 \tValidation Loss 0.01481367 \tTraining Acuuarcy 25.945% \tValidation Acuuarcy 24.965%\n",
      "Epoch: 39 \tTraining Loss: 0.01401796 \tValidation Loss 0.01487268 \tTraining Acuuarcy 26.168% \tValidation Acuuarcy 22.987%\n",
      "Epoch: 40 \tTraining Loss: 0.01399622 \tValidation Loss 0.01486115 \tTraining Acuuarcy 26.418% \tValidation Acuuarcy 23.823%\n",
      "Epoch: 41 \tTraining Loss: 0.01400420 \tValidation Loss 0.01475394 \tTraining Acuuarcy 26.413% \tValidation Acuuarcy 23.572%\n",
      "Epoch: 42 \tTraining Loss: 0.01398193 \tValidation Loss 0.01499485 \tTraining Acuuarcy 26.396% \tValidation Acuuarcy 23.516%\n",
      "Epoch: 43 \tTraining Loss: 0.01394525 \tValidation Loss 0.01521134 \tTraining Acuuarcy 26.703% \tValidation Acuuarcy 23.405%\n",
      "Epoch: 44 \tTraining Loss: 0.01396262 \tValidation Loss 0.01493267 \tTraining Acuuarcy 26.697% \tValidation Acuuarcy 23.572%\n",
      "Epoch: 45 \tTraining Loss: 0.01392532 \tValidation Loss 0.01500986 \tTraining Acuuarcy 27.137% \tValidation Acuuarcy 23.962%\n",
      "Epoch: 46 \tTraining Loss: 0.01393393 \tValidation Loss 0.01503024 \tTraining Acuuarcy 26.931% \tValidation Acuuarcy 23.906%\n",
      "Epoch: 47 \tTraining Loss: 0.01391978 \tValidation Loss 0.01504975 \tTraining Acuuarcy 27.032% \tValidation Acuuarcy 23.656%\n",
      "Epoch: 48 \tTraining Loss: 0.01389943 \tValidation Loss 0.01510802 \tTraining Acuuarcy 26.714% \tValidation Acuuarcy 23.488%\n",
      "Epoch: 49 \tTraining Loss: 0.01384678 \tValidation Loss 0.01504911 \tTraining Acuuarcy 27.305% \tValidation Acuuarcy 24.129%\n",
      "Epoch: 50 \tTraining Loss: 0.01387506 \tValidation Loss 0.01504786 \tTraining Acuuarcy 27.344% \tValidation Acuuarcy 23.154%\n",
      "Epoch: 51 \tTraining Loss: 0.01385723 \tValidation Loss 0.01521063 \tTraining Acuuarcy 27.555% \tValidation Acuuarcy 24.631%\n",
      "Epoch: 52 \tTraining Loss: 0.01385035 \tValidation Loss 0.01497492 \tTraining Acuuarcy 27.260% \tValidation Acuuarcy 23.628%\n",
      "Epoch: 53 \tTraining Loss: 0.01382161 \tValidation Loss 0.01509846 \tTraining Acuuarcy 27.656% \tValidation Acuuarcy 22.680%\n",
      "Epoch: 54 \tTraining Loss: 0.01382399 \tValidation Loss 0.01496806 \tTraining Acuuarcy 27.600% \tValidation Acuuarcy 23.293%\n",
      "Epoch: 55 \tTraining Loss: 0.01381490 \tValidation Loss 0.01523919 \tTraining Acuuarcy 27.667% \tValidation Acuuarcy 23.070%\n",
      "Epoch: 56 \tTraining Loss: 0.01380600 \tValidation Loss 0.01509386 \tTraining Acuuarcy 27.645% \tValidation Acuuarcy 22.764%\n",
      "Epoch: 57 \tTraining Loss: 0.01377611 \tValidation Loss 0.01526543 \tTraining Acuuarcy 28.313% \tValidation Acuuarcy 23.321%\n",
      "Epoch: 58 \tTraining Loss: 0.01378315 \tValidation Loss 0.01520408 \tTraining Acuuarcy 27.912% \tValidation Acuuarcy 23.377%\n",
      "Epoch: 59 \tTraining Loss: 0.01374481 \tValidation Loss 0.01524196 \tTraining Acuuarcy 27.912% \tValidation Acuuarcy 22.708%\n",
      "Epoch: 60 \tTraining Loss: 0.01371172 \tValidation Loss 0.01527480 \tTraining Acuuarcy 28.130% \tValidation Acuuarcy 23.043%\n",
      "Epoch: 61 \tTraining Loss: 0.01375252 \tValidation Loss 0.01516063 \tTraining Acuuarcy 28.074% \tValidation Acuuarcy 23.879%\n",
      "Epoch: 62 \tTraining Loss: 0.01371822 \tValidation Loss 0.01528647 \tTraining Acuuarcy 28.419% \tValidation Acuuarcy 23.070%\n",
      "Epoch: 63 \tTraining Loss: 0.01372854 \tValidation Loss 0.01527363 \tTraining Acuuarcy 28.202% \tValidation Acuuarcy 22.903%\n",
      "Epoch: 64 \tTraining Loss: 0.01369937 \tValidation Loss 0.01541380 \tTraining Acuuarcy 28.464% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 65 \tTraining Loss: 0.01371041 \tValidation Loss 0.01550333 \tTraining Acuuarcy 28.007% \tValidation Acuuarcy 22.987%\n",
      "Epoch: 66 \tTraining Loss: 0.01370786 \tValidation Loss 0.01522413 \tTraining Acuuarcy 28.564% \tValidation Acuuarcy 23.070%\n",
      "Epoch: 67 \tTraining Loss: 0.01370404 \tValidation Loss 0.01534855 \tTraining Acuuarcy 28.330% \tValidation Acuuarcy 23.321%\n",
      "Epoch: 68 \tTraining Loss: 0.01370679 \tValidation Loss 0.01537886 \tTraining Acuuarcy 28.681% \tValidation Acuuarcy 23.210%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 \tTraining Loss: 0.01367668 \tValidation Loss 0.01527162 \tTraining Acuuarcy 28.865% \tValidation Acuuarcy 21.900%\n",
      "Epoch: 70 \tTraining Loss: 0.01367331 \tValidation Loss 0.01533604 \tTraining Acuuarcy 28.559% \tValidation Acuuarcy 22.067%\n",
      "Epoch: 71 \tTraining Loss: 0.01365071 \tValidation Loss 0.01529054 \tTraining Acuuarcy 29.194% \tValidation Acuuarcy 22.541%\n",
      "Epoch: 72 \tTraining Loss: 0.01360640 \tValidation Loss 0.01530465 \tTraining Acuuarcy 29.060% \tValidation Acuuarcy 23.210%\n",
      "Epoch: 73 \tTraining Loss: 0.01360587 \tValidation Loss 0.01537394 \tTraining Acuuarcy 29.110% \tValidation Acuuarcy 22.987%\n",
      "Epoch: 74 \tTraining Loss: 0.01362562 \tValidation Loss 0.01530368 \tTraining Acuuarcy 29.378% \tValidation Acuuarcy 22.987%\n",
      "Epoch: 75 \tTraining Loss: 0.01358652 \tValidation Loss 0.01549976 \tTraining Acuuarcy 29.361% \tValidation Acuuarcy 22.736%\n",
      "Epoch: 76 \tTraining Loss: 0.01356772 \tValidation Loss 0.01534114 \tTraining Acuuarcy 29.423% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 77 \tTraining Loss: 0.01362412 \tValidation Loss 0.01528897 \tTraining Acuuarcy 28.971% \tValidation Acuuarcy 23.572%\n",
      "Epoch: 78 \tTraining Loss: 0.01355554 \tValidation Loss 0.01540472 \tTraining Acuuarcy 29.211% \tValidation Acuuarcy 23.126%\n",
      "Epoch: 79 \tTraining Loss: 0.01352088 \tValidation Loss 0.01538489 \tTraining Acuuarcy 29.774% \tValidation Acuuarcy 23.321%\n",
      "Epoch: 80 \tTraining Loss: 0.01355988 \tValidation Loss 0.01567015 \tTraining Acuuarcy 29.172% \tValidation Acuuarcy 21.845%\n",
      "Epoch: 81 \tTraining Loss: 0.01354579 \tValidation Loss 0.01555816 \tTraining Acuuarcy 29.813% \tValidation Acuuarcy 23.656%\n",
      "Epoch: 82 \tTraining Loss: 0.01353349 \tValidation Loss 0.01545526 \tTraining Acuuarcy 29.718% \tValidation Acuuarcy 22.430%\n",
      "Epoch: 83 \tTraining Loss: 0.01350926 \tValidation Loss 0.01559109 \tTraining Acuuarcy 29.718% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 84 \tTraining Loss: 0.01351982 \tValidation Loss 0.01548615 \tTraining Acuuarcy 29.562% \tValidation Acuuarcy 23.293%\n",
      "Epoch: 85 \tTraining Loss: 0.01350672 \tValidation Loss 0.01575055 \tTraining Acuuarcy 29.573% \tValidation Acuuarcy 21.872%\n",
      "Epoch: 86 \tTraining Loss: 0.01348587 \tValidation Loss 0.01553546 \tTraining Acuuarcy 30.136% \tValidation Acuuarcy 22.541%\n",
      "Epoch: 87 \tTraining Loss: 0.01348440 \tValidation Loss 0.01571501 \tTraining Acuuarcy 30.025% \tValidation Acuuarcy 22.680%\n",
      "Epoch: 88 \tTraining Loss: 0.01349638 \tValidation Loss 0.01565349 \tTraining Acuuarcy 29.952% \tValidation Acuuarcy 23.461%\n",
      "Epoch: 89 \tTraining Loss: 0.01347664 \tValidation Loss 0.01578147 \tTraining Acuuarcy 29.852% \tValidation Acuuarcy 22.625%\n",
      "Epoch: 90 \tTraining Loss: 0.01344924 \tValidation Loss 0.01530793 \tTraining Acuuarcy 30.220% \tValidation Acuuarcy 22.931%\n",
      "Epoch: 91 \tTraining Loss: 0.01345316 \tValidation Loss 0.01570512 \tTraining Acuuarcy 29.696% \tValidation Acuuarcy 21.817%\n",
      "Epoch: 92 \tTraining Loss: 0.01344479 \tValidation Loss 0.01599747 \tTraining Acuuarcy 30.342% \tValidation Acuuarcy 22.402%\n",
      "Epoch: 93 \tTraining Loss: 0.01340416 \tValidation Loss 0.01577066 \tTraining Acuuarcy 30.281% \tValidation Acuuarcy 22.848%\n",
      "Epoch: 94 \tTraining Loss: 0.01338532 \tValidation Loss 0.01551977 \tTraining Acuuarcy 30.259% \tValidation Acuuarcy 23.154%\n",
      "Epoch: 95 \tTraining Loss: 0.01342079 \tValidation Loss 0.01557626 \tTraining Acuuarcy 30.576% \tValidation Acuuarcy 23.126%\n",
      "Epoch: 96 \tTraining Loss: 0.01340724 \tValidation Loss 0.01573305 \tTraining Acuuarcy 30.649% \tValidation Acuuarcy 22.067%\n",
      "Epoch: 97 \tTraining Loss: 0.01344485 \tValidation Loss 0.01569189 \tTraining Acuuarcy 30.197% \tValidation Acuuarcy 22.903%\n",
      "Epoch: 98 \tTraining Loss: 0.01343663 \tValidation Loss 0.01554467 \tTraining Acuuarcy 30.359% \tValidation Acuuarcy 23.823%\n",
      "Epoch: 99 \tTraining Loss: 0.01342170 \tValidation Loss 0.01558550 \tTraining Acuuarcy 30.515% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 100 \tTraining Loss: 0.01334440 \tValidation Loss 0.01586575 \tTraining Acuuarcy 30.849% \tValidation Acuuarcy 23.210%\n",
      "Epoch: 101 \tTraining Loss: 0.01338890 \tValidation Loss 0.01578192 \tTraining Acuuarcy 30.849% \tValidation Acuuarcy 22.040%\n",
      "Epoch: 102 \tTraining Loss: 0.01337052 \tValidation Loss 0.01565882 \tTraining Acuuarcy 30.415% \tValidation Acuuarcy 22.151%\n",
      "Epoch: 103 \tTraining Loss: 0.01336426 \tValidation Loss 0.01562632 \tTraining Acuuarcy 30.593% \tValidation Acuuarcy 23.433%\n",
      "Epoch: 104 \tTraining Loss: 0.01338032 \tValidation Loss 0.01572334 \tTraining Acuuarcy 30.927% \tValidation Acuuarcy 23.600%\n",
      "Epoch: 105 \tTraining Loss: 0.01338834 \tValidation Loss 0.01567109 \tTraining Acuuarcy 30.833% \tValidation Acuuarcy 22.903%\n",
      "Epoch: 106 \tTraining Loss: 0.01332657 \tValidation Loss 0.01574534 \tTraining Acuuarcy 31.217% \tValidation Acuuarcy 21.872%\n",
      "Epoch: 107 \tTraining Loss: 0.01334275 \tValidation Loss 0.01600380 \tTraining Acuuarcy 30.972% \tValidation Acuuarcy 23.739%\n",
      "Epoch: 108 \tTraining Loss: 0.01330497 \tValidation Loss 0.01563696 \tTraining Acuuarcy 31.095% \tValidation Acuuarcy 23.405%\n",
      "Epoch: 109 \tTraining Loss: 0.01330976 \tValidation Loss 0.01573399 \tTraining Acuuarcy 31.301% \tValidation Acuuarcy 22.708%\n",
      "Epoch: 110 \tTraining Loss: 0.01332104 \tValidation Loss 0.01592699 \tTraining Acuuarcy 30.950% \tValidation Acuuarcy 21.956%\n",
      "Epoch: 111 \tTraining Loss: 0.01330601 \tValidation Loss 0.01585419 \tTraining Acuuarcy 31.401% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 112 \tTraining Loss: 0.01333626 \tValidation Loss 0.01589441 \tTraining Acuuarcy 31.139% \tValidation Acuuarcy 22.597%\n",
      "Epoch: 113 \tTraining Loss: 0.01325823 \tValidation Loss 0.01597707 \tTraining Acuuarcy 31.312% \tValidation Acuuarcy 22.875%\n",
      "Epoch: 114 \tTraining Loss: 0.01332528 \tValidation Loss 0.01587135 \tTraining Acuuarcy 31.440% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 115 \tTraining Loss: 0.01332014 \tValidation Loss 0.01575250 \tTraining Acuuarcy 30.933% \tValidation Acuuarcy 22.736%\n",
      "Epoch: 116 \tTraining Loss: 0.01325136 \tValidation Loss 0.01579073 \tTraining Acuuarcy 31.624% \tValidation Acuuarcy 21.928%\n",
      "Epoch: 117 \tTraining Loss: 0.01328004 \tValidation Loss 0.01568652 \tTraining Acuuarcy 31.524% \tValidation Acuuarcy 22.374%\n",
      "Epoch: 118 \tTraining Loss: 0.01329091 \tValidation Loss 0.01574531 \tTraining Acuuarcy 31.524% \tValidation Acuuarcy 22.040%\n",
      "Epoch: 119 \tTraining Loss: 0.01323572 \tValidation Loss 0.01564100 \tTraining Acuuarcy 32.098% \tValidation Acuuarcy 22.764%\n",
      "Epoch: 120 \tTraining Loss: 0.01323634 \tValidation Loss 0.01590951 \tTraining Acuuarcy 31.730% \tValidation Acuuarcy 22.123%\n",
      "Epoch: 121 \tTraining Loss: 0.01326728 \tValidation Loss 0.01588621 \tTraining Acuuarcy 31.580% \tValidation Acuuarcy 21.482%\n",
      "Epoch: 122 \tTraining Loss: 0.01328266 \tValidation Loss 0.01582967 \tTraining Acuuarcy 31.083% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 123 \tTraining Loss: 0.01326424 \tValidation Loss 0.01601254 \tTraining Acuuarcy 31.563% \tValidation Acuuarcy 22.067%\n",
      "Epoch: 124 \tTraining Loss: 0.01320428 \tValidation Loss 0.01579175 \tTraining Acuuarcy 32.260% \tValidation Acuuarcy 21.817%\n",
      "Epoch: 125 \tTraining Loss: 0.01322434 \tValidation Loss 0.01580863 \tTraining Acuuarcy 31.786% \tValidation Acuuarcy 22.736%\n",
      "Epoch: 126 \tTraining Loss: 0.01317728 \tValidation Loss 0.01597422 \tTraining Acuuarcy 32.577% \tValidation Acuuarcy 21.789%\n",
      "Epoch: 127 \tTraining Loss: 0.01319249 \tValidation Loss 0.01601005 \tTraining Acuuarcy 31.602% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 128 \tTraining Loss: 0.01320011 \tValidation Loss 0.01582314 \tTraining Acuuarcy 31.931% \tValidation Acuuarcy 22.708%\n",
      "Epoch: 129 \tTraining Loss: 0.01322671 \tValidation Loss 0.01610852 \tTraining Acuuarcy 31.697% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 130 \tTraining Loss: 0.01320984 \tValidation Loss 0.01609923 \tTraining Acuuarcy 32.048% \tValidation Acuuarcy 22.123%\n",
      "Epoch: 131 \tTraining Loss: 0.01319481 \tValidation Loss 0.01593935 \tTraining Acuuarcy 32.037% \tValidation Acuuarcy 21.900%\n",
      "Epoch: 132 \tTraining Loss: 0.01318077 \tValidation Loss 0.01618036 \tTraining Acuuarcy 32.427% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 133 \tTraining Loss: 0.01319906 \tValidation Loss 0.01601473 \tTraining Acuuarcy 32.014% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 134 \tTraining Loss: 0.01317450 \tValidation Loss 0.01598718 \tTraining Acuuarcy 32.265% \tValidation Acuuarcy 22.095%\n",
      "Epoch: 135 \tTraining Loss: 0.01315941 \tValidation Loss 0.01606683 \tTraining Acuuarcy 32.705% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 136 \tTraining Loss: 0.01318842 \tValidation Loss 0.01585431 \tTraining Acuuarcy 32.198% \tValidation Acuuarcy 21.733%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137 \tTraining Loss: 0.01317364 \tValidation Loss 0.01604264 \tTraining Acuuarcy 32.271% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 138 \tTraining Loss: 0.01316036 \tValidation Loss 0.01604470 \tTraining Acuuarcy 32.248% \tValidation Acuuarcy 22.513%\n",
      "Epoch: 139 \tTraining Loss: 0.01314083 \tValidation Loss 0.01589838 \tTraining Acuuarcy 32.343% \tValidation Acuuarcy 22.903%\n",
      "Epoch: 140 \tTraining Loss: 0.01316923 \tValidation Loss 0.01603243 \tTraining Acuuarcy 32.170% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 141 \tTraining Loss: 0.01312534 \tValidation Loss 0.01609774 \tTraining Acuuarcy 32.978% \tValidation Acuuarcy 22.095%\n",
      "Epoch: 142 \tTraining Loss: 0.01311115 \tValidation Loss 0.01619726 \tTraining Acuuarcy 32.471% \tValidation Acuuarcy 22.318%\n",
      "Epoch: 143 \tTraining Loss: 0.01315248 \tValidation Loss 0.01591727 \tTraining Acuuarcy 32.315% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 144 \tTraining Loss: 0.01313279 \tValidation Loss 0.01600692 \tTraining Acuuarcy 32.109% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 145 \tTraining Loss: 0.01310405 \tValidation Loss 0.01614888 \tTraining Acuuarcy 32.287% \tValidation Acuuarcy 21.900%\n",
      "Epoch: 146 \tTraining Loss: 0.01316812 \tValidation Loss 0.01598995 \tTraining Acuuarcy 32.588% \tValidation Acuuarcy 22.040%\n",
      "Epoch: 147 \tTraining Loss: 0.01313159 \tValidation Loss 0.01610005 \tTraining Acuuarcy 32.494% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 148 \tTraining Loss: 0.01311943 \tValidation Loss 0.01632384 \tTraining Acuuarcy 32.605% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 149 \tTraining Loss: 0.01307748 \tValidation Loss 0.01623086 \tTraining Acuuarcy 32.622% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 150 \tTraining Loss: 0.01312168 \tValidation Loss 0.01612450 \tTraining Acuuarcy 32.505% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 151 \tTraining Loss: 0.01309145 \tValidation Loss 0.01635618 \tTraining Acuuarcy 32.817% \tValidation Acuuarcy 22.318%\n",
      "Epoch: 152 \tTraining Loss: 0.01316390 \tValidation Loss 0.01602396 \tTraining Acuuarcy 32.834% \tValidation Acuuarcy 21.482%\n",
      "Epoch: 153 \tTraining Loss: 0.01315199 \tValidation Loss 0.01597441 \tTraining Acuuarcy 32.505% \tValidation Acuuarcy 21.900%\n",
      "Epoch: 154 \tTraining Loss: 0.01310908 \tValidation Loss 0.01620819 \tTraining Acuuarcy 32.583% \tValidation Acuuarcy 21.845%\n",
      "Epoch: 155 \tTraining Loss: 0.01309767 \tValidation Loss 0.01585199 \tTraining Acuuarcy 32.282% \tValidation Acuuarcy 22.012%\n",
      "Epoch: 156 \tTraining Loss: 0.01310140 \tValidation Loss 0.01630639 \tTraining Acuuarcy 33.018% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 157 \tTraining Loss: 0.01312353 \tValidation Loss 0.01586634 \tTraining Acuuarcy 32.973% \tValidation Acuuarcy 22.653%\n",
      "Epoch: 158 \tTraining Loss: 0.01303151 \tValidation Loss 0.01647040 \tTraining Acuuarcy 33.174% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 159 \tTraining Loss: 0.01310633 \tValidation Loss 0.01618710 \tTraining Acuuarcy 32.945% \tValidation Acuuarcy 22.040%\n",
      "Epoch: 160 \tTraining Loss: 0.01314241 \tValidation Loss 0.01586684 \tTraining Acuuarcy 32.633% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 161 \tTraining Loss: 0.01305534 \tValidation Loss 0.01630562 \tTraining Acuuarcy 33.135% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 162 \tTraining Loss: 0.01304746 \tValidation Loss 0.01616295 \tTraining Acuuarcy 32.856% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 163 \tTraining Loss: 0.01304041 \tValidation Loss 0.01623311 \tTraining Acuuarcy 33.129% \tValidation Acuuarcy 21.984%\n",
      "Epoch: 164 \tTraining Loss: 0.01306266 \tValidation Loss 0.01589016 \tTraining Acuuarcy 32.778% \tValidation Acuuarcy 22.402%\n",
      "Epoch: 165 \tTraining Loss: 0.01305126 \tValidation Loss 0.01601974 \tTraining Acuuarcy 32.951% \tValidation Acuuarcy 22.179%\n",
      "Epoch: 166 \tTraining Loss: 0.01302385 \tValidation Loss 0.01634571 \tTraining Acuuarcy 33.235% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 167 \tTraining Loss: 0.01308357 \tValidation Loss 0.01595172 \tTraining Acuuarcy 33.185% \tValidation Acuuarcy 22.959%\n",
      "Epoch: 168 \tTraining Loss: 0.01304243 \tValidation Loss 0.01601835 \tTraining Acuuarcy 33.335% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 169 \tTraining Loss: 0.01305909 \tValidation Loss 0.01624011 \tTraining Acuuarcy 32.956% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 170 \tTraining Loss: 0.01300538 \tValidation Loss 0.01626943 \tTraining Acuuarcy 33.374% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 171 \tTraining Loss: 0.01306719 \tValidation Loss 0.01618317 \tTraining Acuuarcy 33.201% \tValidation Acuuarcy 21.872%\n",
      "Epoch: 172 \tTraining Loss: 0.01301404 \tValidation Loss 0.01640664 \tTraining Acuuarcy 33.162% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 173 \tTraining Loss: 0.01302701 \tValidation Loss 0.01637821 \tTraining Acuuarcy 33.369% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 174 \tTraining Loss: 0.01304982 \tValidation Loss 0.01618629 \tTraining Acuuarcy 32.717% \tValidation Acuuarcy 21.956%\n",
      "Epoch: 175 \tTraining Loss: 0.01301550 \tValidation Loss 0.01620944 \tTraining Acuuarcy 33.168% \tValidation Acuuarcy 22.290%\n",
      "Epoch: 176 \tTraining Loss: 0.01299841 \tValidation Loss 0.01618721 \tTraining Acuuarcy 33.369% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 177 \tTraining Loss: 0.01303924 \tValidation Loss 0.01638784 \tTraining Acuuarcy 33.246% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 178 \tTraining Loss: 0.01305893 \tValidation Loss 0.01615559 \tTraining Acuuarcy 33.062% \tValidation Acuuarcy 21.761%\n",
      "Epoch: 179 \tTraining Loss: 0.01303147 \tValidation Loss 0.01633606 \tTraining Acuuarcy 33.424% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 180 \tTraining Loss: 0.01298946 \tValidation Loss 0.01616385 \tTraining Acuuarcy 33.475% \tValidation Acuuarcy 21.789%\n",
      "Epoch: 181 \tTraining Loss: 0.01298487 \tValidation Loss 0.01627334 \tTraining Acuuarcy 33.458% \tValidation Acuuarcy 22.458%\n",
      "Epoch: 182 \tTraining Loss: 0.01300329 \tValidation Loss 0.01610239 \tTraining Acuuarcy 33.452% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 183 \tTraining Loss: 0.01304695 \tValidation Loss 0.01614575 \tTraining Acuuarcy 33.001% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 184 \tTraining Loss: 0.01300481 \tValidation Loss 0.01624741 \tTraining Acuuarcy 33.079% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 185 \tTraining Loss: 0.01298593 \tValidation Loss 0.01618543 \tTraining Acuuarcy 33.553% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 186 \tTraining Loss: 0.01304097 \tValidation Loss 0.01635189 \tTraining Acuuarcy 33.519% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 187 \tTraining Loss: 0.01297057 \tValidation Loss 0.01632435 \tTraining Acuuarcy 33.341% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 188 \tTraining Loss: 0.01297999 \tValidation Loss 0.01639273 \tTraining Acuuarcy 33.553% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 189 \tTraining Loss: 0.01298641 \tValidation Loss 0.01606600 \tTraining Acuuarcy 33.040% \tValidation Acuuarcy 21.928%\n",
      "Epoch: 190 \tTraining Loss: 0.01297147 \tValidation Loss 0.01608288 \tTraining Acuuarcy 33.430% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 191 \tTraining Loss: 0.01303403 \tValidation Loss 0.01643427 \tTraining Acuuarcy 33.469% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 192 \tTraining Loss: 0.01297299 \tValidation Loss 0.01618046 \tTraining Acuuarcy 33.363% \tValidation Acuuarcy 21.789%\n",
      "Epoch: 193 \tTraining Loss: 0.01296500 \tValidation Loss 0.01628505 \tTraining Acuuarcy 33.240% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 194 \tTraining Loss: 0.01303236 \tValidation Loss 0.01627310 \tTraining Acuuarcy 33.263% \tValidation Acuuarcy 21.872%\n",
      "Epoch: 195 \tTraining Loss: 0.01300338 \tValidation Loss 0.01632024 \tTraining Acuuarcy 33.736% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 196 \tTraining Loss: 0.01299958 \tValidation Loss 0.01654487 \tTraining Acuuarcy 33.575% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 197 \tTraining Loss: 0.01299968 \tValidation Loss 0.01630585 \tTraining Acuuarcy 33.619% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 198 \tTraining Loss: 0.01296784 \tValidation Loss 0.01638710 \tTraining Acuuarcy 33.352% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 199 \tTraining Loss: 0.01295948 \tValidation Loss 0.01595794 \tTraining Acuuarcy 33.681% \tValidation Acuuarcy 22.067%\n",
      "Epoch: 200 \tTraining Loss: 0.01296929 \tValidation Loss 0.01628746 \tTraining Acuuarcy 34.004% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 201 \tTraining Loss: 0.01299389 \tValidation Loss 0.01622028 \tTraining Acuuarcy 33.123% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 202 \tTraining Loss: 0.01293916 \tValidation Loss 0.01636760 \tTraining Acuuarcy 33.998% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 203 \tTraining Loss: 0.01295310 \tValidation Loss 0.01624769 \tTraining Acuuarcy 33.987% \tValidation Acuuarcy 21.984%\n",
      "Epoch: 204 \tTraining Loss: 0.01291258 \tValidation Loss 0.01627129 \tTraining Acuuarcy 33.876% \tValidation Acuuarcy 20.758%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205 \tTraining Loss: 0.01298249 \tValidation Loss 0.01645825 \tTraining Acuuarcy 33.575% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 206 \tTraining Loss: 0.01291681 \tValidation Loss 0.01663087 \tTraining Acuuarcy 34.132% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 207 \tTraining Loss: 0.01293728 \tValidation Loss 0.01623400 \tTraining Acuuarcy 33.486% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 208 \tTraining Loss: 0.01291194 \tValidation Loss 0.01597776 \tTraining Acuuarcy 34.255% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 209 \tTraining Loss: 0.01291933 \tValidation Loss 0.01638176 \tTraining Acuuarcy 34.132% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 210 \tTraining Loss: 0.01294760 \tValidation Loss 0.01598479 \tTraining Acuuarcy 33.653% \tValidation Acuuarcy 21.956%\n",
      "Epoch: 211 \tTraining Loss: 0.01298370 \tValidation Loss 0.01644131 \tTraining Acuuarcy 33.324% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 212 \tTraining Loss: 0.01297679 \tValidation Loss 0.01632302 \tTraining Acuuarcy 33.736% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 213 \tTraining Loss: 0.01293729 \tValidation Loss 0.01632266 \tTraining Acuuarcy 33.904% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 214 \tTraining Loss: 0.01295164 \tValidation Loss 0.01647803 \tTraining Acuuarcy 33.385% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 215 \tTraining Loss: 0.01291220 \tValidation Loss 0.01631774 \tTraining Acuuarcy 33.820% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 216 \tTraining Loss: 0.01290989 \tValidation Loss 0.01632375 \tTraining Acuuarcy 33.675% \tValidation Acuuarcy 21.928%\n",
      "Epoch: 217 \tTraining Loss: 0.01292165 \tValidation Loss 0.01626725 \tTraining Acuuarcy 34.216% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 218 \tTraining Loss: 0.01290806 \tValidation Loss 0.01622270 \tTraining Acuuarcy 33.720% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 219 \tTraining Loss: 0.01296295 \tValidation Loss 0.01622664 \tTraining Acuuarcy 33.658% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 220 \tTraining Loss: 0.01294663 \tValidation Loss 0.01595735 \tTraining Acuuarcy 33.848% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 221 \tTraining Loss: 0.01292206 \tValidation Loss 0.01651486 \tTraining Acuuarcy 33.703% \tValidation Acuuarcy 21.872%\n",
      "Epoch: 222 \tTraining Loss: 0.01290600 \tValidation Loss 0.01626035 \tTraining Acuuarcy 33.748% \tValidation Acuuarcy 22.207%\n",
      "Epoch: 223 \tTraining Loss: 0.01292163 \tValidation Loss 0.01641595 \tTraining Acuuarcy 33.564% \tValidation Acuuarcy 22.040%\n",
      "Epoch: 224 \tTraining Loss: 0.01292766 \tValidation Loss 0.01597000 \tTraining Acuuarcy 33.893% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 225 \tTraining Loss: 0.01293698 \tValidation Loss 0.01635551 \tTraining Acuuarcy 33.514% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 226 \tTraining Loss: 0.01291551 \tValidation Loss 0.01631173 \tTraining Acuuarcy 34.238% \tValidation Acuuarcy 22.318%\n",
      "Epoch: 227 \tTraining Loss: 0.01287252 \tValidation Loss 0.01651496 \tTraining Acuuarcy 34.188% \tValidation Acuuarcy 22.067%\n",
      "Epoch: 228 \tTraining Loss: 0.01285676 \tValidation Loss 0.01644029 \tTraining Acuuarcy 34.416% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 229 \tTraining Loss: 0.01294769 \tValidation Loss 0.01628819 \tTraining Acuuarcy 33.664% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 230 \tTraining Loss: 0.01295332 \tValidation Loss 0.01648426 \tTraining Acuuarcy 33.436% \tValidation Acuuarcy 22.123%\n",
      "Epoch: 231 \tTraining Loss: 0.01288286 \tValidation Loss 0.01662384 \tTraining Acuuarcy 33.831% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 232 \tTraining Loss: 0.01295507 \tValidation Loss 0.01624996 \tTraining Acuuarcy 34.054% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 233 \tTraining Loss: 0.01285880 \tValidation Loss 0.01618667 \tTraining Acuuarcy 34.322% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 234 \tTraining Loss: 0.01287134 \tValidation Loss 0.01669052 \tTraining Acuuarcy 34.350% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 235 \tTraining Loss: 0.01285413 \tValidation Loss 0.01628207 \tTraining Acuuarcy 34.545% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 236 \tTraining Loss: 0.01292831 \tValidation Loss 0.01632314 \tTraining Acuuarcy 33.614% \tValidation Acuuarcy 22.095%\n",
      "Epoch: 237 \tTraining Loss: 0.01289035 \tValidation Loss 0.01658802 \tTraining Acuuarcy 34.099% \tValidation Acuuarcy 21.845%\n",
      "Epoch: 238 \tTraining Loss: 0.01286829 \tValidation Loss 0.01624974 \tTraining Acuuarcy 34.294% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 239 \tTraining Loss: 0.01284214 \tValidation Loss 0.01627146 \tTraining Acuuarcy 34.455% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 240 \tTraining Loss: 0.01290569 \tValidation Loss 0.01626650 \tTraining Acuuarcy 34.272% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 241 \tTraining Loss: 0.01289351 \tValidation Loss 0.01634262 \tTraining Acuuarcy 34.500% \tValidation Acuuarcy 22.179%\n",
      "Epoch: 242 \tTraining Loss: 0.01288263 \tValidation Loss 0.01624012 \tTraining Acuuarcy 34.065% \tValidation Acuuarcy 21.482%\n",
      "Epoch: 243 \tTraining Loss: 0.01290190 \tValidation Loss 0.01625958 \tTraining Acuuarcy 33.809% \tValidation Acuuarcy 22.625%\n",
      "Epoch: 244 \tTraining Loss: 0.01286634 \tValidation Loss 0.01653029 \tTraining Acuuarcy 34.160% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 245 \tTraining Loss: 0.01288616 \tValidation Loss 0.01620415 \tTraining Acuuarcy 33.948% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 246 \tTraining Loss: 0.01287830 \tValidation Loss 0.01638614 \tTraining Acuuarcy 33.881% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 247 \tTraining Loss: 0.01284645 \tValidation Loss 0.01610614 \tTraining Acuuarcy 34.389% \tValidation Acuuarcy 21.928%\n",
      "Epoch: 248 \tTraining Loss: 0.01283783 \tValidation Loss 0.01631271 \tTraining Acuuarcy 34.288% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 249 \tTraining Loss: 0.01284698 \tValidation Loss 0.01634880 \tTraining Acuuarcy 34.550% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 250 \tTraining Loss: 0.01287807 \tValidation Loss 0.01642513 \tTraining Acuuarcy 34.361% \tValidation Acuuarcy 21.761%\n",
      "Epoch: 251 \tTraining Loss: 0.01284969 \tValidation Loss 0.01642339 \tTraining Acuuarcy 34.533% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 252 \tTraining Loss: 0.01281135 \tValidation Loss 0.01624277 \tTraining Acuuarcy 34.589% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 253 \tTraining Loss: 0.01282710 \tValidation Loss 0.01617189 \tTraining Acuuarcy 34.483% \tValidation Acuuarcy 22.123%\n",
      "Epoch: 254 \tTraining Loss: 0.01286300 \tValidation Loss 0.01638913 \tTraining Acuuarcy 33.954% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 255 \tTraining Loss: 0.01279561 \tValidation Loss 0.01635451 \tTraining Acuuarcy 35.052% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 256 \tTraining Loss: 0.01290335 \tValidation Loss 0.01616794 \tTraining Acuuarcy 34.043% \tValidation Acuuarcy 22.458%\n",
      "Epoch: 257 \tTraining Loss: 0.01286408 \tValidation Loss 0.01626256 \tTraining Acuuarcy 34.099% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 258 \tTraining Loss: 0.01285248 \tValidation Loss 0.01652874 \tTraining Acuuarcy 34.840% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 259 \tTraining Loss: 0.01285269 \tValidation Loss 0.01622959 \tTraining Acuuarcy 34.628% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 260 \tTraining Loss: 0.01287409 \tValidation Loss 0.01623110 \tTraining Acuuarcy 34.071% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 261 \tTraining Loss: 0.01283768 \tValidation Loss 0.01618521 \tTraining Acuuarcy 34.277% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 262 \tTraining Loss: 0.01287765 \tValidation Loss 0.01631834 \tTraining Acuuarcy 34.322% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 263 \tTraining Loss: 0.01282253 \tValidation Loss 0.01625526 \tTraining Acuuarcy 34.494% \tValidation Acuuarcy 21.984%\n",
      "Epoch: 264 \tTraining Loss: 0.01283492 \tValidation Loss 0.01644627 \tTraining Acuuarcy 34.249% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 265 \tTraining Loss: 0.01284383 \tValidation Loss 0.01631021 \tTraining Acuuarcy 34.695% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 266 \tTraining Loss: 0.01281985 \tValidation Loss 0.01640702 \tTraining Acuuarcy 34.478% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 267 \tTraining Loss: 0.01283498 \tValidation Loss 0.01676951 \tTraining Acuuarcy 34.868% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 268 \tTraining Loss: 0.01284065 \tValidation Loss 0.01644537 \tTraining Acuuarcy 34.662% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 269 \tTraining Loss: 0.01283432 \tValidation Loss 0.01658693 \tTraining Acuuarcy 34.021% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 270 \tTraining Loss: 0.01277026 \tValidation Loss 0.01658954 \tTraining Acuuarcy 35.303% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 271 \tTraining Loss: 0.01288318 \tValidation Loss 0.01622284 \tTraining Acuuarcy 33.837% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 272 \tTraining Loss: 0.01284953 \tValidation Loss 0.01653642 \tTraining Acuuarcy 34.311% \tValidation Acuuarcy 21.705%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273 \tTraining Loss: 0.01281657 \tValidation Loss 0.01660647 \tTraining Acuuarcy 34.439% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 274 \tTraining Loss: 0.01279402 \tValidation Loss 0.01662795 \tTraining Acuuarcy 34.952% \tValidation Acuuarcy 22.597%\n",
      "Epoch: 275 \tTraining Loss: 0.01278977 \tValidation Loss 0.01708449 \tTraining Acuuarcy 35.013% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 276 \tTraining Loss: 0.01281897 \tValidation Loss 0.01630067 \tTraining Acuuarcy 34.678% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 277 \tTraining Loss: 0.01280328 \tValidation Loss 0.01642407 \tTraining Acuuarcy 34.533% \tValidation Acuuarcy 21.789%\n",
      "Epoch: 278 \tTraining Loss: 0.01277337 \tValidation Loss 0.01682148 \tTraining Acuuarcy 34.539% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 279 \tTraining Loss: 0.01279092 \tValidation Loss 0.01646313 \tTraining Acuuarcy 34.706% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 280 \tTraining Loss: 0.01280619 \tValidation Loss 0.01652035 \tTraining Acuuarcy 34.272% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 281 \tTraining Loss: 0.01279150 \tValidation Loss 0.01678260 \tTraining Acuuarcy 34.740% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 282 \tTraining Loss: 0.01279008 \tValidation Loss 0.01678162 \tTraining Acuuarcy 34.851% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 283 \tTraining Loss: 0.01282043 \tValidation Loss 0.01651867 \tTraining Acuuarcy 34.567% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 284 \tTraining Loss: 0.01283912 \tValidation Loss 0.01649968 \tTraining Acuuarcy 34.333% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 285 \tTraining Loss: 0.01279660 \tValidation Loss 0.01616883 \tTraining Acuuarcy 34.857% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 286 \tTraining Loss: 0.01282010 \tValidation Loss 0.01653165 \tTraining Acuuarcy 34.768% \tValidation Acuuarcy 22.541%\n",
      "Epoch: 287 \tTraining Loss: 0.01282989 \tValidation Loss 0.01632566 \tTraining Acuuarcy 35.108% \tValidation Acuuarcy 21.482%\n",
      "Epoch: 288 \tTraining Loss: 0.01284282 \tValidation Loss 0.01640924 \tTraining Acuuarcy 34.556% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 289 \tTraining Loss: 0.01278471 \tValidation Loss 0.01626323 \tTraining Acuuarcy 34.935% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 290 \tTraining Loss: 0.01277074 \tValidation Loss 0.01642425 \tTraining Acuuarcy 35.163% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 291 \tTraining Loss: 0.01285461 \tValidation Loss 0.01655321 \tTraining Acuuarcy 34.327% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 292 \tTraining Loss: 0.01281900 \tValidation Loss 0.01638326 \tTraining Acuuarcy 34.667% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 293 \tTraining Loss: 0.01275226 \tValidation Loss 0.01654816 \tTraining Acuuarcy 35.069% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 294 \tTraining Loss: 0.01278574 \tValidation Loss 0.01669553 \tTraining Acuuarcy 34.589% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 295 \tTraining Loss: 0.01278048 \tValidation Loss 0.01650910 \tTraining Acuuarcy 34.924% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 296 \tTraining Loss: 0.01274106 \tValidation Loss 0.01670071 \tTraining Acuuarcy 35.436% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 297 \tTraining Loss: 0.01275550 \tValidation Loss 0.01645423 \tTraining Acuuarcy 35.219% \tValidation Acuuarcy 21.482%\n",
      "Epoch: 298 \tTraining Loss: 0.01281685 \tValidation Loss 0.01641448 \tTraining Acuuarcy 34.862% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 299 \tTraining Loss: 0.01272208 \tValidation Loss 0.01663686 \tTraining Acuuarcy 34.706% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 300 \tTraining Loss: 0.01277776 \tValidation Loss 0.01633383 \tTraining Acuuarcy 34.918% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 301 \tTraining Loss: 0.01274145 \tValidation Loss 0.01641995 \tTraining Acuuarcy 34.756% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 302 \tTraining Loss: 0.01273680 \tValidation Loss 0.01687978 \tTraining Acuuarcy 35.370% \tValidation Acuuarcy 22.207%\n",
      "Epoch: 303 \tTraining Loss: 0.01280862 \tValidation Loss 0.01646699 \tTraining Acuuarcy 34.522% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 304 \tTraining Loss: 0.01273757 \tValidation Loss 0.01652520 \tTraining Acuuarcy 35.030% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 305 \tTraining Loss: 0.01276586 \tValidation Loss 0.01646762 \tTraining Acuuarcy 35.046% \tValidation Acuuarcy 21.845%\n",
      "Epoch: 306 \tTraining Loss: 0.01278482 \tValidation Loss 0.01635666 \tTraining Acuuarcy 34.834% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 307 \tTraining Loss: 0.01272137 \tValidation Loss 0.01681668 \tTraining Acuuarcy 35.074% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 308 \tTraining Loss: 0.01275960 \tValidation Loss 0.01651213 \tTraining Acuuarcy 34.784% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 309 \tTraining Loss: 0.01278002 \tValidation Loss 0.01672561 \tTraining Acuuarcy 35.057% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 310 \tTraining Loss: 0.01271694 \tValidation Loss 0.01648510 \tTraining Acuuarcy 35.436% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 311 \tTraining Loss: 0.01271812 \tValidation Loss 0.01695376 \tTraining Acuuarcy 35.392% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 312 \tTraining Loss: 0.01271450 \tValidation Loss 0.01682590 \tTraining Acuuarcy 35.414% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 313 \tTraining Loss: 0.01283192 \tValidation Loss 0.01641109 \tTraining Acuuarcy 34.645% \tValidation Acuuarcy 21.817%\n",
      "Epoch: 314 \tTraining Loss: 0.01269885 \tValidation Loss 0.01657764 \tTraining Acuuarcy 35.269% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 315 \tTraining Loss: 0.01271154 \tValidation Loss 0.01627305 \tTraining Acuuarcy 34.690% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 316 \tTraining Loss: 0.01271299 \tValidation Loss 0.01673930 \tTraining Acuuarcy 35.710% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 317 \tTraining Loss: 0.01274523 \tValidation Loss 0.01666733 \tTraining Acuuarcy 35.124% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 318 \tTraining Loss: 0.01272876 \tValidation Loss 0.01644999 \tTraining Acuuarcy 34.751% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 319 \tTraining Loss: 0.01273096 \tValidation Loss 0.01656900 \tTraining Acuuarcy 35.336% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 320 \tTraining Loss: 0.01268832 \tValidation Loss 0.01660506 \tTraining Acuuarcy 35.715% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 321 \tTraining Loss: 0.01270931 \tValidation Loss 0.01650892 \tTraining Acuuarcy 35.409% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 322 \tTraining Loss: 0.01275153 \tValidation Loss 0.01670045 \tTraining Acuuarcy 35.007% \tValidation Acuuarcy 21.845%\n",
      "Epoch: 323 \tTraining Loss: 0.01272119 \tValidation Loss 0.01678024 \tTraining Acuuarcy 35.370% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 324 \tTraining Loss: 0.01272064 \tValidation Loss 0.01667307 \tTraining Acuuarcy 35.197% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 325 \tTraining Loss: 0.01267610 \tValidation Loss 0.01661740 \tTraining Acuuarcy 35.069% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 326 \tTraining Loss: 0.01269486 \tValidation Loss 0.01668289 \tTraining Acuuarcy 35.392% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 327 \tTraining Loss: 0.01271844 \tValidation Loss 0.01679474 \tTraining Acuuarcy 35.202% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 328 \tTraining Loss: 0.01273088 \tValidation Loss 0.01652484 \tTraining Acuuarcy 35.057% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 329 \tTraining Loss: 0.01268913 \tValidation Loss 0.01659366 \tTraining Acuuarcy 34.773% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 330 \tTraining Loss: 0.01268797 \tValidation Loss 0.01650291 \tTraining Acuuarcy 35.280% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 331 \tTraining Loss: 0.01268108 \tValidation Loss 0.01720561 \tTraining Acuuarcy 35.754% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 332 \tTraining Loss: 0.01275169 \tValidation Loss 0.01678748 \tTraining Acuuarcy 34.723% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 333 \tTraining Loss: 0.01273039 \tValidation Loss 0.01660013 \tTraining Acuuarcy 35.091% \tValidation Acuuarcy 21.789%\n",
      "Epoch: 334 \tTraining Loss: 0.01266748 \tValidation Loss 0.01657549 \tTraining Acuuarcy 35.314% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 335 \tTraining Loss: 0.01268320 \tValidation Loss 0.01680238 \tTraining Acuuarcy 35.331% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 336 \tTraining Loss: 0.01266199 \tValidation Loss 0.01662974 \tTraining Acuuarcy 35.626% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 337 \tTraining Loss: 0.01268834 \tValidation Loss 0.01683332 \tTraining Acuuarcy 35.236% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 338 \tTraining Loss: 0.01269997 \tValidation Loss 0.01651791 \tTraining Acuuarcy 35.314% \tValidation Acuuarcy 21.761%\n",
      "Epoch: 339 \tTraining Loss: 0.01269706 \tValidation Loss 0.01655632 \tTraining Acuuarcy 35.102% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 340 \tTraining Loss: 0.01264411 \tValidation Loss 0.01695233 \tTraining Acuuarcy 36.189% \tValidation Acuuarcy 21.204%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 341 \tTraining Loss: 0.01272112 \tValidation Loss 0.01664989 \tTraining Acuuarcy 35.264% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 342 \tTraining Loss: 0.01269845 \tValidation Loss 0.01647248 \tTraining Acuuarcy 35.670% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 343 \tTraining Loss: 0.01266276 \tValidation Loss 0.01648721 \tTraining Acuuarcy 35.788% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 344 \tTraining Loss: 0.01266298 \tValidation Loss 0.01675252 \tTraining Acuuarcy 35.637% \tValidation Acuuarcy 22.040%\n",
      "Epoch: 345 \tTraining Loss: 0.01267999 \tValidation Loss 0.01657877 \tTraining Acuuarcy 35.397% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 346 \tTraining Loss: 0.01267428 \tValidation Loss 0.01656236 \tTraining Acuuarcy 35.247% \tValidation Acuuarcy 21.872%\n",
      "Epoch: 347 \tTraining Loss: 0.01265127 \tValidation Loss 0.01654684 \tTraining Acuuarcy 35.581% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 348 \tTraining Loss: 0.01268593 \tValidation Loss 0.01656378 \tTraining Acuuarcy 35.537% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 349 \tTraining Loss: 0.01269113 \tValidation Loss 0.01668608 \tTraining Acuuarcy 35.135% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 350 \tTraining Loss: 0.01267070 \tValidation Loss 0.01661987 \tTraining Acuuarcy 35.208% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 351 \tTraining Loss: 0.01260080 \tValidation Loss 0.01673639 \tTraining Acuuarcy 36.178% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 352 \tTraining Loss: 0.01264012 \tValidation Loss 0.01651649 \tTraining Acuuarcy 35.732% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 353 \tTraining Loss: 0.01269621 \tValidation Loss 0.01641187 \tTraining Acuuarcy 35.643% \tValidation Acuuarcy 22.151%\n",
      "Epoch: 354 \tTraining Loss: 0.01275043 \tValidation Loss 0.01651290 \tTraining Acuuarcy 34.829% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 355 \tTraining Loss: 0.01264498 \tValidation Loss 0.01654489 \tTraining Acuuarcy 35.871% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 356 \tTraining Loss: 0.01263240 \tValidation Loss 0.01670242 \tTraining Acuuarcy 36.010% \tValidation Acuuarcy 21.984%\n",
      "Epoch: 357 \tTraining Loss: 0.01264316 \tValidation Loss 0.01716569 \tTraining Acuuarcy 35.782% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 358 \tTraining Loss: 0.01268193 \tValidation Loss 0.01665195 \tTraining Acuuarcy 35.319% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 359 \tTraining Loss: 0.01261491 \tValidation Loss 0.01668857 \tTraining Acuuarcy 35.676% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 360 \tTraining Loss: 0.01268471 \tValidation Loss 0.01625586 \tTraining Acuuarcy 35.721% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 361 \tTraining Loss: 0.01261952 \tValidation Loss 0.01686427 \tTraining Acuuarcy 35.983% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 362 \tTraining Loss: 0.01258300 \tValidation Loss 0.01690792 \tTraining Acuuarcy 36.005% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 363 \tTraining Loss: 0.01260801 \tValidation Loss 0.01667571 \tTraining Acuuarcy 35.788% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 364 \tTraining Loss: 0.01265667 \tValidation Loss 0.01669829 \tTraining Acuuarcy 35.637% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 365 \tTraining Loss: 0.01258812 \tValidation Loss 0.01657587 \tTraining Acuuarcy 35.927% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 366 \tTraining Loss: 0.01263296 \tValidation Loss 0.01706087 \tTraining Acuuarcy 35.743% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 367 \tTraining Loss: 0.01264611 \tValidation Loss 0.01666857 \tTraining Acuuarcy 35.286% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 368 \tTraining Loss: 0.01264327 \tValidation Loss 0.01687353 \tTraining Acuuarcy 35.871% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 369 \tTraining Loss: 0.01263593 \tValidation Loss 0.01655003 \tTraining Acuuarcy 35.676% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 370 \tTraining Loss: 0.01265378 \tValidation Loss 0.01641688 \tTraining Acuuarcy 35.849% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 371 \tTraining Loss: 0.01259640 \tValidation Loss 0.01672679 \tTraining Acuuarcy 35.921% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 372 \tTraining Loss: 0.01256037 \tValidation Loss 0.01698729 \tTraining Acuuarcy 36.150% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 373 \tTraining Loss: 0.01257687 \tValidation Loss 0.01678726 \tTraining Acuuarcy 36.434% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 374 \tTraining Loss: 0.01258526 \tValidation Loss 0.01679311 \tTraining Acuuarcy 35.938% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 375 \tTraining Loss: 0.01261325 \tValidation Loss 0.01660132 \tTraining Acuuarcy 36.256% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 376 \tTraining Loss: 0.01255962 \tValidation Loss 0.01687617 \tTraining Acuuarcy 36.573% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 377 \tTraining Loss: 0.01256426 \tValidation Loss 0.01647398 \tTraining Acuuarcy 35.955% \tValidation Acuuarcy 22.207%\n",
      "Epoch: 378 \tTraining Loss: 0.01255130 \tValidation Loss 0.01730081 \tTraining Acuuarcy 36.033% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 379 \tTraining Loss: 0.01257395 \tValidation Loss 0.01687780 \tTraining Acuuarcy 35.999% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 380 \tTraining Loss: 0.01260240 \tValidation Loss 0.01684026 \tTraining Acuuarcy 35.704% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 381 \tTraining Loss: 0.01261312 \tValidation Loss 0.01709286 \tTraining Acuuarcy 35.843% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 382 \tTraining Loss: 0.01253935 \tValidation Loss 0.01674897 \tTraining Acuuarcy 36.473% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 383 \tTraining Loss: 0.01261524 \tValidation Loss 0.01677605 \tTraining Acuuarcy 36.211% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 384 \tTraining Loss: 0.01256408 \tValidation Loss 0.01697886 \tTraining Acuuarcy 36.546% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 385 \tTraining Loss: 0.01260850 \tValidation Loss 0.01651647 \tTraining Acuuarcy 35.960% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 386 \tTraining Loss: 0.01258283 \tValidation Loss 0.01655148 \tTraining Acuuarcy 36.451% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 387 \tTraining Loss: 0.01254692 \tValidation Loss 0.01663405 \tTraining Acuuarcy 36.479% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 388 \tTraining Loss: 0.01263167 \tValidation Loss 0.01664407 \tTraining Acuuarcy 35.955% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 389 \tTraining Loss: 0.01259077 \tValidation Loss 0.01706101 \tTraining Acuuarcy 36.217% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 390 \tTraining Loss: 0.01256907 \tValidation Loss 0.01673333 \tTraining Acuuarcy 36.217% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 391 \tTraining Loss: 0.01263301 \tValidation Loss 0.01679964 \tTraining Acuuarcy 36.133% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 392 \tTraining Loss: 0.01260515 \tValidation Loss 0.01699006 \tTraining Acuuarcy 36.044% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 393 \tTraining Loss: 0.01251709 \tValidation Loss 0.01663817 \tTraining Acuuarcy 37.053% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 394 \tTraining Loss: 0.01256536 \tValidation Loss 0.01689262 \tTraining Acuuarcy 36.211% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 395 \tTraining Loss: 0.01256167 \tValidation Loss 0.01683370 \tTraining Acuuarcy 36.373% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 396 \tTraining Loss: 0.01254453 \tValidation Loss 0.01689498 \tTraining Acuuarcy 36.323% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 397 \tTraining Loss: 0.01263603 \tValidation Loss 0.01678508 \tTraining Acuuarcy 35.838% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 398 \tTraining Loss: 0.01253450 \tValidation Loss 0.01671070 \tTraining Acuuarcy 36.451% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 399 \tTraining Loss: 0.01252615 \tValidation Loss 0.01704585 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 400 \tTraining Loss: 0.01250216 \tValidation Loss 0.01711628 \tTraining Acuuarcy 36.869% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 401 \tTraining Loss: 0.01257686 \tValidation Loss 0.01693714 \tTraining Acuuarcy 36.384% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 402 \tTraining Loss: 0.01255424 \tValidation Loss 0.01662113 \tTraining Acuuarcy 36.456% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 403 \tTraining Loss: 0.01253201 \tValidation Loss 0.01707832 \tTraining Acuuarcy 36.256% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 404 \tTraining Loss: 0.01254148 \tValidation Loss 0.01668177 \tTraining Acuuarcy 36.233% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 405 \tTraining Loss: 0.01251108 \tValidation Loss 0.01677231 \tTraining Acuuarcy 36.501% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 406 \tTraining Loss: 0.01248576 \tValidation Loss 0.01685358 \tTraining Acuuarcy 36.306% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 407 \tTraining Loss: 0.01248668 \tValidation Loss 0.01720266 \tTraining Acuuarcy 36.685% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 408 \tTraining Loss: 0.01254314 \tValidation Loss 0.01718671 \tTraining Acuuarcy 36.250% \tValidation Acuuarcy 20.089%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 409 \tTraining Loss: 0.01255394 \tValidation Loss 0.01701581 \tTraining Acuuarcy 36.189% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 410 \tTraining Loss: 0.01251535 \tValidation Loss 0.01694604 \tTraining Acuuarcy 36.802% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 411 \tTraining Loss: 0.01255240 \tValidation Loss 0.01671923 \tTraining Acuuarcy 36.406% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 412 \tTraining Loss: 0.01257018 \tValidation Loss 0.01703221 \tTraining Acuuarcy 36.401% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 413 \tTraining Loss: 0.01246942 \tValidation Loss 0.01719544 \tTraining Acuuarcy 37.159% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 414 \tTraining Loss: 0.01251964 \tValidation Loss 0.01694611 \tTraining Acuuarcy 36.679% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 415 \tTraining Loss: 0.01247204 \tValidation Loss 0.01695283 \tTraining Acuuarcy 37.175% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 416 \tTraining Loss: 0.01251753 \tValidation Loss 0.01667912 \tTraining Acuuarcy 36.206% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 417 \tTraining Loss: 0.01257214 \tValidation Loss 0.01680885 \tTraining Acuuarcy 36.200% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 418 \tTraining Loss: 0.01248027 \tValidation Loss 0.01666881 \tTraining Acuuarcy 37.108% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 419 \tTraining Loss: 0.01247827 \tValidation Loss 0.01689768 \tTraining Acuuarcy 36.635% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 420 \tTraining Loss: 0.01250669 \tValidation Loss 0.01677886 \tTraining Acuuarcy 36.206% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 421 \tTraining Loss: 0.01255643 \tValidation Loss 0.01696256 \tTraining Acuuarcy 36.323% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 422 \tTraining Loss: 0.01249887 \tValidation Loss 0.01694875 \tTraining Acuuarcy 36.780% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 423 \tTraining Loss: 0.01251499 \tValidation Loss 0.01703369 \tTraining Acuuarcy 36.707% \tValidation Acuuarcy 21.817%\n",
      "Epoch: 424 \tTraining Loss: 0.01246217 \tValidation Loss 0.01673970 \tTraining Acuuarcy 36.674% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 425 \tTraining Loss: 0.01244883 \tValidation Loss 0.01713776 \tTraining Acuuarcy 36.557% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 426 \tTraining Loss: 0.01252992 \tValidation Loss 0.01698254 \tTraining Acuuarcy 36.518% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 427 \tTraining Loss: 0.01245697 \tValidation Loss 0.01701332 \tTraining Acuuarcy 36.780% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 428 \tTraining Loss: 0.01253169 \tValidation Loss 0.01689048 \tTraining Acuuarcy 36.434% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 429 \tTraining Loss: 0.01249586 \tValidation Loss 0.01660440 \tTraining Acuuarcy 36.272% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 430 \tTraining Loss: 0.01255429 \tValidation Loss 0.01665468 \tTraining Acuuarcy 36.155% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 431 \tTraining Loss: 0.01251364 \tValidation Loss 0.01687791 \tTraining Acuuarcy 37.131% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 432 \tTraining Loss: 0.01257853 \tValidation Loss 0.01669597 \tTraining Acuuarcy 36.233% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 433 \tTraining Loss: 0.01250333 \tValidation Loss 0.01667700 \tTraining Acuuarcy 36.334% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 434 \tTraining Loss: 0.01245471 \tValidation Loss 0.01706807 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 435 \tTraining Loss: 0.01250685 \tValidation Loss 0.01690028 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 436 \tTraining Loss: 0.01249024 \tValidation Loss 0.01688525 \tTraining Acuuarcy 36.874% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 437 \tTraining Loss: 0.01249402 \tValidation Loss 0.01667834 \tTraining Acuuarcy 36.657% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 438 \tTraining Loss: 0.01248946 \tValidation Loss 0.01723437 \tTraining Acuuarcy 36.724% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 439 \tTraining Loss: 0.01247307 \tValidation Loss 0.01689782 \tTraining Acuuarcy 36.612% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 440 \tTraining Loss: 0.01242663 \tValidation Loss 0.01684976 \tTraining Acuuarcy 36.807% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 441 \tTraining Loss: 0.01237890 \tValidation Loss 0.01689215 \tTraining Acuuarcy 37.944% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 442 \tTraining Loss: 0.01250158 \tValidation Loss 0.01686931 \tTraining Acuuarcy 36.657% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 443 \tTraining Loss: 0.01250847 \tValidation Loss 0.01693129 \tTraining Acuuarcy 36.718% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 444 \tTraining Loss: 0.01249036 \tValidation Loss 0.01704486 \tTraining Acuuarcy 36.741% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 445 \tTraining Loss: 0.01248715 \tValidation Loss 0.01701067 \tTraining Acuuarcy 36.529% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 446 \tTraining Loss: 0.01245382 \tValidation Loss 0.01708097 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 447 \tTraining Loss: 0.01245249 \tValidation Loss 0.01697486 \tTraining Acuuarcy 37.008% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 448 \tTraining Loss: 0.01247205 \tValidation Loss 0.01694974 \tTraining Acuuarcy 36.969% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 449 \tTraining Loss: 0.01244820 \tValidation Loss 0.01728069 \tTraining Acuuarcy 37.276% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 450 \tTraining Loss: 0.01250328 \tValidation Loss 0.01690974 \tTraining Acuuarcy 36.746% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 451 \tTraining Loss: 0.01244359 \tValidation Loss 0.01684093 \tTraining Acuuarcy 37.120% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 452 \tTraining Loss: 0.01247434 \tValidation Loss 0.01738619 \tTraining Acuuarcy 36.785% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 453 \tTraining Loss: 0.01245182 \tValidation Loss 0.01696326 \tTraining Acuuarcy 36.952% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 454 \tTraining Loss: 0.01250344 \tValidation Loss 0.01680808 \tTraining Acuuarcy 36.869% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 455 \tTraining Loss: 0.01246616 \tValidation Loss 0.01676601 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 456 \tTraining Loss: 0.01249411 \tValidation Loss 0.01730017 \tTraining Acuuarcy 36.311% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 457 \tTraining Loss: 0.01247144 \tValidation Loss 0.01667817 \tTraining Acuuarcy 36.991% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 458 \tTraining Loss: 0.01250940 \tValidation Loss 0.01713221 \tTraining Acuuarcy 36.869% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 459 \tTraining Loss: 0.01242604 \tValidation Loss 0.01705618 \tTraining Acuuarcy 36.768% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 460 \tTraining Loss: 0.01241270 \tValidation Loss 0.01710210 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 461 \tTraining Loss: 0.01242452 \tValidation Loss 0.01700728 \tTraining Acuuarcy 37.175% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 462 \tTraining Loss: 0.01245066 \tValidation Loss 0.01666078 \tTraining Acuuarcy 37.382% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 463 \tTraining Loss: 0.01244693 \tValidation Loss 0.01678741 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 464 \tTraining Loss: 0.01240050 \tValidation Loss 0.01712714 \tTraining Acuuarcy 36.796% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 465 \tTraining Loss: 0.01242369 \tValidation Loss 0.01712878 \tTraining Acuuarcy 37.209% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 466 \tTraining Loss: 0.01240559 \tValidation Loss 0.01677022 \tTraining Acuuarcy 37.526% \tValidation Acuuarcy 22.151%\n",
      "Epoch: 467 \tTraining Loss: 0.01244846 \tValidation Loss 0.01709905 \tTraining Acuuarcy 37.270% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 468 \tTraining Loss: 0.01245477 \tValidation Loss 0.01677103 \tTraining Acuuarcy 37.270% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 469 \tTraining Loss: 0.01243447 \tValidation Loss 0.01698442 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 470 \tTraining Loss: 0.01240485 \tValidation Loss 0.01728867 \tTraining Acuuarcy 37.042% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 471 \tTraining Loss: 0.01243755 \tValidation Loss 0.01659145 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 472 \tTraining Loss: 0.01245607 \tValidation Loss 0.01699696 \tTraining Acuuarcy 36.835% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 473 \tTraining Loss: 0.01241698 \tValidation Loss 0.01729956 \tTraining Acuuarcy 36.713% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 474 \tTraining Loss: 0.01241313 \tValidation Loss 0.01732919 \tTraining Acuuarcy 36.757% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 475 \tTraining Loss: 0.01244086 \tValidation Loss 0.01695979 \tTraining Acuuarcy 37.058% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 476 \tTraining Loss: 0.01251015 \tValidation Loss 0.01644350 \tTraining Acuuarcy 36.272% \tValidation Acuuarcy 22.123%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477 \tTraining Loss: 0.01242692 \tValidation Loss 0.01681221 \tTraining Acuuarcy 37.382% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 478 \tTraining Loss: 0.01246548 \tValidation Loss 0.01708843 \tTraining Acuuarcy 37.036% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 479 \tTraining Loss: 0.01240341 \tValidation Loss 0.01693500 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 480 \tTraining Loss: 0.01241581 \tValidation Loss 0.01711258 \tTraining Acuuarcy 36.908% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 481 \tTraining Loss: 0.01245641 \tValidation Loss 0.01709274 \tTraining Acuuarcy 37.047% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 482 \tTraining Loss: 0.01239908 \tValidation Loss 0.01749031 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 483 \tTraining Loss: 0.01243858 \tValidation Loss 0.01695536 \tTraining Acuuarcy 37.108% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 484 \tTraining Loss: 0.01245167 \tValidation Loss 0.01697138 \tTraining Acuuarcy 36.874% \tValidation Acuuarcy 21.789%\n",
      "Epoch: 485 \tTraining Loss: 0.01241017 \tValidation Loss 0.01674244 \tTraining Acuuarcy 37.632% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 486 \tTraining Loss: 0.01236907 \tValidation Loss 0.01707913 \tTraining Acuuarcy 37.549% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 487 \tTraining Loss: 0.01245483 \tValidation Loss 0.01690686 \tTraining Acuuarcy 36.802% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 488 \tTraining Loss: 0.01245421 \tValidation Loss 0.01698455 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 489 \tTraining Loss: 0.01241695 \tValidation Loss 0.01752613 \tTraining Acuuarcy 36.919% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 490 \tTraining Loss: 0.01238551 \tValidation Loss 0.01724883 \tTraining Acuuarcy 37.404% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 491 \tTraining Loss: 0.01242879 \tValidation Loss 0.01690113 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 492 \tTraining Loss: 0.01239101 \tValidation Loss 0.01692344 \tTraining Acuuarcy 37.337% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 493 \tTraining Loss: 0.01234738 \tValidation Loss 0.01721948 \tTraining Acuuarcy 37.671% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 494 \tTraining Loss: 0.01240952 \tValidation Loss 0.01691003 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 495 \tTraining Loss: 0.01239542 \tValidation Loss 0.01709974 \tTraining Acuuarcy 37.811% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 496 \tTraining Loss: 0.01246125 \tValidation Loss 0.01693877 \tTraining Acuuarcy 37.097% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 497 \tTraining Loss: 0.01236903 \tValidation Loss 0.01692939 \tTraining Acuuarcy 37.644% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 498 \tTraining Loss: 0.01241464 \tValidation Loss 0.01742908 \tTraining Acuuarcy 37.315% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 499 \tTraining Loss: 0.01248122 \tValidation Loss 0.01681674 \tTraining Acuuarcy 37.281% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 500 \tTraining Loss: 0.01234931 \tValidation Loss 0.01687037 \tTraining Acuuarcy 38.140% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 501 \tTraining Loss: 0.01239528 \tValidation Loss 0.01695721 \tTraining Acuuarcy 37.460% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 502 \tTraining Loss: 0.01233388 \tValidation Loss 0.01718499 \tTraining Acuuarcy 37.722% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 503 \tTraining Loss: 0.01239912 \tValidation Loss 0.01695766 \tTraining Acuuarcy 37.638% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 504 \tTraining Loss: 0.01239234 \tValidation Loss 0.01699604 \tTraining Acuuarcy 37.226% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 505 \tTraining Loss: 0.01238961 \tValidation Loss 0.01671576 \tTraining Acuuarcy 37.019% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 506 \tTraining Loss: 0.01244421 \tValidation Loss 0.01700858 \tTraining Acuuarcy 36.902% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 507 \tTraining Loss: 0.01240080 \tValidation Loss 0.01744607 \tTraining Acuuarcy 37.811% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 508 \tTraining Loss: 0.01237001 \tValidation Loss 0.01715024 \tTraining Acuuarcy 36.969% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 509 \tTraining Loss: 0.01241167 \tValidation Loss 0.01679926 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 510 \tTraining Loss: 0.01246748 \tValidation Loss 0.01696998 \tTraining Acuuarcy 37.131% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 511 \tTraining Loss: 0.01240022 \tValidation Loss 0.01715472 \tTraining Acuuarcy 37.354% \tValidation Acuuarcy 22.207%\n",
      "Epoch: 512 \tTraining Loss: 0.01239976 \tValidation Loss 0.01725394 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 513 \tTraining Loss: 0.01239789 \tValidation Loss 0.01693138 \tTraining Acuuarcy 37.326% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 514 \tTraining Loss: 0.01235558 \tValidation Loss 0.01700306 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 515 \tTraining Loss: 0.01235471 \tValidation Loss 0.01730193 \tTraining Acuuarcy 37.281% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 516 \tTraining Loss: 0.01241648 \tValidation Loss 0.01707712 \tTraining Acuuarcy 37.387% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 517 \tTraining Loss: 0.01242424 \tValidation Loss 0.01691352 \tTraining Acuuarcy 37.304% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 518 \tTraining Loss: 0.01231256 \tValidation Loss 0.01673646 \tTraining Acuuarcy 37.911% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 519 \tTraining Loss: 0.01236585 \tValidation Loss 0.01686751 \tTraining Acuuarcy 37.621% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 520 \tTraining Loss: 0.01233564 \tValidation Loss 0.01684827 \tTraining Acuuarcy 37.710% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 521 \tTraining Loss: 0.01236776 \tValidation Loss 0.01706410 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 522 \tTraining Loss: 0.01237034 \tValidation Loss 0.01697569 \tTraining Acuuarcy 37.443% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 523 \tTraining Loss: 0.01233410 \tValidation Loss 0.01701542 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 524 \tTraining Loss: 0.01237646 \tValidation Loss 0.01721453 \tTraining Acuuarcy 37.231% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 525 \tTraining Loss: 0.01238582 \tValidation Loss 0.01707897 \tTraining Acuuarcy 37.487% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 526 \tTraining Loss: 0.01233334 \tValidation Loss 0.01747589 \tTraining Acuuarcy 38.050% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 527 \tTraining Loss: 0.01239905 \tValidation Loss 0.01700116 \tTraining Acuuarcy 37.259% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 528 \tTraining Loss: 0.01230437 \tValidation Loss 0.01706800 \tTraining Acuuarcy 38.173% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 529 \tTraining Loss: 0.01239374 \tValidation Loss 0.01707278 \tTraining Acuuarcy 37.237% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 530 \tTraining Loss: 0.01237117 \tValidation Loss 0.01709793 \tTraining Acuuarcy 37.309% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 531 \tTraining Loss: 0.01233263 \tValidation Loss 0.01714656 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 532 \tTraining Loss: 0.01238200 \tValidation Loss 0.01693684 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 533 \tTraining Loss: 0.01236429 \tValidation Loss 0.01698469 \tTraining Acuuarcy 37.638% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 534 \tTraining Loss: 0.01236562 \tValidation Loss 0.01711814 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 535 \tTraining Loss: 0.01236443 \tValidation Loss 0.01714503 \tTraining Acuuarcy 37.733% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 536 \tTraining Loss: 0.01233675 \tValidation Loss 0.01749946 \tTraining Acuuarcy 38.000% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 537 \tTraining Loss: 0.01237147 \tValidation Loss 0.01731919 \tTraining Acuuarcy 37.671% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 538 \tTraining Loss: 0.01233859 \tValidation Loss 0.01766708 \tTraining Acuuarcy 37.777% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 539 \tTraining Loss: 0.01235418 \tValidation Loss 0.01689102 \tTraining Acuuarcy 38.284% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 540 \tTraining Loss: 0.01236256 \tValidation Loss 0.01663198 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 22.123%\n",
      "Epoch: 541 \tTraining Loss: 0.01232426 \tValidation Loss 0.01700500 \tTraining Acuuarcy 37.253% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 542 \tTraining Loss: 0.01234594 \tValidation Loss 0.01767871 \tTraining Acuuarcy 37.972% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 543 \tTraining Loss: 0.01237239 \tValidation Loss 0.01691853 \tTraining Acuuarcy 37.493% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 544 \tTraining Loss: 0.01235130 \tValidation Loss 0.01692058 \tTraining Acuuarcy 37.393% \tValidation Acuuarcy 21.148%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 545 \tTraining Loss: 0.01237360 \tValidation Loss 0.01697223 \tTraining Acuuarcy 37.471% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 546 \tTraining Loss: 0.01238094 \tValidation Loss 0.01681262 \tTraining Acuuarcy 37.343% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 547 \tTraining Loss: 0.01235589 \tValidation Loss 0.01705063 \tTraining Acuuarcy 37.866% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 548 \tTraining Loss: 0.01234177 \tValidation Loss 0.01683338 \tTraining Acuuarcy 38.134% \tValidation Acuuarcy 21.761%\n",
      "Epoch: 549 \tTraining Loss: 0.01231704 \tValidation Loss 0.01687379 \tTraining Acuuarcy 37.554% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 550 \tTraining Loss: 0.01231434 \tValidation Loss 0.01711331 \tTraining Acuuarcy 37.933% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 551 \tTraining Loss: 0.01236051 \tValidation Loss 0.01746592 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 552 \tTraining Loss: 0.01239318 \tValidation Loss 0.01723250 \tTraining Acuuarcy 37.120% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 553 \tTraining Loss: 0.01232976 \tValidation Loss 0.01719850 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 21.900%\n",
      "Epoch: 554 \tTraining Loss: 0.01232069 \tValidation Loss 0.01721167 \tTraining Acuuarcy 38.045% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 555 \tTraining Loss: 0.01237826 \tValidation Loss 0.01736411 \tTraining Acuuarcy 37.967% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 556 \tTraining Loss: 0.01233989 \tValidation Loss 0.01725579 \tTraining Acuuarcy 37.460% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 557 \tTraining Loss: 0.01226253 \tValidation Loss 0.01730405 \tTraining Acuuarcy 38.190% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 558 \tTraining Loss: 0.01235770 \tValidation Loss 0.01718515 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 22.040%\n",
      "Epoch: 559 \tTraining Loss: 0.01234143 \tValidation Loss 0.01732335 \tTraining Acuuarcy 37.839% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 560 \tTraining Loss: 0.01235252 \tValidation Loss 0.01696376 \tTraining Acuuarcy 37.844% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 561 \tTraining Loss: 0.01232454 \tValidation Loss 0.01729838 \tTraining Acuuarcy 37.805% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 562 \tTraining Loss: 0.01237240 \tValidation Loss 0.01729542 \tTraining Acuuarcy 37.805% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 563 \tTraining Loss: 0.01229140 \tValidation Loss 0.01686579 \tTraining Acuuarcy 38.034% \tValidation Acuuarcy 22.123%\n",
      "Epoch: 564 \tTraining Loss: 0.01227394 \tValidation Loss 0.01710356 \tTraining Acuuarcy 38.006% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 565 \tTraining Loss: 0.01231734 \tValidation Loss 0.01696636 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 566 \tTraining Loss: 0.01231158 \tValidation Loss 0.01710903 \tTraining Acuuarcy 38.112% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 567 \tTraining Loss: 0.01234780 \tValidation Loss 0.01692658 \tTraining Acuuarcy 37.354% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 568 \tTraining Loss: 0.01230401 \tValidation Loss 0.01730880 \tTraining Acuuarcy 37.989% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 569 \tTraining Loss: 0.01237763 \tValidation Loss 0.01674603 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 570 \tTraining Loss: 0.01228923 \tValidation Loss 0.01701999 \tTraining Acuuarcy 37.850% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 571 \tTraining Loss: 0.01232685 \tValidation Loss 0.01705899 \tTraining Acuuarcy 38.223% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 572 \tTraining Loss: 0.01235625 \tValidation Loss 0.01754121 \tTraining Acuuarcy 37.928% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 573 \tTraining Loss: 0.01231990 \tValidation Loss 0.01731979 \tTraining Acuuarcy 38.323% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 574 \tTraining Loss: 0.01233263 \tValidation Loss 0.01707306 \tTraining Acuuarcy 38.179% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 575 \tTraining Loss: 0.01232204 \tValidation Loss 0.01720601 \tTraining Acuuarcy 37.705% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 576 \tTraining Loss: 0.01230869 \tValidation Loss 0.01731673 \tTraining Acuuarcy 37.755% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 577 \tTraining Loss: 0.01232101 \tValidation Loss 0.01722170 \tTraining Acuuarcy 38.123% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 578 \tTraining Loss: 0.01229624 \tValidation Loss 0.01683654 \tTraining Acuuarcy 38.257% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 579 \tTraining Loss: 0.01223603 \tValidation Loss 0.01688892 \tTraining Acuuarcy 38.379% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 580 \tTraining Loss: 0.01233940 \tValidation Loss 0.01690736 \tTraining Acuuarcy 37.928% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 581 \tTraining Loss: 0.01231738 \tValidation Loss 0.01739876 \tTraining Acuuarcy 37.944% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 582 \tTraining Loss: 0.01225208 \tValidation Loss 0.01696690 \tTraining Acuuarcy 37.822% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 583 \tTraining Loss: 0.01230757 \tValidation Loss 0.01723554 \tTraining Acuuarcy 38.045% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 584 \tTraining Loss: 0.01236715 \tValidation Loss 0.01692585 \tTraining Acuuarcy 37.593% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 585 \tTraining Loss: 0.01228434 \tValidation Loss 0.01699175 \tTraining Acuuarcy 37.705% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 586 \tTraining Loss: 0.01230752 \tValidation Loss 0.01724787 \tTraining Acuuarcy 37.972% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 587 \tTraining Loss: 0.01226649 \tValidation Loss 0.01716866 \tTraining Acuuarcy 37.956% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 588 \tTraining Loss: 0.01229572 \tValidation Loss 0.01728162 \tTraining Acuuarcy 38.273% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 589 \tTraining Loss: 0.01229613 \tValidation Loss 0.01723840 \tTraining Acuuarcy 37.956% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 590 \tTraining Loss: 0.01230745 \tValidation Loss 0.01711004 \tTraining Acuuarcy 38.218% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 591 \tTraining Loss: 0.01229080 \tValidation Loss 0.01694522 \tTraining Acuuarcy 38.151% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 592 \tTraining Loss: 0.01236873 \tValidation Loss 0.01680118 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 593 \tTraining Loss: 0.01232268 \tValidation Loss 0.01717972 \tTraining Acuuarcy 37.950% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 594 \tTraining Loss: 0.01233241 \tValidation Loss 0.01686308 \tTraining Acuuarcy 37.917% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 595 \tTraining Loss: 0.01234563 \tValidation Loss 0.01700280 \tTraining Acuuarcy 37.878% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 596 \tTraining Loss: 0.01232895 \tValidation Loss 0.01692447 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 597 \tTraining Loss: 0.01233092 \tValidation Loss 0.01698578 \tTraining Acuuarcy 37.844% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 598 \tTraining Loss: 0.01231975 \tValidation Loss 0.01743008 \tTraining Acuuarcy 37.984% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 599 \tTraining Loss: 0.01229982 \tValidation Loss 0.01735829 \tTraining Acuuarcy 38.468% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 600 \tTraining Loss: 0.01230981 \tValidation Loss 0.01714048 \tTraining Acuuarcy 37.805% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 601 \tTraining Loss: 0.01228212 \tValidation Loss 0.01706586 \tTraining Acuuarcy 38.385% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 602 \tTraining Loss: 0.01229645 \tValidation Loss 0.01694493 \tTraining Acuuarcy 37.788% \tValidation Acuuarcy 21.872%\n",
      "Epoch: 603 \tTraining Loss: 0.01228596 \tValidation Loss 0.01740384 \tTraining Acuuarcy 38.229% \tValidation Acuuarcy 22.290%\n",
      "Epoch: 604 \tTraining Loss: 0.01227330 \tValidation Loss 0.01688699 \tTraining Acuuarcy 38.435% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 605 \tTraining Loss: 0.01227843 \tValidation Loss 0.01700718 \tTraining Acuuarcy 37.922% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 606 \tTraining Loss: 0.01227727 \tValidation Loss 0.01758863 \tTraining Acuuarcy 38.023% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 607 \tTraining Loss: 0.01235675 \tValidation Loss 0.01712224 \tTraining Acuuarcy 37.638% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 608 \tTraining Loss: 0.01224513 \tValidation Loss 0.01747813 \tTraining Acuuarcy 38.073% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 609 \tTraining Loss: 0.01228878 \tValidation Loss 0.01750091 \tTraining Acuuarcy 38.140% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 610 \tTraining Loss: 0.01232481 \tValidation Loss 0.01719066 \tTraining Acuuarcy 37.599% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 611 \tTraining Loss: 0.01228506 \tValidation Loss 0.01738459 \tTraining Acuuarcy 37.861% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 612 \tTraining Loss: 0.01230667 \tValidation Loss 0.01705941 \tTraining Acuuarcy 38.123% \tValidation Acuuarcy 21.092%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 613 \tTraining Loss: 0.01230571 \tValidation Loss 0.01688906 \tTraining Acuuarcy 37.827% \tValidation Acuuarcy 22.402%\n",
      "Epoch: 614 \tTraining Loss: 0.01226948 \tValidation Loss 0.01708648 \tTraining Acuuarcy 38.050% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 615 \tTraining Loss: 0.01229619 \tValidation Loss 0.01704974 \tTraining Acuuarcy 37.961% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 616 \tTraining Loss: 0.01230492 \tValidation Loss 0.01749749 \tTraining Acuuarcy 37.582% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 617 \tTraining Loss: 0.01224690 \tValidation Loss 0.01702579 \tTraining Acuuarcy 38.290% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 618 \tTraining Loss: 0.01226026 \tValidation Loss 0.01726944 \tTraining Acuuarcy 37.995% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 619 \tTraining Loss: 0.01227463 \tValidation Loss 0.01707323 \tTraining Acuuarcy 38.296% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 620 \tTraining Loss: 0.01231449 \tValidation Loss 0.01680513 \tTraining Acuuarcy 38.006% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 621 \tTraining Loss: 0.01227267 \tValidation Loss 0.01714687 \tTraining Acuuarcy 38.167% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 622 \tTraining Loss: 0.01231301 \tValidation Loss 0.01694576 \tTraining Acuuarcy 37.443% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 623 \tTraining Loss: 0.01226894 \tValidation Loss 0.01723143 \tTraining Acuuarcy 37.995% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 624 \tTraining Loss: 0.01228403 \tValidation Loss 0.01706758 \tTraining Acuuarcy 38.167% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 625 \tTraining Loss: 0.01224691 \tValidation Loss 0.01713711 \tTraining Acuuarcy 38.335% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 626 \tTraining Loss: 0.01234989 \tValidation Loss 0.01754779 \tTraining Acuuarcy 37.978% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 627 \tTraining Loss: 0.01228218 \tValidation Loss 0.01726425 \tTraining Acuuarcy 37.839% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 628 \tTraining Loss: 0.01230932 \tValidation Loss 0.01708352 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 629 \tTraining Loss: 0.01231862 \tValidation Loss 0.01699932 \tTraining Acuuarcy 37.632% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 630 \tTraining Loss: 0.01228021 \tValidation Loss 0.01736018 \tTraining Acuuarcy 38.413% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 631 \tTraining Loss: 0.01231888 \tValidation Loss 0.01726699 \tTraining Acuuarcy 38.206% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 632 \tTraining Loss: 0.01228943 \tValidation Loss 0.01699987 \tTraining Acuuarcy 38.251% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 633 \tTraining Loss: 0.01226431 \tValidation Loss 0.01666816 \tTraining Acuuarcy 38.173% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 634 \tTraining Loss: 0.01225176 \tValidation Loss 0.01716016 \tTraining Acuuarcy 38.390% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 635 \tTraining Loss: 0.01225952 \tValidation Loss 0.01733102 \tTraining Acuuarcy 38.496% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 636 \tTraining Loss: 0.01229171 \tValidation Loss 0.01722082 \tTraining Acuuarcy 38.179% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 637 \tTraining Loss: 0.01231203 \tValidation Loss 0.01694859 \tTraining Acuuarcy 38.167% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 638 \tTraining Loss: 0.01230208 \tValidation Loss 0.01694452 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 639 \tTraining Loss: 0.01229317 \tValidation Loss 0.01731198 \tTraining Acuuarcy 38.480% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 640 \tTraining Loss: 0.01220419 \tValidation Loss 0.01675227 \tTraining Acuuarcy 38.903% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 641 \tTraining Loss: 0.01233239 \tValidation Loss 0.01719444 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 642 \tTraining Loss: 0.01223661 \tValidation Loss 0.01701044 \tTraining Acuuarcy 38.758% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 643 \tTraining Loss: 0.01222767 \tValidation Loss 0.01745181 \tTraining Acuuarcy 38.491% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 644 \tTraining Loss: 0.01220983 \tValidation Loss 0.01730562 \tTraining Acuuarcy 38.820% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 645 \tTraining Loss: 0.01224454 \tValidation Loss 0.01744344 \tTraining Acuuarcy 38.078% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 646 \tTraining Loss: 0.01226297 \tValidation Loss 0.01732477 \tTraining Acuuarcy 38.106% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 647 \tTraining Loss: 0.01223454 \tValidation Loss 0.01748276 \tTraining Acuuarcy 38.474% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 648 \tTraining Loss: 0.01222343 \tValidation Loss 0.01761660 \tTraining Acuuarcy 38.781% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 649 \tTraining Loss: 0.01228670 \tValidation Loss 0.01731209 \tTraining Acuuarcy 38.312% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 650 \tTraining Loss: 0.01224029 \tValidation Loss 0.01749623 \tTraining Acuuarcy 38.491% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 651 \tTraining Loss: 0.01230999 \tValidation Loss 0.01730231 \tTraining Acuuarcy 38.429% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 652 \tTraining Loss: 0.01229692 \tValidation Loss 0.01757068 \tTraining Acuuarcy 38.206% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 653 \tTraining Loss: 0.01230879 \tValidation Loss 0.01749044 \tTraining Acuuarcy 38.223% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 654 \tTraining Loss: 0.01216609 \tValidation Loss 0.01728719 \tTraining Acuuarcy 38.842% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 655 \tTraining Loss: 0.01226404 \tValidation Loss 0.01700711 \tTraining Acuuarcy 38.023% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 656 \tTraining Loss: 0.01222367 \tValidation Loss 0.01698378 \tTraining Acuuarcy 38.736% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 657 \tTraining Loss: 0.01224396 \tValidation Loss 0.01714200 \tTraining Acuuarcy 38.067% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 658 \tTraining Loss: 0.01222217 \tValidation Loss 0.01746046 \tTraining Acuuarcy 38.558% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 659 \tTraining Loss: 0.01221940 \tValidation Loss 0.01738433 \tTraining Acuuarcy 38.580% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 660 \tTraining Loss: 0.01231670 \tValidation Loss 0.01698946 \tTraining Acuuarcy 38.167% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 661 \tTraining Loss: 0.01222935 \tValidation Loss 0.01738759 \tTraining Acuuarcy 38.396% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 662 \tTraining Loss: 0.01220805 \tValidation Loss 0.01703854 \tTraining Acuuarcy 38.675% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 663 \tTraining Loss: 0.01228284 \tValidation Loss 0.01754988 \tTraining Acuuarcy 38.251% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 664 \tTraining Loss: 0.01227563 \tValidation Loss 0.01701822 \tTraining Acuuarcy 38.546% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 665 \tTraining Loss: 0.01227613 \tValidation Loss 0.01709341 \tTraining Acuuarcy 37.878% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 666 \tTraining Loss: 0.01232958 \tValidation Loss 0.01688197 \tTraining Acuuarcy 37.922% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 667 \tTraining Loss: 0.01219664 \tValidation Loss 0.01694883 \tTraining Acuuarcy 38.730% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 668 \tTraining Loss: 0.01224009 \tValidation Loss 0.01703727 \tTraining Acuuarcy 38.714% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 669 \tTraining Loss: 0.01222348 \tValidation Loss 0.01739385 \tTraining Acuuarcy 38.101% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 670 \tTraining Loss: 0.01226942 \tValidation Loss 0.01760728 \tTraining Acuuarcy 38.262% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 671 \tTraining Loss: 0.01227666 \tValidation Loss 0.01728179 \tTraining Acuuarcy 38.502% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 672 \tTraining Loss: 0.01216236 \tValidation Loss 0.01767625 \tTraining Acuuarcy 38.725% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 673 \tTraining Loss: 0.01227317 \tValidation Loss 0.01709806 \tTraining Acuuarcy 38.039% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 674 \tTraining Loss: 0.01227471 \tValidation Loss 0.01741397 \tTraining Acuuarcy 38.000% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 675 \tTraining Loss: 0.01222663 \tValidation Loss 0.01721834 \tTraining Acuuarcy 38.502% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 676 \tTraining Loss: 0.01225883 \tValidation Loss 0.01712379 \tTraining Acuuarcy 38.502% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 677 \tTraining Loss: 0.01225322 \tValidation Loss 0.01694345 \tTraining Acuuarcy 38.101% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 678 \tTraining Loss: 0.01217345 \tValidation Loss 0.01754613 \tTraining Acuuarcy 39.009% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 679 \tTraining Loss: 0.01225828 \tValidation Loss 0.01741890 \tTraining Acuuarcy 38.530% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 680 \tTraining Loss: 0.01221525 \tValidation Loss 0.01748155 \tTraining Acuuarcy 38.608% \tValidation Acuuarcy 20.424%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 681 \tTraining Loss: 0.01228183 \tValidation Loss 0.01718998 \tTraining Acuuarcy 38.563% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 682 \tTraining Loss: 0.01213944 \tValidation Loss 0.01738794 \tTraining Acuuarcy 38.669% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 683 \tTraining Loss: 0.01220643 \tValidation Loss 0.01763785 \tTraining Acuuarcy 38.619% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 684 \tTraining Loss: 0.01221715 \tValidation Loss 0.01742041 \tTraining Acuuarcy 38.491% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 685 \tTraining Loss: 0.01221248 \tValidation Loss 0.01736771 \tTraining Acuuarcy 38.530% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 686 \tTraining Loss: 0.01220436 \tValidation Loss 0.01770323 \tTraining Acuuarcy 38.624% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 687 \tTraining Loss: 0.01219236 \tValidation Loss 0.01729166 \tTraining Acuuarcy 38.725% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 688 \tTraining Loss: 0.01225405 \tValidation Loss 0.01745995 \tTraining Acuuarcy 38.402% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 689 \tTraining Loss: 0.01219073 \tValidation Loss 0.01724241 \tTraining Acuuarcy 39.031% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 690 \tTraining Loss: 0.01225555 \tValidation Loss 0.01741386 \tTraining Acuuarcy 38.385% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 691 \tTraining Loss: 0.01230661 \tValidation Loss 0.01721280 \tTraining Acuuarcy 37.727% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 692 \tTraining Loss: 0.01225415 \tValidation Loss 0.01734197 \tTraining Acuuarcy 38.485% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 693 \tTraining Loss: 0.01223550 \tValidation Loss 0.01691233 \tTraining Acuuarcy 38.580% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 694 \tTraining Loss: 0.01218806 \tValidation Loss 0.01714960 \tTraining Acuuarcy 39.120% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 695 \tTraining Loss: 0.01226755 \tValidation Loss 0.01744366 \tTraining Acuuarcy 38.546% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 696 \tTraining Loss: 0.01216521 \tValidation Loss 0.01725293 \tTraining Acuuarcy 38.480% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 697 \tTraining Loss: 0.01223163 \tValidation Loss 0.01715525 \tTraining Acuuarcy 38.179% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 698 \tTraining Loss: 0.01223861 \tValidation Loss 0.01720021 \tTraining Acuuarcy 38.675% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 699 \tTraining Loss: 0.01214693 \tValidation Loss 0.01772944 \tTraining Acuuarcy 38.959% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 700 \tTraining Loss: 0.01220152 \tValidation Loss 0.01726741 \tTraining Acuuarcy 38.948% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 701 \tTraining Loss: 0.01219918 \tValidation Loss 0.01740949 \tTraining Acuuarcy 38.602% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 702 \tTraining Loss: 0.01220169 \tValidation Loss 0.01714632 \tTraining Acuuarcy 38.714% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 703 \tTraining Loss: 0.01218388 \tValidation Loss 0.01733369 \tTraining Acuuarcy 38.970% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 704 \tTraining Loss: 0.01219351 \tValidation Loss 0.01732142 \tTraining Acuuarcy 39.277% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 705 \tTraining Loss: 0.01224908 \tValidation Loss 0.01682280 \tTraining Acuuarcy 38.446% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 706 \tTraining Loss: 0.01222083 \tValidation Loss 0.01716665 \tTraining Acuuarcy 38.702% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 707 \tTraining Loss: 0.01221819 \tValidation Loss 0.01773333 \tTraining Acuuarcy 38.842% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 708 \tTraining Loss: 0.01215293 \tValidation Loss 0.01742089 \tTraining Acuuarcy 39.494% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 709 \tTraining Loss: 0.01219098 \tValidation Loss 0.01746888 \tTraining Acuuarcy 38.552% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 710 \tTraining Loss: 0.01224740 \tValidation Loss 0.01731631 \tTraining Acuuarcy 38.017% \tValidation Acuuarcy 21.817%\n",
      "Epoch: 711 \tTraining Loss: 0.01225581 \tValidation Loss 0.01712653 \tTraining Acuuarcy 38.034% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 712 \tTraining Loss: 0.01219587 \tValidation Loss 0.01770458 \tTraining Acuuarcy 38.814% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 713 \tTraining Loss: 0.01226446 \tValidation Loss 0.01713590 \tTraining Acuuarcy 38.468% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 714 \tTraining Loss: 0.01226416 \tValidation Loss 0.01769801 \tTraining Acuuarcy 38.156% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 715 \tTraining Loss: 0.01221185 \tValidation Loss 0.01729050 \tTraining Acuuarcy 38.563% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 716 \tTraining Loss: 0.01228328 \tValidation Loss 0.01728997 \tTraining Acuuarcy 38.374% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 717 \tTraining Loss: 0.01214289 \tValidation Loss 0.01755493 \tTraining Acuuarcy 38.558% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 718 \tTraining Loss: 0.01222094 \tValidation Loss 0.01743056 \tTraining Acuuarcy 38.569% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 719 \tTraining Loss: 0.01218063 \tValidation Loss 0.01744985 \tTraining Acuuarcy 38.747% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 720 \tTraining Loss: 0.01216572 \tValidation Loss 0.01759662 \tTraining Acuuarcy 39.015% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 721 \tTraining Loss: 0.01217397 \tValidation Loss 0.01783332 \tTraining Acuuarcy 39.093% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 722 \tTraining Loss: 0.01216966 \tValidation Loss 0.01707333 \tTraining Acuuarcy 38.976% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 723 \tTraining Loss: 0.01218865 \tValidation Loss 0.01737313 \tTraining Acuuarcy 38.719% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 724 \tTraining Loss: 0.01218884 \tValidation Loss 0.01709443 \tTraining Acuuarcy 38.597% \tValidation Acuuarcy 21.872%\n",
      "Epoch: 725 \tTraining Loss: 0.01216670 \tValidation Loss 0.01753610 \tTraining Acuuarcy 38.786% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 726 \tTraining Loss: 0.01213534 \tValidation Loss 0.01732653 \tTraining Acuuarcy 39.076% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 727 \tTraining Loss: 0.01216663 \tValidation Loss 0.01756852 \tTraining Acuuarcy 38.535% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 728 \tTraining Loss: 0.01217710 \tValidation Loss 0.01753019 \tTraining Acuuarcy 38.781% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 729 \tTraining Loss: 0.01218959 \tValidation Loss 0.01739987 \tTraining Acuuarcy 38.719% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 730 \tTraining Loss: 0.01221870 \tValidation Loss 0.01755687 \tTraining Acuuarcy 38.240% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 731 \tTraining Loss: 0.01210587 \tValidation Loss 0.01800933 \tTraining Acuuarcy 39.232% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 732 \tTraining Loss: 0.01217706 \tValidation Loss 0.01774801 \tTraining Acuuarcy 38.903% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 733 \tTraining Loss: 0.01210556 \tValidation Loss 0.01755200 \tTraining Acuuarcy 39.020% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 734 \tTraining Loss: 0.01216213 \tValidation Loss 0.01738867 \tTraining Acuuarcy 39.187% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 735 \tTraining Loss: 0.01210091 \tValidation Loss 0.01727952 \tTraining Acuuarcy 39.527% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 736 \tTraining Loss: 0.01212468 \tValidation Loss 0.01749012 \tTraining Acuuarcy 38.580% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 737 \tTraining Loss: 0.01211620 \tValidation Loss 0.01750428 \tTraining Acuuarcy 38.948% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 738 \tTraining Loss: 0.01209560 \tValidation Loss 0.01734792 \tTraining Acuuarcy 39.288% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 739 \tTraining Loss: 0.01217245 \tValidation Loss 0.01764312 \tTraining Acuuarcy 38.669% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 740 \tTraining Loss: 0.01214511 \tValidation Loss 0.01712091 \tTraining Acuuarcy 38.853% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 741 \tTraining Loss: 0.01216144 \tValidation Loss 0.01734944 \tTraining Acuuarcy 38.669% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 742 \tTraining Loss: 0.01216826 \tValidation Loss 0.01742306 \tTraining Acuuarcy 39.065% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 743 \tTraining Loss: 0.01216261 \tValidation Loss 0.01707543 \tTraining Acuuarcy 38.870% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 744 \tTraining Loss: 0.01209992 \tValidation Loss 0.01764266 \tTraining Acuuarcy 39.199% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 745 \tTraining Loss: 0.01215941 \tValidation Loss 0.01740270 \tTraining Acuuarcy 39.260% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 746 \tTraining Loss: 0.01217714 \tValidation Loss 0.01741431 \tTraining Acuuarcy 38.647% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 747 \tTraining Loss: 0.01212871 \tValidation Loss 0.01747637 \tTraining Acuuarcy 39.176% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 748 \tTraining Loss: 0.01212121 \tValidation Loss 0.01728925 \tTraining Acuuarcy 39.199% \tValidation Acuuarcy 21.009%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 749 \tTraining Loss: 0.01217744 \tValidation Loss 0.01752518 \tTraining Acuuarcy 38.691% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 750 \tTraining Loss: 0.01218415 \tValidation Loss 0.01739488 \tTraining Acuuarcy 38.814% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 751 \tTraining Loss: 0.01212964 \tValidation Loss 0.01732032 \tTraining Acuuarcy 38.976% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 752 \tTraining Loss: 0.01216497 \tValidation Loss 0.01762183 \tTraining Acuuarcy 39.310% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 753 \tTraining Loss: 0.01206177 \tValidation Loss 0.01754178 \tTraining Acuuarcy 39.472% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 754 \tTraining Loss: 0.01212845 \tValidation Loss 0.01722876 \tTraining Acuuarcy 39.070% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 755 \tTraining Loss: 0.01218119 \tValidation Loss 0.01716067 \tTraining Acuuarcy 38.407% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 756 \tTraining Loss: 0.01206332 \tValidation Loss 0.01763557 \tTraining Acuuarcy 39.839% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 757 \tTraining Loss: 0.01213811 \tValidation Loss 0.01737344 \tTraining Acuuarcy 38.998% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 758 \tTraining Loss: 0.01209807 \tValidation Loss 0.01716798 \tTraining Acuuarcy 39.527% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 759 \tTraining Loss: 0.01214770 \tValidation Loss 0.01771721 \tTraining Acuuarcy 39.081% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 760 \tTraining Loss: 0.01215190 \tValidation Loss 0.01725628 \tTraining Acuuarcy 39.031% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 761 \tTraining Loss: 0.01213480 \tValidation Loss 0.01729886 \tTraining Acuuarcy 38.753% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 762 \tTraining Loss: 0.01215773 \tValidation Loss 0.01739114 \tTraining Acuuarcy 38.998% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 763 \tTraining Loss: 0.01213232 \tValidation Loss 0.01733271 \tTraining Acuuarcy 39.065% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 764 \tTraining Loss: 0.01212306 \tValidation Loss 0.01763875 \tTraining Acuuarcy 39.282% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 765 \tTraining Loss: 0.01209041 \tValidation Loss 0.01729129 \tTraining Acuuarcy 39.355% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 766 \tTraining Loss: 0.01213802 \tValidation Loss 0.01768370 \tTraining Acuuarcy 39.187% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 767 \tTraining Loss: 0.01208694 \tValidation Loss 0.01741005 \tTraining Acuuarcy 39.511% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 768 \tTraining Loss: 0.01212172 \tValidation Loss 0.01770121 \tTraining Acuuarcy 38.658% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 769 \tTraining Loss: 0.01216024 \tValidation Loss 0.01762561 \tTraining Acuuarcy 38.569% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 770 \tTraining Loss: 0.01220607 \tValidation Loss 0.01718393 \tTraining Acuuarcy 38.686% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 771 \tTraining Loss: 0.01219684 \tValidation Loss 0.01766078 \tTraining Acuuarcy 38.485% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 772 \tTraining Loss: 0.01206077 \tValidation Loss 0.01763871 \tTraining Acuuarcy 39.449% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 773 \tTraining Loss: 0.01210485 \tValidation Loss 0.01754296 \tTraining Acuuarcy 39.572% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 774 \tTraining Loss: 0.01211784 \tValidation Loss 0.01748036 \tTraining Acuuarcy 38.959% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 775 \tTraining Loss: 0.01222628 \tValidation Loss 0.01783553 \tTraining Acuuarcy 38.296% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 776 \tTraining Loss: 0.01212135 \tValidation Loss 0.01720815 \tTraining Acuuarcy 39.098% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 777 \tTraining Loss: 0.01216258 \tValidation Loss 0.01730638 \tTraining Acuuarcy 39.109% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 778 \tTraining Loss: 0.01210346 \tValidation Loss 0.01747928 \tTraining Acuuarcy 39.371% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 779 \tTraining Loss: 0.01210925 \tValidation Loss 0.01735134 \tTraining Acuuarcy 39.154% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 780 \tTraining Loss: 0.01209803 \tValidation Loss 0.01753522 \tTraining Acuuarcy 39.065% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 781 \tTraining Loss: 0.01212003 \tValidation Loss 0.01738167 \tTraining Acuuarcy 39.165% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 782 \tTraining Loss: 0.01212945 \tValidation Loss 0.01744071 \tTraining Acuuarcy 38.875% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 783 \tTraining Loss: 0.01212615 \tValidation Loss 0.01725485 \tTraining Acuuarcy 39.271% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 784 \tTraining Loss: 0.01213332 \tValidation Loss 0.01739371 \tTraining Acuuarcy 39.399% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 785 \tTraining Loss: 0.01207316 \tValidation Loss 0.01759593 \tTraining Acuuarcy 39.722% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 786 \tTraining Loss: 0.01217406 \tValidation Loss 0.01753657 \tTraining Acuuarcy 38.714% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 787 \tTraining Loss: 0.01207390 \tValidation Loss 0.01732928 \tTraining Acuuarcy 39.293% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 788 \tTraining Loss: 0.01205923 \tValidation Loss 0.01759754 \tTraining Acuuarcy 39.282% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 789 \tTraining Loss: 0.01214296 \tValidation Loss 0.01771072 \tTraining Acuuarcy 39.221% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 790 \tTraining Loss: 0.01210182 \tValidation Loss 0.01765969 \tTraining Acuuarcy 39.120% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 791 \tTraining Loss: 0.01205877 \tValidation Loss 0.01737369 \tTraining Acuuarcy 39.550% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 792 \tTraining Loss: 0.01208260 \tValidation Loss 0.01776538 \tTraining Acuuarcy 39.455% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 793 \tTraining Loss: 0.01210113 \tValidation Loss 0.01750514 \tTraining Acuuarcy 39.388% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 794 \tTraining Loss: 0.01209715 \tValidation Loss 0.01740245 \tTraining Acuuarcy 39.438% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 795 \tTraining Loss: 0.01217931 \tValidation Loss 0.01723392 \tTraining Acuuarcy 38.725% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 796 \tTraining Loss: 0.01211742 \tValidation Loss 0.01739254 \tTraining Acuuarcy 39.042% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 797 \tTraining Loss: 0.01212887 \tValidation Loss 0.01758644 \tTraining Acuuarcy 39.739% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 798 \tTraining Loss: 0.01213385 \tValidation Loss 0.01749659 \tTraining Acuuarcy 39.003% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 799 \tTraining Loss: 0.01204108 \tValidation Loss 0.01738836 \tTraining Acuuarcy 39.455% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 800 \tTraining Loss: 0.01211819 \tValidation Loss 0.01750226 \tTraining Acuuarcy 39.120% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 801 \tTraining Loss: 0.01208960 \tValidation Loss 0.01781341 \tTraining Acuuarcy 39.360% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 802 \tTraining Loss: 0.01205559 \tValidation Loss 0.01730359 \tTraining Acuuarcy 39.605% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 803 \tTraining Loss: 0.01211773 \tValidation Loss 0.01762405 \tTraining Acuuarcy 39.104% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 804 \tTraining Loss: 0.01206366 \tValidation Loss 0.01779657 \tTraining Acuuarcy 39.332% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 805 \tTraining Loss: 0.01207163 \tValidation Loss 0.01770310 \tTraining Acuuarcy 39.533% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 806 \tTraining Loss: 0.01211253 \tValidation Loss 0.01754295 \tTraining Acuuarcy 39.355% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 807 \tTraining Loss: 0.01210530 \tValidation Loss 0.01700956 \tTraining Acuuarcy 39.360% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 808 \tTraining Loss: 0.01198985 \tValidation Loss 0.01745165 \tTraining Acuuarcy 39.756% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 809 \tTraining Loss: 0.01209140 \tValidation Loss 0.01773224 \tTraining Acuuarcy 39.539% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 810 \tTraining Loss: 0.01209939 \tValidation Loss 0.01804191 \tTraining Acuuarcy 39.238% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 811 \tTraining Loss: 0.01211669 \tValidation Loss 0.01749806 \tTraining Acuuarcy 39.661% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 812 \tTraining Loss: 0.01212258 \tValidation Loss 0.01730433 \tTraining Acuuarcy 39.171% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 813 \tTraining Loss: 0.01212434 \tValidation Loss 0.01773677 \tTraining Acuuarcy 38.964% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 814 \tTraining Loss: 0.01213594 \tValidation Loss 0.01749563 \tTraining Acuuarcy 39.176% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 815 \tTraining Loss: 0.01203896 \tValidation Loss 0.01750077 \tTraining Acuuarcy 39.845% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 816 \tTraining Loss: 0.01207046 \tValidation Loss 0.01765494 \tTraining Acuuarcy 39.232% \tValidation Acuuarcy 21.259%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 817 \tTraining Loss: 0.01216486 \tValidation Loss 0.01777306 \tTraining Acuuarcy 38.864% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 818 \tTraining Loss: 0.01210805 \tValidation Loss 0.01760242 \tTraining Acuuarcy 39.327% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 819 \tTraining Loss: 0.01204075 \tValidation Loss 0.01790152 \tTraining Acuuarcy 39.293% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 820 \tTraining Loss: 0.01208735 \tValidation Loss 0.01756904 \tTraining Acuuarcy 39.522% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 821 \tTraining Loss: 0.01205415 \tValidation Loss 0.01790932 \tTraining Acuuarcy 39.561% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 822 \tTraining Loss: 0.01206823 \tValidation Loss 0.01774116 \tTraining Acuuarcy 39.494% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 823 \tTraining Loss: 0.01211415 \tValidation Loss 0.01777038 \tTraining Acuuarcy 39.254% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 824 \tTraining Loss: 0.01210372 \tValidation Loss 0.01726958 \tTraining Acuuarcy 39.494% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 825 \tTraining Loss: 0.01218684 \tValidation Loss 0.01728662 \tTraining Acuuarcy 38.847% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 826 \tTraining Loss: 0.01212797 \tValidation Loss 0.01744203 \tTraining Acuuarcy 39.332% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 827 \tTraining Loss: 0.01204590 \tValidation Loss 0.01722946 \tTraining Acuuarcy 39.706% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 828 \tTraining Loss: 0.01203196 \tValidation Loss 0.01719529 \tTraining Acuuarcy 39.761% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 829 \tTraining Loss: 0.01214888 \tValidation Loss 0.01750909 \tTraining Acuuarcy 39.210% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 830 \tTraining Loss: 0.01210357 \tValidation Loss 0.01737439 \tTraining Acuuarcy 39.293% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 831 \tTraining Loss: 0.01206805 \tValidation Loss 0.01775028 \tTraining Acuuarcy 39.700% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 832 \tTraining Loss: 0.01205927 \tValidation Loss 0.01756615 \tTraining Acuuarcy 39.421% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 833 \tTraining Loss: 0.01210330 \tValidation Loss 0.01807694 \tTraining Acuuarcy 39.722% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 834 \tTraining Loss: 0.01213472 \tValidation Loss 0.01775699 \tTraining Acuuarcy 39.003% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 835 \tTraining Loss: 0.01203214 \tValidation Loss 0.01735693 \tTraining Acuuarcy 40.001% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 836 \tTraining Loss: 0.01210551 \tValidation Loss 0.01724007 \tTraining Acuuarcy 39.182% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 837 \tTraining Loss: 0.01204846 \tValidation Loss 0.01744899 \tTraining Acuuarcy 39.578% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 838 \tTraining Loss: 0.01207212 \tValidation Loss 0.01735317 \tTraining Acuuarcy 39.578% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 839 \tTraining Loss: 0.01213638 \tValidation Loss 0.01766250 \tTraining Acuuarcy 38.987% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 840 \tTraining Loss: 0.01199095 \tValidation Loss 0.01788978 \tTraining Acuuarcy 39.951% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 841 \tTraining Loss: 0.01214172 \tValidation Loss 0.01741677 \tTraining Acuuarcy 39.761% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 842 \tTraining Loss: 0.01205639 \tValidation Loss 0.01764061 \tTraining Acuuarcy 39.394% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 843 \tTraining Loss: 0.01207862 \tValidation Loss 0.01739495 \tTraining Acuuarcy 39.499% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 844 \tTraining Loss: 0.01213060 \tValidation Loss 0.01742964 \tTraining Acuuarcy 39.104% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 845 \tTraining Loss: 0.01204494 \tValidation Loss 0.01728916 \tTraining Acuuarcy 40.068% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 846 \tTraining Loss: 0.01210769 \tValidation Loss 0.01785888 \tTraining Acuuarcy 39.421% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 847 \tTraining Loss: 0.01201974 \tValidation Loss 0.01769764 \tTraining Acuuarcy 39.288% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 848 \tTraining Loss: 0.01211181 \tValidation Loss 0.01734988 \tTraining Acuuarcy 38.931% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 849 \tTraining Loss: 0.01202591 \tValidation Loss 0.01757692 \tTraining Acuuarcy 39.784% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 850 \tTraining Loss: 0.01208214 \tValidation Loss 0.01750140 \tTraining Acuuarcy 39.410% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 851 \tTraining Loss: 0.01205173 \tValidation Loss 0.01767043 \tTraining Acuuarcy 39.722% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 852 \tTraining Loss: 0.01206138 \tValidation Loss 0.01742865 \tTraining Acuuarcy 39.572% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 853 \tTraining Loss: 0.01201009 \tValidation Loss 0.01803808 \tTraining Acuuarcy 39.583% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 854 \tTraining Loss: 0.01214702 \tValidation Loss 0.01763233 \tTraining Acuuarcy 39.154% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 855 \tTraining Loss: 0.01203736 \tValidation Loss 0.01779636 \tTraining Acuuarcy 39.750% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 856 \tTraining Loss: 0.01201276 \tValidation Loss 0.01771339 \tTraining Acuuarcy 39.516% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 857 \tTraining Loss: 0.01203419 \tValidation Loss 0.01770152 \tTraining Acuuarcy 39.683% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 858 \tTraining Loss: 0.01213866 \tValidation Loss 0.01724180 \tTraining Acuuarcy 38.925% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 859 \tTraining Loss: 0.01206928 \tValidation Loss 0.01769043 \tTraining Acuuarcy 39.566% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 860 \tTraining Loss: 0.01207834 \tValidation Loss 0.01725482 \tTraining Acuuarcy 39.561% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 861 \tTraining Loss: 0.01205900 \tValidation Loss 0.01788357 \tTraining Acuuarcy 39.377% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 862 \tTraining Loss: 0.01209976 \tValidation Loss 0.01760376 \tTraining Acuuarcy 39.382% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 863 \tTraining Loss: 0.01206626 \tValidation Loss 0.01743165 \tTraining Acuuarcy 39.204% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 864 \tTraining Loss: 0.01210179 \tValidation Loss 0.01787512 \tTraining Acuuarcy 39.148% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 865 \tTraining Loss: 0.01210836 \tValidation Loss 0.01745874 \tTraining Acuuarcy 39.416% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 866 \tTraining Loss: 0.01201588 \tValidation Loss 0.01765905 \tTraining Acuuarcy 40.168% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 867 \tTraining Loss: 0.01210060 \tValidation Loss 0.01718430 \tTraining Acuuarcy 39.561% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 868 \tTraining Loss: 0.01212134 \tValidation Loss 0.01742365 \tTraining Acuuarcy 38.898% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 869 \tTraining Loss: 0.01200426 \tValidation Loss 0.01765068 \tTraining Acuuarcy 39.996% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 870 \tTraining Loss: 0.01202331 \tValidation Loss 0.01781956 \tTraining Acuuarcy 40.107% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 871 \tTraining Loss: 0.01211298 \tValidation Loss 0.01736327 \tTraining Acuuarcy 39.215% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 872 \tTraining Loss: 0.01208320 \tValidation Loss 0.01762717 \tTraining Acuuarcy 39.176% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 873 \tTraining Loss: 0.01206721 \tValidation Loss 0.01807113 \tTraining Acuuarcy 39.399% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 874 \tTraining Loss: 0.01206043 \tValidation Loss 0.01780214 \tTraining Acuuarcy 39.277% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 875 \tTraining Loss: 0.01202498 \tValidation Loss 0.01724413 \tTraining Acuuarcy 39.533% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 876 \tTraining Loss: 0.01202973 \tValidation Loss 0.01766017 \tTraining Acuuarcy 39.499% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 877 \tTraining Loss: 0.01210298 \tValidation Loss 0.01786326 \tTraining Acuuarcy 39.511% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 878 \tTraining Loss: 0.01206150 \tValidation Loss 0.01785755 \tTraining Acuuarcy 39.516% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 879 \tTraining Loss: 0.01202588 \tValidation Loss 0.01773622 \tTraining Acuuarcy 39.739% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 880 \tTraining Loss: 0.01205876 \tValidation Loss 0.01797677 \tTraining Acuuarcy 39.605% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 881 \tTraining Loss: 0.01202265 \tValidation Loss 0.01743586 \tTraining Acuuarcy 39.633% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 882 \tTraining Loss: 0.01203627 \tValidation Loss 0.01781725 \tTraining Acuuarcy 39.750% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 883 \tTraining Loss: 0.01207519 \tValidation Loss 0.01758477 \tTraining Acuuarcy 39.722% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 884 \tTraining Loss: 0.01205657 \tValidation Loss 0.01745920 \tTraining Acuuarcy 39.667% \tValidation Acuuarcy 20.841%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 885 \tTraining Loss: 0.01206680 \tValidation Loss 0.01754610 \tTraining Acuuarcy 39.683% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 886 \tTraining Loss: 0.01205173 \tValidation Loss 0.01758228 \tTraining Acuuarcy 39.639% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 887 \tTraining Loss: 0.01201522 \tValidation Loss 0.01794024 \tTraining Acuuarcy 39.778% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 888 \tTraining Loss: 0.01203314 \tValidation Loss 0.01750722 \tTraining Acuuarcy 39.683% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 889 \tTraining Loss: 0.01200121 \tValidation Loss 0.01784719 \tTraining Acuuarcy 39.940% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 890 \tTraining Loss: 0.01202483 \tValidation Loss 0.01764164 \tTraining Acuuarcy 40.029% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 891 \tTraining Loss: 0.01200551 \tValidation Loss 0.01736962 \tTraining Acuuarcy 39.622% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 892 \tTraining Loss: 0.01209484 \tValidation Loss 0.01772919 \tTraining Acuuarcy 39.310% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 893 \tTraining Loss: 0.01203529 \tValidation Loss 0.01772092 \tTraining Acuuarcy 39.605% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 894 \tTraining Loss: 0.01205153 \tValidation Loss 0.01760695 \tTraining Acuuarcy 39.483% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 895 \tTraining Loss: 0.01202851 \tValidation Loss 0.01744287 \tTraining Acuuarcy 39.561% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 896 \tTraining Loss: 0.01206533 \tValidation Loss 0.01761773 \tTraining Acuuarcy 39.862% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 897 \tTraining Loss: 0.01208323 \tValidation Loss 0.01734020 \tTraining Acuuarcy 39.806% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 898 \tTraining Loss: 0.01199009 \tValidation Loss 0.01776362 \tTraining Acuuarcy 39.895% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 899 \tTraining Loss: 0.01208267 \tValidation Loss 0.01789585 \tTraining Acuuarcy 39.449% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 900 \tTraining Loss: 0.01202201 \tValidation Loss 0.01752304 \tTraining Acuuarcy 39.812% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 901 \tTraining Loss: 0.01206157 \tValidation Loss 0.01809054 \tTraining Acuuarcy 39.522% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 902 \tTraining Loss: 0.01206640 \tValidation Loss 0.01769702 \tTraining Acuuarcy 39.851% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 903 \tTraining Loss: 0.01202590 \tValidation Loss 0.01773950 \tTraining Acuuarcy 39.644% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 904 \tTraining Loss: 0.01203830 \tValidation Loss 0.01773830 \tTraining Acuuarcy 39.734% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 905 \tTraining Loss: 0.01211831 \tValidation Loss 0.01775506 \tTraining Acuuarcy 39.332% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 906 \tTraining Loss: 0.01206124 \tValidation Loss 0.01764143 \tTraining Acuuarcy 39.611% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 907 \tTraining Loss: 0.01205014 \tValidation Loss 0.01742072 \tTraining Acuuarcy 39.628% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 908 \tTraining Loss: 0.01200029 \tValidation Loss 0.01719251 \tTraining Acuuarcy 39.667% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 909 \tTraining Loss: 0.01206631 \tValidation Loss 0.01761516 \tTraining Acuuarcy 39.031% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 910 \tTraining Loss: 0.01213098 \tValidation Loss 0.01759610 \tTraining Acuuarcy 39.215% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 911 \tTraining Loss: 0.01198897 \tValidation Loss 0.01769995 \tTraining Acuuarcy 40.135% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 912 \tTraining Loss: 0.01206549 \tValidation Loss 0.01742205 \tTraining Acuuarcy 39.728% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 913 \tTraining Loss: 0.01201985 \tValidation Loss 0.01730329 \tTraining Acuuarcy 39.349% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 914 \tTraining Loss: 0.01212483 \tValidation Loss 0.01735927 \tTraining Acuuarcy 39.199% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 915 \tTraining Loss: 0.01202166 \tValidation Loss 0.01744565 \tTraining Acuuarcy 40.079% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 916 \tTraining Loss: 0.01203453 \tValidation Loss 0.01744543 \tTraining Acuuarcy 39.589% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 917 \tTraining Loss: 0.01195385 \tValidation Loss 0.01768151 \tTraining Acuuarcy 40.297% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 918 \tTraining Loss: 0.01197789 \tValidation Loss 0.01789174 \tTraining Acuuarcy 39.561% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 919 \tTraining Loss: 0.01211494 \tValidation Loss 0.01734405 \tTraining Acuuarcy 39.148% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 920 \tTraining Loss: 0.01206492 \tValidation Loss 0.01747590 \tTraining Acuuarcy 39.661% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 921 \tTraining Loss: 0.01208622 \tValidation Loss 0.01786543 \tTraining Acuuarcy 39.176% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 922 \tTraining Loss: 0.01195625 \tValidation Loss 0.01783671 \tTraining Acuuarcy 40.257% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 923 \tTraining Loss: 0.01207708 \tValidation Loss 0.01778623 \tTraining Acuuarcy 39.416% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 924 \tTraining Loss: 0.01205905 \tValidation Loss 0.01753960 \tTraining Acuuarcy 39.600% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 925 \tTraining Loss: 0.01207615 \tValidation Loss 0.01756082 \tTraining Acuuarcy 39.973% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 926 \tTraining Loss: 0.01201729 \tValidation Loss 0.01733917 \tTraining Acuuarcy 39.321% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 927 \tTraining Loss: 0.01203877 \tValidation Loss 0.01721392 \tTraining Acuuarcy 40.001% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 928 \tTraining Loss: 0.01204343 \tValidation Loss 0.01810582 \tTraining Acuuarcy 39.834% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 929 \tTraining Loss: 0.01194475 \tValidation Loss 0.01796554 \tTraining Acuuarcy 40.469% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 930 \tTraining Loss: 0.01198234 \tValidation Loss 0.01748038 \tTraining Acuuarcy 40.185% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 931 \tTraining Loss: 0.01204230 \tValidation Loss 0.01760521 \tTraining Acuuarcy 39.550% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 932 \tTraining Loss: 0.01203896 \tValidation Loss 0.01738136 \tTraining Acuuarcy 39.957% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 933 \tTraining Loss: 0.01203702 \tValidation Loss 0.01746654 \tTraining Acuuarcy 39.410% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 934 \tTraining Loss: 0.01201865 \tValidation Loss 0.01807643 \tTraining Acuuarcy 39.678% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 935 \tTraining Loss: 0.01203914 \tValidation Loss 0.01764297 \tTraining Acuuarcy 40.218% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 936 \tTraining Loss: 0.01194010 \tValidation Loss 0.01741226 \tTraining Acuuarcy 39.973% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 937 \tTraining Loss: 0.01203991 \tValidation Loss 0.01775682 \tTraining Acuuarcy 39.382% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 938 \tTraining Loss: 0.01203742 \tValidation Loss 0.01771923 \tTraining Acuuarcy 39.683% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 939 \tTraining Loss: 0.01202691 \tValidation Loss 0.01738270 \tTraining Acuuarcy 39.460% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 940 \tTraining Loss: 0.01199592 \tValidation Loss 0.01759936 \tTraining Acuuarcy 39.901% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 941 \tTraining Loss: 0.01201111 \tValidation Loss 0.01766700 \tTraining Acuuarcy 39.566% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 942 \tTraining Loss: 0.01202960 \tValidation Loss 0.01752651 \tTraining Acuuarcy 40.040% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 943 \tTraining Loss: 0.01199625 \tValidation Loss 0.01738984 \tTraining Acuuarcy 40.135% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 944 \tTraining Loss: 0.01199178 \tValidation Loss 0.01777551 \tTraining Acuuarcy 39.984% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 945 \tTraining Loss: 0.01198441 \tValidation Loss 0.01774971 \tTraining Acuuarcy 40.118% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 946 \tTraining Loss: 0.01204553 \tValidation Loss 0.01784348 \tTraining Acuuarcy 39.795% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 947 \tTraining Loss: 0.01203133 \tValidation Loss 0.01787383 \tTraining Acuuarcy 39.789% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 948 \tTraining Loss: 0.01199439 \tValidation Loss 0.01778718 \tTraining Acuuarcy 39.945% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 949 \tTraining Loss: 0.01206506 \tValidation Loss 0.01809939 \tTraining Acuuarcy 39.399% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 950 \tTraining Loss: 0.01193438 \tValidation Loss 0.01732599 \tTraining Acuuarcy 40.046% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 951 \tTraining Loss: 0.01205489 \tValidation Loss 0.01756221 \tTraining Acuuarcy 40.018% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 952 \tTraining Loss: 0.01199498 \tValidation Loss 0.01781762 \tTraining Acuuarcy 40.085% \tValidation Acuuarcy 20.368%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 953 \tTraining Loss: 0.01204775 \tValidation Loss 0.01750557 \tTraining Acuuarcy 39.912% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 954 \tTraining Loss: 0.01197030 \tValidation Loss 0.01732177 \tTraining Acuuarcy 40.386% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 955 \tTraining Loss: 0.01195351 \tValidation Loss 0.01795069 \tTraining Acuuarcy 40.107% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 956 \tTraining Loss: 0.01206080 \tValidation Loss 0.01789838 \tTraining Acuuarcy 39.789% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 957 \tTraining Loss: 0.01194827 \tValidation Loss 0.01736205 \tTraining Acuuarcy 40.179% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 958 \tTraining Loss: 0.01205412 \tValidation Loss 0.01779927 \tTraining Acuuarcy 39.756% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 959 \tTraining Loss: 0.01211020 \tValidation Loss 0.01719490 \tTraining Acuuarcy 39.277% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 960 \tTraining Loss: 0.01200641 \tValidation Loss 0.01770328 \tTraining Acuuarcy 39.851% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 961 \tTraining Loss: 0.01203751 \tValidation Loss 0.01750964 \tTraining Acuuarcy 39.399% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 962 \tTraining Loss: 0.01192436 \tValidation Loss 0.01788921 \tTraining Acuuarcy 40.469% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 963 \tTraining Loss: 0.01197784 \tValidation Loss 0.01747588 \tTraining Acuuarcy 40.363% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 964 \tTraining Loss: 0.01203082 \tValidation Loss 0.01727431 \tTraining Acuuarcy 39.845% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 965 \tTraining Loss: 0.01195682 \tValidation Loss 0.01777740 \tTraining Acuuarcy 40.402% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 966 \tTraining Loss: 0.01195379 \tValidation Loss 0.01804359 \tTraining Acuuarcy 40.453% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 967 \tTraining Loss: 0.01204540 \tValidation Loss 0.01810598 \tTraining Acuuarcy 39.945% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 968 \tTraining Loss: 0.01192675 \tValidation Loss 0.01772976 \tTraining Acuuarcy 40.441% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 969 \tTraining Loss: 0.01199266 \tValidation Loss 0.01809720 \tTraining Acuuarcy 39.789% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 970 \tTraining Loss: 0.01193271 \tValidation Loss 0.01765469 \tTraining Acuuarcy 40.469% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 971 \tTraining Loss: 0.01197237 \tValidation Loss 0.01763766 \tTraining Acuuarcy 39.784% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 972 \tTraining Loss: 0.01196312 \tValidation Loss 0.01788003 \tTraining Acuuarcy 40.453% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 973 \tTraining Loss: 0.01195428 \tValidation Loss 0.01765599 \tTraining Acuuarcy 40.124% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 974 \tTraining Loss: 0.01197080 \tValidation Loss 0.01751014 \tTraining Acuuarcy 40.179% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 975 \tTraining Loss: 0.01194705 \tValidation Loss 0.01787118 \tTraining Acuuarcy 39.962% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 976 \tTraining Loss: 0.01204900 \tValidation Loss 0.01747771 \tTraining Acuuarcy 39.611% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 977 \tTraining Loss: 0.01197951 \tValidation Loss 0.01791767 \tTraining Acuuarcy 40.129% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 978 \tTraining Loss: 0.01205576 \tValidation Loss 0.01750144 \tTraining Acuuarcy 39.795% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 979 \tTraining Loss: 0.01202973 \tValidation Loss 0.01742204 \tTraining Acuuarcy 39.706% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 980 \tTraining Loss: 0.01191590 \tValidation Loss 0.01747921 \tTraining Acuuarcy 40.676% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 981 \tTraining Loss: 0.01198812 \tValidation Loss 0.01821432 \tTraining Acuuarcy 40.213% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 982 \tTraining Loss: 0.01200409 \tValidation Loss 0.01819573 \tTraining Acuuarcy 39.951% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 983 \tTraining Loss: 0.01197321 \tValidation Loss 0.01759940 \tTraining Acuuarcy 40.029% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 984 \tTraining Loss: 0.01188992 \tValidation Loss 0.01809162 \tTraining Acuuarcy 40.313% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 985 \tTraining Loss: 0.01202353 \tValidation Loss 0.01763317 \tTraining Acuuarcy 39.516% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 986 \tTraining Loss: 0.01196292 \tValidation Loss 0.01786197 \tTraining Acuuarcy 40.252% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 987 \tTraining Loss: 0.01197414 \tValidation Loss 0.01762138 \tTraining Acuuarcy 40.135% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 988 \tTraining Loss: 0.01205818 \tValidation Loss 0.01747272 \tTraining Acuuarcy 39.979% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 989 \tTraining Loss: 0.01202280 \tValidation Loss 0.01752202 \tTraining Acuuarcy 39.851% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 990 \tTraining Loss: 0.01189929 \tValidation Loss 0.01783025 \tTraining Acuuarcy 40.793% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 991 \tTraining Loss: 0.01197225 \tValidation Loss 0.01794772 \tTraining Acuuarcy 39.957% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 992 \tTraining Loss: 0.01195757 \tValidation Loss 0.01746149 \tTraining Acuuarcy 40.347% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 993 \tTraining Loss: 0.01193805 \tValidation Loss 0.01756422 \tTraining Acuuarcy 40.926% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 994 \tTraining Loss: 0.01192564 \tValidation Loss 0.01738895 \tTraining Acuuarcy 40.336% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 995 \tTraining Loss: 0.01192018 \tValidation Loss 0.01774610 \tTraining Acuuarcy 40.257% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 996 \tTraining Loss: 0.01202543 \tValidation Loss 0.01806930 \tTraining Acuuarcy 39.890% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 997 \tTraining Loss: 0.01195505 \tValidation Loss 0.01759864 \tTraining Acuuarcy 40.113% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 998 \tTraining Loss: 0.01193963 \tValidation Loss 0.01800279 \tTraining Acuuarcy 40.235% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 999 \tTraining Loss: 0.01190993 \tValidation Loss 0.01826545 \tTraining Acuuarcy 40.709% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1000 \tTraining Loss: 0.01188943 \tValidation Loss 0.01818480 \tTraining Acuuarcy 40.804% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1001 \tTraining Loss: 0.01197500 \tValidation Loss 0.01816288 \tTraining Acuuarcy 40.347% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1002 \tTraining Loss: 0.01191000 \tValidation Loss 0.01796502 \tTraining Acuuarcy 40.715% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1003 \tTraining Loss: 0.01199505 \tValidation Loss 0.01766128 \tTraining Acuuarcy 40.179% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1004 \tTraining Loss: 0.01202882 \tValidation Loss 0.01767277 \tTraining Acuuarcy 39.962% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1005 \tTraining Loss: 0.01201756 \tValidation Loss 0.01776225 \tTraining Acuuarcy 39.778% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1006 \tTraining Loss: 0.01201306 \tValidation Loss 0.01763405 \tTraining Acuuarcy 40.319% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1007 \tTraining Loss: 0.01202333 \tValidation Loss 0.01782808 \tTraining Acuuarcy 39.667% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1008 \tTraining Loss: 0.01193016 \tValidation Loss 0.01786754 \tTraining Acuuarcy 40.280% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1009 \tTraining Loss: 0.01188306 \tValidation Loss 0.01811998 \tTraining Acuuarcy 40.397% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1010 \tTraining Loss: 0.01192945 \tValidation Loss 0.01803353 \tTraining Acuuarcy 40.636% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1011 \tTraining Loss: 0.01196583 \tValidation Loss 0.01788190 \tTraining Acuuarcy 40.057% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1012 \tTraining Loss: 0.01200084 \tValidation Loss 0.01752603 \tTraining Acuuarcy 40.001% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1013 \tTraining Loss: 0.01205675 \tValidation Loss 0.01744852 \tTraining Acuuarcy 39.154% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1014 \tTraining Loss: 0.01196671 \tValidation Loss 0.01813585 \tTraining Acuuarcy 39.561% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1015 \tTraining Loss: 0.01193826 \tValidation Loss 0.01760441 \tTraining Acuuarcy 39.901% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1016 \tTraining Loss: 0.01201101 \tValidation Loss 0.01750034 \tTraining Acuuarcy 39.934% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1017 \tTraining Loss: 0.01193306 \tValidation Loss 0.01770365 \tTraining Acuuarcy 40.202% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1018 \tTraining Loss: 0.01192662 \tValidation Loss 0.01802865 \tTraining Acuuarcy 40.430% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1019 \tTraining Loss: 0.01191019 \tValidation Loss 0.01776218 \tTraining Acuuarcy 40.636% \tValidation Acuuarcy 20.117%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1020 \tTraining Loss: 0.01194545 \tValidation Loss 0.01740564 \tTraining Acuuarcy 40.263% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1021 \tTraining Loss: 0.01191124 \tValidation Loss 0.01812784 \tTraining Acuuarcy 40.659% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1022 \tTraining Loss: 0.01196853 \tValidation Loss 0.01791599 \tTraining Acuuarcy 40.514% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 1023 \tTraining Loss: 0.01198762 \tValidation Loss 0.01775599 \tTraining Acuuarcy 39.951% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1024 \tTraining Loss: 0.01193734 \tValidation Loss 0.01770432 \tTraining Acuuarcy 40.386% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1025 \tTraining Loss: 0.01204574 \tValidation Loss 0.01770361 \tTraining Acuuarcy 39.583% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1026 \tTraining Loss: 0.01195354 \tValidation Loss 0.01739508 \tTraining Acuuarcy 39.761% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1027 \tTraining Loss: 0.01193478 \tValidation Loss 0.01785076 \tTraining Acuuarcy 40.453% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1028 \tTraining Loss: 0.01189852 \tValidation Loss 0.01763648 \tTraining Acuuarcy 40.636% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 1029 \tTraining Loss: 0.01202532 \tValidation Loss 0.01782764 \tTraining Acuuarcy 39.410% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1030 \tTraining Loss: 0.01193625 \tValidation Loss 0.01764742 \tTraining Acuuarcy 40.636% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1031 \tTraining Loss: 0.01200601 \tValidation Loss 0.01767957 \tTraining Acuuarcy 39.773% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1032 \tTraining Loss: 0.01193953 \tValidation Loss 0.01792092 \tTraining Acuuarcy 40.525% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1033 \tTraining Loss: 0.01200457 \tValidation Loss 0.01774711 \tTraining Acuuarcy 39.828% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1034 \tTraining Loss: 0.01198083 \tValidation Loss 0.01781867 \tTraining Acuuarcy 40.285% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1035 \tTraining Loss: 0.01193727 \tValidation Loss 0.01782972 \tTraining Acuuarcy 40.263% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1036 \tTraining Loss: 0.01197524 \tValidation Loss 0.01784798 \tTraining Acuuarcy 39.644% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1037 \tTraining Loss: 0.01199643 \tValidation Loss 0.01759175 \tTraining Acuuarcy 39.929% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1038 \tTraining Loss: 0.01195953 \tValidation Loss 0.01747314 \tTraining Acuuarcy 39.700% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1039 \tTraining Loss: 0.01198476 \tValidation Loss 0.01777710 \tTraining Acuuarcy 39.923% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1040 \tTraining Loss: 0.01196063 \tValidation Loss 0.01838836 \tTraining Acuuarcy 39.878% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1041 \tTraining Loss: 0.01189218 \tValidation Loss 0.01796447 \tTraining Acuuarcy 40.648% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1042 \tTraining Loss: 0.01193940 \tValidation Loss 0.01741741 \tTraining Acuuarcy 40.642% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1043 \tTraining Loss: 0.01195690 \tValidation Loss 0.01774754 \tTraining Acuuarcy 40.603% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1044 \tTraining Loss: 0.01189467 \tValidation Loss 0.01748665 \tTraining Acuuarcy 40.631% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1045 \tTraining Loss: 0.01194851 \tValidation Loss 0.01810626 \tTraining Acuuarcy 39.918% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1046 \tTraining Loss: 0.01197972 \tValidation Loss 0.01734199 \tTraining Acuuarcy 40.001% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1047 \tTraining Loss: 0.01200489 \tValidation Loss 0.01899490 \tTraining Acuuarcy 40.581% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1048 \tTraining Loss: 0.01196154 \tValidation Loss 0.01782841 \tTraining Acuuarcy 40.096% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1049 \tTraining Loss: 0.01195995 \tValidation Loss 0.01783914 \tTraining Acuuarcy 40.207% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1050 \tTraining Loss: 0.01199156 \tValidation Loss 0.01796013 \tTraining Acuuarcy 40.207% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1051 \tTraining Loss: 0.01196240 \tValidation Loss 0.01746123 \tTraining Acuuarcy 40.692% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1052 \tTraining Loss: 0.01194669 \tValidation Loss 0.01784881 \tTraining Acuuarcy 40.492% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1053 \tTraining Loss: 0.01190988 \tValidation Loss 0.01779654 \tTraining Acuuarcy 40.246% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1054 \tTraining Loss: 0.01194286 \tValidation Loss 0.01774612 \tTraining Acuuarcy 40.330% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1055 \tTraining Loss: 0.01197501 \tValidation Loss 0.01764238 \tTraining Acuuarcy 40.062% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1056 \tTraining Loss: 0.01197212 \tValidation Loss 0.01760044 \tTraining Acuuarcy 39.973% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1057 \tTraining Loss: 0.01200568 \tValidation Loss 0.01763867 \tTraining Acuuarcy 39.912% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1058 \tTraining Loss: 0.01193029 \tValidation Loss 0.01842747 \tTraining Acuuarcy 40.770% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1059 \tTraining Loss: 0.01201438 \tValidation Loss 0.01764413 \tTraining Acuuarcy 40.101% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 1060 \tTraining Loss: 0.01190292 \tValidation Loss 0.01778631 \tTraining Acuuarcy 40.770% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1061 \tTraining Loss: 0.01192291 \tValidation Loss 0.01767347 \tTraining Acuuarcy 40.843% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1062 \tTraining Loss: 0.01191396 \tValidation Loss 0.01765917 \tTraining Acuuarcy 40.414% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1063 \tTraining Loss: 0.01189522 \tValidation Loss 0.01794280 \tTraining Acuuarcy 40.759% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1064 \tTraining Loss: 0.01205127 \tValidation Loss 0.01767545 \tTraining Acuuarcy 39.349% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1065 \tTraining Loss: 0.01198082 \tValidation Loss 0.01770771 \tTraining Acuuarcy 40.168% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1066 \tTraining Loss: 0.01197308 \tValidation Loss 0.01780853 \tTraining Acuuarcy 40.369% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1067 \tTraining Loss: 0.01195370 \tValidation Loss 0.01760065 \tTraining Acuuarcy 40.352% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1068 \tTraining Loss: 0.01196720 \tValidation Loss 0.01741445 \tTraining Acuuarcy 40.341% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1069 \tTraining Loss: 0.01200354 \tValidation Loss 0.01768111 \tTraining Acuuarcy 39.806% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1070 \tTraining Loss: 0.01198475 \tValidation Loss 0.01738083 \tTraining Acuuarcy 40.302% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1071 \tTraining Loss: 0.01187630 \tValidation Loss 0.01772830 \tTraining Acuuarcy 40.726% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1072 \tTraining Loss: 0.01196772 \tValidation Loss 0.01735581 \tTraining Acuuarcy 40.285% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1073 \tTraining Loss: 0.01192634 \tValidation Loss 0.01747663 \tTraining Acuuarcy 40.648% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1074 \tTraining Loss: 0.01190962 \tValidation Loss 0.01773816 \tTraining Acuuarcy 40.503% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1075 \tTraining Loss: 0.01195238 \tValidation Loss 0.01803831 \tTraining Acuuarcy 40.408% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1076 \tTraining Loss: 0.01187948 \tValidation Loss 0.01770865 \tTraining Acuuarcy 40.603% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 1077 \tTraining Loss: 0.01190609 \tValidation Loss 0.01789381 \tTraining Acuuarcy 40.715% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 1078 \tTraining Loss: 0.01199547 \tValidation Loss 0.01783725 \tTraining Acuuarcy 40.118% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1079 \tTraining Loss: 0.01197739 \tValidation Loss 0.01740032 \tTraining Acuuarcy 40.269% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1080 \tTraining Loss: 0.01197503 \tValidation Loss 0.01746464 \tTraining Acuuarcy 40.564% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1081 \tTraining Loss: 0.01184183 \tValidation Loss 0.01764277 \tTraining Acuuarcy 40.960% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1082 \tTraining Loss: 0.01192047 \tValidation Loss 0.01732234 \tTraining Acuuarcy 40.419% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1083 \tTraining Loss: 0.01197177 \tValidation Loss 0.01750513 \tTraining Acuuarcy 40.469% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1084 \tTraining Loss: 0.01189412 \tValidation Loss 0.01770924 \tTraining Acuuarcy 40.625% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1085 \tTraining Loss: 0.01196407 \tValidation Loss 0.01822322 \tTraining Acuuarcy 40.497% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1086 \tTraining Loss: 0.01187692 \tValidation Loss 0.01834108 \tTraining Acuuarcy 40.993% \tValidation Acuuarcy 19.755%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1087 \tTraining Loss: 0.01192797 \tValidation Loss 0.01799426 \tTraining Acuuarcy 40.302% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1088 \tTraining Loss: 0.01182809 \tValidation Loss 0.01781304 \tTraining Acuuarcy 40.748% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1089 \tTraining Loss: 0.01191081 \tValidation Loss 0.01750710 \tTraining Acuuarcy 40.447% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1090 \tTraining Loss: 0.01188710 \tValidation Loss 0.01778910 \tTraining Acuuarcy 40.664% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1091 \tTraining Loss: 0.01195705 \tValidation Loss 0.01791901 \tTraining Acuuarcy 40.269% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1092 \tTraining Loss: 0.01187741 \tValidation Loss 0.01777877 \tTraining Acuuarcy 40.703% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1093 \tTraining Loss: 0.01203621 \tValidation Loss 0.01759239 \tTraining Acuuarcy 39.957% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1094 \tTraining Loss: 0.01188752 \tValidation Loss 0.01765395 \tTraining Acuuarcy 40.352% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 1095 \tTraining Loss: 0.01199403 \tValidation Loss 0.01816448 \tTraining Acuuarcy 40.191% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1096 \tTraining Loss: 0.01194134 \tValidation Loss 0.01746795 \tTraining Acuuarcy 40.107% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1097 \tTraining Loss: 0.01195306 \tValidation Loss 0.01799889 \tTraining Acuuarcy 40.118% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1098 \tTraining Loss: 0.01190285 \tValidation Loss 0.01771083 \tTraining Acuuarcy 40.230% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1099 \tTraining Loss: 0.01193851 \tValidation Loss 0.01761847 \tTraining Acuuarcy 40.408% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1100 \tTraining Loss: 0.01193883 \tValidation Loss 0.01769235 \tTraining Acuuarcy 40.447% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1101 \tTraining Loss: 0.01192510 \tValidation Loss 0.01755780 \tTraining Acuuarcy 40.207% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1102 \tTraining Loss: 0.01194179 \tValidation Loss 0.01732027 \tTraining Acuuarcy 40.163% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1103 \tTraining Loss: 0.01194814 \tValidation Loss 0.01788779 \tTraining Acuuarcy 40.542% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1104 \tTraining Loss: 0.01186601 \tValidation Loss 0.01835711 \tTraining Acuuarcy 40.230% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1105 \tTraining Loss: 0.01189765 \tValidation Loss 0.01785359 \tTraining Acuuarcy 40.620% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1106 \tTraining Loss: 0.01189728 \tValidation Loss 0.01776667 \tTraining Acuuarcy 40.759% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 1107 \tTraining Loss: 0.01196015 \tValidation Loss 0.01761128 \tTraining Acuuarcy 39.795% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1108 \tTraining Loss: 0.01193042 \tValidation Loss 0.01748037 \tTraining Acuuarcy 40.230% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1109 \tTraining Loss: 0.01190280 \tValidation Loss 0.01747281 \tTraining Acuuarcy 40.241% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1110 \tTraining Loss: 0.01191567 \tValidation Loss 0.01756367 \tTraining Acuuarcy 40.453% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1111 \tTraining Loss: 0.01191186 \tValidation Loss 0.01749551 \tTraining Acuuarcy 40.497% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1112 \tTraining Loss: 0.01192584 \tValidation Loss 0.01801254 \tTraining Acuuarcy 40.363% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1113 \tTraining Loss: 0.01194071 \tValidation Loss 0.01810253 \tTraining Acuuarcy 40.308% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1114 \tTraining Loss: 0.01185894 \tValidation Loss 0.01794114 \tTraining Acuuarcy 40.157% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1115 \tTraining Loss: 0.01189286 \tValidation Loss 0.01740696 \tTraining Acuuarcy 40.196% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1116 \tTraining Loss: 0.01191078 \tValidation Loss 0.01782193 \tTraining Acuuarcy 40.592% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1117 \tTraining Loss: 0.01188583 \tValidation Loss 0.01762749 \tTraining Acuuarcy 40.480% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1118 \tTraining Loss: 0.01191218 \tValidation Loss 0.01777264 \tTraining Acuuarcy 40.062% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1119 \tTraining Loss: 0.01189493 \tValidation Loss 0.01765483 \tTraining Acuuarcy 40.581% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 1120 \tTraining Loss: 0.01194318 \tValidation Loss 0.01785881 \tTraining Acuuarcy 40.631% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1121 \tTraining Loss: 0.01193389 \tValidation Loss 0.01753952 \tTraining Acuuarcy 40.347% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1122 \tTraining Loss: 0.01193099 \tValidation Loss 0.01759164 \tTraining Acuuarcy 40.531% \tValidation Acuuarcy 21.817%\n",
      "Epoch: 1123 \tTraining Loss: 0.01191363 \tValidation Loss 0.01812215 \tTraining Acuuarcy 40.391% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1124 \tTraining Loss: 0.01189595 \tValidation Loss 0.01775480 \tTraining Acuuarcy 40.230% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1125 \tTraining Loss: 0.01189747 \tValidation Loss 0.01778385 \tTraining Acuuarcy 40.681% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1126 \tTraining Loss: 0.01193904 \tValidation Loss 0.01796234 \tTraining Acuuarcy 40.564% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1127 \tTraining Loss: 0.01187213 \tValidation Loss 0.01762685 \tTraining Acuuarcy 40.871% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1128 \tTraining Loss: 0.01184269 \tValidation Loss 0.01775180 \tTraining Acuuarcy 41.355% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1129 \tTraining Loss: 0.01192928 \tValidation Loss 0.01778928 \tTraining Acuuarcy 40.380% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1130 \tTraining Loss: 0.01189549 \tValidation Loss 0.01749824 \tTraining Acuuarcy 40.876% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1131 \tTraining Loss: 0.01192204 \tValidation Loss 0.01773811 \tTraining Acuuarcy 40.826% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1132 \tTraining Loss: 0.01191112 \tValidation Loss 0.01759831 \tTraining Acuuarcy 40.085% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1133 \tTraining Loss: 0.01187137 \tValidation Loss 0.01770884 \tTraining Acuuarcy 40.871% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 1134 \tTraining Loss: 0.01190726 \tValidation Loss 0.01802987 \tTraining Acuuarcy 40.475% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1135 \tTraining Loss: 0.01182788 \tValidation Loss 0.01769058 \tTraining Acuuarcy 41.094% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1136 \tTraining Loss: 0.01189854 \tValidation Loss 0.01822970 \tTraining Acuuarcy 40.609% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1137 \tTraining Loss: 0.01183150 \tValidation Loss 0.01814120 \tTraining Acuuarcy 40.893% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1138 \tTraining Loss: 0.01189427 \tValidation Loss 0.01845435 \tTraining Acuuarcy 40.698% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1139 \tTraining Loss: 0.01202717 \tValidation Loss 0.01773228 \tTraining Acuuarcy 40.302% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1140 \tTraining Loss: 0.01197384 \tValidation Loss 0.01816468 \tTraining Acuuarcy 40.441% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1141 \tTraining Loss: 0.01183135 \tValidation Loss 0.01824766 \tTraining Acuuarcy 40.960% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1142 \tTraining Loss: 0.01183704 \tValidation Loss 0.01785775 \tTraining Acuuarcy 40.804% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 1143 \tTraining Loss: 0.01195440 \tValidation Loss 0.01768115 \tTraining Acuuarcy 40.246% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1144 \tTraining Loss: 0.01191423 \tValidation Loss 0.01743388 \tTraining Acuuarcy 40.575% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1145 \tTraining Loss: 0.01192808 \tValidation Loss 0.01767623 \tTraining Acuuarcy 40.358% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1146 \tTraining Loss: 0.01194477 \tValidation Loss 0.01738885 \tTraining Acuuarcy 40.269% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1147 \tTraining Loss: 0.01184082 \tValidation Loss 0.01760731 \tTraining Acuuarcy 40.882% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 1148 \tTraining Loss: 0.01194326 \tValidation Loss 0.01798378 \tTraining Acuuarcy 40.135% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1149 \tTraining Loss: 0.01190008 \tValidation Loss 0.01750502 \tTraining Acuuarcy 40.631% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1150 \tTraining Loss: 0.01185244 \tValidation Loss 0.01796113 \tTraining Acuuarcy 40.531% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1151 \tTraining Loss: 0.01195394 \tValidation Loss 0.01767199 \tTraining Acuuarcy 40.163% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1152 \tTraining Loss: 0.01186775 \tValidation Loss 0.01771721 \tTraining Acuuarcy 40.826% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1153 \tTraining Loss: 0.01187714 \tValidation Loss 0.01793774 \tTraining Acuuarcy 40.430% \tValidation Acuuarcy 19.532%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1154 \tTraining Loss: 0.01186230 \tValidation Loss 0.01782051 \tTraining Acuuarcy 40.898% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1155 \tTraining Loss: 0.01192487 \tValidation Loss 0.01726408 \tTraining Acuuarcy 40.235% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 1156 \tTraining Loss: 0.01185686 \tValidation Loss 0.01781008 \tTraining Acuuarcy 40.592% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1157 \tTraining Loss: 0.01192354 \tValidation Loss 0.01794155 \tTraining Acuuarcy 40.480% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 1158 \tTraining Loss: 0.01190939 \tValidation Loss 0.01793415 \tTraining Acuuarcy 40.592% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1159 \tTraining Loss: 0.01187080 \tValidation Loss 0.01782397 \tTraining Acuuarcy 40.447% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1160 \tTraining Loss: 0.01188029 \tValidation Loss 0.01849775 \tTraining Acuuarcy 40.798% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1161 \tTraining Loss: 0.01190044 \tValidation Loss 0.01731084 \tTraining Acuuarcy 40.976% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1162 \tTraining Loss: 0.01195950 \tValidation Loss 0.01759792 \tTraining Acuuarcy 40.514% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1163 \tTraining Loss: 0.01185529 \tValidation Loss 0.01812211 \tTraining Acuuarcy 41.205% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1164 \tTraining Loss: 0.01182703 \tValidation Loss 0.01824051 \tTraining Acuuarcy 41.060% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1165 \tTraining Loss: 0.01186576 \tValidation Loss 0.01793834 \tTraining Acuuarcy 40.681% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 1166 \tTraining Loss: 0.01185110 \tValidation Loss 0.01805956 \tTraining Acuuarcy 41.021% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1167 \tTraining Loss: 0.01185616 \tValidation Loss 0.01794304 \tTraining Acuuarcy 41.049% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1168 \tTraining Loss: 0.01196325 \tValidation Loss 0.01748273 \tTraining Acuuarcy 40.207% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1169 \tTraining Loss: 0.01183697 \tValidation Loss 0.01780446 \tTraining Acuuarcy 40.519% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1170 \tTraining Loss: 0.01189835 \tValidation Loss 0.01795773 \tTraining Acuuarcy 40.620% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1171 \tTraining Loss: 0.01189420 \tValidation Loss 0.01811956 \tTraining Acuuarcy 40.402% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1172 \tTraining Loss: 0.01191093 \tValidation Loss 0.01796808 \tTraining Acuuarcy 40.832% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1173 \tTraining Loss: 0.01187381 \tValidation Loss 0.01741233 \tTraining Acuuarcy 40.882% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1174 \tTraining Loss: 0.01187388 \tValidation Loss 0.01821731 \tTraining Acuuarcy 40.898% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1175 \tTraining Loss: 0.01189774 \tValidation Loss 0.01773623 \tTraining Acuuarcy 40.887% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1176 \tTraining Loss: 0.01190689 \tValidation Loss 0.01823298 \tTraining Acuuarcy 40.302% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1177 \tTraining Loss: 0.01189684 \tValidation Loss 0.01799159 \tTraining Acuuarcy 40.843% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1178 \tTraining Loss: 0.01189069 \tValidation Loss 0.01748237 \tTraining Acuuarcy 40.742% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1179 \tTraining Loss: 0.01191789 \tValidation Loss 0.01783448 \tTraining Acuuarcy 40.291% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1180 \tTraining Loss: 0.01186547 \tValidation Loss 0.01804258 \tTraining Acuuarcy 40.837% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1181 \tTraining Loss: 0.01185177 \tValidation Loss 0.01745749 \tTraining Acuuarcy 40.776% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1182 \tTraining Loss: 0.01184949 \tValidation Loss 0.01769367 \tTraining Acuuarcy 41.668% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1183 \tTraining Loss: 0.01192900 \tValidation Loss 0.01784889 \tTraining Acuuarcy 40.625% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1184 \tTraining Loss: 0.01196917 \tValidation Loss 0.01769036 \tTraining Acuuarcy 40.163% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1185 \tTraining Loss: 0.01195589 \tValidation Loss 0.01736186 \tTraining Acuuarcy 40.391% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1186 \tTraining Loss: 0.01191581 \tValidation Loss 0.01776722 \tTraining Acuuarcy 40.781% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1187 \tTraining Loss: 0.01188930 \tValidation Loss 0.01780708 \tTraining Acuuarcy 40.692% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1188 \tTraining Loss: 0.01193012 \tValidation Loss 0.01774684 \tTraining Acuuarcy 40.564% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1189 \tTraining Loss: 0.01183791 \tValidation Loss 0.01785546 \tTraining Acuuarcy 41.099% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1190 \tTraining Loss: 0.01189021 \tValidation Loss 0.01766484 \tTraining Acuuarcy 40.709% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1191 \tTraining Loss: 0.01189684 \tValidation Loss 0.01791563 \tTraining Acuuarcy 40.347% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1192 \tTraining Loss: 0.01189887 \tValidation Loss 0.01808938 \tTraining Acuuarcy 40.207% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1193 \tTraining Loss: 0.01190284 \tValidation Loss 0.01790669 \tTraining Acuuarcy 40.469% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1194 \tTraining Loss: 0.01185635 \tValidation Loss 0.01765495 \tTraining Acuuarcy 40.341% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1195 \tTraining Loss: 0.01180322 \tValidation Loss 0.01767511 \tTraining Acuuarcy 40.954% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1196 \tTraining Loss: 0.01193097 \tValidation Loss 0.01746548 \tTraining Acuuarcy 40.815% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 1197 \tTraining Loss: 0.01185743 \tValidation Loss 0.01785828 \tTraining Acuuarcy 40.943% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 1198 \tTraining Loss: 0.01190867 \tValidation Loss 0.01756877 \tTraining Acuuarcy 40.871% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1199 \tTraining Loss: 0.01182033 \tValidation Loss 0.01795578 \tTraining Acuuarcy 41.461% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1200 \tTraining Loss: 0.01193467 \tValidation Loss 0.01764528 \tTraining Acuuarcy 40.564% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1201 \tTraining Loss: 0.01188492 \tValidation Loss 0.01815341 \tTraining Acuuarcy 40.648% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1202 \tTraining Loss: 0.01187561 \tValidation Loss 0.01806215 \tTraining Acuuarcy 40.525% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1203 \tTraining Loss: 0.01184717 \tValidation Loss 0.01784169 \tTraining Acuuarcy 41.629% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1204 \tTraining Loss: 0.01187715 \tValidation Loss 0.01781435 \tTraining Acuuarcy 40.698% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1205 \tTraining Loss: 0.01185765 \tValidation Loss 0.01798481 \tTraining Acuuarcy 41.032% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1206 \tTraining Loss: 0.01186938 \tValidation Loss 0.01777995 \tTraining Acuuarcy 40.737% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 1207 \tTraining Loss: 0.01192130 \tValidation Loss 0.01770687 \tTraining Acuuarcy 40.553% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1208 \tTraining Loss: 0.01191135 \tValidation Loss 0.01826007 \tTraining Acuuarcy 40.341% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1209 \tTraining Loss: 0.01195392 \tValidation Loss 0.01794469 \tTraining Acuuarcy 40.252% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1210 \tTraining Loss: 0.01181011 \tValidation Loss 0.01795536 \tTraining Acuuarcy 40.965% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1211 \tTraining Loss: 0.01190604 \tValidation Loss 0.01824795 \tTraining Acuuarcy 40.475% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1212 \tTraining Loss: 0.01183108 \tValidation Loss 0.01773094 \tTraining Acuuarcy 40.742% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1213 \tTraining Loss: 0.01192478 \tValidation Loss 0.01764396 \tTraining Acuuarcy 40.291% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1214 \tTraining Loss: 0.01188796 \tValidation Loss 0.01774930 \tTraining Acuuarcy 40.475% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1215 \tTraining Loss: 0.01196003 \tValidation Loss 0.01779213 \tTraining Acuuarcy 40.692% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1216 \tTraining Loss: 0.01191907 \tValidation Loss 0.01827868 \tTraining Acuuarcy 40.497% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1217 \tTraining Loss: 0.01193946 \tValidation Loss 0.01770415 \tTraining Acuuarcy 40.787% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1218 \tTraining Loss: 0.01192520 \tValidation Loss 0.01775246 \tTraining Acuuarcy 40.475% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1219 \tTraining Loss: 0.01195226 \tValidation Loss 0.01791859 \tTraining Acuuarcy 40.046% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 1220 \tTraining Loss: 0.01188396 \tValidation Loss 0.01799579 \tTraining Acuuarcy 40.982% \tValidation Acuuarcy 20.201%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1221 \tTraining Loss: 0.01189993 \tValidation Loss 0.01759517 \tTraining Acuuarcy 40.653% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1222 \tTraining Loss: 0.01181725 \tValidation Loss 0.01771694 \tTraining Acuuarcy 40.820% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1223 \tTraining Loss: 0.01185145 \tValidation Loss 0.01802131 \tTraining Acuuarcy 41.082% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1224 \tTraining Loss: 0.01193750 \tValidation Loss 0.01812449 \tTraining Acuuarcy 40.174% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1225 \tTraining Loss: 0.01187200 \tValidation Loss 0.01785589 \tTraining Acuuarcy 41.015% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1226 \tTraining Loss: 0.01188075 \tValidation Loss 0.01787004 \tTraining Acuuarcy 40.347% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1227 \tTraining Loss: 0.01185931 \tValidation Loss 0.01763303 \tTraining Acuuarcy 41.350% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1228 \tTraining Loss: 0.01187711 \tValidation Loss 0.01779658 \tTraining Acuuarcy 40.865% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1229 \tTraining Loss: 0.01188373 \tValidation Loss 0.01839825 \tTraining Acuuarcy 40.597% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1230 \tTraining Loss: 0.01187395 \tValidation Loss 0.01810094 \tTraining Acuuarcy 40.737% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1231 \tTraining Loss: 0.01192633 \tValidation Loss 0.01775706 \tTraining Acuuarcy 40.531% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1232 \tTraining Loss: 0.01186964 \tValidation Loss 0.01779841 \tTraining Acuuarcy 40.804% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1233 \tTraining Loss: 0.01186745 \tValidation Loss 0.01784365 \tTraining Acuuarcy 41.155% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1234 \tTraining Loss: 0.01194191 \tValidation Loss 0.01758490 \tTraining Acuuarcy 40.581% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1235 \tTraining Loss: 0.01198968 \tValidation Loss 0.01764963 \tTraining Acuuarcy 40.163% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 1236 \tTraining Loss: 0.01188051 \tValidation Loss 0.01786166 \tTraining Acuuarcy 40.765% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1237 \tTraining Loss: 0.01188372 \tValidation Loss 0.01799375 \tTraining Acuuarcy 40.625% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1238 \tTraining Loss: 0.01194977 \tValidation Loss 0.01805964 \tTraining Acuuarcy 40.336% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1239 \tTraining Loss: 0.01187505 \tValidation Loss 0.01818161 \tTraining Acuuarcy 40.715% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1240 \tTraining Loss: 0.01188116 \tValidation Loss 0.01770609 \tTraining Acuuarcy 40.430% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1241 \tTraining Loss: 0.01181064 \tValidation Loss 0.01795287 \tTraining Acuuarcy 41.055% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1242 \tTraining Loss: 0.01190552 \tValidation Loss 0.01765463 \tTraining Acuuarcy 40.776% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1243 \tTraining Loss: 0.01188859 \tValidation Loss 0.01782650 \tTraining Acuuarcy 40.937% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1244 \tTraining Loss: 0.01186397 \tValidation Loss 0.01780493 \tTraining Acuuarcy 41.077% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1245 \tTraining Loss: 0.01186130 \tValidation Loss 0.01807208 \tTraining Acuuarcy 41.350% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1246 \tTraining Loss: 0.01189795 \tValidation Loss 0.01754117 \tTraining Acuuarcy 40.904% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1247 \tTraining Loss: 0.01181600 \tValidation Loss 0.01739274 \tTraining Acuuarcy 40.826% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 1248 \tTraining Loss: 0.01189780 \tValidation Loss 0.01781729 \tTraining Acuuarcy 40.492% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 1249 \tTraining Loss: 0.01179243 \tValidation Loss 0.01779753 \tTraining Acuuarcy 41.116% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1250 \tTraining Loss: 0.01191401 \tValidation Loss 0.01730328 \tTraining Acuuarcy 40.832% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1251 \tTraining Loss: 0.01192795 \tValidation Loss 0.01751724 \tTraining Acuuarcy 40.430% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1252 \tTraining Loss: 0.01190200 \tValidation Loss 0.01804371 \tTraining Acuuarcy 40.575% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1253 \tTraining Loss: 0.01185053 \tValidation Loss 0.01802473 \tTraining Acuuarcy 40.960% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1254 \tTraining Loss: 0.01184034 \tValidation Loss 0.01756081 \tTraining Acuuarcy 41.250% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1255 \tTraining Loss: 0.01184291 \tValidation Loss 0.01781128 \tTraining Acuuarcy 40.965% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1256 \tTraining Loss: 0.01182198 \tValidation Loss 0.01802089 \tTraining Acuuarcy 41.043% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1257 \tTraining Loss: 0.01197954 \tValidation Loss 0.01756946 \tTraining Acuuarcy 40.375% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1258 \tTraining Loss: 0.01182816 \tValidation Loss 0.01815667 \tTraining Acuuarcy 40.503% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1259 \tTraining Loss: 0.01192296 \tValidation Loss 0.01768137 \tTraining Acuuarcy 40.592% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1260 \tTraining Loss: 0.01186683 \tValidation Loss 0.01767920 \tTraining Acuuarcy 40.781% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 1261 \tTraining Loss: 0.01183388 \tValidation Loss 0.01811118 \tTraining Acuuarcy 40.921% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1262 \tTraining Loss: 0.01183845 \tValidation Loss 0.01798222 \tTraining Acuuarcy 40.859% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1263 \tTraining Loss: 0.01179695 \tValidation Loss 0.01818429 \tTraining Acuuarcy 41.233% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1264 \tTraining Loss: 0.01180543 \tValidation Loss 0.01753249 \tTraining Acuuarcy 40.943% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1265 \tTraining Loss: 0.01190299 \tValidation Loss 0.01746093 \tTraining Acuuarcy 40.347% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1266 \tTraining Loss: 0.01185811 \tValidation Loss 0.01776802 \tTraining Acuuarcy 40.603% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1267 \tTraining Loss: 0.01187930 \tValidation Loss 0.01820916 \tTraining Acuuarcy 40.436% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1268 \tTraining Loss: 0.01184061 \tValidation Loss 0.01766494 \tTraining Acuuarcy 41.021% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1269 \tTraining Loss: 0.01189016 \tValidation Loss 0.01798743 \tTraining Acuuarcy 40.787% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1270 \tTraining Loss: 0.01183793 \tValidation Loss 0.01789903 \tTraining Acuuarcy 40.915% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1271 \tTraining Loss: 0.01181669 \tValidation Loss 0.01801596 \tTraining Acuuarcy 40.882% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1272 \tTraining Loss: 0.01180026 \tValidation Loss 0.01808362 \tTraining Acuuarcy 41.394% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1273 \tTraining Loss: 0.01187327 \tValidation Loss 0.01757928 \tTraining Acuuarcy 41.127% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1274 \tTraining Loss: 0.01184863 \tValidation Loss 0.01765370 \tTraining Acuuarcy 41.155% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 1275 \tTraining Loss: 0.01186403 \tValidation Loss 0.01806041 \tTraining Acuuarcy 40.837% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1276 \tTraining Loss: 0.01185670 \tValidation Loss 0.01761349 \tTraining Acuuarcy 40.993% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1277 \tTraining Loss: 0.01175016 \tValidation Loss 0.01785819 \tTraining Acuuarcy 41.595% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1278 \tTraining Loss: 0.01186285 \tValidation Loss 0.01775751 \tTraining Acuuarcy 41.250% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1279 \tTraining Loss: 0.01188027 \tValidation Loss 0.01797188 \tTraining Acuuarcy 40.575% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1280 \tTraining Loss: 0.01189500 \tValidation Loss 0.01772401 \tTraining Acuuarcy 40.720% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 1281 \tTraining Loss: 0.01185250 \tValidation Loss 0.01800670 \tTraining Acuuarcy 40.547% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1282 \tTraining Loss: 0.01183390 \tValidation Loss 0.01765647 \tTraining Acuuarcy 41.316% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1283 \tTraining Loss: 0.01183756 \tValidation Loss 0.01777427 \tTraining Acuuarcy 41.116% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1284 \tTraining Loss: 0.01188801 \tValidation Loss 0.01775118 \tTraining Acuuarcy 40.553% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1285 \tTraining Loss: 0.01184196 \tValidation Loss 0.01796750 \tTraining Acuuarcy 40.988% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1286 \tTraining Loss: 0.01190078 \tValidation Loss 0.01794377 \tTraining Acuuarcy 40.592% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1287 \tTraining Loss: 0.01182104 \tValidation Loss 0.01781382 \tTraining Acuuarcy 41.172% \tValidation Acuuarcy 19.671%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1288 \tTraining Loss: 0.01191177 \tValidation Loss 0.01796361 \tTraining Acuuarcy 40.676% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 1289 \tTraining Loss: 0.01185547 \tValidation Loss 0.01796942 \tTraining Acuuarcy 40.971% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1290 \tTraining Loss: 0.01186078 \tValidation Loss 0.01785394 \tTraining Acuuarcy 40.441% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1291 \tTraining Loss: 0.01185050 \tValidation Loss 0.01786208 \tTraining Acuuarcy 40.776% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1292 \tTraining Loss: 0.01188878 \tValidation Loss 0.01813148 \tTraining Acuuarcy 40.993% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1293 \tTraining Loss: 0.01191091 \tValidation Loss 0.01772290 \tTraining Acuuarcy 40.575% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1294 \tTraining Loss: 0.01182751 \tValidation Loss 0.01829679 \tTraining Acuuarcy 40.937% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1295 \tTraining Loss: 0.01187669 \tValidation Loss 0.01826492 \tTraining Acuuarcy 40.748% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1296 \tTraining Loss: 0.01182527 \tValidation Loss 0.01778314 \tTraining Acuuarcy 41.015% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1297 \tTraining Loss: 0.01183654 \tValidation Loss 0.01820606 \tTraining Acuuarcy 40.893% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1298 \tTraining Loss: 0.01180789 \tValidation Loss 0.01825770 \tTraining Acuuarcy 41.545% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1299 \tTraining Loss: 0.01179627 \tValidation Loss 0.01788832 \tTraining Acuuarcy 41.952% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1300 \tTraining Loss: 0.01185787 \tValidation Loss 0.01768408 \tTraining Acuuarcy 40.832% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1301 \tTraining Loss: 0.01185143 \tValidation Loss 0.01776014 \tTraining Acuuarcy 40.592% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1302 \tTraining Loss: 0.01191780 \tValidation Loss 0.01737443 \tTraining Acuuarcy 40.664% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 1303 \tTraining Loss: 0.01184402 \tValidation Loss 0.01787623 \tTraining Acuuarcy 40.848% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1304 \tTraining Loss: 0.01185506 \tValidation Loss 0.01734076 \tTraining Acuuarcy 40.960% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 1305 \tTraining Loss: 0.01182657 \tValidation Loss 0.01800615 \tTraining Acuuarcy 41.305% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1306 \tTraining Loss: 0.01181435 \tValidation Loss 0.01797166 \tTraining Acuuarcy 40.932% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1307 \tTraining Loss: 0.01183346 \tValidation Loss 0.01806920 \tTraining Acuuarcy 41.311% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1308 \tTraining Loss: 0.01176531 \tValidation Loss 0.01773251 \tTraining Acuuarcy 41.294% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1309 \tTraining Loss: 0.01180294 \tValidation Loss 0.01786692 \tTraining Acuuarcy 41.534% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1310 \tTraining Loss: 0.01188696 \tValidation Loss 0.01762908 \tTraining Acuuarcy 40.486% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 1311 \tTraining Loss: 0.01182267 \tValidation Loss 0.01792664 \tTraining Acuuarcy 41.411% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1312 \tTraining Loss: 0.01175350 \tValidation Loss 0.01811411 \tTraining Acuuarcy 41.852% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1313 \tTraining Loss: 0.01183324 \tValidation Loss 0.01795326 \tTraining Acuuarcy 41.027% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1314 \tTraining Loss: 0.01188360 \tValidation Loss 0.01776633 \tTraining Acuuarcy 40.793% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1315 \tTraining Loss: 0.01189000 \tValidation Loss 0.01816663 \tTraining Acuuarcy 40.859% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1316 \tTraining Loss: 0.01182749 \tValidation Loss 0.01773110 \tTraining Acuuarcy 41.021% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1317 \tTraining Loss: 0.01186829 \tValidation Loss 0.01784350 \tTraining Acuuarcy 40.642% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1318 \tTraining Loss: 0.01189061 \tValidation Loss 0.01820242 \tTraining Acuuarcy 40.703% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1319 \tTraining Loss: 0.01187572 \tValidation Loss 0.01760978 \tTraining Acuuarcy 40.653% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1320 \tTraining Loss: 0.01184635 \tValidation Loss 0.01819681 \tTraining Acuuarcy 41.183% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1321 \tTraining Loss: 0.01182895 \tValidation Loss 0.01823998 \tTraining Acuuarcy 41.088% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1322 \tTraining Loss: 0.01188867 \tValidation Loss 0.01796654 \tTraining Acuuarcy 40.369% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1323 \tTraining Loss: 0.01187248 \tValidation Loss 0.01767694 \tTraining Acuuarcy 41.227% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1324 \tTraining Loss: 0.01187828 \tValidation Loss 0.01789034 \tTraining Acuuarcy 40.692% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1325 \tTraining Loss: 0.01187895 \tValidation Loss 0.01752138 \tTraining Acuuarcy 40.692% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1326 \tTraining Loss: 0.01185088 \tValidation Loss 0.01819599 \tTraining Acuuarcy 40.820% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1327 \tTraining Loss: 0.01183046 \tValidation Loss 0.01826704 \tTraining Acuuarcy 40.798% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1328 \tTraining Loss: 0.01183242 \tValidation Loss 0.01800019 \tTraining Acuuarcy 41.355% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1329 \tTraining Loss: 0.01182186 \tValidation Loss 0.01789422 \tTraining Acuuarcy 40.999% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1330 \tTraining Loss: 0.01190234 \tValidation Loss 0.01776061 \tTraining Acuuarcy 40.865% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 1331 \tTraining Loss: 0.01175575 \tValidation Loss 0.01817654 \tTraining Acuuarcy 41.679% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1332 \tTraining Loss: 0.01186453 \tValidation Loss 0.01796746 \tTraining Acuuarcy 40.715% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1333 \tTraining Loss: 0.01181200 \tValidation Loss 0.01752398 \tTraining Acuuarcy 41.110% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1334 \tTraining Loss: 0.01185201 \tValidation Loss 0.01816925 \tTraining Acuuarcy 40.932% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1335 \tTraining Loss: 0.01174909 \tValidation Loss 0.01796372 \tTraining Acuuarcy 41.578% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1336 \tTraining Loss: 0.01196016 \tValidation Loss 0.01777624 \tTraining Acuuarcy 40.319% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1337 \tTraining Loss: 0.01173184 \tValidation Loss 0.01794854 \tTraining Acuuarcy 41.556% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1338 \tTraining Loss: 0.01188981 \tValidation Loss 0.01781374 \tTraining Acuuarcy 40.207% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 1339 \tTraining Loss: 0.01189887 \tValidation Loss 0.01832624 \tTraining Acuuarcy 40.765% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1340 \tTraining Loss: 0.01184737 \tValidation Loss 0.01831922 \tTraining Acuuarcy 41.244% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1341 \tTraining Loss: 0.01182691 \tValidation Loss 0.01795082 \tTraining Acuuarcy 41.049% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1342 \tTraining Loss: 0.01184367 \tValidation Loss 0.01783984 \tTraining Acuuarcy 41.049% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1343 \tTraining Loss: 0.01189122 \tValidation Loss 0.01753152 \tTraining Acuuarcy 40.620% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1344 \tTraining Loss: 0.01176630 \tValidation Loss 0.01797073 \tTraining Acuuarcy 41.261% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1345 \tTraining Loss: 0.01181516 \tValidation Loss 0.01814305 \tTraining Acuuarcy 41.250% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1346 \tTraining Loss: 0.01188674 \tValidation Loss 0.01768979 \tTraining Acuuarcy 40.503% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1347 \tTraining Loss: 0.01188504 \tValidation Loss 0.01758455 \tTraining Acuuarcy 41.110% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1348 \tTraining Loss: 0.01180456 \tValidation Loss 0.01776676 \tTraining Acuuarcy 41.266% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1349 \tTraining Loss: 0.01190737 \tValidation Loss 0.01768924 \tTraining Acuuarcy 40.754% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1350 \tTraining Loss: 0.01185098 \tValidation Loss 0.01791911 \tTraining Acuuarcy 41.105% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1351 \tTraining Loss: 0.01180291 \tValidation Loss 0.01755594 \tTraining Acuuarcy 41.506% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1352 \tTraining Loss: 0.01188765 \tValidation Loss 0.01782566 \tTraining Acuuarcy 40.837% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1353 \tTraining Loss: 0.01193281 \tValidation Loss 0.01776720 \tTraining Acuuarcy 40.464% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1354 \tTraining Loss: 0.01183050 \tValidation Loss 0.01848136 \tTraining Acuuarcy 40.815% \tValidation Acuuarcy 19.309%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1355 \tTraining Loss: 0.01183190 \tValidation Loss 0.01755115 \tTraining Acuuarcy 41.389% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1356 \tTraining Loss: 0.01187910 \tValidation Loss 0.01788102 \tTraining Acuuarcy 40.793% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1357 \tTraining Loss: 0.01186325 \tValidation Loss 0.01795139 \tTraining Acuuarcy 40.653% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1358 \tTraining Loss: 0.01185692 \tValidation Loss 0.01804187 \tTraining Acuuarcy 40.871% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1359 \tTraining Loss: 0.01182422 \tValidation Loss 0.01799522 \tTraining Acuuarcy 40.703% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1360 \tTraining Loss: 0.01177266 \tValidation Loss 0.01794016 \tTraining Acuuarcy 41.974% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1361 \tTraining Loss: 0.01189074 \tValidation Loss 0.01765593 \tTraining Acuuarcy 40.196% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1362 \tTraining Loss: 0.01181006 \tValidation Loss 0.01797142 \tTraining Acuuarcy 41.411% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1363 \tTraining Loss: 0.01185537 \tValidation Loss 0.01808440 \tTraining Acuuarcy 40.976% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1364 \tTraining Loss: 0.01174861 \tValidation Loss 0.01776303 \tTraining Acuuarcy 41.227% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1365 \tTraining Loss: 0.01185247 \tValidation Loss 0.01782940 \tTraining Acuuarcy 40.937% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1366 \tTraining Loss: 0.01184123 \tValidation Loss 0.01826988 \tTraining Acuuarcy 40.453% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1367 \tTraining Loss: 0.01189493 \tValidation Loss 0.01763741 \tTraining Acuuarcy 40.531% \tValidation Acuuarcy 21.761%\n",
      "Epoch: 1368 \tTraining Loss: 0.01180720 \tValidation Loss 0.01815755 \tTraining Acuuarcy 40.754% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1369 \tTraining Loss: 0.01175854 \tValidation Loss 0.01800216 \tTraining Acuuarcy 41.394% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1370 \tTraining Loss: 0.01187136 \tValidation Loss 0.01784149 \tTraining Acuuarcy 40.976% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1371 \tTraining Loss: 0.01185042 \tValidation Loss 0.01752363 \tTraining Acuuarcy 40.809% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1372 \tTraining Loss: 0.01179449 \tValidation Loss 0.01740957 \tTraining Acuuarcy 41.116% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 1373 \tTraining Loss: 0.01181537 \tValidation Loss 0.01802784 \tTraining Acuuarcy 40.910% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1374 \tTraining Loss: 0.01178835 \tValidation Loss 0.01788996 \tTraining Acuuarcy 41.205% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1375 \tTraining Loss: 0.01174919 \tValidation Loss 0.01801548 \tTraining Acuuarcy 41.846% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1376 \tTraining Loss: 0.01181998 \tValidation Loss 0.01802776 \tTraining Acuuarcy 41.149% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1377 \tTraining Loss: 0.01179749 \tValidation Loss 0.01803517 \tTraining Acuuarcy 41.010% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1378 \tTraining Loss: 0.01172636 \tValidation Loss 0.01797519 \tTraining Acuuarcy 41.484% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1379 \tTraining Loss: 0.01180127 \tValidation Loss 0.01749097 \tTraining Acuuarcy 41.729% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1380 \tTraining Loss: 0.01183639 \tValidation Loss 0.01767052 \tTraining Acuuarcy 40.793% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1381 \tTraining Loss: 0.01177658 \tValidation Loss 0.01805984 \tTraining Acuuarcy 41.277% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1382 \tTraining Loss: 0.01182615 \tValidation Loss 0.01810199 \tTraining Acuuarcy 40.787% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1383 \tTraining Loss: 0.01179197 \tValidation Loss 0.01787610 \tTraining Acuuarcy 41.021% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1384 \tTraining Loss: 0.01183749 \tValidation Loss 0.01828786 \tTraining Acuuarcy 41.166% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1385 \tTraining Loss: 0.01189961 \tValidation Loss 0.01785985 \tTraining Acuuarcy 40.436% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1386 \tTraining Loss: 0.01190104 \tValidation Loss 0.01822914 \tTraining Acuuarcy 41.099% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1387 \tTraining Loss: 0.01178440 \tValidation Loss 0.01821341 \tTraining Acuuarcy 41.077% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1388 \tTraining Loss: 0.01179914 \tValidation Loss 0.01783602 \tTraining Acuuarcy 41.021% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1389 \tTraining Loss: 0.01192612 \tValidation Loss 0.01795425 \tTraining Acuuarcy 40.592% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1390 \tTraining Loss: 0.01184319 \tValidation Loss 0.01823177 \tTraining Acuuarcy 41.211% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1391 \tTraining Loss: 0.01177822 \tValidation Loss 0.01796641 \tTraining Acuuarcy 40.592% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1392 \tTraining Loss: 0.01172166 \tValidation Loss 0.01776709 \tTraining Acuuarcy 41.578% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 1393 \tTraining Loss: 0.01184746 \tValidation Loss 0.01805014 \tTraining Acuuarcy 40.826% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1394 \tTraining Loss: 0.01178383 \tValidation Loss 0.01791807 \tTraining Acuuarcy 41.110% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1395 \tTraining Loss: 0.01181711 \tValidation Loss 0.01788105 \tTraining Acuuarcy 41.328% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1396 \tTraining Loss: 0.01180895 \tValidation Loss 0.01755553 \tTraining Acuuarcy 41.773% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1397 \tTraining Loss: 0.01177415 \tValidation Loss 0.01787469 \tTraining Acuuarcy 41.434% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1398 \tTraining Loss: 0.01182518 \tValidation Loss 0.01789205 \tTraining Acuuarcy 41.138% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1399 \tTraining Loss: 0.01186942 \tValidation Loss 0.01766933 \tTraining Acuuarcy 40.971% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1400 \tTraining Loss: 0.01183402 \tValidation Loss 0.01826720 \tTraining Acuuarcy 41.461% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1401 \tTraining Loss: 0.01183928 \tValidation Loss 0.01799748 \tTraining Acuuarcy 41.038% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1402 \tTraining Loss: 0.01190607 \tValidation Loss 0.01803262 \tTraining Acuuarcy 40.932% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 1403 \tTraining Loss: 0.01178489 \tValidation Loss 0.01764815 \tTraining Acuuarcy 41.684% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1404 \tTraining Loss: 0.01176207 \tValidation Loss 0.01788773 \tTraining Acuuarcy 41.601% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1405 \tTraining Loss: 0.01178367 \tValidation Loss 0.01830674 \tTraining Acuuarcy 41.567% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1406 \tTraining Loss: 0.01184786 \tValidation Loss 0.01780075 \tTraining Acuuarcy 40.776% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1407 \tTraining Loss: 0.01177635 \tValidation Loss 0.01793967 \tTraining Acuuarcy 41.305% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 1408 \tTraining Loss: 0.01185579 \tValidation Loss 0.01772122 \tTraining Acuuarcy 41.071% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1409 \tTraining Loss: 0.01182012 \tValidation Loss 0.01798936 \tTraining Acuuarcy 41.099% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1410 \tTraining Loss: 0.01174124 \tValidation Loss 0.01812735 \tTraining Acuuarcy 41.712% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 1411 \tTraining Loss: 0.01184907 \tValidation Loss 0.01776666 \tTraining Acuuarcy 41.038% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1412 \tTraining Loss: 0.01174937 \tValidation Loss 0.01790835 \tTraining Acuuarcy 41.980% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1413 \tTraining Loss: 0.01186068 \tValidation Loss 0.01846977 \tTraining Acuuarcy 40.949% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 1414 \tTraining Loss: 0.01186523 \tValidation Loss 0.01832646 \tTraining Acuuarcy 40.854% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1415 \tTraining Loss: 0.01175152 \tValidation Loss 0.01819068 \tTraining Acuuarcy 41.088% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1416 \tTraining Loss: 0.01177881 \tValidation Loss 0.01785816 \tTraining Acuuarcy 41.757% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1417 \tTraining Loss: 0.01176943 \tValidation Loss 0.01784570 \tTraining Acuuarcy 41.935% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1418 \tTraining Loss: 0.01179635 \tValidation Loss 0.01798435 \tTraining Acuuarcy 41.344% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1419 \tTraining Loss: 0.01179977 \tValidation Loss 0.01797278 \tTraining Acuuarcy 41.718% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1420 \tTraining Loss: 0.01176485 \tValidation Loss 0.01827498 \tTraining Acuuarcy 41.751% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1421 \tTraining Loss: 0.01184422 \tValidation Loss 0.01773152 \tTraining Acuuarcy 41.038% \tValidation Acuuarcy 20.061%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1422 \tTraining Loss: 0.01178644 \tValidation Loss 0.01865063 \tTraining Acuuarcy 41.316% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1423 \tTraining Loss: 0.01175770 \tValidation Loss 0.01769977 \tTraining Acuuarcy 41.734% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1424 \tTraining Loss: 0.01186515 \tValidation Loss 0.01794607 \tTraining Acuuarcy 41.010% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1425 \tTraining Loss: 0.01174486 \tValidation Loss 0.01755349 \tTraining Acuuarcy 41.355% \tValidation Acuuarcy 22.680%\n",
      "Epoch: 1426 \tTraining Loss: 0.01185564 \tValidation Loss 0.01792080 \tTraining Acuuarcy 40.976% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1427 \tTraining Loss: 0.01177571 \tValidation Loss 0.01818535 \tTraining Acuuarcy 41.467% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1428 \tTraining Loss: 0.01184427 \tValidation Loss 0.01743337 \tTraining Acuuarcy 40.893% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1429 \tTraining Loss: 0.01181301 \tValidation Loss 0.01809901 \tTraining Acuuarcy 41.283% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1430 \tTraining Loss: 0.01185396 \tValidation Loss 0.01785230 \tTraining Acuuarcy 40.525% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1431 \tTraining Loss: 0.01179452 \tValidation Loss 0.01838731 \tTraining Acuuarcy 41.551% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1432 \tTraining Loss: 0.01178756 \tValidation Loss 0.01777499 \tTraining Acuuarcy 41.573% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1433 \tTraining Loss: 0.01188483 \tValidation Loss 0.01788051 \tTraining Acuuarcy 40.848% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1434 \tTraining Loss: 0.01175529 \tValidation Loss 0.01785199 \tTraining Acuuarcy 41.277% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1435 \tTraining Loss: 0.01186220 \tValidation Loss 0.01770303 \tTraining Acuuarcy 40.915% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1436 \tTraining Loss: 0.01172010 \tValidation Loss 0.01817818 \tTraining Acuuarcy 41.946% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1437 \tTraining Loss: 0.01179602 \tValidation Loss 0.01771400 \tTraining Acuuarcy 41.539% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 1438 \tTraining Loss: 0.01179578 \tValidation Loss 0.01789878 \tTraining Acuuarcy 41.004% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1439 \tTraining Loss: 0.01173792 \tValidation Loss 0.01843178 \tTraining Acuuarcy 41.495% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 1440 \tTraining Loss: 0.01188819 \tValidation Loss 0.01800455 \tTraining Acuuarcy 40.765% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1441 \tTraining Loss: 0.01178985 \tValidation Loss 0.01775302 \tTraining Acuuarcy 41.428% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 1442 \tTraining Loss: 0.01176989 \tValidation Loss 0.01793946 \tTraining Acuuarcy 41.266% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1443 \tTraining Loss: 0.01186471 \tValidation Loss 0.01758578 \tTraining Acuuarcy 40.765% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1444 \tTraining Loss: 0.01178937 \tValidation Loss 0.01810074 \tTraining Acuuarcy 41.277% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1445 \tTraining Loss: 0.01180728 \tValidation Loss 0.01778604 \tTraining Acuuarcy 40.971% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1446 \tTraining Loss: 0.01178402 \tValidation Loss 0.01784006 \tTraining Acuuarcy 41.015% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1447 \tTraining Loss: 0.01185783 \tValidation Loss 0.01799986 \tTraining Acuuarcy 40.932% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1448 \tTraining Loss: 0.01181941 \tValidation Loss 0.01805063 \tTraining Acuuarcy 41.233% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1449 \tTraining Loss: 0.01181245 \tValidation Loss 0.01805306 \tTraining Acuuarcy 40.781% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1450 \tTraining Loss: 0.01175012 \tValidation Loss 0.01776081 \tTraining Acuuarcy 41.512% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1451 \tTraining Loss: 0.01172649 \tValidation Loss 0.01864185 \tTraining Acuuarcy 41.662% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1452 \tTraining Loss: 0.01182320 \tValidation Loss 0.01790549 \tTraining Acuuarcy 40.871% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1453 \tTraining Loss: 0.01175276 \tValidation Loss 0.01838432 \tTraining Acuuarcy 41.434% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1454 \tTraining Loss: 0.01183621 \tValidation Loss 0.01752012 \tTraining Acuuarcy 40.837% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1455 \tTraining Loss: 0.01175331 \tValidation Loss 0.01804367 \tTraining Acuuarcy 41.339% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 1456 \tTraining Loss: 0.01178865 \tValidation Loss 0.01805009 \tTraining Acuuarcy 41.205% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 1457 \tTraining Loss: 0.01175337 \tValidation Loss 0.01763867 \tTraining Acuuarcy 41.952% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1458 \tTraining Loss: 0.01175073 \tValidation Loss 0.01793499 \tTraining Acuuarcy 42.125% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 1459 \tTraining Loss: 0.01177497 \tValidation Loss 0.01779467 \tTraining Acuuarcy 41.734% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1460 \tTraining Loss: 0.01171370 \tValidation Loss 0.01810269 \tTraining Acuuarcy 41.601% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1461 \tTraining Loss: 0.01185112 \tValidation Loss 0.01780281 \tTraining Acuuarcy 41.495% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1462 \tTraining Loss: 0.01182161 \tValidation Loss 0.01762808 \tTraining Acuuarcy 41.160% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1463 \tTraining Loss: 0.01173173 \tValidation Loss 0.01820582 \tTraining Acuuarcy 41.523% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1464 \tTraining Loss: 0.01172786 \tValidation Loss 0.01832473 \tTraining Acuuarcy 41.879% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1465 \tTraining Loss: 0.01178083 \tValidation Loss 0.01792858 \tTraining Acuuarcy 40.976% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1466 \tTraining Loss: 0.01169860 \tValidation Loss 0.01833016 \tTraining Acuuarcy 42.024% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1467 \tTraining Loss: 0.01182521 \tValidation Loss 0.01799996 \tTraining Acuuarcy 40.937% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1468 \tTraining Loss: 0.01176004 \tValidation Loss 0.01816018 \tTraining Acuuarcy 41.467% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1469 \tTraining Loss: 0.01177377 \tValidation Loss 0.01805885 \tTraining Acuuarcy 41.434% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 1470 \tTraining Loss: 0.01178429 \tValidation Loss 0.01782997 \tTraining Acuuarcy 41.417% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 1471 \tTraining Loss: 0.01178371 \tValidation Loss 0.01813779 \tTraining Acuuarcy 41.094% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1472 \tTraining Loss: 0.01183226 \tValidation Loss 0.01801649 \tTraining Acuuarcy 40.988% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1473 \tTraining Loss: 0.01178375 \tValidation Loss 0.01797224 \tTraining Acuuarcy 40.636% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1474 \tTraining Loss: 0.01174433 \tValidation Loss 0.01794769 \tTraining Acuuarcy 41.623% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 1475 \tTraining Loss: 0.01172552 \tValidation Loss 0.01822990 \tTraining Acuuarcy 41.980% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 1476 \tTraining Loss: 0.01175112 \tValidation Loss 0.01786747 \tTraining Acuuarcy 41.138% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1477 \tTraining Loss: 0.01173283 \tValidation Loss 0.01822898 \tTraining Acuuarcy 41.606% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1478 \tTraining Loss: 0.01168117 \tValidation Loss 0.01812427 \tTraining Acuuarcy 41.651% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1479 \tTraining Loss: 0.01190230 \tValidation Loss 0.01759774 \tTraining Acuuarcy 40.324% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1480 \tTraining Loss: 0.01173914 \tValidation Loss 0.01816776 \tTraining Acuuarcy 41.629% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1481 \tTraining Loss: 0.01173817 \tValidation Loss 0.01816427 \tTraining Acuuarcy 41.807% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1482 \tTraining Loss: 0.01180997 \tValidation Loss 0.01771313 \tTraining Acuuarcy 40.893% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 1483 \tTraining Loss: 0.01188023 \tValidation Loss 0.01803229 \tTraining Acuuarcy 40.770% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1484 \tTraining Loss: 0.01183354 \tValidation Loss 0.01758088 \tTraining Acuuarcy 41.105% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1485 \tTraining Loss: 0.01178589 \tValidation Loss 0.01828179 \tTraining Acuuarcy 41.194% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1486 \tTraining Loss: 0.01174891 \tValidation Loss 0.01811970 \tTraining Acuuarcy 41.679% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1487 \tTraining Loss: 0.01178146 \tValidation Loss 0.01767877 \tTraining Acuuarcy 41.517% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1488 \tTraining Loss: 0.01187111 \tValidation Loss 0.01793474 \tTraining Acuuarcy 40.921% \tValidation Acuuarcy 19.030%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1489 \tTraining Loss: 0.01175484 \tValidation Loss 0.01747330 \tTraining Acuuarcy 41.718% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1490 \tTraining Loss: 0.01185560 \tValidation Loss 0.01791313 \tTraining Acuuarcy 40.687% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1491 \tTraining Loss: 0.01174020 \tValidation Loss 0.01773126 \tTraining Acuuarcy 41.567% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1492 \tTraining Loss: 0.01178098 \tValidation Loss 0.01829863 \tTraining Acuuarcy 41.489% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1493 \tTraining Loss: 0.01179078 \tValidation Loss 0.01785589 \tTraining Acuuarcy 41.461% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1494 \tTraining Loss: 0.01183760 \tValidation Loss 0.01828304 \tTraining Acuuarcy 40.854% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1495 \tTraining Loss: 0.01178056 \tValidation Loss 0.01807611 \tTraining Acuuarcy 41.523% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1496 \tTraining Loss: 0.01164656 \tValidation Loss 0.01789380 \tTraining Acuuarcy 42.164% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1497 \tTraining Loss: 0.01177349 \tValidation Loss 0.01798816 \tTraining Acuuarcy 41.695% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1498 \tTraining Loss: 0.01182458 \tValidation Loss 0.01807881 \tTraining Acuuarcy 40.988% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 1499 \tTraining Loss: 0.01179686 \tValidation Loss 0.01780957 \tTraining Acuuarcy 41.277% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1500 \tTraining Loss: 0.01176551 \tValidation Loss 0.01814475 \tTraining Acuuarcy 41.283% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1501 \tTraining Loss: 0.01185172 \tValidation Loss 0.01805452 \tTraining Acuuarcy 40.653% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1502 \tTraining Loss: 0.01179808 \tValidation Loss 0.01756639 \tTraining Acuuarcy 41.038% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1503 \tTraining Loss: 0.01182080 \tValidation Loss 0.01847259 \tTraining Acuuarcy 40.809% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1504 \tTraining Loss: 0.01174038 \tValidation Loss 0.01789458 \tTraining Acuuarcy 41.422% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1505 \tTraining Loss: 0.01175805 \tValidation Loss 0.01813761 \tTraining Acuuarcy 41.344% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1506 \tTraining Loss: 0.01180624 \tValidation Loss 0.01853857 \tTraining Acuuarcy 41.205% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 1507 \tTraining Loss: 0.01174818 \tValidation Loss 0.01788897 \tTraining Acuuarcy 41.473% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1508 \tTraining Loss: 0.01182506 \tValidation Loss 0.01803970 \tTraining Acuuarcy 41.467% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1509 \tTraining Loss: 0.01188073 \tValidation Loss 0.01796257 \tTraining Acuuarcy 40.837% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1510 \tTraining Loss: 0.01175749 \tValidation Loss 0.01796087 \tTraining Acuuarcy 41.796% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 1511 \tTraining Loss: 0.01180665 \tValidation Loss 0.01825237 \tTraining Acuuarcy 41.077% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1512 \tTraining Loss: 0.01174710 \tValidation Loss 0.01847732 \tTraining Acuuarcy 41.238% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1513 \tTraining Loss: 0.01180344 \tValidation Loss 0.01802212 \tTraining Acuuarcy 41.127% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1514 \tTraining Loss: 0.01169274 \tValidation Loss 0.01782770 \tTraining Acuuarcy 41.818% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1515 \tTraining Loss: 0.01177143 \tValidation Loss 0.01824362 \tTraining Acuuarcy 41.227% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1516 \tTraining Loss: 0.01172803 \tValidation Loss 0.01785942 \tTraining Acuuarcy 41.350% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1517 \tTraining Loss: 0.01177224 \tValidation Loss 0.01816204 \tTraining Acuuarcy 41.289% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1518 \tTraining Loss: 0.01173519 \tValidation Loss 0.01784727 \tTraining Acuuarcy 41.367% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1519 \tTraining Loss: 0.01179457 \tValidation Loss 0.01799634 \tTraining Acuuarcy 41.316% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1520 \tTraining Loss: 0.01182272 \tValidation Loss 0.01799830 \tTraining Acuuarcy 41.467% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1521 \tTraining Loss: 0.01169593 \tValidation Loss 0.01779943 \tTraining Acuuarcy 42.058% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1522 \tTraining Loss: 0.01177084 \tValidation Loss 0.01780391 \tTraining Acuuarcy 41.818% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1523 \tTraining Loss: 0.01183793 \tValidation Loss 0.01778075 \tTraining Acuuarcy 41.233% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1524 \tTraining Loss: 0.01173688 \tValidation Loss 0.01803199 \tTraining Acuuarcy 41.367% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1525 \tTraining Loss: 0.01180628 \tValidation Loss 0.01798229 \tTraining Acuuarcy 41.478% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1526 \tTraining Loss: 0.01165131 \tValidation Loss 0.01816683 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1527 \tTraining Loss: 0.01173450 \tValidation Loss 0.01784474 \tTraining Acuuarcy 41.907% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1528 \tTraining Loss: 0.01168521 \tValidation Loss 0.01781182 \tTraining Acuuarcy 42.158% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1529 \tTraining Loss: 0.01178367 \tValidation Loss 0.01816929 \tTraining Acuuarcy 41.400% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1530 \tTraining Loss: 0.01181884 \tValidation Loss 0.01816106 \tTraining Acuuarcy 40.904% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1531 \tTraining Loss: 0.01178093 \tValidation Loss 0.01811677 \tTraining Acuuarcy 41.355% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1532 \tTraining Loss: 0.01168132 \tValidation Loss 0.01772202 \tTraining Acuuarcy 42.041% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1533 \tTraining Loss: 0.01176613 \tValidation Loss 0.01844399 \tTraining Acuuarcy 41.762% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1534 \tTraining Loss: 0.01175214 \tValidation Loss 0.01796525 \tTraining Acuuarcy 41.573% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 1535 \tTraining Loss: 0.01172427 \tValidation Loss 0.01819943 \tTraining Acuuarcy 41.740% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1536 \tTraining Loss: 0.01176682 \tValidation Loss 0.01782841 \tTraining Acuuarcy 41.584% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1537 \tTraining Loss: 0.01171340 \tValidation Loss 0.01829395 \tTraining Acuuarcy 41.082% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1538 \tTraining Loss: 0.01175541 \tValidation Loss 0.01789609 \tTraining Acuuarcy 40.943% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 1539 \tTraining Loss: 0.01182479 \tValidation Loss 0.01781143 \tTraining Acuuarcy 41.015% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1540 \tTraining Loss: 0.01183608 \tValidation Loss 0.01794776 \tTraining Acuuarcy 40.709% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1541 \tTraining Loss: 0.01177670 \tValidation Loss 0.01779446 \tTraining Acuuarcy 41.751% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1542 \tTraining Loss: 0.01175947 \tValidation Loss 0.01803370 \tTraining Acuuarcy 41.456% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1543 \tTraining Loss: 0.01169673 \tValidation Loss 0.01813802 \tTraining Acuuarcy 41.422% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1544 \tTraining Loss: 0.01179755 \tValidation Loss 0.01765274 \tTraining Acuuarcy 41.055% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 1545 \tTraining Loss: 0.01173735 \tValidation Loss 0.01808792 \tTraining Acuuarcy 41.361% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1546 \tTraining Loss: 0.01175233 \tValidation Loss 0.01816489 \tTraining Acuuarcy 41.684% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1547 \tTraining Loss: 0.01183933 \tValidation Loss 0.01798472 \tTraining Acuuarcy 40.820% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 1548 \tTraining Loss: 0.01173384 \tValidation Loss 0.01801187 \tTraining Acuuarcy 41.539% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1549 \tTraining Loss: 0.01171939 \tValidation Loss 0.01801077 \tTraining Acuuarcy 41.584% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1550 \tTraining Loss: 0.01172410 \tValidation Loss 0.01787871 \tTraining Acuuarcy 41.640% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1551 \tTraining Loss: 0.01177177 \tValidation Loss 0.01823994 \tTraining Acuuarcy 41.857% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1552 \tTraining Loss: 0.01176720 \tValidation Loss 0.01795117 \tTraining Acuuarcy 41.885% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1553 \tTraining Loss: 0.01175230 \tValidation Loss 0.01831937 \tTraining Acuuarcy 41.584% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1554 \tTraining Loss: 0.01183908 \tValidation Loss 0.01802876 \tTraining Acuuarcy 40.943% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1555 \tTraining Loss: 0.01181054 \tValidation Loss 0.01794238 \tTraining Acuuarcy 41.406% \tValidation Acuuarcy 20.061%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1556 \tTraining Loss: 0.01171515 \tValidation Loss 0.01838042 \tTraining Acuuarcy 41.172% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1557 \tTraining Loss: 0.01170169 \tValidation Loss 0.01841410 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1558 \tTraining Loss: 0.01174392 \tValidation Loss 0.01768071 \tTraining Acuuarcy 41.785% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1559 \tTraining Loss: 0.01175446 \tValidation Loss 0.01816262 \tTraining Acuuarcy 41.673% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1560 \tTraining Loss: 0.01176734 \tValidation Loss 0.01803696 \tTraining Acuuarcy 41.590% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1561 \tTraining Loss: 0.01174506 \tValidation Loss 0.01829294 \tTraining Acuuarcy 41.773% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 1562 \tTraining Loss: 0.01173663 \tValidation Loss 0.01862254 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1563 \tTraining Loss: 0.01177436 \tValidation Loss 0.01789083 \tTraining Acuuarcy 41.601% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1564 \tTraining Loss: 0.01168135 \tValidation Loss 0.01788809 \tTraining Acuuarcy 41.857% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1565 \tTraining Loss: 0.01180362 \tValidation Loss 0.01789159 \tTraining Acuuarcy 41.116% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1566 \tTraining Loss: 0.01169373 \tValidation Loss 0.01821816 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1567 \tTraining Loss: 0.01179568 \tValidation Loss 0.01787586 \tTraining Acuuarcy 41.211% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1568 \tTraining Loss: 0.01174261 \tValidation Loss 0.01814129 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1569 \tTraining Loss: 0.01174908 \tValidation Loss 0.01774570 \tTraining Acuuarcy 41.461% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1570 \tTraining Loss: 0.01176932 \tValidation Loss 0.01794741 \tTraining Acuuarcy 41.439% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1571 \tTraining Loss: 0.01171058 \tValidation Loss 0.01816283 \tTraining Acuuarcy 41.779% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1572 \tTraining Loss: 0.01176588 \tValidation Loss 0.01836366 \tTraining Acuuarcy 41.127% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1573 \tTraining Loss: 0.01178560 \tValidation Loss 0.01766709 \tTraining Acuuarcy 41.679% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1574 \tTraining Loss: 0.01173780 \tValidation Loss 0.01827265 \tTraining Acuuarcy 41.957% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1575 \tTraining Loss: 0.01170609 \tValidation Loss 0.01784613 \tTraining Acuuarcy 42.080% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 1576 \tTraining Loss: 0.01176568 \tValidation Loss 0.01786361 \tTraining Acuuarcy 41.874% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1577 \tTraining Loss: 0.01175402 \tValidation Loss 0.01841169 \tTraining Acuuarcy 41.690% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1578 \tTraining Loss: 0.01177388 \tValidation Loss 0.01798879 \tTraining Acuuarcy 41.684% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1579 \tTraining Loss: 0.01184088 \tValidation Loss 0.01797043 \tTraining Acuuarcy 40.965% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 1580 \tTraining Loss: 0.01177001 \tValidation Loss 0.01804378 \tTraining Acuuarcy 41.372% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1581 \tTraining Loss: 0.01166563 \tValidation Loss 0.01801845 \tTraining Acuuarcy 42.125% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1582 \tTraining Loss: 0.01173336 \tValidation Loss 0.01792559 \tTraining Acuuarcy 41.233% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1583 \tTraining Loss: 0.01177569 \tValidation Loss 0.01762031 \tTraining Acuuarcy 41.801% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1584 \tTraining Loss: 0.01174494 \tValidation Loss 0.01820807 \tTraining Acuuarcy 41.578% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1585 \tTraining Loss: 0.01172990 \tValidation Loss 0.01809394 \tTraining Acuuarcy 41.623% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1586 \tTraining Loss: 0.01174704 \tValidation Loss 0.01845848 \tTraining Acuuarcy 41.562% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1587 \tTraining Loss: 0.01181285 \tValidation Loss 0.01767544 \tTraining Acuuarcy 40.715% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 1588 \tTraining Loss: 0.01183318 \tValidation Loss 0.01786754 \tTraining Acuuarcy 41.333% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1589 \tTraining Loss: 0.01181299 \tValidation Loss 0.01790921 \tTraining Acuuarcy 41.567% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1590 \tTraining Loss: 0.01180608 \tValidation Loss 0.01795469 \tTraining Acuuarcy 41.244% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1591 \tTraining Loss: 0.01170647 \tValidation Loss 0.01811126 \tTraining Acuuarcy 42.086% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1592 \tTraining Loss: 0.01176183 \tValidation Loss 0.01824127 \tTraining Acuuarcy 41.233% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1593 \tTraining Loss: 0.01178341 \tValidation Loss 0.01829395 \tTraining Acuuarcy 41.350% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1594 \tTraining Loss: 0.01164641 \tValidation Loss 0.01833999 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1595 \tTraining Loss: 0.01178456 \tValidation Loss 0.01837031 \tTraining Acuuarcy 41.523% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1596 \tTraining Loss: 0.01178091 \tValidation Loss 0.01842893 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1597 \tTraining Loss: 0.01173748 \tValidation Loss 0.01796318 \tTraining Acuuarcy 41.723% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1598 \tTraining Loss: 0.01176408 \tValidation Loss 0.01821426 \tTraining Acuuarcy 41.740% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1599 \tTraining Loss: 0.01171054 \tValidation Loss 0.01823425 \tTraining Acuuarcy 41.673% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1600 \tTraining Loss: 0.01175490 \tValidation Loss 0.01811732 \tTraining Acuuarcy 41.729% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 1601 \tTraining Loss: 0.01172155 \tValidation Loss 0.01827121 \tTraining Acuuarcy 42.052% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1602 \tTraining Loss: 0.01170408 \tValidation Loss 0.01831074 \tTraining Acuuarcy 41.746% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 1603 \tTraining Loss: 0.01166663 \tValidation Loss 0.01810051 \tTraining Acuuarcy 41.980% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1604 \tTraining Loss: 0.01176069 \tValidation Loss 0.01838161 \tTraining Acuuarcy 41.266% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1605 \tTraining Loss: 0.01171500 \tValidation Loss 0.01834238 \tTraining Acuuarcy 41.829% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1606 \tTraining Loss: 0.01176704 \tValidation Loss 0.01753255 \tTraining Acuuarcy 41.261% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1607 \tTraining Loss: 0.01172205 \tValidation Loss 0.01877155 \tTraining Acuuarcy 41.824% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1608 \tTraining Loss: 0.01171800 \tValidation Loss 0.01812287 \tTraining Acuuarcy 41.255% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1609 \tTraining Loss: 0.01184609 \tValidation Loss 0.01807360 \tTraining Acuuarcy 41.116% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1610 \tTraining Loss: 0.01168981 \tValidation Loss 0.01871738 \tTraining Acuuarcy 41.394% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1611 \tTraining Loss: 0.01177549 \tValidation Loss 0.01771856 \tTraining Acuuarcy 41.244% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1612 \tTraining Loss: 0.01168330 \tValidation Loss 0.01801142 \tTraining Acuuarcy 41.868% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1613 \tTraining Loss: 0.01182590 \tValidation Loss 0.01775951 \tTraining Acuuarcy 41.250% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1614 \tTraining Loss: 0.01176391 \tValidation Loss 0.01827097 \tTraining Acuuarcy 41.389% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1615 \tTraining Loss: 0.01172300 \tValidation Loss 0.01784290 \tTraining Acuuarcy 41.506% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1616 \tTraining Loss: 0.01169834 \tValidation Loss 0.01782907 \tTraining Acuuarcy 41.300% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1617 \tTraining Loss: 0.01179536 \tValidation Loss 0.01819937 \tTraining Acuuarcy 40.904% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1618 \tTraining Loss: 0.01164746 \tValidation Loss 0.01839675 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1619 \tTraining Loss: 0.01167226 \tValidation Loss 0.01848550 \tTraining Acuuarcy 41.946% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1620 \tTraining Loss: 0.01183917 \tValidation Loss 0.01819050 \tTraining Acuuarcy 41.199% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1621 \tTraining Loss: 0.01169311 \tValidation Loss 0.01803037 \tTraining Acuuarcy 41.740% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 1622 \tTraining Loss: 0.01171482 \tValidation Loss 0.01773200 \tTraining Acuuarcy 42.080% \tValidation Acuuarcy 20.368%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1623 \tTraining Loss: 0.01176857 \tValidation Loss 0.01777558 \tTraining Acuuarcy 41.322% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1624 \tTraining Loss: 0.01173075 \tValidation Loss 0.01814846 \tTraining Acuuarcy 41.985% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1625 \tTraining Loss: 0.01168589 \tValidation Loss 0.01844699 \tTraining Acuuarcy 41.835% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1626 \tTraining Loss: 0.01172639 \tValidation Loss 0.01829682 \tTraining Acuuarcy 41.952% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1627 \tTraining Loss: 0.01177409 \tValidation Loss 0.01805670 \tTraining Acuuarcy 41.790% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1628 \tTraining Loss: 0.01174562 \tValidation Loss 0.01789379 \tTraining Acuuarcy 41.996% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1629 \tTraining Loss: 0.01170698 \tValidation Loss 0.01811985 \tTraining Acuuarcy 41.896% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1630 \tTraining Loss: 0.01186274 \tValidation Loss 0.01788086 \tTraining Acuuarcy 41.467% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1631 \tTraining Loss: 0.01167636 \tValidation Loss 0.01788613 \tTraining Acuuarcy 41.734% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1632 \tTraining Loss: 0.01173512 \tValidation Loss 0.01811671 \tTraining Acuuarcy 42.041% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1633 \tTraining Loss: 0.01175802 \tValidation Loss 0.01847880 \tTraining Acuuarcy 41.411% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1634 \tTraining Loss: 0.01183710 \tValidation Loss 0.01814127 \tTraining Acuuarcy 41.400% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1635 \tTraining Loss: 0.01177766 \tValidation Loss 0.01825691 \tTraining Acuuarcy 41.500% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1636 \tTraining Loss: 0.01177104 \tValidation Loss 0.01801541 \tTraining Acuuarcy 41.829% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1637 \tTraining Loss: 0.01176303 \tValidation Loss 0.01831397 \tTraining Acuuarcy 41.523% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1638 \tTraining Loss: 0.01176380 \tValidation Loss 0.01813907 \tTraining Acuuarcy 41.127% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1639 \tTraining Loss: 0.01170157 \tValidation Loss 0.01785425 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1640 \tTraining Loss: 0.01182092 \tValidation Loss 0.01801426 \tTraining Acuuarcy 41.489% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1641 \tTraining Loss: 0.01171513 \tValidation Loss 0.01846747 \tTraining Acuuarcy 41.729% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1642 \tTraining Loss: 0.01165830 \tValidation Loss 0.01856257 \tTraining Acuuarcy 41.835% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1643 \tTraining Loss: 0.01171291 \tValidation Loss 0.01854684 \tTraining Acuuarcy 41.584% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1644 \tTraining Loss: 0.01174351 \tValidation Loss 0.01795012 \tTraining Acuuarcy 41.383% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 1645 \tTraining Loss: 0.01176701 \tValidation Loss 0.01832720 \tTraining Acuuarcy 41.690% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1646 \tTraining Loss: 0.01177188 \tValidation Loss 0.01781615 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1647 \tTraining Loss: 0.01170561 \tValidation Loss 0.01792956 \tTraining Acuuarcy 41.751% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1648 \tTraining Loss: 0.01180836 \tValidation Loss 0.01825795 \tTraining Acuuarcy 41.244% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1649 \tTraining Loss: 0.01168903 \tValidation Loss 0.01789157 \tTraining Acuuarcy 41.807% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1650 \tTraining Loss: 0.01177363 \tValidation Loss 0.01747896 \tTraining Acuuarcy 41.300% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1651 \tTraining Loss: 0.01170754 \tValidation Loss 0.01825907 \tTraining Acuuarcy 41.996% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1652 \tTraining Loss: 0.01172726 \tValidation Loss 0.01807543 \tTraining Acuuarcy 41.617% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1653 \tTraining Loss: 0.01178973 \tValidation Loss 0.01802757 \tTraining Acuuarcy 40.988% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 1654 \tTraining Loss: 0.01173466 \tValidation Loss 0.01768326 \tTraining Acuuarcy 41.684% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1655 \tTraining Loss: 0.01177958 \tValidation Loss 0.01772738 \tTraining Acuuarcy 41.796% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1656 \tTraining Loss: 0.01164536 \tValidation Loss 0.01807767 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1657 \tTraining Loss: 0.01185382 \tValidation Loss 0.01826602 \tTraining Acuuarcy 40.558% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1658 \tTraining Loss: 0.01175543 \tValidation Loss 0.01806432 \tTraining Acuuarcy 41.584% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1659 \tTraining Loss: 0.01173507 \tValidation Loss 0.01782853 \tTraining Acuuarcy 41.885% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1660 \tTraining Loss: 0.01178860 \tValidation Loss 0.01774026 \tTraining Acuuarcy 41.344% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1661 \tTraining Loss: 0.01169751 \tValidation Loss 0.01792009 \tTraining Acuuarcy 41.461% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1662 \tTraining Loss: 0.01174966 \tValidation Loss 0.01813730 \tTraining Acuuarcy 41.595% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1663 \tTraining Loss: 0.01169474 \tValidation Loss 0.01840641 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1664 \tTraining Loss: 0.01172608 \tValidation Loss 0.01811976 \tTraining Acuuarcy 41.796% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1665 \tTraining Loss: 0.01175893 \tValidation Loss 0.01842449 \tTraining Acuuarcy 41.606% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1666 \tTraining Loss: 0.01176351 \tValidation Loss 0.01787122 \tTraining Acuuarcy 41.651% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1667 \tTraining Loss: 0.01178805 \tValidation Loss 0.01796124 \tTraining Acuuarcy 40.954% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1668 \tTraining Loss: 0.01174014 \tValidation Loss 0.01826138 \tTraining Acuuarcy 41.328% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1669 \tTraining Loss: 0.01170100 \tValidation Loss 0.01787477 \tTraining Acuuarcy 41.846% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 1670 \tTraining Loss: 0.01170556 \tValidation Loss 0.01820622 \tTraining Acuuarcy 41.668% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1671 \tTraining Loss: 0.01168973 \tValidation Loss 0.01837612 \tTraining Acuuarcy 42.119% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1672 \tTraining Loss: 0.01173675 \tValidation Loss 0.01828268 \tTraining Acuuarcy 41.634% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1673 \tTraining Loss: 0.01170624 \tValidation Loss 0.01798682 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1674 \tTraining Loss: 0.01171373 \tValidation Loss 0.01851908 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1675 \tTraining Loss: 0.01174197 \tValidation Loss 0.01808420 \tTraining Acuuarcy 42.052% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1676 \tTraining Loss: 0.01175189 \tValidation Loss 0.01874366 \tTraining Acuuarcy 41.489% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1677 \tTraining Loss: 0.01174728 \tValidation Loss 0.01777486 \tTraining Acuuarcy 41.729% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1678 \tTraining Loss: 0.01179413 \tValidation Loss 0.01804918 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1679 \tTraining Loss: 0.01168868 \tValidation Loss 0.01798952 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1680 \tTraining Loss: 0.01174464 \tValidation Loss 0.01788702 \tTraining Acuuarcy 41.578% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1681 \tTraining Loss: 0.01177004 \tValidation Loss 0.01772798 \tTraining Acuuarcy 41.283% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1682 \tTraining Loss: 0.01170734 \tValidation Loss 0.01788520 \tTraining Acuuarcy 41.902% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1683 \tTraining Loss: 0.01174155 \tValidation Loss 0.01807097 \tTraining Acuuarcy 41.450% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1684 \tTraining Loss: 0.01178168 \tValidation Loss 0.01789435 \tTraining Acuuarcy 41.417% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1685 \tTraining Loss: 0.01168587 \tValidation Loss 0.01900207 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1686 \tTraining Loss: 0.01166015 \tValidation Loss 0.01833721 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1687 \tTraining Loss: 0.01165882 \tValidation Loss 0.01799043 \tTraining Acuuarcy 41.924% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1688 \tTraining Loss: 0.01175904 \tValidation Loss 0.01897196 \tTraining Acuuarcy 41.506% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 1689 \tTraining Loss: 0.01166773 \tValidation Loss 0.01821010 \tTraining Acuuarcy 41.740% \tValidation Acuuarcy 20.591%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1690 \tTraining Loss: 0.01181540 \tValidation Loss 0.01786708 \tTraining Acuuarcy 41.534% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1691 \tTraining Loss: 0.01167808 \tValidation Loss 0.01803456 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1692 \tTraining Loss: 0.01176755 \tValidation Loss 0.01785169 \tTraining Acuuarcy 41.355% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 1693 \tTraining Loss: 0.01177135 \tValidation Loss 0.01795464 \tTraining Acuuarcy 41.712% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1694 \tTraining Loss: 0.01178755 \tValidation Loss 0.01853308 \tTraining Acuuarcy 41.478% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1695 \tTraining Loss: 0.01176492 \tValidation Loss 0.01812339 \tTraining Acuuarcy 41.389% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 1696 \tTraining Loss: 0.01184432 \tValidation Loss 0.01760746 \tTraining Acuuarcy 41.199% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1697 \tTraining Loss: 0.01168011 \tValidation Loss 0.01840398 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1698 \tTraining Loss: 0.01166840 \tValidation Loss 0.01845343 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1699 \tTraining Loss: 0.01171089 \tValidation Loss 0.01826494 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1700 \tTraining Loss: 0.01171135 \tValidation Loss 0.01775105 \tTraining Acuuarcy 41.740% \tValidation Acuuarcy 21.845%\n",
      "Epoch: 1701 \tTraining Loss: 0.01179029 \tValidation Loss 0.01824716 \tTraining Acuuarcy 41.406% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1702 \tTraining Loss: 0.01176930 \tValidation Loss 0.01823886 \tTraining Acuuarcy 41.857% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1703 \tTraining Loss: 0.01174192 \tValidation Loss 0.01755736 \tTraining Acuuarcy 41.350% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1704 \tTraining Loss: 0.01173889 \tValidation Loss 0.01812706 \tTraining Acuuarcy 41.428% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1705 \tTraining Loss: 0.01168178 \tValidation Loss 0.01803521 \tTraining Acuuarcy 42.448% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1706 \tTraining Loss: 0.01179901 \tValidation Loss 0.01810834 \tTraining Acuuarcy 41.255% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1707 \tTraining Loss: 0.01176485 \tValidation Loss 0.01787137 \tTraining Acuuarcy 41.868% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1708 \tTraining Loss: 0.01175980 \tValidation Loss 0.01790522 \tTraining Acuuarcy 41.785% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1709 \tTraining Loss: 0.01173153 \tValidation Loss 0.01829622 \tTraining Acuuarcy 41.757% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1710 \tTraining Loss: 0.01179869 \tValidation Loss 0.01816392 \tTraining Acuuarcy 41.066% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1711 \tTraining Loss: 0.01170989 \tValidation Loss 0.01814579 \tTraining Acuuarcy 41.840% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1712 \tTraining Loss: 0.01173756 \tValidation Loss 0.01849417 \tTraining Acuuarcy 41.434% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1713 \tTraining Loss: 0.01173631 \tValidation Loss 0.01862164 \tTraining Acuuarcy 41.924% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1714 \tTraining Loss: 0.01170528 \tValidation Loss 0.01786454 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1715 \tTraining Loss: 0.01175873 \tValidation Loss 0.01769205 \tTraining Acuuarcy 41.517% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1716 \tTraining Loss: 0.01171282 \tValidation Loss 0.01768984 \tTraining Acuuarcy 41.634% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 1717 \tTraining Loss: 0.01173761 \tValidation Loss 0.01804029 \tTraining Acuuarcy 42.052% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1718 \tTraining Loss: 0.01166157 \tValidation Loss 0.01862457 \tTraining Acuuarcy 42.448% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1719 \tTraining Loss: 0.01176288 \tValidation Loss 0.01824188 \tTraining Acuuarcy 41.238% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1720 \tTraining Loss: 0.01165761 \tValidation Loss 0.01822699 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1721 \tTraining Loss: 0.01167004 \tValidation Loss 0.01850227 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1722 \tTraining Loss: 0.01176010 \tValidation Loss 0.01828444 \tTraining Acuuarcy 41.172% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1723 \tTraining Loss: 0.01158530 \tValidation Loss 0.01828558 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1724 \tTraining Loss: 0.01166884 \tValidation Loss 0.01817501 \tTraining Acuuarcy 42.309% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1725 \tTraining Loss: 0.01178053 \tValidation Loss 0.01779264 \tTraining Acuuarcy 41.818% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1726 \tTraining Loss: 0.01169913 \tValidation Loss 0.01818313 \tTraining Acuuarcy 41.762% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1727 \tTraining Loss: 0.01172245 \tValidation Loss 0.01825948 \tTraining Acuuarcy 41.623% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1728 \tTraining Loss: 0.01178055 \tValidation Loss 0.01828963 \tTraining Acuuarcy 40.848% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1729 \tTraining Loss: 0.01170616 \tValidation Loss 0.01786845 \tTraining Acuuarcy 42.086% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1730 \tTraining Loss: 0.01173201 \tValidation Loss 0.01815087 \tTraining Acuuarcy 41.768% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1731 \tTraining Loss: 0.01168659 \tValidation Loss 0.01823600 \tTraining Acuuarcy 42.041% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 1732 \tTraining Loss: 0.01170163 \tValidation Loss 0.01788326 \tTraining Acuuarcy 41.757% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1733 \tTraining Loss: 0.01168880 \tValidation Loss 0.01799416 \tTraining Acuuarcy 41.734% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1734 \tTraining Loss: 0.01172057 \tValidation Loss 0.01809092 \tTraining Acuuarcy 41.790% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1735 \tTraining Loss: 0.01171515 \tValidation Loss 0.01856075 \tTraining Acuuarcy 41.634% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1736 \tTraining Loss: 0.01173900 \tValidation Loss 0.01827534 \tTraining Acuuarcy 41.762% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1737 \tTraining Loss: 0.01166066 \tValidation Loss 0.01839269 \tTraining Acuuarcy 42.052% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1738 \tTraining Loss: 0.01172289 \tValidation Loss 0.01792882 \tTraining Acuuarcy 41.885% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1739 \tTraining Loss: 0.01172038 \tValidation Loss 0.01774280 \tTraining Acuuarcy 41.874% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 1740 \tTraining Loss: 0.01169246 \tValidation Loss 0.01868586 \tTraining Acuuarcy 41.629% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1741 \tTraining Loss: 0.01171797 \tValidation Loss 0.01818026 \tTraining Acuuarcy 41.824% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1742 \tTraining Loss: 0.01168867 \tValidation Loss 0.01847628 \tTraining Acuuarcy 41.907% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1743 \tTraining Loss: 0.01169481 \tValidation Loss 0.01825858 \tTraining Acuuarcy 41.746% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1744 \tTraining Loss: 0.01174877 \tValidation Loss 0.01839109 \tTraining Acuuarcy 41.590% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1745 \tTraining Loss: 0.01168818 \tValidation Loss 0.01856248 \tTraining Acuuarcy 42.180% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1746 \tTraining Loss: 0.01178808 \tValidation Loss 0.01784465 \tTraining Acuuarcy 41.506% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1747 \tTraining Loss: 0.01172753 \tValidation Loss 0.01812691 \tTraining Acuuarcy 41.840% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1748 \tTraining Loss: 0.01178735 \tValidation Loss 0.01776608 \tTraining Acuuarcy 41.133% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1749 \tTraining Loss: 0.01172470 \tValidation Loss 0.01790358 \tTraining Acuuarcy 41.539% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1750 \tTraining Loss: 0.01169897 \tValidation Loss 0.01785705 \tTraining Acuuarcy 42.231% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1751 \tTraining Loss: 0.01171943 \tValidation Loss 0.01854593 \tTraining Acuuarcy 41.813% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1752 \tTraining Loss: 0.01171512 \tValidation Loss 0.01824376 \tTraining Acuuarcy 41.868% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1753 \tTraining Loss: 0.01173818 \tValidation Loss 0.01787334 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1754 \tTraining Loss: 0.01168209 \tValidation Loss 0.01829265 \tTraining Acuuarcy 41.690% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1755 \tTraining Loss: 0.01167053 \tValidation Loss 0.01880601 \tTraining Acuuarcy 42.281% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1756 \tTraining Loss: 0.01177283 \tValidation Loss 0.01810918 \tTraining Acuuarcy 41.762% \tValidation Acuuarcy 20.396%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1757 \tTraining Loss: 0.01171895 \tValidation Loss 0.01794019 \tTraining Acuuarcy 41.980% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1758 \tTraining Loss: 0.01170024 \tValidation Loss 0.01824410 \tTraining Acuuarcy 41.918% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1759 \tTraining Loss: 0.01167948 \tValidation Loss 0.01812774 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1760 \tTraining Loss: 0.01165343 \tValidation Loss 0.01833712 \tTraining Acuuarcy 42.364% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1761 \tTraining Loss: 0.01180340 \tValidation Loss 0.01782813 \tTraining Acuuarcy 41.606% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 1762 \tTraining Loss: 0.01173107 \tValidation Loss 0.01793054 \tTraining Acuuarcy 41.957% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1763 \tTraining Loss: 0.01169435 \tValidation Loss 0.01826281 \tTraining Acuuarcy 41.629% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1764 \tTraining Loss: 0.01182005 \tValidation Loss 0.01782921 \tTraining Acuuarcy 41.038% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1765 \tTraining Loss: 0.01163773 \tValidation Loss 0.01809598 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1766 \tTraining Loss: 0.01170121 \tValidation Loss 0.01804766 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1767 \tTraining Loss: 0.01165273 \tValidation Loss 0.01821863 \tTraining Acuuarcy 42.041% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1768 \tTraining Loss: 0.01167980 \tValidation Loss 0.01836158 \tTraining Acuuarcy 42.074% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1769 \tTraining Loss: 0.01167571 \tValidation Loss 0.01838824 \tTraining Acuuarcy 41.729% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1770 \tTraining Loss: 0.01160964 \tValidation Loss 0.01830043 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1771 \tTraining Loss: 0.01180122 \tValidation Loss 0.01804760 \tTraining Acuuarcy 41.612% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1772 \tTraining Loss: 0.01178302 \tValidation Loss 0.01840492 \tTraining Acuuarcy 42.141% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1773 \tTraining Loss: 0.01176436 \tValidation Loss 0.01817275 \tTraining Acuuarcy 41.606% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1774 \tTraining Loss: 0.01175048 \tValidation Loss 0.01830098 \tTraining Acuuarcy 41.116% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1775 \tTraining Loss: 0.01159011 \tValidation Loss 0.01827917 \tTraining Acuuarcy 43.016% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1776 \tTraining Loss: 0.01167282 \tValidation Loss 0.01824183 \tTraining Acuuarcy 41.868% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1777 \tTraining Loss: 0.01166165 \tValidation Loss 0.01835755 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1778 \tTraining Loss: 0.01173936 \tValidation Loss 0.01790605 \tTraining Acuuarcy 41.562% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1779 \tTraining Loss: 0.01175373 \tValidation Loss 0.01811901 \tTraining Acuuarcy 41.545% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1780 \tTraining Loss: 0.01171517 \tValidation Loss 0.01800094 \tTraining Acuuarcy 41.378% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1781 \tTraining Loss: 0.01174482 \tValidation Loss 0.01790506 \tTraining Acuuarcy 41.785% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1782 \tTraining Loss: 0.01165462 \tValidation Loss 0.01813760 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 1783 \tTraining Loss: 0.01171092 \tValidation Loss 0.01820790 \tTraining Acuuarcy 41.790% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1784 \tTraining Loss: 0.01166651 \tValidation Loss 0.01785003 \tTraining Acuuarcy 42.253% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1785 \tTraining Loss: 0.01165954 \tValidation Loss 0.01803282 \tTraining Acuuarcy 41.846% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1786 \tTraining Loss: 0.01170866 \tValidation Loss 0.01814165 \tTraining Acuuarcy 41.907% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1787 \tTraining Loss: 0.01171082 \tValidation Loss 0.01840259 \tTraining Acuuarcy 41.868% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1788 \tTraining Loss: 0.01166779 \tValidation Loss 0.01801700 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1789 \tTraining Loss: 0.01171909 \tValidation Loss 0.01805274 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1790 \tTraining Loss: 0.01168841 \tValidation Loss 0.01856635 \tTraining Acuuarcy 42.147% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1791 \tTraining Loss: 0.01173189 \tValidation Loss 0.01809072 \tTraining Acuuarcy 41.617% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1792 \tTraining Loss: 0.01170682 \tValidation Loss 0.01804575 \tTraining Acuuarcy 41.790% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1793 \tTraining Loss: 0.01180187 \tValidation Loss 0.01814524 \tTraining Acuuarcy 41.322% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1794 \tTraining Loss: 0.01170674 \tValidation Loss 0.01832082 \tTraining Acuuarcy 41.801% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1795 \tTraining Loss: 0.01166663 \tValidation Loss 0.01821554 \tTraining Acuuarcy 42.102% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1796 \tTraining Loss: 0.01175892 \tValidation Loss 0.01799467 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1797 \tTraining Loss: 0.01159226 \tValidation Loss 0.01807822 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1798 \tTraining Loss: 0.01171257 \tValidation Loss 0.01805833 \tTraining Acuuarcy 41.868% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1799 \tTraining Loss: 0.01164683 \tValidation Loss 0.01827786 \tTraining Acuuarcy 42.192% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1800 \tTraining Loss: 0.01172933 \tValidation Loss 0.01834820 \tTraining Acuuarcy 41.863% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1801 \tTraining Loss: 0.01169364 \tValidation Loss 0.01867821 \tTraining Acuuarcy 41.662% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1802 \tTraining Loss: 0.01171758 \tValidation Loss 0.01822717 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 1803 \tTraining Loss: 0.01158282 \tValidation Loss 0.01841517 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1804 \tTraining Loss: 0.01174958 \tValidation Loss 0.01809760 \tTraining Acuuarcy 41.612% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1805 \tTraining Loss: 0.01162269 \tValidation Loss 0.01809424 \tTraining Acuuarcy 42.442% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 1806 \tTraining Loss: 0.01166025 \tValidation Loss 0.01856760 \tTraining Acuuarcy 42.392% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1807 \tTraining Loss: 0.01174884 \tValidation Loss 0.01796976 \tTraining Acuuarcy 41.584% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1808 \tTraining Loss: 0.01165794 \tValidation Loss 0.01818529 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1809 \tTraining Loss: 0.01168503 \tValidation Loss 0.01862500 \tTraining Acuuarcy 41.746% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 1810 \tTraining Loss: 0.01164313 \tValidation Loss 0.01829167 \tTraining Acuuarcy 42.348% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1811 \tTraining Loss: 0.01174241 \tValidation Loss 0.01848569 \tTraining Acuuarcy 41.746% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1812 \tTraining Loss: 0.01164605 \tValidation Loss 0.01771036 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1813 \tTraining Loss: 0.01167993 \tValidation Loss 0.01793992 \tTraining Acuuarcy 42.214% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1814 \tTraining Loss: 0.01172096 \tValidation Loss 0.01808587 \tTraining Acuuarcy 41.863% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1815 \tTraining Loss: 0.01167808 \tValidation Loss 0.01864423 \tTraining Acuuarcy 41.863% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1816 \tTraining Loss: 0.01172012 \tValidation Loss 0.01840804 \tTraining Acuuarcy 41.773% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1817 \tTraining Loss: 0.01179818 \tValidation Loss 0.01785285 \tTraining Acuuarcy 40.993% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1818 \tTraining Loss: 0.01171547 \tValidation Loss 0.01785618 \tTraining Acuuarcy 41.712% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1819 \tTraining Loss: 0.01171406 \tValidation Loss 0.01812168 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1820 \tTraining Loss: 0.01168524 \tValidation Loss 0.01812408 \tTraining Acuuarcy 41.495% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1821 \tTraining Loss: 0.01162214 \tValidation Loss 0.01799473 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1822 \tTraining Loss: 0.01173319 \tValidation Loss 0.01820143 \tTraining Acuuarcy 41.489% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1823 \tTraining Loss: 0.01173925 \tValidation Loss 0.01799254 \tTraining Acuuarcy 41.684% \tValidation Acuuarcy 20.033%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1824 \tTraining Loss: 0.01172275 \tValidation Loss 0.01801692 \tTraining Acuuarcy 41.567% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1825 \tTraining Loss: 0.01175924 \tValidation Loss 0.01844337 \tTraining Acuuarcy 41.651% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1826 \tTraining Loss: 0.01170297 \tValidation Loss 0.01803941 \tTraining Acuuarcy 41.930% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 1827 \tTraining Loss: 0.01176616 \tValidation Loss 0.01808362 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1828 \tTraining Loss: 0.01178190 \tValidation Loss 0.01801611 \tTraining Acuuarcy 41.617% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1829 \tTraining Loss: 0.01168115 \tValidation Loss 0.01840260 \tTraining Acuuarcy 41.601% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1830 \tTraining Loss: 0.01169152 \tValidation Loss 0.01800858 \tTraining Acuuarcy 41.656% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1831 \tTraining Loss: 0.01180602 \tValidation Loss 0.01779824 \tTraining Acuuarcy 41.244% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1832 \tTraining Loss: 0.01167354 \tValidation Loss 0.01836990 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1833 \tTraining Loss: 0.01169908 \tValidation Loss 0.01786702 \tTraining Acuuarcy 41.718% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1834 \tTraining Loss: 0.01164585 \tValidation Loss 0.01823319 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1835 \tTraining Loss: 0.01173701 \tValidation Loss 0.01779472 \tTraining Acuuarcy 41.974% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1836 \tTraining Loss: 0.01168170 \tValidation Loss 0.01806331 \tTraining Acuuarcy 41.729% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1837 \tTraining Loss: 0.01163507 \tValidation Loss 0.01814411 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 1838 \tTraining Loss: 0.01172277 \tValidation Loss 0.01807095 \tTraining Acuuarcy 41.662% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 1839 \tTraining Loss: 0.01168173 \tValidation Loss 0.01792366 \tTraining Acuuarcy 41.723% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1840 \tTraining Loss: 0.01162823 \tValidation Loss 0.01810120 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1841 \tTraining Loss: 0.01166663 \tValidation Loss 0.01819898 \tTraining Acuuarcy 42.080% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1842 \tTraining Loss: 0.01171886 \tValidation Loss 0.01793620 \tTraining Acuuarcy 41.434% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 1843 \tTraining Loss: 0.01172588 \tValidation Loss 0.01833640 \tTraining Acuuarcy 41.512% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1844 \tTraining Loss: 0.01168936 \tValidation Loss 0.01860695 \tTraining Acuuarcy 42.286% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1845 \tTraining Loss: 0.01173600 \tValidation Loss 0.01824018 \tTraining Acuuarcy 41.456% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1846 \tTraining Loss: 0.01169670 \tValidation Loss 0.01795970 \tTraining Acuuarcy 42.119% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1847 \tTraining Loss: 0.01169250 \tValidation Loss 0.01822259 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1848 \tTraining Loss: 0.01167097 \tValidation Loss 0.01817469 \tTraining Acuuarcy 42.214% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1849 \tTraining Loss: 0.01171449 \tValidation Loss 0.01811238 \tTraining Acuuarcy 41.411% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1850 \tTraining Loss: 0.01167601 \tValidation Loss 0.01805567 \tTraining Acuuarcy 42.197% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1851 \tTraining Loss: 0.01172713 \tValidation Loss 0.01813854 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1852 \tTraining Loss: 0.01172688 \tValidation Loss 0.01821068 \tTraining Acuuarcy 41.746% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1853 \tTraining Loss: 0.01168875 \tValidation Loss 0.01784854 \tTraining Acuuarcy 41.679% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1854 \tTraining Loss: 0.01166533 \tValidation Loss 0.01830964 \tTraining Acuuarcy 42.242% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1855 \tTraining Loss: 0.01172353 \tValidation Loss 0.01815883 \tTraining Acuuarcy 41.852% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1856 \tTraining Loss: 0.01163569 \tValidation Loss 0.01848484 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1857 \tTraining Loss: 0.01171256 \tValidation Loss 0.01794452 \tTraining Acuuarcy 41.517% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1858 \tTraining Loss: 0.01172158 \tValidation Loss 0.01776818 \tTraining Acuuarcy 41.623% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 1859 \tTraining Loss: 0.01169312 \tValidation Loss 0.01803458 \tTraining Acuuarcy 41.590% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 1860 \tTraining Loss: 0.01170813 \tValidation Loss 0.01790665 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1861 \tTraining Loss: 0.01176929 \tValidation Loss 0.01820635 \tTraining Acuuarcy 41.160% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1862 \tTraining Loss: 0.01166716 \tValidation Loss 0.01788724 \tTraining Acuuarcy 41.885% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1863 \tTraining Loss: 0.01157425 \tValidation Loss 0.01818809 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1864 \tTraining Loss: 0.01174842 \tValidation Loss 0.01803712 \tTraining Acuuarcy 41.790% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1865 \tTraining Loss: 0.01171526 \tValidation Loss 0.01855525 \tTraining Acuuarcy 41.874% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1866 \tTraining Loss: 0.01172047 \tValidation Loss 0.01824120 \tTraining Acuuarcy 42.058% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1867 \tTraining Loss: 0.01167444 \tValidation Loss 0.01789779 \tTraining Acuuarcy 42.470% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1868 \tTraining Loss: 0.01168322 \tValidation Loss 0.01828991 \tTraining Acuuarcy 42.052% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1869 \tTraining Loss: 0.01176012 \tValidation Loss 0.01816516 \tTraining Acuuarcy 41.512% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1870 \tTraining Loss: 0.01171669 \tValidation Loss 0.01826900 \tTraining Acuuarcy 41.523% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1871 \tTraining Loss: 0.01170329 \tValidation Loss 0.01770682 \tTraining Acuuarcy 41.484% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1872 \tTraining Loss: 0.01173310 \tValidation Loss 0.01778556 \tTraining Acuuarcy 42.019% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1873 \tTraining Loss: 0.01165078 \tValidation Loss 0.01810023 \tTraining Acuuarcy 41.718% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1874 \tTraining Loss: 0.01166308 \tValidation Loss 0.01829754 \tTraining Acuuarcy 42.236% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1875 \tTraining Loss: 0.01167771 \tValidation Loss 0.01842860 \tTraining Acuuarcy 42.236% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1876 \tTraining Loss: 0.01167216 \tValidation Loss 0.01844475 \tTraining Acuuarcy 41.879% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1877 \tTraining Loss: 0.01170879 \tValidation Loss 0.01767844 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1878 \tTraining Loss: 0.01174144 \tValidation Loss 0.01805353 \tTraining Acuuarcy 41.188% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1879 \tTraining Loss: 0.01168666 \tValidation Loss 0.01798884 \tTraining Acuuarcy 42.309% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1880 \tTraining Loss: 0.01162724 \tValidation Loss 0.01801254 \tTraining Acuuarcy 42.208% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1881 \tTraining Loss: 0.01164597 \tValidation Loss 0.01835450 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1882 \tTraining Loss: 0.01169185 \tValidation Loss 0.01825441 \tTraining Acuuarcy 41.924% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1883 \tTraining Loss: 0.01166050 \tValidation Loss 0.01852055 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1884 \tTraining Loss: 0.01165040 \tValidation Loss 0.01814612 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1885 \tTraining Loss: 0.01161330 \tValidation Loss 0.01836193 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1886 \tTraining Loss: 0.01172447 \tValidation Loss 0.01835639 \tTraining Acuuarcy 41.294% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1887 \tTraining Loss: 0.01170660 \tValidation Loss 0.01809580 \tTraining Acuuarcy 41.684% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1888 \tTraining Loss: 0.01168421 \tValidation Loss 0.01831076 \tTraining Acuuarcy 41.974% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1889 \tTraining Loss: 0.01163652 \tValidation Loss 0.01779505 \tTraining Acuuarcy 41.840% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1890 \tTraining Loss: 0.01174531 \tValidation Loss 0.01856194 \tTraining Acuuarcy 41.930% \tValidation Acuuarcy 20.897%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1891 \tTraining Loss: 0.01171425 \tValidation Loss 0.01836430 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1892 \tTraining Loss: 0.01168138 \tValidation Loss 0.01815340 \tTraining Acuuarcy 41.868% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1893 \tTraining Loss: 0.01174383 \tValidation Loss 0.01829039 \tTraining Acuuarcy 41.974% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1894 \tTraining Loss: 0.01168669 \tValidation Loss 0.01811473 \tTraining Acuuarcy 41.874% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1895 \tTraining Loss: 0.01170367 \tValidation Loss 0.01796735 \tTraining Acuuarcy 41.813% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1896 \tTraining Loss: 0.01165806 \tValidation Loss 0.01822804 \tTraining Acuuarcy 42.080% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1897 \tTraining Loss: 0.01178429 \tValidation Loss 0.01829223 \tTraining Acuuarcy 41.378% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1898 \tTraining Loss: 0.01169474 \tValidation Loss 0.01831762 \tTraining Acuuarcy 42.058% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1899 \tTraining Loss: 0.01171589 \tValidation Loss 0.01809900 \tTraining Acuuarcy 41.863% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1900 \tTraining Loss: 0.01171229 \tValidation Loss 0.01862666 \tTraining Acuuarcy 41.434% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1901 \tTraining Loss: 0.01174458 \tValidation Loss 0.01787924 \tTraining Acuuarcy 41.779% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1902 \tTraining Loss: 0.01163981 \tValidation Loss 0.01833844 \tTraining Acuuarcy 42.097% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1903 \tTraining Loss: 0.01169903 \tValidation Loss 0.01793735 \tTraining Acuuarcy 42.063% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 1904 \tTraining Loss: 0.01165327 \tValidation Loss 0.01825914 \tTraining Acuuarcy 41.818% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1905 \tTraining Loss: 0.01165248 \tValidation Loss 0.01819115 \tTraining Acuuarcy 42.498% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1906 \tTraining Loss: 0.01164839 \tValidation Loss 0.01807046 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1907 \tTraining Loss: 0.01171685 \tValidation Loss 0.01798919 \tTraining Acuuarcy 41.495% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 1908 \tTraining Loss: 0.01169799 \tValidation Loss 0.01818073 \tTraining Acuuarcy 42.019% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1909 \tTraining Loss: 0.01167711 \tValidation Loss 0.01804879 \tTraining Acuuarcy 41.551% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1910 \tTraining Loss: 0.01169358 \tValidation Loss 0.01782202 \tTraining Acuuarcy 41.517% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1911 \tTraining Loss: 0.01173544 \tValidation Loss 0.01794755 \tTraining Acuuarcy 41.322% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1912 \tTraining Loss: 0.01164565 \tValidation Loss 0.01861147 \tTraining Acuuarcy 42.359% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1913 \tTraining Loss: 0.01174144 \tValidation Loss 0.01804741 \tTraining Acuuarcy 41.790% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1914 \tTraining Loss: 0.01160759 \tValidation Loss 0.01832673 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1915 \tTraining Loss: 0.01166580 \tValidation Loss 0.01822114 \tTraining Acuuarcy 41.846% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1916 \tTraining Loss: 0.01167555 \tValidation Loss 0.01806002 \tTraining Acuuarcy 42.526% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1917 \tTraining Loss: 0.01168583 \tValidation Loss 0.01839627 \tTraining Acuuarcy 41.974% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1918 \tTraining Loss: 0.01168475 \tValidation Loss 0.01806524 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1919 \tTraining Loss: 0.01164642 \tValidation Loss 0.01817005 \tTraining Acuuarcy 41.974% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1920 \tTraining Loss: 0.01160947 \tValidation Loss 0.01778815 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 1921 \tTraining Loss: 0.01166870 \tValidation Loss 0.01833295 \tTraining Acuuarcy 42.024% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1922 \tTraining Loss: 0.01164878 \tValidation Loss 0.01786547 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1923 \tTraining Loss: 0.01162363 \tValidation Loss 0.01818746 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1924 \tTraining Loss: 0.01168948 \tValidation Loss 0.01858302 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 1925 \tTraining Loss: 0.01174221 \tValidation Loss 0.01817756 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1926 \tTraining Loss: 0.01172533 \tValidation Loss 0.01842061 \tTraining Acuuarcy 41.623% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1927 \tTraining Loss: 0.01167475 \tValidation Loss 0.01804865 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1928 \tTraining Loss: 0.01167910 \tValidation Loss 0.01784240 \tTraining Acuuarcy 41.785% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1929 \tTraining Loss: 0.01164870 \tValidation Loss 0.01824094 \tTraining Acuuarcy 42.359% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1930 \tTraining Loss: 0.01167526 \tValidation Loss 0.01802535 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1931 \tTraining Loss: 0.01166410 \tValidation Loss 0.01855298 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1932 \tTraining Loss: 0.01168138 \tValidation Loss 0.01845630 \tTraining Acuuarcy 42.414% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1933 \tTraining Loss: 0.01166995 \tValidation Loss 0.01844169 \tTraining Acuuarcy 41.840% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1934 \tTraining Loss: 0.01164906 \tValidation Loss 0.01811814 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 1935 \tTraining Loss: 0.01162275 \tValidation Loss 0.01785274 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1936 \tTraining Loss: 0.01169759 \tValidation Loss 0.01795866 \tTraining Acuuarcy 41.723% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1937 \tTraining Loss: 0.01164678 \tValidation Loss 0.01807399 \tTraining Acuuarcy 42.108% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1938 \tTraining Loss: 0.01173285 \tValidation Loss 0.01793218 \tTraining Acuuarcy 41.723% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1939 \tTraining Loss: 0.01154021 \tValidation Loss 0.01895103 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1940 \tTraining Loss: 0.01170332 \tValidation Loss 0.01751962 \tTraining Acuuarcy 41.907% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 1941 \tTraining Loss: 0.01176923 \tValidation Loss 0.01789401 \tTraining Acuuarcy 41.495% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1942 \tTraining Loss: 0.01170867 \tValidation Loss 0.01805004 \tTraining Acuuarcy 41.773% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1943 \tTraining Loss: 0.01177915 \tValidation Loss 0.01803617 \tTraining Acuuarcy 41.562% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1944 \tTraining Loss: 0.01165393 \tValidation Loss 0.01771451 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1945 \tTraining Loss: 0.01169690 \tValidation Loss 0.01818444 \tTraining Acuuarcy 42.309% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1946 \tTraining Loss: 0.01170680 \tValidation Loss 0.01795849 \tTraining Acuuarcy 41.757% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1947 \tTraining Loss: 0.01163297 \tValidation Loss 0.01795628 \tTraining Acuuarcy 41.930% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1948 \tTraining Loss: 0.01171442 \tValidation Loss 0.01816187 \tTraining Acuuarcy 41.857% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1949 \tTraining Loss: 0.01172948 \tValidation Loss 0.01796838 \tTraining Acuuarcy 41.773% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1950 \tTraining Loss: 0.01166812 \tValidation Loss 0.01835679 \tTraining Acuuarcy 42.314% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1951 \tTraining Loss: 0.01167611 \tValidation Loss 0.01829163 \tTraining Acuuarcy 41.835% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1952 \tTraining Loss: 0.01166822 \tValidation Loss 0.01892455 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1953 \tTraining Loss: 0.01163161 \tValidation Loss 0.01823402 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1954 \tTraining Loss: 0.01170705 \tValidation Loss 0.01852954 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 1955 \tTraining Loss: 0.01166504 \tValidation Loss 0.01846135 \tTraining Acuuarcy 42.019% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1956 \tTraining Loss: 0.01175503 \tValidation Loss 0.01774695 \tTraining Acuuarcy 42.063% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1957 \tTraining Loss: 0.01163603 \tValidation Loss 0.01780763 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 20.897%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1958 \tTraining Loss: 0.01166821 \tValidation Loss 0.01850506 \tTraining Acuuarcy 42.013% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1959 \tTraining Loss: 0.01171736 \tValidation Loss 0.01819318 \tTraining Acuuarcy 41.595% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1960 \tTraining Loss: 0.01171391 \tValidation Loss 0.01764934 \tTraining Acuuarcy 41.684% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1961 \tTraining Loss: 0.01164324 \tValidation Loss 0.01803889 \tTraining Acuuarcy 42.359% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 1962 \tTraining Loss: 0.01167344 \tValidation Loss 0.01829316 \tTraining Acuuarcy 42.242% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1963 \tTraining Loss: 0.01164511 \tValidation Loss 0.01780445 \tTraining Acuuarcy 41.879% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1964 \tTraining Loss: 0.01168981 \tValidation Loss 0.01818525 \tTraining Acuuarcy 42.141% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1965 \tTraining Loss: 0.01165119 \tValidation Loss 0.01815723 \tTraining Acuuarcy 42.058% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1966 \tTraining Loss: 0.01165181 \tValidation Loss 0.01825024 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1967 \tTraining Loss: 0.01168727 \tValidation Loss 0.01813643 \tTraining Acuuarcy 41.746% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1968 \tTraining Loss: 0.01163221 \tValidation Loss 0.01822165 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1969 \tTraining Loss: 0.01164600 \tValidation Loss 0.01819092 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1970 \tTraining Loss: 0.01156241 \tValidation Loss 0.01791255 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1971 \tTraining Loss: 0.01172813 \tValidation Loss 0.01804282 \tTraining Acuuarcy 41.996% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1972 \tTraining Loss: 0.01167162 \tValidation Loss 0.01839805 \tTraining Acuuarcy 41.957% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1973 \tTraining Loss: 0.01172677 \tValidation Loss 0.01818462 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1974 \tTraining Loss: 0.01166121 \tValidation Loss 0.01842224 \tTraining Acuuarcy 42.364% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1975 \tTraining Loss: 0.01166620 \tValidation Loss 0.01782248 \tTraining Acuuarcy 42.052% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1976 \tTraining Loss: 0.01163123 \tValidation Loss 0.01866469 \tTraining Acuuarcy 41.996% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1977 \tTraining Loss: 0.01163707 \tValidation Loss 0.01854149 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1978 \tTraining Loss: 0.01166538 \tValidation Loss 0.01834059 \tTraining Acuuarcy 41.813% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1979 \tTraining Loss: 0.01162919 \tValidation Loss 0.01816168 \tTraining Acuuarcy 41.835% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1980 \tTraining Loss: 0.01171706 \tValidation Loss 0.01803955 \tTraining Acuuarcy 42.013% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1981 \tTraining Loss: 0.01157263 \tValidation Loss 0.01833589 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1982 \tTraining Loss: 0.01163072 \tValidation Loss 0.01815101 \tTraining Acuuarcy 41.874% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 1983 \tTraining Loss: 0.01171331 \tValidation Loss 0.01860701 \tTraining Acuuarcy 41.918% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1984 \tTraining Loss: 0.01167766 \tValidation Loss 0.01807473 \tTraining Acuuarcy 42.392% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1985 \tTraining Loss: 0.01167980 \tValidation Loss 0.01809698 \tTraining Acuuarcy 41.707% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1986 \tTraining Loss: 0.01166040 \tValidation Loss 0.01827225 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 1987 \tTraining Loss: 0.01169144 \tValidation Loss 0.01849024 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1988 \tTraining Loss: 0.01162759 \tValidation Loss 0.01855744 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 1989 \tTraining Loss: 0.01164958 \tValidation Loss 0.01772729 \tTraining Acuuarcy 41.712% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 1990 \tTraining Loss: 0.01165371 \tValidation Loss 0.01778857 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1991 \tTraining Loss: 0.01178161 \tValidation Loss 0.01810543 \tTraining Acuuarcy 41.004% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 1992 \tTraining Loss: 0.01171338 \tValidation Loss 0.01816543 \tTraining Acuuarcy 41.205% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1993 \tTraining Loss: 0.01164036 \tValidation Loss 0.01816069 \tTraining Acuuarcy 42.063% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1994 \tTraining Loss: 0.01172338 \tValidation Loss 0.01809469 \tTraining Acuuarcy 41.495% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1995 \tTraining Loss: 0.01173463 \tValidation Loss 0.01788578 \tTraining Acuuarcy 41.573% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1996 \tTraining Loss: 0.01164278 \tValidation Loss 0.01807371 \tTraining Acuuarcy 41.946% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1997 \tTraining Loss: 0.01178041 \tValidation Loss 0.01759597 \tTraining Acuuarcy 41.623% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1998 \tTraining Loss: 0.01174695 \tValidation Loss 0.01770460 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1999 \tTraining Loss: 0.01164877 \tValidation Loss 0.01783235 \tTraining Acuuarcy 42.108% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2000 \tTraining Loss: 0.01168704 \tValidation Loss 0.01877454 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 2001 \tTraining Loss: 0.01156445 \tValidation Loss 0.01799867 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2002 \tTraining Loss: 0.01166363 \tValidation Loss 0.01836805 \tTraining Acuuarcy 42.047% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2003 \tTraining Loss: 0.01164096 \tValidation Loss 0.01838360 \tTraining Acuuarcy 41.734% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 2004 \tTraining Loss: 0.01167706 \tValidation Loss 0.01858894 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2005 \tTraining Loss: 0.01162547 \tValidation Loss 0.01848428 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2006 \tTraining Loss: 0.01169218 \tValidation Loss 0.01798115 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2007 \tTraining Loss: 0.01168829 \tValidation Loss 0.01784144 \tTraining Acuuarcy 42.242% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2008 \tTraining Loss: 0.01169326 \tValidation Loss 0.01810052 \tTraining Acuuarcy 42.336% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 2009 \tTraining Loss: 0.01170655 \tValidation Loss 0.01823898 \tTraining Acuuarcy 41.489% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2010 \tTraining Loss: 0.01167821 \tValidation Loss 0.01840555 \tTraining Acuuarcy 42.141% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2011 \tTraining Loss: 0.01169433 \tValidation Loss 0.01824767 \tTraining Acuuarcy 41.734% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 2012 \tTraining Loss: 0.01173622 \tValidation Loss 0.01794716 \tTraining Acuuarcy 41.383% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2013 \tTraining Loss: 0.01165860 \tValidation Loss 0.01838657 \tTraining Acuuarcy 41.796% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2014 \tTraining Loss: 0.01167280 \tValidation Loss 0.01811519 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2015 \tTraining Loss: 0.01161622 \tValidation Loss 0.01826745 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2016 \tTraining Loss: 0.01167114 \tValidation Loss 0.01824838 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2017 \tTraining Loss: 0.01167577 \tValidation Loss 0.01839173 \tTraining Acuuarcy 41.946% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2018 \tTraining Loss: 0.01157666 \tValidation Loss 0.01845161 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2019 \tTraining Loss: 0.01168750 \tValidation Loss 0.01815770 \tTraining Acuuarcy 42.208% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2020 \tTraining Loss: 0.01166788 \tValidation Loss 0.01794285 \tTraining Acuuarcy 42.164% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2021 \tTraining Loss: 0.01163613 \tValidation Loss 0.01814725 \tTraining Acuuarcy 42.063% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2022 \tTraining Loss: 0.01174330 \tValidation Loss 0.01748420 \tTraining Acuuarcy 41.250% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 2023 \tTraining Loss: 0.01164044 \tValidation Loss 0.01834898 \tTraining Acuuarcy 41.634% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2024 \tTraining Loss: 0.01170501 \tValidation Loss 0.01822481 \tTraining Acuuarcy 41.857% \tValidation Acuuarcy 19.420%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2025 \tTraining Loss: 0.01165721 \tValidation Loss 0.01823336 \tTraining Acuuarcy 42.180% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2026 \tTraining Loss: 0.01152940 \tValidation Loss 0.01850482 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2027 \tTraining Loss: 0.01176953 \tValidation Loss 0.01795235 \tTraining Acuuarcy 41.712% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2028 \tTraining Loss: 0.01166869 \tValidation Loss 0.01783485 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2029 \tTraining Loss: 0.01171459 \tValidation Loss 0.01785362 \tTraining Acuuarcy 41.679% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2030 \tTraining Loss: 0.01158665 \tValidation Loss 0.01859347 \tTraining Acuuarcy 42.364% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2031 \tTraining Loss: 0.01160485 \tValidation Loss 0.01825470 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2032 \tTraining Loss: 0.01170718 \tValidation Loss 0.01797151 \tTraining Acuuarcy 41.902% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2033 \tTraining Loss: 0.01165844 \tValidation Loss 0.01820849 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2034 \tTraining Loss: 0.01166163 \tValidation Loss 0.01818056 \tTraining Acuuarcy 41.662% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2035 \tTraining Loss: 0.01164943 \tValidation Loss 0.01821003 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2036 \tTraining Loss: 0.01166903 \tValidation Loss 0.01847877 \tTraining Acuuarcy 42.214% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2037 \tTraining Loss: 0.01171516 \tValidation Loss 0.01800744 \tTraining Acuuarcy 42.130% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2038 \tTraining Loss: 0.01163451 \tValidation Loss 0.01818956 \tTraining Acuuarcy 42.470% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2039 \tTraining Loss: 0.01166378 \tValidation Loss 0.01808772 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2040 \tTraining Loss: 0.01162513 \tValidation Loss 0.01816963 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2041 \tTraining Loss: 0.01169395 \tValidation Loss 0.01850673 \tTraining Acuuarcy 42.125% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 2042 \tTraining Loss: 0.01168303 \tValidation Loss 0.01805844 \tTraining Acuuarcy 41.874% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2043 \tTraining Loss: 0.01176010 \tValidation Loss 0.01807692 \tTraining Acuuarcy 41.640% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2044 \tTraining Loss: 0.01164666 \tValidation Loss 0.01836516 \tTraining Acuuarcy 42.264% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2045 \tTraining Loss: 0.01157623 \tValidation Loss 0.01870164 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2046 \tTraining Loss: 0.01172355 \tValidation Loss 0.01849386 \tTraining Acuuarcy 41.612% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2047 \tTraining Loss: 0.01165757 \tValidation Loss 0.01802107 \tTraining Acuuarcy 42.130% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2048 \tTraining Loss: 0.01167680 \tValidation Loss 0.01842237 \tTraining Acuuarcy 41.874% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2049 \tTraining Loss: 0.01167497 \tValidation Loss 0.01792774 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 2050 \tTraining Loss: 0.01163766 \tValidation Loss 0.01844585 \tTraining Acuuarcy 42.069% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2051 \tTraining Loss: 0.01166588 \tValidation Loss 0.01820010 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2052 \tTraining Loss: 0.01172672 \tValidation Loss 0.01820694 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2053 \tTraining Loss: 0.01162022 \tValidation Loss 0.01816241 \tTraining Acuuarcy 42.158% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2054 \tTraining Loss: 0.01166668 \tValidation Loss 0.01826423 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 2055 \tTraining Loss: 0.01167201 \tValidation Loss 0.01815164 \tTraining Acuuarcy 42.125% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2056 \tTraining Loss: 0.01172353 \tValidation Loss 0.01789969 \tTraining Acuuarcy 41.177% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2057 \tTraining Loss: 0.01165783 \tValidation Loss 0.01829702 \tTraining Acuuarcy 42.186% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2058 \tTraining Loss: 0.01163211 \tValidation Loss 0.01818375 \tTraining Acuuarcy 42.476% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2059 \tTraining Loss: 0.01169733 \tValidation Loss 0.01809859 \tTraining Acuuarcy 41.852% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 2060 \tTraining Loss: 0.01167621 \tValidation Loss 0.01791065 \tTraining Acuuarcy 41.913% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2061 \tTraining Loss: 0.01163031 \tValidation Loss 0.01796939 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 2062 \tTraining Loss: 0.01172314 \tValidation Loss 0.01801666 \tTraining Acuuarcy 41.651% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2063 \tTraining Loss: 0.01160109 \tValidation Loss 0.01795821 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2064 \tTraining Loss: 0.01167636 \tValidation Loss 0.01774951 \tTraining Acuuarcy 41.651% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2065 \tTraining Loss: 0.01167030 \tValidation Loss 0.01842107 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2066 \tTraining Loss: 0.01156947 \tValidation Loss 0.01862637 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2067 \tTraining Loss: 0.01162543 \tValidation Loss 0.01795603 \tTraining Acuuarcy 41.807% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2068 \tTraining Loss: 0.01172585 \tValidation Loss 0.01823251 \tTraining Acuuarcy 41.835% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2069 \tTraining Loss: 0.01163568 \tValidation Loss 0.01822922 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2070 \tTraining Loss: 0.01160071 \tValidation Loss 0.01865109 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 2071 \tTraining Loss: 0.01163926 \tValidation Loss 0.01789912 \tTraining Acuuarcy 41.857% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 2072 \tTraining Loss: 0.01160412 \tValidation Loss 0.01868421 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2073 \tTraining Loss: 0.01157703 \tValidation Loss 0.01847190 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2074 \tTraining Loss: 0.01168178 \tValidation Loss 0.01823424 \tTraining Acuuarcy 41.924% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2075 \tTraining Loss: 0.01165050 \tValidation Loss 0.01809892 \tTraining Acuuarcy 41.651% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2076 \tTraining Loss: 0.01167430 \tValidation Loss 0.01785059 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 2077 \tTraining Loss: 0.01167150 \tValidation Loss 0.01780453 \tTraining Acuuarcy 41.740% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2078 \tTraining Loss: 0.01166703 \tValidation Loss 0.01843991 \tTraining Acuuarcy 42.264% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2079 \tTraining Loss: 0.01170055 \tValidation Loss 0.01812248 \tTraining Acuuarcy 41.394% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2080 \tTraining Loss: 0.01163598 \tValidation Loss 0.01864717 \tTraining Acuuarcy 42.041% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2081 \tTraining Loss: 0.01160626 \tValidation Loss 0.01843970 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2082 \tTraining Loss: 0.01165089 \tValidation Loss 0.01798857 \tTraining Acuuarcy 42.264% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2083 \tTraining Loss: 0.01170553 \tValidation Loss 0.01801411 \tTraining Acuuarcy 41.874% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2084 \tTraining Loss: 0.01167625 \tValidation Loss 0.01833674 \tTraining Acuuarcy 41.762% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2085 \tTraining Loss: 0.01162620 \tValidation Loss 0.01842188 \tTraining Acuuarcy 42.526% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2086 \tTraining Loss: 0.01170359 \tValidation Loss 0.01795065 \tTraining Acuuarcy 42.136% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2087 \tTraining Loss: 0.01169305 \tValidation Loss 0.01799039 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2088 \tTraining Loss: 0.01161106 \tValidation Loss 0.01863432 \tTraining Acuuarcy 41.902% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2089 \tTraining Loss: 0.01162439 \tValidation Loss 0.01868095 \tTraining Acuuarcy 42.136% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2090 \tTraining Loss: 0.01157697 \tValidation Loss 0.01813665 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2091 \tTraining Loss: 0.01157747 \tValidation Loss 0.01808969 \tTraining Acuuarcy 42.214% \tValidation Acuuarcy 19.504%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2092 \tTraining Loss: 0.01170391 \tValidation Loss 0.01815736 \tTraining Acuuarcy 41.612% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 2093 \tTraining Loss: 0.01161179 \tValidation Loss 0.01830235 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2094 \tTraining Loss: 0.01162784 \tValidation Loss 0.01807574 \tTraining Acuuarcy 41.645% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2095 \tTraining Loss: 0.01171522 \tValidation Loss 0.01803604 \tTraining Acuuarcy 41.590% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2096 \tTraining Loss: 0.01165262 \tValidation Loss 0.01792548 \tTraining Acuuarcy 42.336% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2097 \tTraining Loss: 0.01164574 \tValidation Loss 0.01798966 \tTraining Acuuarcy 41.746% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2098 \tTraining Loss: 0.01164201 \tValidation Loss 0.01830694 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2099 \tTraining Loss: 0.01160367 \tValidation Loss 0.01826820 \tTraining Acuuarcy 42.470% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2100 \tTraining Loss: 0.01162768 \tValidation Loss 0.01815171 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2101 \tTraining Loss: 0.01164552 \tValidation Loss 0.01819682 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2102 \tTraining Loss: 0.01167733 \tValidation Loss 0.01834491 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2103 \tTraining Loss: 0.01165454 \tValidation Loss 0.01851416 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2104 \tTraining Loss: 0.01164977 \tValidation Loss 0.01832056 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2105 \tTraining Loss: 0.01161063 \tValidation Loss 0.01792217 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2106 \tTraining Loss: 0.01158836 \tValidation Loss 0.01782191 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 2107 \tTraining Loss: 0.01171745 \tValidation Loss 0.01825637 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2108 \tTraining Loss: 0.01171128 \tValidation Loss 0.01851850 \tTraining Acuuarcy 41.734% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2109 \tTraining Loss: 0.01168239 \tValidation Loss 0.01804961 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2110 \tTraining Loss: 0.01163882 \tValidation Loss 0.01784008 \tTraining Acuuarcy 41.941% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2111 \tTraining Loss: 0.01168124 \tValidation Loss 0.01799336 \tTraining Acuuarcy 42.498% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2112 \tTraining Loss: 0.01155286 \tValidation Loss 0.01809836 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2113 \tTraining Loss: 0.01172337 \tValidation Loss 0.01792650 \tTraining Acuuarcy 41.879% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2114 \tTraining Loss: 0.01161275 \tValidation Loss 0.01828127 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 2115 \tTraining Loss: 0.01170124 \tValidation Loss 0.01855682 \tTraining Acuuarcy 41.695% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2116 \tTraining Loss: 0.01164640 \tValidation Loss 0.01794859 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2117 \tTraining Loss: 0.01163017 \tValidation Loss 0.01798653 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2118 \tTraining Loss: 0.01165525 \tValidation Loss 0.01823137 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2119 \tTraining Loss: 0.01160839 \tValidation Loss 0.01824649 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 2120 \tTraining Loss: 0.01165109 \tValidation Loss 0.01841933 \tTraining Acuuarcy 41.952% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2121 \tTraining Loss: 0.01154675 \tValidation Loss 0.01911654 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2122 \tTraining Loss: 0.01173102 \tValidation Loss 0.01852186 \tTraining Acuuarcy 41.478% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 2123 \tTraining Loss: 0.01174219 \tValidation Loss 0.01770299 \tTraining Acuuarcy 41.918% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 2124 \tTraining Loss: 0.01160459 \tValidation Loss 0.01857445 \tTraining Acuuarcy 42.437% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2125 \tTraining Loss: 0.01169521 \tValidation Loss 0.01791498 \tTraining Acuuarcy 41.695% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2126 \tTraining Loss: 0.01166795 \tValidation Loss 0.01794253 \tTraining Acuuarcy 42.130% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2127 \tTraining Loss: 0.01168247 \tValidation Loss 0.01831953 \tTraining Acuuarcy 42.286% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 2128 \tTraining Loss: 0.01163320 \tValidation Loss 0.01852910 \tTraining Acuuarcy 42.414% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2129 \tTraining Loss: 0.01162460 \tValidation Loss 0.01792977 \tTraining Acuuarcy 42.008% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2130 \tTraining Loss: 0.01163428 \tValidation Loss 0.01878597 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2131 \tTraining Loss: 0.01160455 \tValidation Loss 0.01789264 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2132 \tTraining Loss: 0.01168948 \tValidation Loss 0.01791688 \tTraining Acuuarcy 42.164% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2133 \tTraining Loss: 0.01169000 \tValidation Loss 0.01815819 \tTraining Acuuarcy 41.957% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2134 \tTraining Loss: 0.01169858 \tValidation Loss 0.01831021 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 2135 \tTraining Loss: 0.01169623 \tValidation Loss 0.01818343 \tTraining Acuuarcy 41.757% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2136 \tTraining Loss: 0.01168437 \tValidation Loss 0.01845059 \tTraining Acuuarcy 41.902% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2137 \tTraining Loss: 0.01160607 \tValidation Loss 0.01800494 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2138 \tTraining Loss: 0.01154032 \tValidation Loss 0.01798829 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2139 \tTraining Loss: 0.01165126 \tValidation Loss 0.01835999 \tTraining Acuuarcy 41.818% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2140 \tTraining Loss: 0.01164406 \tValidation Loss 0.01805815 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 2141 \tTraining Loss: 0.01167136 \tValidation Loss 0.01819637 \tTraining Acuuarcy 42.208% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2142 \tTraining Loss: 0.01163640 \tValidation Loss 0.01826831 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 2143 \tTraining Loss: 0.01162226 \tValidation Loss 0.01835204 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2144 \tTraining Loss: 0.01162540 \tValidation Loss 0.01849582 \tTraining Acuuarcy 41.996% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2145 \tTraining Loss: 0.01161102 \tValidation Loss 0.01827459 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2146 \tTraining Loss: 0.01164717 \tValidation Loss 0.01820144 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 2147 \tTraining Loss: 0.01166239 \tValidation Loss 0.01840944 \tTraining Acuuarcy 41.980% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2148 \tTraining Loss: 0.01166819 \tValidation Loss 0.01829628 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2149 \tTraining Loss: 0.01171760 \tValidation Loss 0.01829754 \tTraining Acuuarcy 41.712% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 2150 \tTraining Loss: 0.01165863 \tValidation Loss 0.01835427 \tTraining Acuuarcy 41.852% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2151 \tTraining Loss: 0.01160954 \tValidation Loss 0.01799624 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2152 \tTraining Loss: 0.01173610 \tValidation Loss 0.01864456 \tTraining Acuuarcy 41.790% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2153 \tTraining Loss: 0.01168108 \tValidation Loss 0.01796883 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2154 \tTraining Loss: 0.01169299 \tValidation Loss 0.01815402 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2155 \tTraining Loss: 0.01163446 \tValidation Loss 0.01825385 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2156 \tTraining Loss: 0.01158521 \tValidation Loss 0.01847896 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2157 \tTraining Loss: 0.01157009 \tValidation Loss 0.01869443 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2158 \tTraining Loss: 0.01166381 \tValidation Loss 0.01804522 \tTraining Acuuarcy 42.047% \tValidation Acuuarcy 21.817%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2159 \tTraining Loss: 0.01171715 \tValidation Loss 0.01789390 \tTraining Acuuarcy 42.141% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2160 \tTraining Loss: 0.01170098 \tValidation Loss 0.01792843 \tTraining Acuuarcy 41.941% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2161 \tTraining Loss: 0.01171540 \tValidation Loss 0.01872318 \tTraining Acuuarcy 41.935% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 2162 \tTraining Loss: 0.01164515 \tValidation Loss 0.01846496 \tTraining Acuuarcy 42.175% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2163 \tTraining Loss: 0.01167071 \tValidation Loss 0.01803089 \tTraining Acuuarcy 41.790% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 2164 \tTraining Loss: 0.01156450 \tValidation Loss 0.01793606 \tTraining Acuuarcy 42.214% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2165 \tTraining Loss: 0.01162663 \tValidation Loss 0.01808425 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2166 \tTraining Loss: 0.01162227 \tValidation Loss 0.01806160 \tTraining Acuuarcy 42.253% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2167 \tTraining Loss: 0.01154417 \tValidation Loss 0.01847057 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2168 \tTraining Loss: 0.01167260 \tValidation Loss 0.01813766 \tTraining Acuuarcy 42.247% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2169 \tTraining Loss: 0.01162845 \tValidation Loss 0.01816355 \tTraining Acuuarcy 41.874% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2170 \tTraining Loss: 0.01154402 \tValidation Loss 0.01838486 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2171 \tTraining Loss: 0.01166131 \tValidation Loss 0.01887134 \tTraining Acuuarcy 42.197% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2172 \tTraining Loss: 0.01159856 \tValidation Loss 0.01856173 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2173 \tTraining Loss: 0.01163391 \tValidation Loss 0.01804953 \tTraining Acuuarcy 42.175% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 2174 \tTraining Loss: 0.01170106 \tValidation Loss 0.01852148 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2175 \tTraining Loss: 0.01170673 \tValidation Loss 0.01813825 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2176 \tTraining Loss: 0.01167407 \tValidation Loss 0.01772185 \tTraining Acuuarcy 42.041% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2177 \tTraining Loss: 0.01161321 \tValidation Loss 0.01814711 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2178 \tTraining Loss: 0.01164304 \tValidation Loss 0.01814067 \tTraining Acuuarcy 41.846% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2179 \tTraining Loss: 0.01167426 \tValidation Loss 0.01792088 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2180 \tTraining Loss: 0.01165945 \tValidation Loss 0.01811641 \tTraining Acuuarcy 42.264% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2181 \tTraining Loss: 0.01162178 \tValidation Loss 0.01877286 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2182 \tTraining Loss: 0.01172764 \tValidation Loss 0.01784836 \tTraining Acuuarcy 41.523% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2183 \tTraining Loss: 0.01165482 \tValidation Loss 0.01809389 \tTraining Acuuarcy 42.242% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2184 \tTraining Loss: 0.01168992 \tValidation Loss 0.01825003 \tTraining Acuuarcy 42.047% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2185 \tTraining Loss: 0.01165005 \tValidation Loss 0.01805004 \tTraining Acuuarcy 42.715% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2186 \tTraining Loss: 0.01169283 \tValidation Loss 0.01806778 \tTraining Acuuarcy 42.164% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2187 \tTraining Loss: 0.01165017 \tValidation Loss 0.01844512 \tTraining Acuuarcy 42.292% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2188 \tTraining Loss: 0.01164786 \tValidation Loss 0.01853431 \tTraining Acuuarcy 41.879% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2189 \tTraining Loss: 0.01161653 \tValidation Loss 0.01872376 \tTraining Acuuarcy 42.292% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2190 \tTraining Loss: 0.01169829 \tValidation Loss 0.01820798 \tTraining Acuuarcy 42.175% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2191 \tTraining Loss: 0.01157481 \tValidation Loss 0.01795723 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2192 \tTraining Loss: 0.01164317 \tValidation Loss 0.01817220 \tTraining Acuuarcy 42.453% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2193 \tTraining Loss: 0.01171252 \tValidation Loss 0.01814600 \tTraining Acuuarcy 42.008% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2194 \tTraining Loss: 0.01169165 \tValidation Loss 0.01793880 \tTraining Acuuarcy 41.757% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2195 \tTraining Loss: 0.01163136 \tValidation Loss 0.01816242 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2196 \tTraining Loss: 0.01159049 \tValidation Loss 0.01835983 \tTraining Acuuarcy 42.125% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2197 \tTraining Loss: 0.01155779 \tValidation Loss 0.01855175 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2198 \tTraining Loss: 0.01172604 \tValidation Loss 0.01852943 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2199 \tTraining Loss: 0.01163770 \tValidation Loss 0.01859600 \tTraining Acuuarcy 42.008% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2200 \tTraining Loss: 0.01162765 \tValidation Loss 0.01807541 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2201 \tTraining Loss: 0.01165540 \tValidation Loss 0.01822440 \tTraining Acuuarcy 41.773% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2202 \tTraining Loss: 0.01165919 \tValidation Loss 0.01827446 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2203 \tTraining Loss: 0.01156885 \tValidation Loss 0.01854631 \tTraining Acuuarcy 42.364% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2204 \tTraining Loss: 0.01167405 \tValidation Loss 0.01826724 \tTraining Acuuarcy 42.141% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 2205 \tTraining Loss: 0.01160642 \tValidation Loss 0.01809797 \tTraining Acuuarcy 42.147% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 2206 \tTraining Loss: 0.01159167 \tValidation Loss 0.01825514 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2207 \tTraining Loss: 0.01162216 \tValidation Loss 0.01852608 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2208 \tTraining Loss: 0.01158938 \tValidation Loss 0.01837200 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2209 \tTraining Loss: 0.01165687 \tValidation Loss 0.01824055 \tTraining Acuuarcy 42.030% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2210 \tTraining Loss: 0.01167852 \tValidation Loss 0.01784764 \tTraining Acuuarcy 41.963% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2211 \tTraining Loss: 0.01171807 \tValidation Loss 0.01815544 \tTraining Acuuarcy 41.734% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 2212 \tTraining Loss: 0.01159148 \tValidation Loss 0.01843120 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 2213 \tTraining Loss: 0.01162487 \tValidation Loss 0.01825182 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2214 \tTraining Loss: 0.01157245 \tValidation Loss 0.01795245 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 2215 \tTraining Loss: 0.01159220 \tValidation Loss 0.01801591 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2216 \tTraining Loss: 0.01169147 \tValidation Loss 0.01801551 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2217 \tTraining Loss: 0.01157442 \tValidation Loss 0.01805799 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2218 \tTraining Loss: 0.01159390 \tValidation Loss 0.01807047 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2219 \tTraining Loss: 0.01161496 \tValidation Loss 0.01865276 \tTraining Acuuarcy 42.208% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2220 \tTraining Loss: 0.01157469 \tValidation Loss 0.01825775 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2221 \tTraining Loss: 0.01161018 \tValidation Loss 0.01800599 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2222 \tTraining Loss: 0.01155563 \tValidation Loss 0.01865355 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2223 \tTraining Loss: 0.01163180 \tValidation Loss 0.01848643 \tTraining Acuuarcy 42.080% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2224 \tTraining Loss: 0.01161198 \tValidation Loss 0.01816333 \tTraining Acuuarcy 42.286% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 2225 \tTraining Loss: 0.01163165 \tValidation Loss 0.01845226 \tTraining Acuuarcy 42.086% \tValidation Acuuarcy 19.783%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2226 \tTraining Loss: 0.01158143 \tValidation Loss 0.01801469 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2227 \tTraining Loss: 0.01157727 \tValidation Loss 0.01818431 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2228 \tTraining Loss: 0.01173298 \tValidation Loss 0.01829334 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2229 \tTraining Loss: 0.01167308 \tValidation Loss 0.01808980 \tTraining Acuuarcy 41.651% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2230 \tTraining Loss: 0.01170310 \tValidation Loss 0.01836841 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 2231 \tTraining Loss: 0.01163200 \tValidation Loss 0.01812402 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2232 \tTraining Loss: 0.01162654 \tValidation Loss 0.01866078 \tTraining Acuuarcy 42.414% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2233 \tTraining Loss: 0.01168088 \tValidation Loss 0.01799624 \tTraining Acuuarcy 42.236% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2234 \tTraining Loss: 0.01170309 \tValidation Loss 0.01794717 \tTraining Acuuarcy 41.662% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2235 \tTraining Loss: 0.01161140 \tValidation Loss 0.01827896 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 2236 \tTraining Loss: 0.01168267 \tValidation Loss 0.01797770 \tTraining Acuuarcy 41.885% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2237 \tTraining Loss: 0.01158169 \tValidation Loss 0.01769254 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 2238 \tTraining Loss: 0.01167390 \tValidation Loss 0.01779909 \tTraining Acuuarcy 42.158% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2239 \tTraining Loss: 0.01163132 \tValidation Loss 0.01845422 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2240 \tTraining Loss: 0.01158996 \tValidation Loss 0.01883579 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2241 \tTraining Loss: 0.01157958 \tValidation Loss 0.01871751 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2242 \tTraining Loss: 0.01154427 \tValidation Loss 0.01838636 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2243 \tTraining Loss: 0.01157197 \tValidation Loss 0.01856158 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2244 \tTraining Loss: 0.01170266 \tValidation Loss 0.01813862 \tTraining Acuuarcy 42.113% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2245 \tTraining Loss: 0.01163398 \tValidation Loss 0.01832099 \tTraining Acuuarcy 42.052% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2246 \tTraining Loss: 0.01161453 \tValidation Loss 0.01826890 \tTraining Acuuarcy 42.069% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2247 \tTraining Loss: 0.01157612 \tValidation Loss 0.01884684 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 2248 \tTraining Loss: 0.01166862 \tValidation Loss 0.01825275 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2249 \tTraining Loss: 0.01167914 \tValidation Loss 0.01813326 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2250 \tTraining Loss: 0.01162096 \tValidation Loss 0.01815076 \tTraining Acuuarcy 42.414% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2251 \tTraining Loss: 0.01163487 \tValidation Loss 0.01806699 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2252 \tTraining Loss: 0.01168618 \tValidation Loss 0.01812394 \tTraining Acuuarcy 41.723% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2253 \tTraining Loss: 0.01165023 \tValidation Loss 0.01825261 \tTraining Acuuarcy 42.041% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2254 \tTraining Loss: 0.01170698 \tValidation Loss 0.01797751 \tTraining Acuuarcy 41.829% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2255 \tTraining Loss: 0.01154516 \tValidation Loss 0.01819021 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2256 \tTraining Loss: 0.01157658 \tValidation Loss 0.01822991 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2257 \tTraining Loss: 0.01161857 \tValidation Loss 0.01881141 \tTraining Acuuarcy 42.119% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2258 \tTraining Loss: 0.01169499 \tValidation Loss 0.01820160 \tTraining Acuuarcy 42.141% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2259 \tTraining Loss: 0.01163371 \tValidation Loss 0.01801785 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2260 \tTraining Loss: 0.01163808 \tValidation Loss 0.01808755 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2261 \tTraining Loss: 0.01158227 \tValidation Loss 0.01832133 \tTraining Acuuarcy 42.403% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2262 \tTraining Loss: 0.01166280 \tValidation Loss 0.01793720 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2263 \tTraining Loss: 0.01168946 \tValidation Loss 0.01839652 \tTraining Acuuarcy 41.930% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2264 \tTraining Loss: 0.01167515 \tValidation Loss 0.01815901 \tTraining Acuuarcy 41.930% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2265 \tTraining Loss: 0.01172677 \tValidation Loss 0.01811547 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2266 \tTraining Loss: 0.01160202 \tValidation Loss 0.01813537 \tTraining Acuuarcy 42.348% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2267 \tTraining Loss: 0.01155479 \tValidation Loss 0.01807281 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2268 \tTraining Loss: 0.01166508 \tValidation Loss 0.01776229 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2269 \tTraining Loss: 0.01160291 \tValidation Loss 0.01800646 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2270 \tTraining Loss: 0.01154548 \tValidation Loss 0.01851405 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2271 \tTraining Loss: 0.01170407 \tValidation Loss 0.01803417 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 2272 \tTraining Loss: 0.01159322 \tValidation Loss 0.01873883 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2273 \tTraining Loss: 0.01163885 \tValidation Loss 0.01820642 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2274 \tTraining Loss: 0.01163646 \tValidation Loss 0.01826053 \tTraining Acuuarcy 42.292% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2275 \tTraining Loss: 0.01164879 \tValidation Loss 0.01924235 \tTraining Acuuarcy 42.136% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2276 \tTraining Loss: 0.01161570 \tValidation Loss 0.01802922 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 2277 \tTraining Loss: 0.01170194 \tValidation Loss 0.01786798 \tTraining Acuuarcy 42.102% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2278 \tTraining Loss: 0.01159475 \tValidation Loss 0.01833998 \tTraining Acuuarcy 42.186% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2279 \tTraining Loss: 0.01166948 \tValidation Loss 0.01819018 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2280 \tTraining Loss: 0.01164973 \tValidation Loss 0.01793618 \tTraining Acuuarcy 42.113% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2281 \tTraining Loss: 0.01168368 \tValidation Loss 0.01803716 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2282 \tTraining Loss: 0.01165538 \tValidation Loss 0.01812846 \tTraining Acuuarcy 42.013% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2283 \tTraining Loss: 0.01164267 \tValidation Loss 0.01815749 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 2284 \tTraining Loss: 0.01163679 \tValidation Loss 0.01825142 \tTraining Acuuarcy 41.935% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 2285 \tTraining Loss: 0.01159271 \tValidation Loss 0.01790597 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2286 \tTraining Loss: 0.01158750 \tValidation Loss 0.01834123 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2287 \tTraining Loss: 0.01161440 \tValidation Loss 0.01833618 \tTraining Acuuarcy 42.448% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 2288 \tTraining Loss: 0.01155945 \tValidation Loss 0.01831806 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2289 \tTraining Loss: 0.01162102 \tValidation Loss 0.01842787 \tTraining Acuuarcy 42.392% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2290 \tTraining Loss: 0.01161475 \tValidation Loss 0.01805524 \tTraining Acuuarcy 42.314% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2291 \tTraining Loss: 0.01167581 \tValidation Loss 0.01811992 \tTraining Acuuarcy 42.113% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2292 \tTraining Loss: 0.01156118 \tValidation Loss 0.01808542 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 17.888%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2293 \tTraining Loss: 0.01155563 \tValidation Loss 0.01820397 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2294 \tTraining Loss: 0.01164368 \tValidation Loss 0.01794725 \tTraining Acuuarcy 42.236% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2295 \tTraining Loss: 0.01176055 \tValidation Loss 0.01804493 \tTraining Acuuarcy 41.266% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2296 \tTraining Loss: 0.01164396 \tValidation Loss 0.01853402 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2297 \tTraining Loss: 0.01159818 \tValidation Loss 0.01872181 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2298 \tTraining Loss: 0.01159226 \tValidation Loss 0.01853674 \tTraining Acuuarcy 42.253% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2299 \tTraining Loss: 0.01158021 \tValidation Loss 0.01822292 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2300 \tTraining Loss: 0.01164578 \tValidation Loss 0.01805882 \tTraining Acuuarcy 41.863% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2301 \tTraining Loss: 0.01168731 \tValidation Loss 0.01794643 \tTraining Acuuarcy 41.757% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2302 \tTraining Loss: 0.01161168 \tValidation Loss 0.01805946 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 2303 \tTraining Loss: 0.01174982 \tValidation Loss 0.01805820 \tTraining Acuuarcy 42.086% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2304 \tTraining Loss: 0.01157967 \tValidation Loss 0.01865984 \tTraining Acuuarcy 42.147% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2305 \tTraining Loss: 0.01160026 \tValidation Loss 0.01864257 \tTraining Acuuarcy 42.353% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 2306 \tTraining Loss: 0.01157866 \tValidation Loss 0.01820698 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2307 \tTraining Loss: 0.01164214 \tValidation Loss 0.01814191 \tTraining Acuuarcy 42.097% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2308 \tTraining Loss: 0.01157217 \tValidation Loss 0.01822466 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2309 \tTraining Loss: 0.01158287 \tValidation Loss 0.01859479 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2310 \tTraining Loss: 0.01163782 \tValidation Loss 0.01814937 \tTraining Acuuarcy 42.231% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2311 \tTraining Loss: 0.01166346 \tValidation Loss 0.01858935 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2312 \tTraining Loss: 0.01163064 \tValidation Loss 0.01794756 \tTraining Acuuarcy 42.264% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2313 \tTraining Loss: 0.01166296 \tValidation Loss 0.01839824 \tTraining Acuuarcy 41.863% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2314 \tTraining Loss: 0.01163951 \tValidation Loss 0.01852367 \tTraining Acuuarcy 42.448% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2315 \tTraining Loss: 0.01167374 \tValidation Loss 0.01848400 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2316 \tTraining Loss: 0.01158041 \tValidation Loss 0.01810961 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2317 \tTraining Loss: 0.01161539 \tValidation Loss 0.01807821 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2318 \tTraining Loss: 0.01167044 \tValidation Loss 0.01873350 \tTraining Acuuarcy 42.192% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2319 \tTraining Loss: 0.01170229 \tValidation Loss 0.01814357 \tTraining Acuuarcy 41.980% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2320 \tTraining Loss: 0.01156838 \tValidation Loss 0.01840310 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2321 \tTraining Loss: 0.01165770 \tValidation Loss 0.01855583 \tTraining Acuuarcy 42.498% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2322 \tTraining Loss: 0.01168974 \tValidation Loss 0.01849611 \tTraining Acuuarcy 41.985% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 2323 \tTraining Loss: 0.01162935 \tValidation Loss 0.01852340 \tTraining Acuuarcy 42.431% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2324 \tTraining Loss: 0.01162338 \tValidation Loss 0.01815724 \tTraining Acuuarcy 42.448% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 2325 \tTraining Loss: 0.01156434 \tValidation Loss 0.01797871 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2326 \tTraining Loss: 0.01159933 \tValidation Loss 0.01789899 \tTraining Acuuarcy 42.102% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 2327 \tTraining Loss: 0.01170125 \tValidation Loss 0.01835878 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2328 \tTraining Loss: 0.01164163 \tValidation Loss 0.01794343 \tTraining Acuuarcy 42.030% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2329 \tTraining Loss: 0.01159218 \tValidation Loss 0.01834794 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2330 \tTraining Loss: 0.01168258 \tValidation Loss 0.01832953 \tTraining Acuuarcy 41.946% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2331 \tTraining Loss: 0.01160580 \tValidation Loss 0.01831967 \tTraining Acuuarcy 42.197% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2332 \tTraining Loss: 0.01165144 \tValidation Loss 0.01799407 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2333 \tTraining Loss: 0.01159108 \tValidation Loss 0.01860857 \tTraining Acuuarcy 42.426% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2334 \tTraining Loss: 0.01165573 \tValidation Loss 0.01841971 \tTraining Acuuarcy 42.074% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2335 \tTraining Loss: 0.01167631 \tValidation Loss 0.01817967 \tTraining Acuuarcy 42.130% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2336 \tTraining Loss: 0.01167366 \tValidation Loss 0.01849712 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2337 \tTraining Loss: 0.01160535 \tValidation Loss 0.01857195 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2338 \tTraining Loss: 0.01170905 \tValidation Loss 0.01802569 \tTraining Acuuarcy 41.996% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2339 \tTraining Loss: 0.01163492 \tValidation Loss 0.01825717 \tTraining Acuuarcy 42.286% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2340 \tTraining Loss: 0.01172413 \tValidation Loss 0.01861803 \tTraining Acuuarcy 41.573% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2341 \tTraining Loss: 0.01164084 \tValidation Loss 0.01812743 \tTraining Acuuarcy 42.052% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2342 \tTraining Loss: 0.01160299 \tValidation Loss 0.01851395 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2343 \tTraining Loss: 0.01163480 \tValidation Loss 0.01794125 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2344 \tTraining Loss: 0.01163577 \tValidation Loss 0.01812513 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2345 \tTraining Loss: 0.01165191 \tValidation Loss 0.01851374 \tTraining Acuuarcy 41.807% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2346 \tTraining Loss: 0.01166188 \tValidation Loss 0.01820949 \tTraining Acuuarcy 41.807% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2347 \tTraining Loss: 0.01157490 \tValidation Loss 0.01844593 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2348 \tTraining Loss: 0.01166177 \tValidation Loss 0.01836197 \tTraining Acuuarcy 42.286% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2349 \tTraining Loss: 0.01163096 \tValidation Loss 0.01838569 \tTraining Acuuarcy 42.130% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2350 \tTraining Loss: 0.01160579 \tValidation Loss 0.01853241 \tTraining Acuuarcy 42.437% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2351 \tTraining Loss: 0.01167070 \tValidation Loss 0.01815849 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 2352 \tTraining Loss: 0.01169506 \tValidation Loss 0.01804860 \tTraining Acuuarcy 41.891% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2353 \tTraining Loss: 0.01154818 \tValidation Loss 0.01893167 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2354 \tTraining Loss: 0.01150509 \tValidation Loss 0.01821001 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2355 \tTraining Loss: 0.01158636 \tValidation Loss 0.01884840 \tTraining Acuuarcy 42.476% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2356 \tTraining Loss: 0.01159581 \tValidation Loss 0.01787049 \tTraining Acuuarcy 42.158% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2357 \tTraining Loss: 0.01162460 \tValidation Loss 0.01864457 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2358 \tTraining Loss: 0.01154801 \tValidation Loss 0.01796222 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2359 \tTraining Loss: 0.01158622 \tValidation Loss 0.01849266 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 18.334%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2360 \tTraining Loss: 0.01157440 \tValidation Loss 0.01836470 \tTraining Acuuarcy 42.414% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2361 \tTraining Loss: 0.01160037 \tValidation Loss 0.01833854 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2362 \tTraining Loss: 0.01166292 \tValidation Loss 0.01808029 \tTraining Acuuarcy 41.673% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2363 \tTraining Loss: 0.01153738 \tValidation Loss 0.01826278 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2364 \tTraining Loss: 0.01162505 \tValidation Loss 0.01810150 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2365 \tTraining Loss: 0.01166713 \tValidation Loss 0.01843665 \tTraining Acuuarcy 41.785% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2366 \tTraining Loss: 0.01158273 \tValidation Loss 0.01819392 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2367 \tTraining Loss: 0.01166667 \tValidation Loss 0.01839669 \tTraining Acuuarcy 41.829% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2368 \tTraining Loss: 0.01160823 \tValidation Loss 0.01821040 \tTraining Acuuarcy 42.164% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2369 \tTraining Loss: 0.01160873 \tValidation Loss 0.01811366 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 2370 \tTraining Loss: 0.01159371 \tValidation Loss 0.01840906 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2371 \tTraining Loss: 0.01160898 \tValidation Loss 0.01874789 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2372 \tTraining Loss: 0.01172357 \tValidation Loss 0.01798957 \tTraining Acuuarcy 42.192% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2373 \tTraining Loss: 0.01163193 \tValidation Loss 0.01832143 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2374 \tTraining Loss: 0.01162946 \tValidation Loss 0.01814101 \tTraining Acuuarcy 42.069% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2375 \tTraining Loss: 0.01168578 \tValidation Loss 0.01828049 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2376 \tTraining Loss: 0.01165057 \tValidation Loss 0.01824977 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 2377 \tTraining Loss: 0.01164012 \tValidation Loss 0.01860729 \tTraining Acuuarcy 42.247% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2378 \tTraining Loss: 0.01162108 \tValidation Loss 0.01837430 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2379 \tTraining Loss: 0.01158931 \tValidation Loss 0.01833781 \tTraining Acuuarcy 42.665% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2380 \tTraining Loss: 0.01164773 \tValidation Loss 0.01785780 \tTraining Acuuarcy 41.985% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2381 \tTraining Loss: 0.01161743 \tValidation Loss 0.01800245 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2382 \tTraining Loss: 0.01172564 \tValidation Loss 0.01795095 \tTraining Acuuarcy 41.740% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2383 \tTraining Loss: 0.01157693 \tValidation Loss 0.01827017 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 2384 \tTraining Loss: 0.01152558 \tValidation Loss 0.01850514 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2385 \tTraining Loss: 0.01166928 \tValidation Loss 0.01823991 \tTraining Acuuarcy 41.450% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2386 \tTraining Loss: 0.01158463 \tValidation Loss 0.01879610 \tTraining Acuuarcy 42.515% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2387 \tTraining Loss: 0.01154763 \tValidation Loss 0.01821138 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2388 \tTraining Loss: 0.01161433 \tValidation Loss 0.01817082 \tTraining Acuuarcy 42.426% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2389 \tTraining Loss: 0.01157645 \tValidation Loss 0.01884394 \tTraining Acuuarcy 42.403% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2390 \tTraining Loss: 0.01153989 \tValidation Loss 0.01861575 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2391 \tTraining Loss: 0.01162572 \tValidation Loss 0.01765311 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2392 \tTraining Loss: 0.01161300 \tValidation Loss 0.01841077 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 2393 \tTraining Loss: 0.01155115 \tValidation Loss 0.01810342 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2394 \tTraining Loss: 0.01153161 \tValidation Loss 0.01862971 \tTraining Acuuarcy 42.654% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2395 \tTraining Loss: 0.01161328 \tValidation Loss 0.01867111 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2396 \tTraining Loss: 0.01167118 \tValidation Loss 0.01851862 \tTraining Acuuarcy 42.080% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2397 \tTraining Loss: 0.01175589 \tValidation Loss 0.01833385 \tTraining Acuuarcy 40.971% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2398 \tTraining Loss: 0.01159311 \tValidation Loss 0.01863764 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2399 \tTraining Loss: 0.01154003 \tValidation Loss 0.01835515 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2400 \tTraining Loss: 0.01150500 \tValidation Loss 0.01861149 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2401 \tTraining Loss: 0.01158429 \tValidation Loss 0.01824149 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2402 \tTraining Loss: 0.01156271 \tValidation Loss 0.01820189 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2403 \tTraining Loss: 0.01154670 \tValidation Loss 0.01781854 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2404 \tTraining Loss: 0.01152996 \tValidation Loss 0.01822454 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2405 \tTraining Loss: 0.01163006 \tValidation Loss 0.01867822 \tTraining Acuuarcy 42.130% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2406 \tTraining Loss: 0.01166926 \tValidation Loss 0.01801450 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 2407 \tTraining Loss: 0.01156799 \tValidation Loss 0.01860391 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2408 \tTraining Loss: 0.01156941 \tValidation Loss 0.01837369 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2409 \tTraining Loss: 0.01158498 \tValidation Loss 0.01861511 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2410 \tTraining Loss: 0.01162084 \tValidation Loss 0.01834504 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2411 \tTraining Loss: 0.01170001 \tValidation Loss 0.01845657 \tTraining Acuuarcy 41.701% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2412 \tTraining Loss: 0.01160425 \tValidation Loss 0.01758764 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2413 \tTraining Loss: 0.01157202 \tValidation Loss 0.01823484 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2414 \tTraining Loss: 0.01154602 \tValidation Loss 0.01837156 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2415 \tTraining Loss: 0.01152311 \tValidation Loss 0.01882174 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2416 \tTraining Loss: 0.01165266 \tValidation Loss 0.01830684 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2417 \tTraining Loss: 0.01159306 \tValidation Loss 0.01842207 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2418 \tTraining Loss: 0.01161561 \tValidation Loss 0.01836102 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2419 \tTraining Loss: 0.01149247 \tValidation Loss 0.01852744 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2420 \tTraining Loss: 0.01168164 \tValidation Loss 0.01810955 \tTraining Acuuarcy 42.364% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2421 \tTraining Loss: 0.01162175 \tValidation Loss 0.01803939 \tTraining Acuuarcy 42.292% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 2422 \tTraining Loss: 0.01165888 \tValidation Loss 0.01851085 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2423 \tTraining Loss: 0.01163170 \tValidation Loss 0.01813573 \tTraining Acuuarcy 42.476% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 2424 \tTraining Loss: 0.01160150 \tValidation Loss 0.01802776 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2425 \tTraining Loss: 0.01161284 \tValidation Loss 0.01852201 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2426 \tTraining Loss: 0.01161732 \tValidation Loss 0.01800119 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 20.145%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2427 \tTraining Loss: 0.01151902 \tValidation Loss 0.01877704 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2428 \tTraining Loss: 0.01164264 \tValidation Loss 0.01845031 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2429 \tTraining Loss: 0.01160472 \tValidation Loss 0.01838645 \tTraining Acuuarcy 42.231% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2430 \tTraining Loss: 0.01160587 \tValidation Loss 0.01860554 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2431 \tTraining Loss: 0.01154098 \tValidation Loss 0.01829452 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2432 \tTraining Loss: 0.01164524 \tValidation Loss 0.01860285 \tTraining Acuuarcy 41.879% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2433 \tTraining Loss: 0.01166703 \tValidation Loss 0.01777379 \tTraining Acuuarcy 41.718% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 2434 \tTraining Loss: 0.01160730 \tValidation Loss 0.01835350 \tTraining Acuuarcy 42.097% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2435 \tTraining Loss: 0.01162046 \tValidation Loss 0.01835066 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2436 \tTraining Loss: 0.01163133 \tValidation Loss 0.01830892 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2437 \tTraining Loss: 0.01156519 \tValidation Loss 0.01839350 \tTraining Acuuarcy 42.598% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2438 \tTraining Loss: 0.01155832 \tValidation Loss 0.01848150 \tTraining Acuuarcy 42.810% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2439 \tTraining Loss: 0.01166201 \tValidation Loss 0.01795778 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2440 \tTraining Loss: 0.01155657 \tValidation Loss 0.01824404 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2441 \tTraining Loss: 0.01170648 \tValidation Loss 0.01780454 \tTraining Acuuarcy 41.590% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2442 \tTraining Loss: 0.01156800 \tValidation Loss 0.01818599 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2443 \tTraining Loss: 0.01163956 \tValidation Loss 0.01815409 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 2444 \tTraining Loss: 0.01166521 \tValidation Loss 0.01836668 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2445 \tTraining Loss: 0.01157182 \tValidation Loss 0.01882771 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2446 \tTraining Loss: 0.01162197 \tValidation Loss 0.01820171 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2447 \tTraining Loss: 0.01162403 \tValidation Loss 0.01809122 \tTraining Acuuarcy 42.414% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2448 \tTraining Loss: 0.01173225 \tValidation Loss 0.01790068 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 2449 \tTraining Loss: 0.01152824 \tValidation Loss 0.01866631 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2450 \tTraining Loss: 0.01152182 \tValidation Loss 0.01834472 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 2451 \tTraining Loss: 0.01155571 \tValidation Loss 0.01818176 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2452 \tTraining Loss: 0.01162695 \tValidation Loss 0.01823115 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2453 \tTraining Loss: 0.01171799 \tValidation Loss 0.01836191 \tTraining Acuuarcy 41.773% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 2454 \tTraining Loss: 0.01166246 \tValidation Loss 0.01777248 \tTraining Acuuarcy 41.746% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2455 \tTraining Loss: 0.01155738 \tValidation Loss 0.01817364 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2456 \tTraining Loss: 0.01157451 \tValidation Loss 0.01858059 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2457 \tTraining Loss: 0.01157084 \tValidation Loss 0.01841134 \tTraining Acuuarcy 42.526% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2458 \tTraining Loss: 0.01155443 \tValidation Loss 0.01865539 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 2459 \tTraining Loss: 0.01155686 \tValidation Loss 0.01840311 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2460 \tTraining Loss: 0.01159560 \tValidation Loss 0.01833853 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2461 \tTraining Loss: 0.01153645 \tValidation Loss 0.01867533 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 2462 \tTraining Loss: 0.01165541 \tValidation Loss 0.01865130 \tTraining Acuuarcy 41.852% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2463 \tTraining Loss: 0.01167859 \tValidation Loss 0.01818178 \tTraining Acuuarcy 42.152% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2464 \tTraining Loss: 0.01159323 \tValidation Loss 0.01808157 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2465 \tTraining Loss: 0.01169921 \tValidation Loss 0.01834127 \tTraining Acuuarcy 41.773% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2466 \tTraining Loss: 0.01163742 \tValidation Loss 0.01789570 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2467 \tTraining Loss: 0.01166728 \tValidation Loss 0.01779877 \tTraining Acuuarcy 41.952% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2468 \tTraining Loss: 0.01162126 \tValidation Loss 0.01860357 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2469 \tTraining Loss: 0.01157824 \tValidation Loss 0.01808187 \tTraining Acuuarcy 42.342% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2470 \tTraining Loss: 0.01164110 \tValidation Loss 0.01846107 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2471 \tTraining Loss: 0.01166700 \tValidation Loss 0.01836364 \tTraining Acuuarcy 42.314% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2472 \tTraining Loss: 0.01163783 \tValidation Loss 0.01802414 \tTraining Acuuarcy 42.236% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2473 \tTraining Loss: 0.01162276 \tValidation Loss 0.01831240 \tTraining Acuuarcy 42.403% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 2474 \tTraining Loss: 0.01159242 \tValidation Loss 0.01832751 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2475 \tTraining Loss: 0.01163237 \tValidation Loss 0.01865055 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2476 \tTraining Loss: 0.01161176 \tValidation Loss 0.01827600 \tTraining Acuuarcy 41.924% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2477 \tTraining Loss: 0.01166715 \tValidation Loss 0.01845443 \tTraining Acuuarcy 42.186% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2478 \tTraining Loss: 0.01153034 \tValidation Loss 0.01851962 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2479 \tTraining Loss: 0.01162489 \tValidation Loss 0.01822462 \tTraining Acuuarcy 42.453% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2480 \tTraining Loss: 0.01158002 \tValidation Loss 0.01861309 \tTraining Acuuarcy 42.392% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 2481 \tTraining Loss: 0.01155426 \tValidation Loss 0.01860911 \tTraining Acuuarcy 42.470% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2482 \tTraining Loss: 0.01163997 \tValidation Loss 0.01823479 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2483 \tTraining Loss: 0.01158070 \tValidation Loss 0.01833004 \tTraining Acuuarcy 42.403% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2484 \tTraining Loss: 0.01159422 \tValidation Loss 0.01768319 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 2485 \tTraining Loss: 0.01162948 \tValidation Loss 0.01823377 \tTraining Acuuarcy 42.113% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 2486 \tTraining Loss: 0.01164728 \tValidation Loss 0.01861130 \tTraining Acuuarcy 41.879% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2487 \tTraining Loss: 0.01166312 \tValidation Loss 0.01804674 \tTraining Acuuarcy 41.930% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2488 \tTraining Loss: 0.01158598 \tValidation Loss 0.01806394 \tTraining Acuuarcy 42.453% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 2489 \tTraining Loss: 0.01158119 \tValidation Loss 0.01837775 \tTraining Acuuarcy 42.264% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2490 \tTraining Loss: 0.01160190 \tValidation Loss 0.01812233 \tTraining Acuuarcy 42.119% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2491 \tTraining Loss: 0.01151073 \tValidation Loss 0.01780844 \tTraining Acuuarcy 42.777% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2492 \tTraining Loss: 0.01163844 \tValidation Loss 0.01807276 \tTraining Acuuarcy 42.164% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2493 \tTraining Loss: 0.01161327 \tValidation Loss 0.01828738 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 18.640%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2494 \tTraining Loss: 0.01160049 \tValidation Loss 0.01825711 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2495 \tTraining Loss: 0.01161274 \tValidation Loss 0.01868763 \tTraining Acuuarcy 42.141% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2496 \tTraining Loss: 0.01169992 \tValidation Loss 0.01869967 \tTraining Acuuarcy 42.102% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2497 \tTraining Loss: 0.01154773 \tValidation Loss 0.01849298 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 2498 \tTraining Loss: 0.01168012 \tValidation Loss 0.01824384 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2499 \tTraining Loss: 0.01159627 \tValidation Loss 0.01795073 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2500 \tTraining Loss: 0.01155624 \tValidation Loss 0.01847246 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 2501 \tTraining Loss: 0.01154013 \tValidation Loss 0.01925760 \tTraining Acuuarcy 42.548% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2502 \tTraining Loss: 0.01162161 \tValidation Loss 0.01829385 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2503 \tTraining Loss: 0.01168194 \tValidation Loss 0.01842094 \tTraining Acuuarcy 41.907% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2504 \tTraining Loss: 0.01161561 \tValidation Loss 0.01819908 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 2505 \tTraining Loss: 0.01165236 \tValidation Loss 0.01831382 \tTraining Acuuarcy 42.437% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2506 \tTraining Loss: 0.01151003 \tValidation Loss 0.01878639 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2507 \tTraining Loss: 0.01161545 \tValidation Loss 0.01833896 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2508 \tTraining Loss: 0.01151713 \tValidation Loss 0.01855133 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2509 \tTraining Loss: 0.01164374 \tValidation Loss 0.01874049 \tTraining Acuuarcy 42.158% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2510 \tTraining Loss: 0.01163489 \tValidation Loss 0.01809784 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2511 \tTraining Loss: 0.01162498 \tValidation Loss 0.01809883 \tTraining Acuuarcy 42.147% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2512 \tTraining Loss: 0.01155830 \tValidation Loss 0.01823238 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 2513 \tTraining Loss: 0.01165883 \tValidation Loss 0.01797403 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2514 \tTraining Loss: 0.01150446 \tValidation Loss 0.01869455 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 2515 \tTraining Loss: 0.01164690 \tValidation Loss 0.01804238 \tTraining Acuuarcy 42.442% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2516 \tTraining Loss: 0.01169745 \tValidation Loss 0.01838743 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2517 \tTraining Loss: 0.01164757 \tValidation Loss 0.01812788 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2518 \tTraining Loss: 0.01155925 \tValidation Loss 0.01834815 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2519 \tTraining Loss: 0.01165355 \tValidation Loss 0.01839913 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2520 \tTraining Loss: 0.01161590 \tValidation Loss 0.01784296 \tTraining Acuuarcy 42.125% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2521 \tTraining Loss: 0.01161176 \tValidation Loss 0.01820540 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2522 \tTraining Loss: 0.01161679 \tValidation Loss 0.01824095 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 2523 \tTraining Loss: 0.01161281 \tValidation Loss 0.01850131 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2524 \tTraining Loss: 0.01155644 \tValidation Loss 0.01868661 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2525 \tTraining Loss: 0.01154929 \tValidation Loss 0.01834366 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 2526 \tTraining Loss: 0.01155146 \tValidation Loss 0.01889481 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2527 \tTraining Loss: 0.01157950 \tValidation Loss 0.01800443 \tTraining Acuuarcy 42.197% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2528 \tTraining Loss: 0.01160220 \tValidation Loss 0.01840951 \tTraining Acuuarcy 42.348% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2529 \tTraining Loss: 0.01156994 \tValidation Loss 0.01843030 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2530 \tTraining Loss: 0.01171750 \tValidation Loss 0.01800927 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2531 \tTraining Loss: 0.01154656 \tValidation Loss 0.01820488 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2532 \tTraining Loss: 0.01158361 \tValidation Loss 0.01871479 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2533 \tTraining Loss: 0.01163504 \tValidation Loss 0.01804225 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2534 \tTraining Loss: 0.01165123 \tValidation Loss 0.01848652 \tTraining Acuuarcy 42.063% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 2535 \tTraining Loss: 0.01161843 \tValidation Loss 0.01865870 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2536 \tTraining Loss: 0.01162485 \tValidation Loss 0.01817069 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2537 \tTraining Loss: 0.01153813 \tValidation Loss 0.01850850 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 2538 \tTraining Loss: 0.01161276 \tValidation Loss 0.01859142 \tTraining Acuuarcy 42.063% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2539 \tTraining Loss: 0.01159485 \tValidation Loss 0.01842415 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2540 \tTraining Loss: 0.01159940 \tValidation Loss 0.01822304 \tTraining Acuuarcy 42.024% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2541 \tTraining Loss: 0.01153874 \tValidation Loss 0.01807083 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2542 \tTraining Loss: 0.01158826 \tValidation Loss 0.01867555 \tTraining Acuuarcy 42.414% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2543 \tTraining Loss: 0.01158486 \tValidation Loss 0.01811635 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2544 \tTraining Loss: 0.01157802 \tValidation Loss 0.01845778 \tTraining Acuuarcy 42.097% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2545 \tTraining Loss: 0.01154982 \tValidation Loss 0.01794952 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 2546 \tTraining Loss: 0.01161485 \tValidation Loss 0.01837309 \tTraining Acuuarcy 41.896% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2547 \tTraining Loss: 0.01160545 \tValidation Loss 0.01842932 \tTraining Acuuarcy 42.598% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2548 \tTraining Loss: 0.01161238 \tValidation Loss 0.01880340 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2549 \tTraining Loss: 0.01155142 \tValidation Loss 0.01840612 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2550 \tTraining Loss: 0.01158219 \tValidation Loss 0.01912216 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 2551 \tTraining Loss: 0.01160154 \tValidation Loss 0.01857579 \tTraining Acuuarcy 42.192% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2552 \tTraining Loss: 0.01161309 \tValidation Loss 0.01832854 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2553 \tTraining Loss: 0.01165946 \tValidation Loss 0.01793400 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 2554 \tTraining Loss: 0.01168150 \tValidation Loss 0.01859930 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2555 \tTraining Loss: 0.01169335 \tValidation Loss 0.01805848 \tTraining Acuuarcy 41.790% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2556 \tTraining Loss: 0.01157518 \tValidation Loss 0.01849957 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 2557 \tTraining Loss: 0.01158208 \tValidation Loss 0.01820078 \tTraining Acuuarcy 42.704% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 2558 \tTraining Loss: 0.01155666 \tValidation Loss 0.01805491 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2559 \tTraining Loss: 0.01167016 \tValidation Loss 0.01805261 \tTraining Acuuarcy 42.526% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2560 \tTraining Loss: 0.01159836 \tValidation Loss 0.01807016 \tTraining Acuuarcy 41.723% \tValidation Acuuarcy 19.309%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2561 \tTraining Loss: 0.01152806 \tValidation Loss 0.01792978 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2562 \tTraining Loss: 0.01165411 \tValidation Loss 0.01907908 \tTraining Acuuarcy 42.164% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2563 \tTraining Loss: 0.01160710 \tValidation Loss 0.01854220 \tTraining Acuuarcy 42.342% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2564 \tTraining Loss: 0.01163582 \tValidation Loss 0.01832448 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2565 \tTraining Loss: 0.01158953 \tValidation Loss 0.01835180 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2566 \tTraining Loss: 0.01155797 \tValidation Loss 0.01851716 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2567 \tTraining Loss: 0.01163711 \tValidation Loss 0.01835515 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 2568 \tTraining Loss: 0.01164428 \tValidation Loss 0.01833540 \tTraining Acuuarcy 41.941% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2569 \tTraining Loss: 0.01162185 \tValidation Loss 0.01829732 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2570 \tTraining Loss: 0.01171120 \tValidation Loss 0.01827273 \tTraining Acuuarcy 41.539% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 2571 \tTraining Loss: 0.01167885 \tValidation Loss 0.01802369 \tTraining Acuuarcy 42.119% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2572 \tTraining Loss: 0.01160365 \tValidation Loss 0.01835618 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2573 \tTraining Loss: 0.01157829 \tValidation Loss 0.01845698 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2574 \tTraining Loss: 0.01157471 \tValidation Loss 0.01795762 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 2575 \tTraining Loss: 0.01163401 \tValidation Loss 0.01872680 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2576 \tTraining Loss: 0.01161806 \tValidation Loss 0.01852525 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2577 \tTraining Loss: 0.01155281 \tValidation Loss 0.01799938 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 2578 \tTraining Loss: 0.01152963 \tValidation Loss 0.01837128 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2579 \tTraining Loss: 0.01161871 \tValidation Loss 0.01782985 \tTraining Acuuarcy 42.231% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2580 \tTraining Loss: 0.01159665 \tValidation Loss 0.01846650 \tTraining Acuuarcy 42.164% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2581 \tTraining Loss: 0.01168919 \tValidation Loss 0.01803441 \tTraining Acuuarcy 42.353% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2582 \tTraining Loss: 0.01160630 \tValidation Loss 0.01827373 \tTraining Acuuarcy 42.247% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2583 \tTraining Loss: 0.01159950 \tValidation Loss 0.01828638 \tTraining Acuuarcy 42.030% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2584 \tTraining Loss: 0.01157010 \tValidation Loss 0.01855271 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2585 \tTraining Loss: 0.01156730 \tValidation Loss 0.01818887 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2586 \tTraining Loss: 0.01170004 \tValidation Loss 0.01822163 \tTraining Acuuarcy 41.773% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2587 \tTraining Loss: 0.01159012 \tValidation Loss 0.01834420 \tTraining Acuuarcy 42.186% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2588 \tTraining Loss: 0.01163638 \tValidation Loss 0.01799354 \tTraining Acuuarcy 42.069% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 2589 \tTraining Loss: 0.01154916 \tValidation Loss 0.01814799 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2590 \tTraining Loss: 0.01166790 \tValidation Loss 0.01928803 \tTraining Acuuarcy 41.824% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2591 \tTraining Loss: 0.01156669 \tValidation Loss 0.01806989 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2592 \tTraining Loss: 0.01166653 \tValidation Loss 0.01835944 \tTraining Acuuarcy 41.840% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2593 \tTraining Loss: 0.01158622 \tValidation Loss 0.01861207 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2594 \tTraining Loss: 0.01152492 \tValidation Loss 0.01826520 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2595 \tTraining Loss: 0.01161145 \tValidation Loss 0.01842205 \tTraining Acuuarcy 42.364% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2596 \tTraining Loss: 0.01164865 \tValidation Loss 0.01789458 \tTraining Acuuarcy 42.141% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2597 \tTraining Loss: 0.01141600 \tValidation Loss 0.01808085 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2598 \tTraining Loss: 0.01159329 \tValidation Loss 0.01844825 \tTraining Acuuarcy 42.777% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2599 \tTraining Loss: 0.01151344 \tValidation Loss 0.01868136 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2600 \tTraining Loss: 0.01160837 \tValidation Loss 0.01821609 \tTraining Acuuarcy 42.387% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2601 \tTraining Loss: 0.01161822 \tValidation Loss 0.01784823 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2602 \tTraining Loss: 0.01163870 \tValidation Loss 0.01857151 \tTraining Acuuarcy 41.974% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2603 \tTraining Loss: 0.01158884 \tValidation Loss 0.01830275 \tTraining Acuuarcy 42.654% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2604 \tTraining Loss: 0.01162578 \tValidation Loss 0.01833427 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2605 \tTraining Loss: 0.01157891 \tValidation Loss 0.01769640 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 2606 \tTraining Loss: 0.01162662 \tValidation Loss 0.01826355 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2607 \tTraining Loss: 0.01162311 \tValidation Loss 0.01804463 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2608 \tTraining Loss: 0.01155618 \tValidation Loss 0.01835202 \tTraining Acuuarcy 42.476% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2609 \tTraining Loss: 0.01164167 \tValidation Loss 0.01881083 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2610 \tTraining Loss: 0.01163217 \tValidation Loss 0.01834050 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2611 \tTraining Loss: 0.01165466 \tValidation Loss 0.01831777 \tTraining Acuuarcy 42.186% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 2612 \tTraining Loss: 0.01150903 \tValidation Loss 0.01820438 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2613 \tTraining Loss: 0.01159564 \tValidation Loss 0.01776382 \tTraining Acuuarcy 42.548% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 2614 \tTraining Loss: 0.01161210 \tValidation Loss 0.01848529 \tTraining Acuuarcy 42.431% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2615 \tTraining Loss: 0.01154723 \tValidation Loss 0.01834921 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2616 \tTraining Loss: 0.01160203 \tValidation Loss 0.01825678 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2617 \tTraining Loss: 0.01162781 \tValidation Loss 0.01789211 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2618 \tTraining Loss: 0.01157812 \tValidation Loss 0.01772006 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 2619 \tTraining Loss: 0.01162267 \tValidation Loss 0.01790865 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2620 \tTraining Loss: 0.01162071 \tValidation Loss 0.01816456 \tTraining Acuuarcy 42.236% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2621 \tTraining Loss: 0.01160042 \tValidation Loss 0.01895752 \tTraining Acuuarcy 42.526% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2622 \tTraining Loss: 0.01172245 \tValidation Loss 0.01837164 \tTraining Acuuarcy 42.203% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2623 \tTraining Loss: 0.01154631 \tValidation Loss 0.01844213 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 2624 \tTraining Loss: 0.01151419 \tValidation Loss 0.01848847 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2625 \tTraining Loss: 0.01159242 \tValidation Loss 0.01878454 \tTraining Acuuarcy 42.180% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2626 \tTraining Loss: 0.01152140 \tValidation Loss 0.01850220 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2627 \tTraining Loss: 0.01155632 \tValidation Loss 0.01830419 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.699%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2628 \tTraining Loss: 0.01160818 \tValidation Loss 0.01845801 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2629 \tTraining Loss: 0.01160988 \tValidation Loss 0.01816792 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2630 \tTraining Loss: 0.01160432 \tValidation Loss 0.01796790 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2631 \tTraining Loss: 0.01158555 \tValidation Loss 0.01826819 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2632 \tTraining Loss: 0.01157357 \tValidation Loss 0.01838624 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2633 \tTraining Loss: 0.01158140 \tValidation Loss 0.01810112 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2634 \tTraining Loss: 0.01165298 \tValidation Loss 0.01839151 \tTraining Acuuarcy 42.292% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 2635 \tTraining Loss: 0.01153151 \tValidation Loss 0.01875372 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2636 \tTraining Loss: 0.01158475 \tValidation Loss 0.01853339 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2637 \tTraining Loss: 0.01157611 \tValidation Loss 0.01824622 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2638 \tTraining Loss: 0.01155865 \tValidation Loss 0.01859924 \tTraining Acuuarcy 42.453% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2639 \tTraining Loss: 0.01154912 \tValidation Loss 0.01814445 \tTraining Acuuarcy 43.016% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2640 \tTraining Loss: 0.01153577 \tValidation Loss 0.01850670 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2641 \tTraining Loss: 0.01170932 \tValidation Loss 0.01806483 \tTraining Acuuarcy 41.718% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2642 \tTraining Loss: 0.01158313 \tValidation Loss 0.01790426 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2643 \tTraining Loss: 0.01159772 \tValidation Loss 0.01838316 \tTraining Acuuarcy 42.348% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2644 \tTraining Loss: 0.01160612 \tValidation Loss 0.01877902 \tTraining Acuuarcy 42.113% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2645 \tTraining Loss: 0.01163260 \tValidation Loss 0.01874192 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2646 \tTraining Loss: 0.01155822 \tValidation Loss 0.01792544 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2647 \tTraining Loss: 0.01160491 \tValidation Loss 0.01852274 \tTraining Acuuarcy 42.086% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 2648 \tTraining Loss: 0.01157104 \tValidation Loss 0.01843450 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2649 \tTraining Loss: 0.01158034 \tValidation Loss 0.01827256 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2650 \tTraining Loss: 0.01164795 \tValidation Loss 0.01811214 \tTraining Acuuarcy 42.069% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2651 \tTraining Loss: 0.01169841 \tValidation Loss 0.01846736 \tTraining Acuuarcy 41.712% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2652 \tTraining Loss: 0.01160386 \tValidation Loss 0.01854308 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2653 \tTraining Loss: 0.01157704 \tValidation Loss 0.01817632 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2654 \tTraining Loss: 0.01152607 \tValidation Loss 0.01837933 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2655 \tTraining Loss: 0.01156977 \tValidation Loss 0.01823256 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 2656 \tTraining Loss: 0.01154509 \tValidation Loss 0.01839620 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2657 \tTraining Loss: 0.01149976 \tValidation Loss 0.01805576 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2658 \tTraining Loss: 0.01153201 \tValidation Loss 0.01850751 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2659 \tTraining Loss: 0.01146215 \tValidation Loss 0.01843845 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 2660 \tTraining Loss: 0.01159250 \tValidation Loss 0.01845829 \tTraining Acuuarcy 42.476% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2661 \tTraining Loss: 0.01164870 \tValidation Loss 0.01849533 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2662 \tTraining Loss: 0.01161139 \tValidation Loss 0.01837672 \tTraining Acuuarcy 42.074% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2663 \tTraining Loss: 0.01154440 \tValidation Loss 0.01824051 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 2664 \tTraining Loss: 0.01162099 \tValidation Loss 0.01825226 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2665 \tTraining Loss: 0.01158077 \tValidation Loss 0.01865057 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2666 \tTraining Loss: 0.01152878 \tValidation Loss 0.01806666 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2667 \tTraining Loss: 0.01153585 \tValidation Loss 0.01846575 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2668 \tTraining Loss: 0.01158267 \tValidation Loss 0.01828534 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2669 \tTraining Loss: 0.01161776 \tValidation Loss 0.01801241 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 2670 \tTraining Loss: 0.01155148 \tValidation Loss 0.01812234 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2671 \tTraining Loss: 0.01158042 \tValidation Loss 0.01802195 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2672 \tTraining Loss: 0.01163166 \tValidation Loss 0.01832612 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2673 \tTraining Loss: 0.01159304 \tValidation Loss 0.01853013 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2674 \tTraining Loss: 0.01155077 \tValidation Loss 0.01846064 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2675 \tTraining Loss: 0.01162082 \tValidation Loss 0.01853789 \tTraining Acuuarcy 42.314% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2676 \tTraining Loss: 0.01157000 \tValidation Loss 0.01813670 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2677 \tTraining Loss: 0.01160287 \tValidation Loss 0.01833346 \tTraining Acuuarcy 42.309% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2678 \tTraining Loss: 0.01151464 \tValidation Loss 0.01867991 \tTraining Acuuarcy 42.654% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2679 \tTraining Loss: 0.01159556 \tValidation Loss 0.01802998 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 2680 \tTraining Loss: 0.01155197 \tValidation Loss 0.01789330 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2681 \tTraining Loss: 0.01161517 \tValidation Loss 0.01814030 \tTraining Acuuarcy 42.576% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2682 \tTraining Loss: 0.01160909 \tValidation Loss 0.01830867 \tTraining Acuuarcy 42.242% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2683 \tTraining Loss: 0.01161086 \tValidation Loss 0.01815304 \tTraining Acuuarcy 42.074% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2684 \tTraining Loss: 0.01163043 \tValidation Loss 0.01828372 \tTraining Acuuarcy 42.342% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2685 \tTraining Loss: 0.01154585 \tValidation Loss 0.01834690 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2686 \tTraining Loss: 0.01162418 \tValidation Loss 0.01838486 \tTraining Acuuarcy 42.175% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2687 \tTraining Loss: 0.01158621 \tValidation Loss 0.01832452 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2688 \tTraining Loss: 0.01148984 \tValidation Loss 0.01864878 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 2689 \tTraining Loss: 0.01163071 \tValidation Loss 0.01841733 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2690 \tTraining Loss: 0.01160118 \tValidation Loss 0.01815493 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2691 \tTraining Loss: 0.01161197 \tValidation Loss 0.01797245 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2692 \tTraining Loss: 0.01160091 \tValidation Loss 0.01828401 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2693 \tTraining Loss: 0.01147245 \tValidation Loss 0.01812143 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2694 \tTraining Loss: 0.01153059 \tValidation Loss 0.01838819 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 20.563%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2695 \tTraining Loss: 0.01154427 \tValidation Loss 0.01825887 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 2696 \tTraining Loss: 0.01155786 \tValidation Loss 0.01900919 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2697 \tTraining Loss: 0.01156277 \tValidation Loss 0.01835048 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2698 \tTraining Loss: 0.01150308 \tValidation Loss 0.01872655 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2699 \tTraining Loss: 0.01162521 \tValidation Loss 0.01817392 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 2700 \tTraining Loss: 0.01165978 \tValidation Loss 0.01809468 \tTraining Acuuarcy 42.253% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 2701 \tTraining Loss: 0.01160732 \tValidation Loss 0.01887872 \tTraining Acuuarcy 42.063% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2702 \tTraining Loss: 0.01162292 \tValidation Loss 0.01870426 \tTraining Acuuarcy 42.359% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2703 \tTraining Loss: 0.01159632 \tValidation Loss 0.01819935 \tTraining Acuuarcy 42.498% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2704 \tTraining Loss: 0.01163908 \tValidation Loss 0.01805889 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2705 \tTraining Loss: 0.01155376 \tValidation Loss 0.01864576 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2706 \tTraining Loss: 0.01157195 \tValidation Loss 0.01848884 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2707 \tTraining Loss: 0.01156239 \tValidation Loss 0.01805233 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2708 \tTraining Loss: 0.01162805 \tValidation Loss 0.01806185 \tTraining Acuuarcy 42.192% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2709 \tTraining Loss: 0.01166059 \tValidation Loss 0.01783136 \tTraining Acuuarcy 42.331% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2710 \tTraining Loss: 0.01158055 \tValidation Loss 0.01875521 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2711 \tTraining Loss: 0.01154857 \tValidation Loss 0.01834402 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 2712 \tTraining Loss: 0.01165853 \tValidation Loss 0.01817724 \tTraining Acuuarcy 41.768% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2713 \tTraining Loss: 0.01161602 \tValidation Loss 0.01823106 \tTraining Acuuarcy 42.013% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2714 \tTraining Loss: 0.01161621 \tValidation Loss 0.01859204 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2715 \tTraining Loss: 0.01162936 \tValidation Loss 0.01815507 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2716 \tTraining Loss: 0.01149613 \tValidation Loss 0.01840414 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2717 \tTraining Loss: 0.01153140 \tValidation Loss 0.01860642 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2718 \tTraining Loss: 0.01153643 \tValidation Loss 0.01852405 \tTraining Acuuarcy 43.016% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2719 \tTraining Loss: 0.01163221 \tValidation Loss 0.01818518 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2720 \tTraining Loss: 0.01164721 \tValidation Loss 0.01820037 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2721 \tTraining Loss: 0.01160049 \tValidation Loss 0.01779078 \tTraining Acuuarcy 42.403% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2722 \tTraining Loss: 0.01163119 \tValidation Loss 0.01794929 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2723 \tTraining Loss: 0.01167125 \tValidation Loss 0.01821022 \tTraining Acuuarcy 41.913% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 2724 \tTraining Loss: 0.01158496 \tValidation Loss 0.01851642 \tTraining Acuuarcy 42.777% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 2725 \tTraining Loss: 0.01166674 \tValidation Loss 0.01861382 \tTraining Acuuarcy 41.941% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 2726 \tTraining Loss: 0.01159974 \tValidation Loss 0.01854872 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 2727 \tTraining Loss: 0.01150553 \tValidation Loss 0.01808251 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2728 \tTraining Loss: 0.01151285 \tValidation Loss 0.01838224 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2729 \tTraining Loss: 0.01151640 \tValidation Loss 0.01831464 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2730 \tTraining Loss: 0.01156298 \tValidation Loss 0.01808515 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 2731 \tTraining Loss: 0.01164966 \tValidation Loss 0.01801165 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2732 \tTraining Loss: 0.01163047 \tValidation Loss 0.01813965 \tTraining Acuuarcy 42.208% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 2733 \tTraining Loss: 0.01158168 \tValidation Loss 0.01872505 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2734 \tTraining Loss: 0.01152459 \tValidation Loss 0.01817652 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2735 \tTraining Loss: 0.01161808 \tValidation Loss 0.01816725 \tTraining Acuuarcy 42.147% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2736 \tTraining Loss: 0.01159753 \tValidation Loss 0.01858958 \tTraining Acuuarcy 42.041% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2737 \tTraining Loss: 0.01151748 \tValidation Loss 0.01879872 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2738 \tTraining Loss: 0.01158470 \tValidation Loss 0.01915779 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2739 \tTraining Loss: 0.01158310 \tValidation Loss 0.01829406 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2740 \tTraining Loss: 0.01157175 \tValidation Loss 0.01826616 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 2741 \tTraining Loss: 0.01155777 \tValidation Loss 0.01856634 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2742 \tTraining Loss: 0.01153579 \tValidation Loss 0.01851288 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 2743 \tTraining Loss: 0.01145595 \tValidation Loss 0.01856728 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2744 \tTraining Loss: 0.01160541 \tValidation Loss 0.01862695 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2745 \tTraining Loss: 0.01156505 \tValidation Loss 0.01825952 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2746 \tTraining Loss: 0.01167129 \tValidation Loss 0.01788800 \tTraining Acuuarcy 42.158% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2747 \tTraining Loss: 0.01154129 \tValidation Loss 0.01848922 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2748 \tTraining Loss: 0.01154391 \tValidation Loss 0.01817023 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2749 \tTraining Loss: 0.01155828 \tValidation Loss 0.01877162 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2750 \tTraining Loss: 0.01162524 \tValidation Loss 0.01841153 \tTraining Acuuarcy 42.141% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 2751 \tTraining Loss: 0.01161595 \tValidation Loss 0.01868299 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 2752 \tTraining Loss: 0.01153440 \tValidation Loss 0.01867437 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2753 \tTraining Loss: 0.01158784 \tValidation Loss 0.01826114 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2754 \tTraining Loss: 0.01160925 \tValidation Loss 0.01819773 \tTraining Acuuarcy 42.426% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 2755 \tTraining Loss: 0.01153389 \tValidation Loss 0.01854974 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2756 \tTraining Loss: 0.01156457 \tValidation Loss 0.01833563 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2757 \tTraining Loss: 0.01160399 \tValidation Loss 0.01847130 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 2758 \tTraining Loss: 0.01159439 \tValidation Loss 0.01799578 \tTraining Acuuarcy 42.604% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2759 \tTraining Loss: 0.01144597 \tValidation Loss 0.01854866 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2760 \tTraining Loss: 0.01158491 \tValidation Loss 0.01823863 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2761 \tTraining Loss: 0.01148749 \tValidation Loss 0.01940505 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.142%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2762 \tTraining Loss: 0.01155501 \tValidation Loss 0.01827825 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2763 \tTraining Loss: 0.01152325 \tValidation Loss 0.01854730 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2764 \tTraining Loss: 0.01163529 \tValidation Loss 0.01806803 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2765 \tTraining Loss: 0.01156935 \tValidation Loss 0.01811020 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2766 \tTraining Loss: 0.01157394 \tValidation Loss 0.01850414 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2767 \tTraining Loss: 0.01156029 \tValidation Loss 0.01843075 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 2768 \tTraining Loss: 0.01157987 \tValidation Loss 0.01836055 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 2769 \tTraining Loss: 0.01157719 \tValidation Loss 0.01849529 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2770 \tTraining Loss: 0.01170255 \tValidation Loss 0.01842462 \tTraining Acuuarcy 41.668% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2771 \tTraining Loss: 0.01153330 \tValidation Loss 0.01829090 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2772 \tTraining Loss: 0.01153109 \tValidation Loss 0.01850720 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2773 \tTraining Loss: 0.01157040 \tValidation Loss 0.01833057 \tTraining Acuuarcy 42.437% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2774 \tTraining Loss: 0.01152200 \tValidation Loss 0.01850171 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2775 \tTraining Loss: 0.01159910 \tValidation Loss 0.01812597 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 2776 \tTraining Loss: 0.01152310 \tValidation Loss 0.01867144 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2777 \tTraining Loss: 0.01165864 \tValidation Loss 0.01802494 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2778 \tTraining Loss: 0.01156170 \tValidation Loss 0.01840534 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2779 \tTraining Loss: 0.01166615 \tValidation Loss 0.01777507 \tTraining Acuuarcy 41.885% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2780 \tTraining Loss: 0.01158165 \tValidation Loss 0.01852093 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2781 \tTraining Loss: 0.01172547 \tValidation Loss 0.01778801 \tTraining Acuuarcy 41.907% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2782 \tTraining Loss: 0.01158437 \tValidation Loss 0.01822742 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2783 \tTraining Loss: 0.01155758 \tValidation Loss 0.01807779 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2784 \tTraining Loss: 0.01155532 \tValidation Loss 0.01843349 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2785 \tTraining Loss: 0.01147873 \tValidation Loss 0.01813799 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 2786 \tTraining Loss: 0.01162265 \tValidation Loss 0.01807512 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2787 \tTraining Loss: 0.01152589 \tValidation Loss 0.01808776 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 2788 \tTraining Loss: 0.01158849 \tValidation Loss 0.01845349 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2789 \tTraining Loss: 0.01156911 \tValidation Loss 0.01828177 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 2790 \tTraining Loss: 0.01158250 \tValidation Loss 0.01843995 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2791 \tTraining Loss: 0.01150685 \tValidation Loss 0.01835918 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2792 \tTraining Loss: 0.01154659 \tValidation Loss 0.01897754 \tTraining Acuuarcy 42.810% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2793 \tTraining Loss: 0.01161659 \tValidation Loss 0.01870526 \tTraining Acuuarcy 42.442% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 2794 \tTraining Loss: 0.01157199 \tValidation Loss 0.01856895 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2795 \tTraining Loss: 0.01156206 \tValidation Loss 0.01834504 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 2796 \tTraining Loss: 0.01159999 \tValidation Loss 0.01784463 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2797 \tTraining Loss: 0.01153962 \tValidation Loss 0.01815728 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 2798 \tTraining Loss: 0.01155814 \tValidation Loss 0.01856410 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2799 \tTraining Loss: 0.01160710 \tValidation Loss 0.01877394 \tTraining Acuuarcy 42.777% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2800 \tTraining Loss: 0.01154777 \tValidation Loss 0.01817138 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2801 \tTraining Loss: 0.01151557 \tValidation Loss 0.01847568 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2802 \tTraining Loss: 0.01152970 \tValidation Loss 0.01879531 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2803 \tTraining Loss: 0.01167304 \tValidation Loss 0.01800307 \tTraining Acuuarcy 42.247% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2804 \tTraining Loss: 0.01154343 \tValidation Loss 0.01828137 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2805 \tTraining Loss: 0.01150619 \tValidation Loss 0.01873422 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2806 \tTraining Loss: 0.01157846 \tValidation Loss 0.01859137 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 2807 \tTraining Loss: 0.01156400 \tValidation Loss 0.01805217 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2808 \tTraining Loss: 0.01157926 \tValidation Loss 0.01849257 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2809 \tTraining Loss: 0.01155944 \tValidation Loss 0.01834942 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2810 \tTraining Loss: 0.01170042 \tValidation Loss 0.01814727 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2811 \tTraining Loss: 0.01159673 \tValidation Loss 0.01803990 \tTraining Acuuarcy 42.426% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2812 \tTraining Loss: 0.01153780 \tValidation Loss 0.01877940 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2813 \tTraining Loss: 0.01155339 \tValidation Loss 0.01824016 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2814 \tTraining Loss: 0.01151865 \tValidation Loss 0.01828991 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2815 \tTraining Loss: 0.01163768 \tValidation Loss 0.01840551 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2816 \tTraining Loss: 0.01156698 \tValidation Loss 0.01821157 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 2817 \tTraining Loss: 0.01153108 \tValidation Loss 0.01880314 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2818 \tTraining Loss: 0.01164944 \tValidation Loss 0.01831081 \tTraining Acuuarcy 42.236% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2819 \tTraining Loss: 0.01156919 \tValidation Loss 0.01809299 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2820 \tTraining Loss: 0.01157146 \tValidation Loss 0.01909828 \tTraining Acuuarcy 42.336% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2821 \tTraining Loss: 0.01166444 \tValidation Loss 0.01835052 \tTraining Acuuarcy 42.342% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2822 \tTraining Loss: 0.01155780 \tValidation Loss 0.01843986 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2823 \tTraining Loss: 0.01155372 \tValidation Loss 0.01864881 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2824 \tTraining Loss: 0.01157927 \tValidation Loss 0.01821601 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2825 \tTraining Loss: 0.01144463 \tValidation Loss 0.01816243 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2826 \tTraining Loss: 0.01160728 \tValidation Loss 0.01857646 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2827 \tTraining Loss: 0.01164069 \tValidation Loss 0.01784824 \tTraining Acuuarcy 42.247% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 2828 \tTraining Loss: 0.01158477 \tValidation Loss 0.01873874 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 20.256%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2829 \tTraining Loss: 0.01157429 \tValidation Loss 0.01836442 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2830 \tTraining Loss: 0.01153784 \tValidation Loss 0.01830651 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2831 \tTraining Loss: 0.01156914 \tValidation Loss 0.01819937 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2832 \tTraining Loss: 0.01165193 \tValidation Loss 0.01847635 \tTraining Acuuarcy 42.052% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2833 \tTraining Loss: 0.01158998 \tValidation Loss 0.01838127 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2834 \tTraining Loss: 0.01163217 \tValidation Loss 0.01797511 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2835 \tTraining Loss: 0.01155860 \tValidation Loss 0.01820446 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2836 \tTraining Loss: 0.01161593 \tValidation Loss 0.01839941 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2837 \tTraining Loss: 0.01154740 \tValidation Loss 0.01840711 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 2838 \tTraining Loss: 0.01161264 \tValidation Loss 0.01867833 \tTraining Acuuarcy 42.275% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2839 \tTraining Loss: 0.01157154 \tValidation Loss 0.01813071 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2840 \tTraining Loss: 0.01159130 \tValidation Loss 0.01912398 \tTraining Acuuarcy 42.342% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 2841 \tTraining Loss: 0.01151746 \tValidation Loss 0.01814888 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2842 \tTraining Loss: 0.01159169 \tValidation Loss 0.01851531 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2843 \tTraining Loss: 0.01156822 \tValidation Loss 0.01803112 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2844 \tTraining Loss: 0.01153720 \tValidation Loss 0.01783778 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 2845 \tTraining Loss: 0.01161085 \tValidation Loss 0.01831047 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2846 \tTraining Loss: 0.01164208 \tValidation Loss 0.01792750 \tTraining Acuuarcy 42.342% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2847 \tTraining Loss: 0.01163447 \tValidation Loss 0.01818083 \tTraining Acuuarcy 42.437% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2848 \tTraining Loss: 0.01153478 \tValidation Loss 0.01792664 \tTraining Acuuarcy 42.665% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2849 \tTraining Loss: 0.01154597 \tValidation Loss 0.01878791 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2850 \tTraining Loss: 0.01158909 \tValidation Loss 0.01850240 \tTraining Acuuarcy 42.476% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 2851 \tTraining Loss: 0.01155291 \tValidation Loss 0.01829231 \tTraining Acuuarcy 42.348% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2852 \tTraining Loss: 0.01161763 \tValidation Loss 0.01817366 \tTraining Acuuarcy 42.470% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2853 \tTraining Loss: 0.01159482 \tValidation Loss 0.01809159 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2854 \tTraining Loss: 0.01154973 \tValidation Loss 0.01818937 \tTraining Acuuarcy 42.086% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2855 \tTraining Loss: 0.01153725 \tValidation Loss 0.01863996 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2856 \tTraining Loss: 0.01147729 \tValidation Loss 0.01813641 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2857 \tTraining Loss: 0.01157849 \tValidation Loss 0.01825146 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2858 \tTraining Loss: 0.01154323 \tValidation Loss 0.01826594 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 2859 \tTraining Loss: 0.01145893 \tValidation Loss 0.01839152 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2860 \tTraining Loss: 0.01156675 \tValidation Loss 0.01780994 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2861 \tTraining Loss: 0.01158089 \tValidation Loss 0.01834151 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2862 \tTraining Loss: 0.01161574 \tValidation Loss 0.01839927 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2863 \tTraining Loss: 0.01150501 \tValidation Loss 0.01861106 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2864 \tTraining Loss: 0.01155129 \tValidation Loss 0.01820831 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2865 \tTraining Loss: 0.01157422 \tValidation Loss 0.01885727 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2866 \tTraining Loss: 0.01157332 \tValidation Loss 0.01807832 \tTraining Acuuarcy 42.309% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2867 \tTraining Loss: 0.01155384 \tValidation Loss 0.01813851 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2868 \tTraining Loss: 0.01160003 \tValidation Loss 0.01860920 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2869 \tTraining Loss: 0.01157557 \tValidation Loss 0.01830082 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2870 \tTraining Loss: 0.01147943 \tValidation Loss 0.01908296 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2871 \tTraining Loss: 0.01154275 \tValidation Loss 0.01862153 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2872 \tTraining Loss: 0.01164276 \tValidation Loss 0.01846210 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2873 \tTraining Loss: 0.01158466 \tValidation Loss 0.01852322 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2874 \tTraining Loss: 0.01160306 \tValidation Loss 0.01807345 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2875 \tTraining Loss: 0.01168498 \tValidation Loss 0.01809034 \tTraining Acuuarcy 42.047% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 2876 \tTraining Loss: 0.01161279 \tValidation Loss 0.01857346 \tTraining Acuuarcy 42.437% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2877 \tTraining Loss: 0.01150200 \tValidation Loss 0.01800827 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2878 \tTraining Loss: 0.01153016 \tValidation Loss 0.01905171 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 2879 \tTraining Loss: 0.01153325 \tValidation Loss 0.01793056 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 2880 \tTraining Loss: 0.01156067 \tValidation Loss 0.01811788 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2881 \tTraining Loss: 0.01152669 \tValidation Loss 0.01897134 \tTraining Acuuarcy 42.665% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2882 \tTraining Loss: 0.01158552 \tValidation Loss 0.01879639 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2883 \tTraining Loss: 0.01160519 \tValidation Loss 0.01813273 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2884 \tTraining Loss: 0.01153407 \tValidation Loss 0.01826760 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2885 \tTraining Loss: 0.01150395 \tValidation Loss 0.01852068 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2886 \tTraining Loss: 0.01157758 \tValidation Loss 0.01843997 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2887 \tTraining Loss: 0.01153694 \tValidation Loss 0.01815064 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 2888 \tTraining Loss: 0.01156775 \tValidation Loss 0.01844369 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 2889 \tTraining Loss: 0.01156966 \tValidation Loss 0.01823101 \tTraining Acuuarcy 42.715% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2890 \tTraining Loss: 0.01161642 \tValidation Loss 0.01845123 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 2891 \tTraining Loss: 0.01165092 \tValidation Loss 0.01818848 \tTraining Acuuarcy 42.136% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2892 \tTraining Loss: 0.01151771 \tValidation Loss 0.01884069 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2893 \tTraining Loss: 0.01165973 \tValidation Loss 0.01819867 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2894 \tTraining Loss: 0.01158101 \tValidation Loss 0.01861675 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2895 \tTraining Loss: 0.01146105 \tValidation Loss 0.01848894 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 18.752%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2896 \tTraining Loss: 0.01159696 \tValidation Loss 0.01792854 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 2897 \tTraining Loss: 0.01157442 \tValidation Loss 0.01813811 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2898 \tTraining Loss: 0.01150122 \tValidation Loss 0.01809491 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2899 \tTraining Loss: 0.01164849 \tValidation Loss 0.01803762 \tTraining Acuuarcy 42.353% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2900 \tTraining Loss: 0.01154008 \tValidation Loss 0.01876094 \tTraining Acuuarcy 42.442% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2901 \tTraining Loss: 0.01152777 \tValidation Loss 0.01797632 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 2902 \tTraining Loss: 0.01164640 \tValidation Loss 0.01813031 \tTraining Acuuarcy 41.924% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2903 \tTraining Loss: 0.01150235 \tValidation Loss 0.01868834 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 2904 \tTraining Loss: 0.01159723 \tValidation Loss 0.01778563 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2905 \tTraining Loss: 0.01155191 \tValidation Loss 0.01827479 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2906 \tTraining Loss: 0.01156374 \tValidation Loss 0.01838085 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2907 \tTraining Loss: 0.01153769 \tValidation Loss 0.01861329 \tTraining Acuuarcy 42.810% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2908 \tTraining Loss: 0.01165374 \tValidation Loss 0.01787407 \tTraining Acuuarcy 42.336% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2909 \tTraining Loss: 0.01161124 \tValidation Loss 0.01852551 \tTraining Acuuarcy 42.682% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2910 \tTraining Loss: 0.01157265 \tValidation Loss 0.01808885 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2911 \tTraining Loss: 0.01166030 \tValidation Loss 0.01810771 \tTraining Acuuarcy 42.231% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2912 \tTraining Loss: 0.01157862 \tValidation Loss 0.01851234 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2913 \tTraining Loss: 0.01153616 \tValidation Loss 0.01849597 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2914 \tTraining Loss: 0.01160061 \tValidation Loss 0.01827346 \tTraining Acuuarcy 42.548% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2915 \tTraining Loss: 0.01153307 \tValidation Loss 0.01930733 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2916 \tTraining Loss: 0.01160585 \tValidation Loss 0.01842288 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 2917 \tTraining Loss: 0.01149343 \tValidation Loss 0.01827836 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2918 \tTraining Loss: 0.01162834 \tValidation Loss 0.01869030 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2919 \tTraining Loss: 0.01160759 \tValidation Loss 0.01822269 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2920 \tTraining Loss: 0.01154689 \tValidation Loss 0.01846871 \tTraining Acuuarcy 42.933% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2921 \tTraining Loss: 0.01164040 \tValidation Loss 0.01800177 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2922 \tTraining Loss: 0.01155033 \tValidation Loss 0.01826928 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2923 \tTraining Loss: 0.01155013 \tValidation Loss 0.01805785 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2924 \tTraining Loss: 0.01158032 \tValidation Loss 0.01862794 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2925 \tTraining Loss: 0.01157954 \tValidation Loss 0.01807913 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2926 \tTraining Loss: 0.01148478 \tValidation Loss 0.01812834 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2927 \tTraining Loss: 0.01154701 \tValidation Loss 0.01892369 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2928 \tTraining Loss: 0.01150049 \tValidation Loss 0.01818213 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2929 \tTraining Loss: 0.01148390 \tValidation Loss 0.01834233 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2930 \tTraining Loss: 0.01148651 \tValidation Loss 0.01875197 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2931 \tTraining Loss: 0.01147810 \tValidation Loss 0.01836806 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2932 \tTraining Loss: 0.01155332 \tValidation Loss 0.01790485 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2933 \tTraining Loss: 0.01161981 \tValidation Loss 0.01787085 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2934 \tTraining Loss: 0.01151513 \tValidation Loss 0.01852514 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2935 \tTraining Loss: 0.01160606 \tValidation Loss 0.01819892 \tTraining Acuuarcy 42.387% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 2936 \tTraining Loss: 0.01153413 \tValidation Loss 0.01842832 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2937 \tTraining Loss: 0.01157073 \tValidation Loss 0.01817956 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2938 \tTraining Loss: 0.01158000 \tValidation Loss 0.01831128 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2939 \tTraining Loss: 0.01160018 \tValidation Loss 0.01834978 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2940 \tTraining Loss: 0.01161919 \tValidation Loss 0.01870623 \tTraining Acuuarcy 42.097% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2941 \tTraining Loss: 0.01155027 \tValidation Loss 0.01838789 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2942 \tTraining Loss: 0.01161825 \tValidation Loss 0.01829057 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 2943 \tTraining Loss: 0.01160037 \tValidation Loss 0.01847470 \tTraining Acuuarcy 42.414% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2944 \tTraining Loss: 0.01157040 \tValidation Loss 0.01855217 \tTraining Acuuarcy 42.665% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2945 \tTraining Loss: 0.01153510 \tValidation Loss 0.01843210 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2946 \tTraining Loss: 0.01169109 \tValidation Loss 0.01815114 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 2947 \tTraining Loss: 0.01158398 \tValidation Loss 0.01804022 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2948 \tTraining Loss: 0.01154418 \tValidation Loss 0.01817997 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2949 \tTraining Loss: 0.01162080 \tValidation Loss 0.01851113 \tTraining Acuuarcy 42.336% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2950 \tTraining Loss: 0.01160345 \tValidation Loss 0.01827271 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2951 \tTraining Loss: 0.01164250 \tValidation Loss 0.01820164 \tTraining Acuuarcy 42.403% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2952 \tTraining Loss: 0.01155938 \tValidation Loss 0.01819208 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2953 \tTraining Loss: 0.01150523 \tValidation Loss 0.01853101 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2954 \tTraining Loss: 0.01161576 \tValidation Loss 0.01809682 \tTraining Acuuarcy 42.448% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2955 \tTraining Loss: 0.01153276 \tValidation Loss 0.01848739 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 2956 \tTraining Loss: 0.01149747 \tValidation Loss 0.01843795 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2957 \tTraining Loss: 0.01156674 \tValidation Loss 0.01836188 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2958 \tTraining Loss: 0.01156640 \tValidation Loss 0.01890252 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2959 \tTraining Loss: 0.01152266 \tValidation Loss 0.01814521 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2960 \tTraining Loss: 0.01157808 \tValidation Loss 0.01811780 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 2961 \tTraining Loss: 0.01154118 \tValidation Loss 0.01875211 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 2962 \tTraining Loss: 0.01150896 \tValidation Loss 0.01826015 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 19.086%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2963 \tTraining Loss: 0.01150694 \tValidation Loss 0.01827415 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2964 \tTraining Loss: 0.01158247 \tValidation Loss 0.01860021 \tTraining Acuuarcy 42.682% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2965 \tTraining Loss: 0.01154606 \tValidation Loss 0.01777957 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2966 \tTraining Loss: 0.01160223 \tValidation Loss 0.01828790 \tTraining Acuuarcy 42.242% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2967 \tTraining Loss: 0.01159800 \tValidation Loss 0.01814953 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2968 \tTraining Loss: 0.01161653 \tValidation Loss 0.01792117 \tTraining Acuuarcy 42.253% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 2969 \tTraining Loss: 0.01160178 \tValidation Loss 0.01804425 \tTraining Acuuarcy 42.598% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2970 \tTraining Loss: 0.01153629 \tValidation Loss 0.01863826 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2971 \tTraining Loss: 0.01158490 \tValidation Loss 0.01798812 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2972 \tTraining Loss: 0.01155047 \tValidation Loss 0.01864353 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2973 \tTraining Loss: 0.01147731 \tValidation Loss 0.01822589 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2974 \tTraining Loss: 0.01154175 \tValidation Loss 0.01805227 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2975 \tTraining Loss: 0.01162074 \tValidation Loss 0.01810856 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2976 \tTraining Loss: 0.01161082 \tValidation Loss 0.01812798 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2977 \tTraining Loss: 0.01155377 \tValidation Loss 0.01816867 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2978 \tTraining Loss: 0.01152771 \tValidation Loss 0.01809553 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2979 \tTraining Loss: 0.01163177 \tValidation Loss 0.01802435 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 2980 \tTraining Loss: 0.01153020 \tValidation Loss 0.01837357 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2981 \tTraining Loss: 0.01150606 \tValidation Loss 0.01853298 \tTraining Acuuarcy 42.526% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2982 \tTraining Loss: 0.01156754 \tValidation Loss 0.01813247 \tTraining Acuuarcy 42.453% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2983 \tTraining Loss: 0.01156826 \tValidation Loss 0.01854217 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2984 \tTraining Loss: 0.01155720 \tValidation Loss 0.01812599 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2985 \tTraining Loss: 0.01150733 \tValidation Loss 0.01836495 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 2986 \tTraining Loss: 0.01152821 \tValidation Loss 0.01861967 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2987 \tTraining Loss: 0.01155002 \tValidation Loss 0.01839435 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2988 \tTraining Loss: 0.01156987 \tValidation Loss 0.01814320 \tTraining Acuuarcy 42.309% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2989 \tTraining Loss: 0.01149444 \tValidation Loss 0.01854799 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 2990 \tTraining Loss: 0.01159247 \tValidation Loss 0.01823509 \tTraining Acuuarcy 42.180% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 2991 \tTraining Loss: 0.01147917 \tValidation Loss 0.01834728 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2992 \tTraining Loss: 0.01153502 \tValidation Loss 0.01783074 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2993 \tTraining Loss: 0.01152208 \tValidation Loss 0.01861163 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2994 \tTraining Loss: 0.01156669 \tValidation Loss 0.01911828 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2995 \tTraining Loss: 0.01151712 \tValidation Loss 0.01834809 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2996 \tTraining Loss: 0.01159529 \tValidation Loss 0.01842580 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2997 \tTraining Loss: 0.01159940 \tValidation Loss 0.01873488 \tTraining Acuuarcy 42.074% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2998 \tTraining Loss: 0.01152975 \tValidation Loss 0.01816559 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2999 \tTraining Loss: 0.01153256 \tValidation Loss 0.01839787 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 3000 \tTraining Loss: 0.01156537 \tValidation Loss 0.01819142 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 3001 \tTraining Loss: 0.01156037 \tValidation Loss 0.01830063 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3002 \tTraining Loss: 0.01157798 \tValidation Loss 0.01849519 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3003 \tTraining Loss: 0.01151660 \tValidation Loss 0.01855241 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3004 \tTraining Loss: 0.01158133 \tValidation Loss 0.01858325 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3005 \tTraining Loss: 0.01164356 \tValidation Loss 0.01824327 \tTraining Acuuarcy 42.208% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3006 \tTraining Loss: 0.01161134 \tValidation Loss 0.01883552 \tTraining Acuuarcy 42.442% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3007 \tTraining Loss: 0.01148362 \tValidation Loss 0.01879633 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3008 \tTraining Loss: 0.01154985 \tValidation Loss 0.01829418 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3009 \tTraining Loss: 0.01150351 \tValidation Loss 0.01873912 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 3010 \tTraining Loss: 0.01154686 \tValidation Loss 0.01824182 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3011 \tTraining Loss: 0.01153425 \tValidation Loss 0.01840580 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 3012 \tTraining Loss: 0.01161527 \tValidation Loss 0.01789967 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3013 \tTraining Loss: 0.01161260 \tValidation Loss 0.01812428 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3014 \tTraining Loss: 0.01155273 \tValidation Loss 0.01837017 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3015 \tTraining Loss: 0.01150054 \tValidation Loss 0.01847832 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 3016 \tTraining Loss: 0.01149861 \tValidation Loss 0.01823910 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3017 \tTraining Loss: 0.01158561 \tValidation Loss 0.01840909 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 3018 \tTraining Loss: 0.01160372 \tValidation Loss 0.01823205 \tTraining Acuuarcy 42.008% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3019 \tTraining Loss: 0.01151873 \tValidation Loss 0.01854325 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 3020 \tTraining Loss: 0.01159715 \tValidation Loss 0.01854927 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3021 \tTraining Loss: 0.01155371 \tValidation Loss 0.01840649 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 3022 \tTraining Loss: 0.01163123 \tValidation Loss 0.01838977 \tTraining Acuuarcy 42.063% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 3023 \tTraining Loss: 0.01166153 \tValidation Loss 0.01842189 \tTraining Acuuarcy 42.069% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3024 \tTraining Loss: 0.01160095 \tValidation Loss 0.01821229 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 3025 \tTraining Loss: 0.01156668 \tValidation Loss 0.01835267 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3026 \tTraining Loss: 0.01162553 \tValidation Loss 0.01816347 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 3027 \tTraining Loss: 0.01150036 \tValidation Loss 0.01777423 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 3028 \tTraining Loss: 0.01154637 \tValidation Loss 0.01850011 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3029 \tTraining Loss: 0.01151932 \tValidation Loss 0.01855555 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 21.037%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3030 \tTraining Loss: 0.01164705 \tValidation Loss 0.01809733 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3031 \tTraining Loss: 0.01156897 \tValidation Loss 0.01857950 \tTraining Acuuarcy 42.704% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3032 \tTraining Loss: 0.01167708 \tValidation Loss 0.01762554 \tTraining Acuuarcy 41.807% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3033 \tTraining Loss: 0.01148525 \tValidation Loss 0.01839493 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 3034 \tTraining Loss: 0.01155952 \tValidation Loss 0.01812425 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3035 \tTraining Loss: 0.01152653 \tValidation Loss 0.01865033 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3036 \tTraining Loss: 0.01159379 \tValidation Loss 0.01872202 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3037 \tTraining Loss: 0.01152303 \tValidation Loss 0.01835830 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3038 \tTraining Loss: 0.01152564 \tValidation Loss 0.01796121 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3039 \tTraining Loss: 0.01153419 \tValidation Loss 0.01856462 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3040 \tTraining Loss: 0.01161968 \tValidation Loss 0.01803281 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 3041 \tTraining Loss: 0.01152381 \tValidation Loss 0.01869413 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3042 \tTraining Loss: 0.01148912 \tValidation Loss 0.01907903 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3043 \tTraining Loss: 0.01155713 \tValidation Loss 0.01849526 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3044 \tTraining Loss: 0.01152123 \tValidation Loss 0.01809663 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3045 \tTraining Loss: 0.01148475 \tValidation Loss 0.01884587 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 3046 \tTraining Loss: 0.01164881 \tValidation Loss 0.01869216 \tTraining Acuuarcy 42.281% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3047 \tTraining Loss: 0.01157615 \tValidation Loss 0.01787281 \tTraining Acuuarcy 42.192% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3048 \tTraining Loss: 0.01150731 \tValidation Loss 0.01868984 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3049 \tTraining Loss: 0.01152798 \tValidation Loss 0.01842124 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3050 \tTraining Loss: 0.01154504 \tValidation Loss 0.01836236 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3051 \tTraining Loss: 0.01152220 \tValidation Loss 0.01889151 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3052 \tTraining Loss: 0.01161870 \tValidation Loss 0.01880776 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 3053 \tTraining Loss: 0.01153571 \tValidation Loss 0.01821524 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 3054 \tTraining Loss: 0.01148238 \tValidation Loss 0.01921502 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 3055 \tTraining Loss: 0.01154756 \tValidation Loss 0.01839926 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3056 \tTraining Loss: 0.01159639 \tValidation Loss 0.01812818 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3057 \tTraining Loss: 0.01156329 \tValidation Loss 0.01849929 \tTraining Acuuarcy 42.576% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 3058 \tTraining Loss: 0.01160994 \tValidation Loss 0.01848678 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3059 \tTraining Loss: 0.01159320 \tValidation Loss 0.01855809 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3060 \tTraining Loss: 0.01153809 \tValidation Loss 0.01873839 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 3061 \tTraining Loss: 0.01160918 \tValidation Loss 0.01825313 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3062 \tTraining Loss: 0.01151082 \tValidation Loss 0.01858140 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3063 \tTraining Loss: 0.01155754 \tValidation Loss 0.01876890 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 3064 \tTraining Loss: 0.01156675 \tValidation Loss 0.01877223 \tTraining Acuuarcy 42.777% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3065 \tTraining Loss: 0.01160243 \tValidation Loss 0.01819892 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3066 \tTraining Loss: 0.01149378 \tValidation Loss 0.01834184 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3067 \tTraining Loss: 0.01153335 \tValidation Loss 0.01846656 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3068 \tTraining Loss: 0.01154959 \tValidation Loss 0.01842701 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 3069 \tTraining Loss: 0.01153845 \tValidation Loss 0.01812724 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3070 \tTraining Loss: 0.01153291 \tValidation Loss 0.01815729 \tTraining Acuuarcy 42.710% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 3071 \tTraining Loss: 0.01150812 \tValidation Loss 0.01877606 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 3072 \tTraining Loss: 0.01150429 \tValidation Loss 0.01859040 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3073 \tTraining Loss: 0.01152643 \tValidation Loss 0.01860374 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3074 \tTraining Loss: 0.01159741 \tValidation Loss 0.01837108 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3075 \tTraining Loss: 0.01148880 \tValidation Loss 0.01801958 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3076 \tTraining Loss: 0.01164164 \tValidation Loss 0.01821921 \tTraining Acuuarcy 41.991% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3077 \tTraining Loss: 0.01153972 \tValidation Loss 0.01837375 \tTraining Acuuarcy 42.777% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3078 \tTraining Loss: 0.01154366 \tValidation Loss 0.01817152 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3079 \tTraining Loss: 0.01153880 \tValidation Loss 0.01843875 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3080 \tTraining Loss: 0.01156791 \tValidation Loss 0.01807330 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3081 \tTraining Loss: 0.01160296 \tValidation Loss 0.01815148 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 3082 \tTraining Loss: 0.01162374 \tValidation Loss 0.01842464 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3083 \tTraining Loss: 0.01152125 \tValidation Loss 0.01837145 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3084 \tTraining Loss: 0.01161872 \tValidation Loss 0.01823003 \tTraining Acuuarcy 42.548% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 3085 \tTraining Loss: 0.01153654 \tValidation Loss 0.01866856 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3086 \tTraining Loss: 0.01145452 \tValidation Loss 0.01906675 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3087 \tTraining Loss: 0.01153323 \tValidation Loss 0.01828133 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3088 \tTraining Loss: 0.01154152 \tValidation Loss 0.01828797 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 3089 \tTraining Loss: 0.01156301 \tValidation Loss 0.01837263 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3090 \tTraining Loss: 0.01154599 \tValidation Loss 0.01866958 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3091 \tTraining Loss: 0.01149677 \tValidation Loss 0.01867612 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3092 \tTraining Loss: 0.01155373 \tValidation Loss 0.01853705 \tTraining Acuuarcy 42.498% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3093 \tTraining Loss: 0.01159930 \tValidation Loss 0.01855109 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3094 \tTraining Loss: 0.01162474 \tValidation Loss 0.01820909 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 3095 \tTraining Loss: 0.01153977 \tValidation Loss 0.01890140 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3096 \tTraining Loss: 0.01154944 \tValidation Loss 0.01820909 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 20.061%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3097 \tTraining Loss: 0.01157160 \tValidation Loss 0.01877835 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3098 \tTraining Loss: 0.01150115 \tValidation Loss 0.01860911 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3099 \tTraining Loss: 0.01156289 \tValidation Loss 0.01848440 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3100 \tTraining Loss: 0.01160456 \tValidation Loss 0.01793219 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 3101 \tTraining Loss: 0.01155133 \tValidation Loss 0.01820477 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3102 \tTraining Loss: 0.01151520 \tValidation Loss 0.01819032 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3103 \tTraining Loss: 0.01158057 \tValidation Loss 0.01808895 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3104 \tTraining Loss: 0.01152116 \tValidation Loss 0.01867824 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3105 \tTraining Loss: 0.01145357 \tValidation Loss 0.01831392 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3106 \tTraining Loss: 0.01151250 \tValidation Loss 0.01855206 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 3107 \tTraining Loss: 0.01149902 \tValidation Loss 0.01826266 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3108 \tTraining Loss: 0.01145356 \tValidation Loss 0.01823895 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3109 \tTraining Loss: 0.01153472 \tValidation Loss 0.01865178 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 3110 \tTraining Loss: 0.01150195 \tValidation Loss 0.01905149 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3111 \tTraining Loss: 0.01166515 \tValidation Loss 0.01839541 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 3112 \tTraining Loss: 0.01153355 \tValidation Loss 0.01817141 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3113 \tTraining Loss: 0.01147314 \tValidation Loss 0.01822331 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3114 \tTraining Loss: 0.01151872 \tValidation Loss 0.01859975 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3115 \tTraining Loss: 0.01154685 \tValidation Loss 0.01826597 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3116 \tTraining Loss: 0.01158177 \tValidation Loss 0.01779807 \tTraining Acuuarcy 42.526% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 3117 \tTraining Loss: 0.01156978 \tValidation Loss 0.01869935 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3118 \tTraining Loss: 0.01150323 \tValidation Loss 0.01829112 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3119 \tTraining Loss: 0.01155111 \tValidation Loss 0.01836064 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3120 \tTraining Loss: 0.01153978 \tValidation Loss 0.01853133 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3121 \tTraining Loss: 0.01159305 \tValidation Loss 0.01862450 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3122 \tTraining Loss: 0.01155386 \tValidation Loss 0.01851926 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3123 \tTraining Loss: 0.01159477 \tValidation Loss 0.01797088 \tTraining Acuuarcy 42.336% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 3124 \tTraining Loss: 0.01158705 \tValidation Loss 0.01843520 \tTraining Acuuarcy 42.403% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3125 \tTraining Loss: 0.01146848 \tValidation Loss 0.01841147 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3126 \tTraining Loss: 0.01162102 \tValidation Loss 0.01808668 \tTraining Acuuarcy 42.164% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 3127 \tTraining Loss: 0.01157154 \tValidation Loss 0.01850281 \tTraining Acuuarcy 42.375% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 3128 \tTraining Loss: 0.01153723 \tValidation Loss 0.01860890 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 3129 \tTraining Loss: 0.01150235 \tValidation Loss 0.01848113 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 3130 \tTraining Loss: 0.01162748 \tValidation Loss 0.01797854 \tTraining Acuuarcy 42.197% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3131 \tTraining Loss: 0.01150735 \tValidation Loss 0.01817403 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3132 \tTraining Loss: 0.01148306 \tValidation Loss 0.01885999 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 3133 \tTraining Loss: 0.01153845 \tValidation Loss 0.01851459 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 3134 \tTraining Loss: 0.01155610 \tValidation Loss 0.01905923 \tTraining Acuuarcy 42.498% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3135 \tTraining Loss: 0.01161054 \tValidation Loss 0.01845387 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3136 \tTraining Loss: 0.01146670 \tValidation Loss 0.01908049 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3137 \tTraining Loss: 0.01151797 \tValidation Loss 0.01817403 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 3138 \tTraining Loss: 0.01156293 \tValidation Loss 0.01857779 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 3139 \tTraining Loss: 0.01164517 \tValidation Loss 0.01848353 \tTraining Acuuarcy 42.035% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3140 \tTraining Loss: 0.01150237 \tValidation Loss 0.01819530 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3141 \tTraining Loss: 0.01164874 \tValidation Loss 0.01817936 \tTraining Acuuarcy 42.119% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 3142 \tTraining Loss: 0.01153103 \tValidation Loss 0.01842340 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 3143 \tTraining Loss: 0.01155176 \tValidation Loss 0.01878555 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3144 \tTraining Loss: 0.01159875 \tValidation Loss 0.01822778 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 3145 \tTraining Loss: 0.01157284 \tValidation Loss 0.01840482 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3146 \tTraining Loss: 0.01159412 \tValidation Loss 0.01845926 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3147 \tTraining Loss: 0.01152900 \tValidation Loss 0.01851568 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3148 \tTraining Loss: 0.01161534 \tValidation Loss 0.01839058 \tTraining Acuuarcy 42.476% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 3149 \tTraining Loss: 0.01158458 \tValidation Loss 0.01826006 \tTraining Acuuarcy 42.710% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3150 \tTraining Loss: 0.01153016 \tValidation Loss 0.01867547 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3151 \tTraining Loss: 0.01161690 \tValidation Loss 0.01843191 \tTraining Acuuarcy 42.442% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 3152 \tTraining Loss: 0.01146571 \tValidation Loss 0.01888593 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3153 \tTraining Loss: 0.01150255 \tValidation Loss 0.01897831 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3154 \tTraining Loss: 0.01152289 \tValidation Loss 0.01870705 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 3155 \tTraining Loss: 0.01148469 \tValidation Loss 0.01863132 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 3156 \tTraining Loss: 0.01167633 \tValidation Loss 0.01841531 \tTraining Acuuarcy 41.924% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3157 \tTraining Loss: 0.01153103 \tValidation Loss 0.01851232 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3158 \tTraining Loss: 0.01158773 \tValidation Loss 0.01847311 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3159 \tTraining Loss: 0.01150265 \tValidation Loss 0.01869368 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 3160 \tTraining Loss: 0.01162465 \tValidation Loss 0.01825138 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3161 \tTraining Loss: 0.01160793 \tValidation Loss 0.01851618 \tTraining Acuuarcy 42.381% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3162 \tTraining Loss: 0.01157561 \tValidation Loss 0.01811480 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 3163 \tTraining Loss: 0.01156433 \tValidation Loss 0.01839639 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 19.365%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3164 \tTraining Loss: 0.01155396 \tValidation Loss 0.01829816 \tTraining Acuuarcy 42.777% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3165 \tTraining Loss: 0.01150783 \tValidation Loss 0.01881200 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3166 \tTraining Loss: 0.01147774 \tValidation Loss 0.01848010 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 3167 \tTraining Loss: 0.01154225 \tValidation Loss 0.01818491 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3168 \tTraining Loss: 0.01168795 \tValidation Loss 0.01815813 \tTraining Acuuarcy 42.002% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3169 \tTraining Loss: 0.01158620 \tValidation Loss 0.01811584 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3170 \tTraining Loss: 0.01155745 \tValidation Loss 0.01818535 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 3171 \tTraining Loss: 0.01160164 \tValidation Loss 0.01798996 \tTraining Acuuarcy 42.442% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3172 \tTraining Loss: 0.01158427 \tValidation Loss 0.01826472 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3173 \tTraining Loss: 0.01150784 \tValidation Loss 0.01843764 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3174 \tTraining Loss: 0.01156782 \tValidation Loss 0.01847992 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 3175 \tTraining Loss: 0.01158296 \tValidation Loss 0.01829356 \tTraining Acuuarcy 42.309% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 3176 \tTraining Loss: 0.01160255 \tValidation Loss 0.01840445 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 3177 \tTraining Loss: 0.01155203 \tValidation Loss 0.01842040 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3178 \tTraining Loss: 0.01155728 \tValidation Loss 0.01824740 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3179 \tTraining Loss: 0.01148822 \tValidation Loss 0.01876325 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3180 \tTraining Loss: 0.01154452 \tValidation Loss 0.01849276 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3181 \tTraining Loss: 0.01154486 \tValidation Loss 0.01853135 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3182 \tTraining Loss: 0.01159909 \tValidation Loss 0.01808916 \tTraining Acuuarcy 42.821% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3183 \tTraining Loss: 0.01149172 \tValidation Loss 0.01830730 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3184 \tTraining Loss: 0.01154274 \tValidation Loss 0.01835908 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3185 \tTraining Loss: 0.01144982 \tValidation Loss 0.01814635 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3186 \tTraining Loss: 0.01161205 \tValidation Loss 0.01862153 \tTraining Acuuarcy 42.682% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3187 \tTraining Loss: 0.01150938 \tValidation Loss 0.01825270 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3188 \tTraining Loss: 0.01150917 \tValidation Loss 0.01841196 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3189 \tTraining Loss: 0.01161496 \tValidation Loss 0.01902215 \tTraining Acuuarcy 41.985% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3190 \tTraining Loss: 0.01150443 \tValidation Loss 0.01829540 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3191 \tTraining Loss: 0.01153649 \tValidation Loss 0.01917387 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 3192 \tTraining Loss: 0.01164665 \tValidation Loss 0.01850845 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3193 \tTraining Loss: 0.01153190 \tValidation Loss 0.01824257 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3194 \tTraining Loss: 0.01186320 \tValidation Loss 0.01792022 \tTraining Acuuarcy 40.531% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 3195 \tTraining Loss: 0.01151144 \tValidation Loss 0.01819712 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3196 \tTraining Loss: 0.01157639 \tValidation Loss 0.01852749 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3197 \tTraining Loss: 0.01158978 \tValidation Loss 0.01849047 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3198 \tTraining Loss: 0.01154385 \tValidation Loss 0.01827583 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3199 \tTraining Loss: 0.01156652 \tValidation Loss 0.01850863 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3200 \tTraining Loss: 0.01152251 \tValidation Loss 0.01854168 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3201 \tTraining Loss: 0.01153564 \tValidation Loss 0.01870991 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3202 \tTraining Loss: 0.01151598 \tValidation Loss 0.01826320 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 3203 \tTraining Loss: 0.01146748 \tValidation Loss 0.01875157 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 3204 \tTraining Loss: 0.01154297 \tValidation Loss 0.01879724 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3205 \tTraining Loss: 0.01148613 \tValidation Loss 0.01852804 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 3206 \tTraining Loss: 0.01149203 \tValidation Loss 0.01878813 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3207 \tTraining Loss: 0.01153111 \tValidation Loss 0.01836471 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3208 \tTraining Loss: 0.01149172 \tValidation Loss 0.01910686 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3209 \tTraining Loss: 0.01167531 \tValidation Loss 0.01820511 \tTraining Acuuarcy 41.913% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 3210 \tTraining Loss: 0.01156755 \tValidation Loss 0.01865406 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3211 \tTraining Loss: 0.01162871 \tValidation Loss 0.01830155 \tTraining Acuuarcy 42.309% \tValidation Acuuarcy 21.482%\n",
      "Epoch: 3212 \tTraining Loss: 0.01147909 \tValidation Loss 0.01869874 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3213 \tTraining Loss: 0.01145266 \tValidation Loss 0.01833054 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 3214 \tTraining Loss: 0.01146864 \tValidation Loss 0.01856449 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3215 \tTraining Loss: 0.01158222 \tValidation Loss 0.01814952 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 3216 \tTraining Loss: 0.01146324 \tValidation Loss 0.01825558 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3217 \tTraining Loss: 0.01159612 \tValidation Loss 0.01838803 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3218 \tTraining Loss: 0.01148383 \tValidation Loss 0.01892980 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 3219 \tTraining Loss: 0.01158404 \tValidation Loss 0.01836909 \tTraining Acuuarcy 42.431% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3220 \tTraining Loss: 0.01147115 \tValidation Loss 0.01866459 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3221 \tTraining Loss: 0.01150105 \tValidation Loss 0.01872390 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3222 \tTraining Loss: 0.01153703 \tValidation Loss 0.01841200 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3223 \tTraining Loss: 0.01147096 \tValidation Loss 0.01878272 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3224 \tTraining Loss: 0.01154990 \tValidation Loss 0.01806854 \tTraining Acuuarcy 42.437% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3225 \tTraining Loss: 0.01156362 \tValidation Loss 0.01812555 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3226 \tTraining Loss: 0.01159171 \tValidation Loss 0.01807598 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3227 \tTraining Loss: 0.01148287 \tValidation Loss 0.01904024 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3228 \tTraining Loss: 0.01154689 \tValidation Loss 0.01833259 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3229 \tTraining Loss: 0.01152882 \tValidation Loss 0.01884451 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3230 \tTraining Loss: 0.01158396 \tValidation Loss 0.01840434 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 18.724%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3231 \tTraining Loss: 0.01154599 \tValidation Loss 0.01888218 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 3232 \tTraining Loss: 0.01150098 \tValidation Loss 0.01856925 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3233 \tTraining Loss: 0.01161250 \tValidation Loss 0.01825782 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3234 \tTraining Loss: 0.01164559 \tValidation Loss 0.01814829 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3235 \tTraining Loss: 0.01159447 \tValidation Loss 0.01848324 \tTraining Acuuarcy 42.219% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3236 \tTraining Loss: 0.01148977 \tValidation Loss 0.01837811 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3237 \tTraining Loss: 0.01161577 \tValidation Loss 0.01857003 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3238 \tTraining Loss: 0.01161264 \tValidation Loss 0.01857993 \tTraining Acuuarcy 42.431% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3239 \tTraining Loss: 0.01152667 \tValidation Loss 0.01807137 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3240 \tTraining Loss: 0.01152386 \tValidation Loss 0.01806293 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 3241 \tTraining Loss: 0.01157350 \tValidation Loss 0.01853533 \tTraining Acuuarcy 42.654% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3242 \tTraining Loss: 0.01156185 \tValidation Loss 0.01817412 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3243 \tTraining Loss: 0.01153475 \tValidation Loss 0.01836526 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3244 \tTraining Loss: 0.01157354 \tValidation Loss 0.01835284 \tTraining Acuuarcy 42.715% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3245 \tTraining Loss: 0.01149347 \tValidation Loss 0.01848399 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3246 \tTraining Loss: 0.01159616 \tValidation Loss 0.01802706 \tTraining Acuuarcy 42.270% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 3247 \tTraining Loss: 0.01152780 \tValidation Loss 0.01821683 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3248 \tTraining Loss: 0.01150672 \tValidation Loss 0.01870473 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3249 \tTraining Loss: 0.01152236 \tValidation Loss 0.01895558 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 3250 \tTraining Loss: 0.01153474 \tValidation Loss 0.01907330 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3251 \tTraining Loss: 0.01152672 \tValidation Loss 0.01823870 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3252 \tTraining Loss: 0.01159847 \tValidation Loss 0.01873711 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 3253 \tTraining Loss: 0.01146287 \tValidation Loss 0.01825547 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3254 \tTraining Loss: 0.01169260 \tValidation Loss 0.01793051 \tTraining Acuuarcy 42.192% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3255 \tTraining Loss: 0.01148599 \tValidation Loss 0.01827005 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3256 \tTraining Loss: 0.01151011 \tValidation Loss 0.01849024 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3257 \tTraining Loss: 0.01153085 \tValidation Loss 0.01827725 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3258 \tTraining Loss: 0.01162404 \tValidation Loss 0.01816345 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3259 \tTraining Loss: 0.01158539 \tValidation Loss 0.01802308 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3260 \tTraining Loss: 0.01150906 \tValidation Loss 0.01856031 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3261 \tTraining Loss: 0.01157107 \tValidation Loss 0.01803793 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 3262 \tTraining Loss: 0.01155460 \tValidation Loss 0.01852500 \tTraining Acuuarcy 42.453% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3263 \tTraining Loss: 0.01163550 \tValidation Loss 0.01817646 \tTraining Acuuarcy 42.242% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3264 \tTraining Loss: 0.01158009 \tValidation Loss 0.01786943 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3265 \tTraining Loss: 0.01150170 \tValidation Loss 0.01877823 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 3266 \tTraining Loss: 0.01154349 \tValidation Loss 0.01840382 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3267 \tTraining Loss: 0.01151027 \tValidation Loss 0.01841193 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3268 \tTraining Loss: 0.01154110 \tValidation Loss 0.01840008 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3269 \tTraining Loss: 0.01152496 \tValidation Loss 0.01873091 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3270 \tTraining Loss: 0.01150811 \tValidation Loss 0.01797060 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 3271 \tTraining Loss: 0.01155426 \tValidation Loss 0.01828911 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3272 \tTraining Loss: 0.01150836 \tValidation Loss 0.01878685 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3273 \tTraining Loss: 0.01159009 \tValidation Loss 0.01881512 \tTraining Acuuarcy 42.598% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3274 \tTraining Loss: 0.01154967 \tValidation Loss 0.01860503 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 3275 \tTraining Loss: 0.01148118 \tValidation Loss 0.01872623 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3276 \tTraining Loss: 0.01148499 \tValidation Loss 0.01857479 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3277 \tTraining Loss: 0.01150909 \tValidation Loss 0.01852721 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3278 \tTraining Loss: 0.01147149 \tValidation Loss 0.01875673 \tTraining Acuuarcy 42.933% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3279 \tTraining Loss: 0.01146606 \tValidation Loss 0.01846823 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 3280 \tTraining Loss: 0.01151442 \tValidation Loss 0.01801487 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3281 \tTraining Loss: 0.01161550 \tValidation Loss 0.01823632 \tTraining Acuuarcy 42.665% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3282 \tTraining Loss: 0.01150711 \tValidation Loss 0.01852310 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 3283 \tTraining Loss: 0.01150185 \tValidation Loss 0.01851133 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3284 \tTraining Loss: 0.01160284 \tValidation Loss 0.01822323 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 3285 \tTraining Loss: 0.01149864 \tValidation Loss 0.01846620 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 3286 \tTraining Loss: 0.01152900 \tValidation Loss 0.01793379 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 3287 \tTraining Loss: 0.01152966 \tValidation Loss 0.01827006 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3288 \tTraining Loss: 0.01156410 \tValidation Loss 0.01858917 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3289 \tTraining Loss: 0.01154168 \tValidation Loss 0.01863258 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 3290 \tTraining Loss: 0.01150087 \tValidation Loss 0.01827030 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3291 \tTraining Loss: 0.01151114 \tValidation Loss 0.01830962 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 3292 \tTraining Loss: 0.01150529 \tValidation Loss 0.01810655 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3293 \tTraining Loss: 0.01149656 \tValidation Loss 0.01868240 \tTraining Acuuarcy 43.412% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3294 \tTraining Loss: 0.01159881 \tValidation Loss 0.01803233 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3295 \tTraining Loss: 0.01155488 \tValidation Loss 0.01797859 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3296 \tTraining Loss: 0.01150766 \tValidation Loss 0.01852479 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3297 \tTraining Loss: 0.01157312 \tValidation Loss 0.01863002 \tTraining Acuuarcy 42.387% \tValidation Acuuarcy 20.061%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3298 \tTraining Loss: 0.01150569 \tValidation Loss 0.01860454 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3299 \tTraining Loss: 0.01157351 \tValidation Loss 0.01831288 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3300 \tTraining Loss: 0.01155258 \tValidation Loss 0.01883700 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3301 \tTraining Loss: 0.01152009 \tValidation Loss 0.01868200 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3302 \tTraining Loss: 0.01148632 \tValidation Loss 0.01851329 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 3303 \tTraining Loss: 0.01153861 \tValidation Loss 0.01882841 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3304 \tTraining Loss: 0.01156497 \tValidation Loss 0.01866901 \tTraining Acuuarcy 42.715% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3305 \tTraining Loss: 0.01155641 \tValidation Loss 0.01853800 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3306 \tTraining Loss: 0.01155764 \tValidation Loss 0.01854257 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3307 \tTraining Loss: 0.01156762 \tValidation Loss 0.01853647 \tTraining Acuuarcy 42.286% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3308 \tTraining Loss: 0.01149076 \tValidation Loss 0.01807435 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3309 \tTraining Loss: 0.01145421 \tValidation Loss 0.01839502 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3310 \tTraining Loss: 0.01155265 \tValidation Loss 0.01850785 \tTraining Acuuarcy 42.359% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3311 \tTraining Loss: 0.01154791 \tValidation Loss 0.01817034 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 3312 \tTraining Loss: 0.01146283 \tValidation Loss 0.01868546 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 3313 \tTraining Loss: 0.01162792 \tValidation Loss 0.01867993 \tTraining Acuuarcy 42.169% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3314 \tTraining Loss: 0.01147599 \tValidation Loss 0.01825700 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 3315 \tTraining Loss: 0.01147360 \tValidation Loss 0.01834338 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3316 \tTraining Loss: 0.01148043 \tValidation Loss 0.01855036 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3317 \tTraining Loss: 0.01146077 \tValidation Loss 0.01856263 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3318 \tTraining Loss: 0.01153990 \tValidation Loss 0.01834230 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 3319 \tTraining Loss: 0.01166560 \tValidation Loss 0.01830963 \tTraining Acuuarcy 42.348% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3320 \tTraining Loss: 0.01145539 \tValidation Loss 0.01861240 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3321 \tTraining Loss: 0.01162282 \tValidation Loss 0.01823512 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 3322 \tTraining Loss: 0.01155884 \tValidation Loss 0.01855178 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 3323 \tTraining Loss: 0.01162833 \tValidation Loss 0.01845168 \tTraining Acuuarcy 42.264% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 3324 \tTraining Loss: 0.01147731 \tValidation Loss 0.01846907 \tTraining Acuuarcy 42.715% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3325 \tTraining Loss: 0.01153155 \tValidation Loss 0.01861867 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3326 \tTraining Loss: 0.01156627 \tValidation Loss 0.01887274 \tTraining Acuuarcy 42.598% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 3327 \tTraining Loss: 0.01167266 \tValidation Loss 0.01808722 \tTraining Acuuarcy 42.108% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3328 \tTraining Loss: 0.01157375 \tValidation Loss 0.01853156 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3329 \tTraining Loss: 0.01151117 \tValidation Loss 0.01840385 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3330 \tTraining Loss: 0.01153506 \tValidation Loss 0.01849947 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3331 \tTraining Loss: 0.01145795 \tValidation Loss 0.01828076 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3332 \tTraining Loss: 0.01147566 \tValidation Loss 0.01829126 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3333 \tTraining Loss: 0.01155649 \tValidation Loss 0.01823531 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3334 \tTraining Loss: 0.01158335 \tValidation Loss 0.01835575 \tTraining Acuuarcy 42.247% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3335 \tTraining Loss: 0.01152753 \tValidation Loss 0.01833641 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3336 \tTraining Loss: 0.01147135 \tValidation Loss 0.01855721 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3337 \tTraining Loss: 0.01165341 \tValidation Loss 0.01877103 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 3338 \tTraining Loss: 0.01151602 \tValidation Loss 0.01861907 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3339 \tTraining Loss: 0.01151926 \tValidation Loss 0.01854928 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3340 \tTraining Loss: 0.01144672 \tValidation Loss 0.01887716 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3341 \tTraining Loss: 0.01148292 \tValidation Loss 0.01844596 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3342 \tTraining Loss: 0.01158868 \tValidation Loss 0.01870123 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3343 \tTraining Loss: 0.01153513 \tValidation Loss 0.01790630 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 3344 \tTraining Loss: 0.01157890 \tValidation Loss 0.01799558 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3345 \tTraining Loss: 0.01145166 \tValidation Loss 0.01842318 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3346 \tTraining Loss: 0.01151642 \tValidation Loss 0.01866097 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 3347 \tTraining Loss: 0.01145126 \tValidation Loss 0.01829120 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 3348 \tTraining Loss: 0.01154398 \tValidation Loss 0.01813622 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3349 \tTraining Loss: 0.01150663 \tValidation Loss 0.01844031 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3350 \tTraining Loss: 0.01155177 \tValidation Loss 0.01903948 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3351 \tTraining Loss: 0.01156522 \tValidation Loss 0.01841140 \tTraining Acuuarcy 42.821% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3352 \tTraining Loss: 0.01151412 \tValidation Loss 0.01874153 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3353 \tTraining Loss: 0.01166612 \tValidation Loss 0.01811601 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 3354 \tTraining Loss: 0.01156944 \tValidation Loss 0.01854005 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3355 \tTraining Loss: 0.01153976 \tValidation Loss 0.01808787 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3356 \tTraining Loss: 0.01155352 \tValidation Loss 0.01847978 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 3357 \tTraining Loss: 0.01149671 \tValidation Loss 0.01812497 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3358 \tTraining Loss: 0.01147575 \tValidation Loss 0.01834279 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3359 \tTraining Loss: 0.01148829 \tValidation Loss 0.01833196 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3360 \tTraining Loss: 0.01144957 \tValidation Loss 0.01830509 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3361 \tTraining Loss: 0.01147076 \tValidation Loss 0.01863481 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3362 \tTraining Loss: 0.01152650 \tValidation Loss 0.01839350 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3363 \tTraining Loss: 0.01159482 \tValidation Loss 0.01852865 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 3364 \tTraining Loss: 0.01159057 \tValidation Loss 0.01812489 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 19.532%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3365 \tTraining Loss: 0.01148581 \tValidation Loss 0.01909221 \tTraining Acuuarcy 42.682% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3366 \tTraining Loss: 0.01149928 \tValidation Loss 0.01849417 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3367 \tTraining Loss: 0.01156646 \tValidation Loss 0.01816616 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3368 \tTraining Loss: 0.01145337 \tValidation Loss 0.01849358 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3369 \tTraining Loss: 0.01149843 \tValidation Loss 0.01851889 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3370 \tTraining Loss: 0.01158392 \tValidation Loss 0.01819787 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3371 \tTraining Loss: 0.01147488 \tValidation Loss 0.01815327 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 3372 \tTraining Loss: 0.01153100 \tValidation Loss 0.01837225 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3373 \tTraining Loss: 0.01153257 \tValidation Loss 0.01816536 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3374 \tTraining Loss: 0.01155023 \tValidation Loss 0.01842230 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 3375 \tTraining Loss: 0.01151953 \tValidation Loss 0.01864613 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3376 \tTraining Loss: 0.01149158 \tValidation Loss 0.01813374 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3377 \tTraining Loss: 0.01155047 \tValidation Loss 0.01838386 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 3378 \tTraining Loss: 0.01144644 \tValidation Loss 0.01865950 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3379 \tTraining Loss: 0.01161687 \tValidation Loss 0.01820295 \tTraining Acuuarcy 42.448% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3380 \tTraining Loss: 0.01155810 \tValidation Loss 0.01844910 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3381 \tTraining Loss: 0.01154081 \tValidation Loss 0.01885704 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 3382 \tTraining Loss: 0.01163131 \tValidation Loss 0.01838899 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3383 \tTraining Loss: 0.01153929 \tValidation Loss 0.01830750 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3384 \tTraining Loss: 0.01150781 \tValidation Loss 0.01856981 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3385 \tTraining Loss: 0.01161605 \tValidation Loss 0.01825828 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3386 \tTraining Loss: 0.01150096 \tValidation Loss 0.01805646 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3387 \tTraining Loss: 0.01154174 \tValidation Loss 0.01794848 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 3388 \tTraining Loss: 0.01152721 \tValidation Loss 0.01873963 \tTraining Acuuarcy 42.821% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3389 \tTraining Loss: 0.01151650 \tValidation Loss 0.01900568 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3390 \tTraining Loss: 0.01167429 \tValidation Loss 0.01830061 \tTraining Acuuarcy 42.074% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3391 \tTraining Loss: 0.01161448 \tValidation Loss 0.01891459 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3392 \tTraining Loss: 0.01148766 \tValidation Loss 0.01826488 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3393 \tTraining Loss: 0.01148108 \tValidation Loss 0.01842399 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3394 \tTraining Loss: 0.01162130 \tValidation Loss 0.01812572 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 3395 \tTraining Loss: 0.01143225 \tValidation Loss 0.01853606 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 3396 \tTraining Loss: 0.01150608 \tValidation Loss 0.01875217 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 3397 \tTraining Loss: 0.01161420 \tValidation Loss 0.01847837 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3398 \tTraining Loss: 0.01156887 \tValidation Loss 0.01871309 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3399 \tTraining Loss: 0.01153599 \tValidation Loss 0.01895386 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3400 \tTraining Loss: 0.01156135 \tValidation Loss 0.01822923 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3401 \tTraining Loss: 0.01151686 \tValidation Loss 0.01852979 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3402 \tTraining Loss: 0.01142684 \tValidation Loss 0.01841946 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3403 \tTraining Loss: 0.01144855 \tValidation Loss 0.01842543 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3404 \tTraining Loss: 0.01146269 \tValidation Loss 0.01837218 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3405 \tTraining Loss: 0.01161079 \tValidation Loss 0.01813667 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3406 \tTraining Loss: 0.01153353 \tValidation Loss 0.01862109 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3407 \tTraining Loss: 0.01149570 \tValidation Loss 0.01845793 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3408 \tTraining Loss: 0.01149158 \tValidation Loss 0.01850597 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3409 \tTraining Loss: 0.01151163 \tValidation Loss 0.01847538 \tTraining Acuuarcy 42.710% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3410 \tTraining Loss: 0.01147828 \tValidation Loss 0.01870776 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3411 \tTraining Loss: 0.01151774 \tValidation Loss 0.01892319 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3412 \tTraining Loss: 0.01159375 \tValidation Loss 0.01860473 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3413 \tTraining Loss: 0.01150083 \tValidation Loss 0.01851543 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 3414 \tTraining Loss: 0.01159251 \tValidation Loss 0.01857544 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3415 \tTraining Loss: 0.01152094 \tValidation Loss 0.01846248 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3416 \tTraining Loss: 0.01147443 \tValidation Loss 0.01806992 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 3417 \tTraining Loss: 0.01157379 \tValidation Loss 0.01811708 \tTraining Acuuarcy 42.353% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 3418 \tTraining Loss: 0.01149237 \tValidation Loss 0.01885820 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3419 \tTraining Loss: 0.01151139 \tValidation Loss 0.01823652 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 3420 \tTraining Loss: 0.01158236 \tValidation Loss 0.01775913 \tTraining Acuuarcy 42.426% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3421 \tTraining Loss: 0.01160547 \tValidation Loss 0.01818934 \tTraining Acuuarcy 42.359% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 3422 \tTraining Loss: 0.01152204 \tValidation Loss 0.01829976 \tTraining Acuuarcy 42.933% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 3423 \tTraining Loss: 0.01156009 \tValidation Loss 0.01831785 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 3424 \tTraining Loss: 0.01152713 \tValidation Loss 0.01863227 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3425 \tTraining Loss: 0.01155567 \tValidation Loss 0.01786649 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3426 \tTraining Loss: 0.01150265 \tValidation Loss 0.01799021 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3427 \tTraining Loss: 0.01156084 \tValidation Loss 0.01812914 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3428 \tTraining Loss: 0.01152275 \tValidation Loss 0.01841276 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3429 \tTraining Loss: 0.01155318 \tValidation Loss 0.01855485 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 3430 \tTraining Loss: 0.01146026 \tValidation Loss 0.01884332 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3431 \tTraining Loss: 0.01151612 \tValidation Loss 0.01822887 \tTraining Acuuarcy 42.821% \tValidation Acuuarcy 20.479%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3432 \tTraining Loss: 0.01158750 \tValidation Loss 0.01874483 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3433 \tTraining Loss: 0.01151346 \tValidation Loss 0.01838524 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3434 \tTraining Loss: 0.01153379 \tValidation Loss 0.01826100 \tTraining Acuuarcy 42.453% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3435 \tTraining Loss: 0.01153541 \tValidation Loss 0.01846460 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3436 \tTraining Loss: 0.01144406 \tValidation Loss 0.01835855 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3437 \tTraining Loss: 0.01148506 \tValidation Loss 0.01847502 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 3438 \tTraining Loss: 0.01148205 \tValidation Loss 0.01864516 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 3439 \tTraining Loss: 0.01151734 \tValidation Loss 0.01898992 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 3440 \tTraining Loss: 0.01148902 \tValidation Loss 0.01800742 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3441 \tTraining Loss: 0.01156429 \tValidation Loss 0.01865765 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 3442 \tTraining Loss: 0.01154417 \tValidation Loss 0.01879650 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3443 \tTraining Loss: 0.01155735 \tValidation Loss 0.01808561 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 3444 \tTraining Loss: 0.01150396 \tValidation Loss 0.01803756 \tTraining Acuuarcy 42.426% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3445 \tTraining Loss: 0.01146594 \tValidation Loss 0.01851866 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3446 \tTraining Loss: 0.01146837 \tValidation Loss 0.01872977 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3447 \tTraining Loss: 0.01161274 \tValidation Loss 0.01859410 \tTraining Acuuarcy 42.008% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 3448 \tTraining Loss: 0.01145316 \tValidation Loss 0.01838475 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3449 \tTraining Loss: 0.01143838 \tValidation Loss 0.01828277 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 3450 \tTraining Loss: 0.01155605 \tValidation Loss 0.01844171 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 3451 \tTraining Loss: 0.01145611 \tValidation Loss 0.01818321 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 3452 \tTraining Loss: 0.01156389 \tValidation Loss 0.01830607 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3453 \tTraining Loss: 0.01162402 \tValidation Loss 0.01855293 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3454 \tTraining Loss: 0.01155864 \tValidation Loss 0.01817992 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 3455 \tTraining Loss: 0.01148765 \tValidation Loss 0.01877152 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3456 \tTraining Loss: 0.01155113 \tValidation Loss 0.01837224 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3457 \tTraining Loss: 0.01157720 \tValidation Loss 0.01832021 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3458 \tTraining Loss: 0.01150422 \tValidation Loss 0.01802911 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3459 \tTraining Loss: 0.01146110 \tValidation Loss 0.01857337 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3460 \tTraining Loss: 0.01161640 \tValidation Loss 0.01869351 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3461 \tTraining Loss: 0.01144980 \tValidation Loss 0.01823339 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3462 \tTraining Loss: 0.01143663 \tValidation Loss 0.01891706 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3463 \tTraining Loss: 0.01150781 \tValidation Loss 0.01858747 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3464 \tTraining Loss: 0.01153716 \tValidation Loss 0.01908587 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3465 \tTraining Loss: 0.01168775 \tValidation Loss 0.01845242 \tTraining Acuuarcy 41.996% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3466 \tTraining Loss: 0.01147710 \tValidation Loss 0.01847142 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3467 \tTraining Loss: 0.01157811 \tValidation Loss 0.01873133 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3468 \tTraining Loss: 0.01150553 \tValidation Loss 0.01843535 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3469 \tTraining Loss: 0.01152407 \tValidation Loss 0.01816939 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3470 \tTraining Loss: 0.01159043 \tValidation Loss 0.01797627 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3471 \tTraining Loss: 0.01148736 \tValidation Loss 0.01881272 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3472 \tTraining Loss: 0.01143826 \tValidation Loss 0.01845737 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3473 \tTraining Loss: 0.01160518 \tValidation Loss 0.01834272 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3474 \tTraining Loss: 0.01149547 \tValidation Loss 0.01872587 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 3475 \tTraining Loss: 0.01152981 \tValidation Loss 0.01867014 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3476 \tTraining Loss: 0.01152276 \tValidation Loss 0.01846940 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 3477 \tTraining Loss: 0.01150586 \tValidation Loss 0.01839771 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3478 \tTraining Loss: 0.01153335 \tValidation Loss 0.01841285 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3479 \tTraining Loss: 0.01148254 \tValidation Loss 0.01856742 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3480 \tTraining Loss: 0.01147638 \tValidation Loss 0.01838468 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3481 \tTraining Loss: 0.01151149 \tValidation Loss 0.01844919 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3482 \tTraining Loss: 0.01155980 \tValidation Loss 0.01866362 \tTraining Acuuarcy 42.231% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3483 \tTraining Loss: 0.01149661 \tValidation Loss 0.01842503 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 3484 \tTraining Loss: 0.01146725 \tValidation Loss 0.01806070 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3485 \tTraining Loss: 0.01147491 \tValidation Loss 0.01888420 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3486 \tTraining Loss: 0.01148873 \tValidation Loss 0.01830464 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3487 \tTraining Loss: 0.01149212 \tValidation Loss 0.01819395 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3488 \tTraining Loss: 0.01144810 \tValidation Loss 0.01918919 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3489 \tTraining Loss: 0.01152255 \tValidation Loss 0.01842303 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3490 \tTraining Loss: 0.01150772 \tValidation Loss 0.01846434 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3491 \tTraining Loss: 0.01159924 \tValidation Loss 0.01822973 \tTraining Acuuarcy 42.498% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3492 \tTraining Loss: 0.01156199 \tValidation Loss 0.01846512 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3493 \tTraining Loss: 0.01143761 \tValidation Loss 0.01814795 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3494 \tTraining Loss: 0.01147087 \tValidation Loss 0.01866839 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3495 \tTraining Loss: 0.01153849 \tValidation Loss 0.01942419 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3496 \tTraining Loss: 0.01154639 \tValidation Loss 0.01870995 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3497 \tTraining Loss: 0.01148502 \tValidation Loss 0.01861039 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3498 \tTraining Loss: 0.01146211 \tValidation Loss 0.01890280 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 20.368%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3499 \tTraining Loss: 0.01153012 \tValidation Loss 0.01823786 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3500 \tTraining Loss: 0.01160199 \tValidation Loss 0.01849963 \tTraining Acuuarcy 42.598% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 3501 \tTraining Loss: 0.01146674 \tValidation Loss 0.01841965 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 3502 \tTraining Loss: 0.01159646 \tValidation Loss 0.01789670 \tTraining Acuuarcy 42.175% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3503 \tTraining Loss: 0.01147635 \tValidation Loss 0.01881012 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 3504 \tTraining Loss: 0.01153320 \tValidation Loss 0.01806615 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3505 \tTraining Loss: 0.01151928 \tValidation Loss 0.01822850 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3506 \tTraining Loss: 0.01162083 \tValidation Loss 0.01855631 \tTraining Acuuarcy 42.364% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3507 \tTraining Loss: 0.01149202 \tValidation Loss 0.01828197 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3508 \tTraining Loss: 0.01143015 \tValidation Loss 0.01885312 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3509 \tTraining Loss: 0.01160883 \tValidation Loss 0.01837656 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 3510 \tTraining Loss: 0.01153186 \tValidation Loss 0.01841906 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3511 \tTraining Loss: 0.01148165 \tValidation Loss 0.01813679 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3512 \tTraining Loss: 0.01153175 \tValidation Loss 0.01839618 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 3513 \tTraining Loss: 0.01157310 \tValidation Loss 0.01889512 \tTraining Acuuarcy 43.016% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 3514 \tTraining Loss: 0.01150551 \tValidation Loss 0.01907790 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3515 \tTraining Loss: 0.01142601 \tValidation Loss 0.01847000 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3516 \tTraining Loss: 0.01150377 \tValidation Loss 0.01821433 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3517 \tTraining Loss: 0.01154251 \tValidation Loss 0.01841486 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3518 \tTraining Loss: 0.01145844 \tValidation Loss 0.01856200 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3519 \tTraining Loss: 0.01154586 \tValidation Loss 0.01839128 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3520 \tTraining Loss: 0.01151706 \tValidation Loss 0.01863611 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 3521 \tTraining Loss: 0.01145924 \tValidation Loss 0.01867502 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3522 \tTraining Loss: 0.01146678 \tValidation Loss 0.01843139 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 3523 \tTraining Loss: 0.01156065 \tValidation Loss 0.01842012 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3524 \tTraining Loss: 0.01150911 \tValidation Loss 0.01829698 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3525 \tTraining Loss: 0.01154513 \tValidation Loss 0.01867609 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3526 \tTraining Loss: 0.01146679 \tValidation Loss 0.01875648 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3527 \tTraining Loss: 0.01150999 \tValidation Loss 0.01857190 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3528 \tTraining Loss: 0.01157944 \tValidation Loss 0.01814238 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3529 \tTraining Loss: 0.01152033 \tValidation Loss 0.01841221 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3530 \tTraining Loss: 0.01154375 \tValidation Loss 0.01871370 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 3531 \tTraining Loss: 0.01157776 \tValidation Loss 0.01844436 \tTraining Acuuarcy 42.236% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3532 \tTraining Loss: 0.01159689 \tValidation Loss 0.01852648 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 3533 \tTraining Loss: 0.01148533 \tValidation Loss 0.01888268 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3534 \tTraining Loss: 0.01159734 \tValidation Loss 0.01818797 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3535 \tTraining Loss: 0.01147821 \tValidation Loss 0.01892095 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3536 \tTraining Loss: 0.01148361 \tValidation Loss 0.01897789 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3537 \tTraining Loss: 0.01155815 \tValidation Loss 0.01812535 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3538 \tTraining Loss: 0.01146393 \tValidation Loss 0.01876752 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3539 \tTraining Loss: 0.01142747 \tValidation Loss 0.01845462 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3540 \tTraining Loss: 0.01161223 \tValidation Loss 0.01837717 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3541 \tTraining Loss: 0.01147807 \tValidation Loss 0.01851904 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 3542 \tTraining Loss: 0.01146888 \tValidation Loss 0.01855353 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3543 \tTraining Loss: 0.01145123 \tValidation Loss 0.01811043 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3544 \tTraining Loss: 0.01152961 \tValidation Loss 0.01946215 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 3545 \tTraining Loss: 0.01141620 \tValidation Loss 0.01849529 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3546 \tTraining Loss: 0.01155881 \tValidation Loss 0.01833904 \tTraining Acuuarcy 42.498% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3547 \tTraining Loss: 0.01146186 \tValidation Loss 0.01834725 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3548 \tTraining Loss: 0.01158790 \tValidation Loss 0.01778138 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3549 \tTraining Loss: 0.01148745 \tValidation Loss 0.01850096 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3550 \tTraining Loss: 0.01147421 \tValidation Loss 0.01856560 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 3551 \tTraining Loss: 0.01144016 \tValidation Loss 0.01869148 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 3552 \tTraining Loss: 0.01154504 \tValidation Loss 0.01823036 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 3553 \tTraining Loss: 0.01152658 \tValidation Loss 0.01921523 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3554 \tTraining Loss: 0.01159016 \tValidation Loss 0.01828124 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 3555 \tTraining Loss: 0.01150699 \tValidation Loss 0.01874853 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 3556 \tTraining Loss: 0.01151487 \tValidation Loss 0.01839866 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3557 \tTraining Loss: 0.01153549 \tValidation Loss 0.01851857 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3558 \tTraining Loss: 0.01148161 \tValidation Loss 0.01870612 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3559 \tTraining Loss: 0.01151145 \tValidation Loss 0.01840897 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 3560 \tTraining Loss: 0.01149593 \tValidation Loss 0.01844010 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 3561 \tTraining Loss: 0.01149563 \tValidation Loss 0.01813545 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3562 \tTraining Loss: 0.01152197 \tValidation Loss 0.01827945 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 3563 \tTraining Loss: 0.01143765 \tValidation Loss 0.01834603 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3564 \tTraining Loss: 0.01157213 \tValidation Loss 0.01847370 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3565 \tTraining Loss: 0.01145425 \tValidation Loss 0.01882518 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 18.947%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3566 \tTraining Loss: 0.01159646 \tValidation Loss 0.01807208 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3567 \tTraining Loss: 0.01142066 \tValidation Loss 0.01843580 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 3568 \tTraining Loss: 0.01145644 \tValidation Loss 0.01891124 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 17.219%\n",
      "Epoch: 3569 \tTraining Loss: 0.01144236 \tValidation Loss 0.01860961 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 3570 \tTraining Loss: 0.01149616 \tValidation Loss 0.01816470 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3571 \tTraining Loss: 0.01153890 \tValidation Loss 0.01875368 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3572 \tTraining Loss: 0.01147115 \tValidation Loss 0.01833866 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3573 \tTraining Loss: 0.01158670 \tValidation Loss 0.01840172 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3574 \tTraining Loss: 0.01158987 \tValidation Loss 0.01847264 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3575 \tTraining Loss: 0.01152463 \tValidation Loss 0.01806169 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 3576 \tTraining Loss: 0.01160550 \tValidation Loss 0.01855761 \tTraining Acuuarcy 42.320% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3577 \tTraining Loss: 0.01150441 \tValidation Loss 0.01859325 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3578 \tTraining Loss: 0.01151462 \tValidation Loss 0.01864744 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3579 \tTraining Loss: 0.01156663 \tValidation Loss 0.01831923 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3580 \tTraining Loss: 0.01155155 \tValidation Loss 0.01834068 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3581 \tTraining Loss: 0.01141989 \tValidation Loss 0.01882538 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3582 \tTraining Loss: 0.01157391 \tValidation Loss 0.01850712 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3583 \tTraining Loss: 0.01157956 \tValidation Loss 0.01861456 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3584 \tTraining Loss: 0.01152837 \tValidation Loss 0.01835295 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 3585 \tTraining Loss: 0.01154806 \tValidation Loss 0.01842947 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 3586 \tTraining Loss: 0.01149101 \tValidation Loss 0.01858046 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3587 \tTraining Loss: 0.01152289 \tValidation Loss 0.01790235 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3588 \tTraining Loss: 0.01159236 \tValidation Loss 0.01845142 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3589 \tTraining Loss: 0.01148329 \tValidation Loss 0.01872238 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3590 \tTraining Loss: 0.01144515 \tValidation Loss 0.01846230 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 3591 \tTraining Loss: 0.01163330 \tValidation Loss 0.01818564 \tTraining Acuuarcy 42.119% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3592 \tTraining Loss: 0.01151417 \tValidation Loss 0.01825740 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3593 \tTraining Loss: 0.01157130 \tValidation Loss 0.01839188 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3594 \tTraining Loss: 0.01150548 \tValidation Loss 0.01860496 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 3595 \tTraining Loss: 0.01153024 \tValidation Loss 0.01849345 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3596 \tTraining Loss: 0.01157791 \tValidation Loss 0.01832068 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3597 \tTraining Loss: 0.01152283 \tValidation Loss 0.01849155 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3598 \tTraining Loss: 0.01146099 \tValidation Loss 0.01881071 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3599 \tTraining Loss: 0.01157182 \tValidation Loss 0.01802370 \tTraining Acuuarcy 42.370% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3600 \tTraining Loss: 0.01164676 \tValidation Loss 0.01838175 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3601 \tTraining Loss: 0.01164720 \tValidation Loss 0.01837060 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 3602 \tTraining Loss: 0.01156498 \tValidation Loss 0.01871016 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3603 \tTraining Loss: 0.01147577 \tValidation Loss 0.01865447 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3604 \tTraining Loss: 0.01145692 \tValidation Loss 0.01815584 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3605 \tTraining Loss: 0.01157181 \tValidation Loss 0.01868003 \tTraining Acuuarcy 42.453% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 3606 \tTraining Loss: 0.01143228 \tValidation Loss 0.01867821 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3607 \tTraining Loss: 0.01140237 \tValidation Loss 0.01832262 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3608 \tTraining Loss: 0.01148462 \tValidation Loss 0.01839201 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 3609 \tTraining Loss: 0.01142067 \tValidation Loss 0.01855383 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3610 \tTraining Loss: 0.01161803 \tValidation Loss 0.01849620 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3611 \tTraining Loss: 0.01151600 \tValidation Loss 0.01818042 \tTraining Acuuarcy 42.721% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 3612 \tTraining Loss: 0.01143294 \tValidation Loss 0.01875511 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3613 \tTraining Loss: 0.01147743 \tValidation Loss 0.01895387 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3614 \tTraining Loss: 0.01150690 \tValidation Loss 0.01845049 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3615 \tTraining Loss: 0.01151926 \tValidation Loss 0.01852284 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3616 \tTraining Loss: 0.01151119 \tValidation Loss 0.01820588 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3617 \tTraining Loss: 0.01157245 \tValidation Loss 0.01835645 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3618 \tTraining Loss: 0.01149605 \tValidation Loss 0.01837799 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3619 \tTraining Loss: 0.01145520 \tValidation Loss 0.01858888 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 3620 \tTraining Loss: 0.01147312 \tValidation Loss 0.01879911 \tTraining Acuuarcy 42.821% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 3621 \tTraining Loss: 0.01151038 \tValidation Loss 0.01826625 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3622 \tTraining Loss: 0.01160955 \tValidation Loss 0.01816139 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3623 \tTraining Loss: 0.01146433 \tValidation Loss 0.01822729 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3624 \tTraining Loss: 0.01145878 \tValidation Loss 0.01869481 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3625 \tTraining Loss: 0.01151193 \tValidation Loss 0.01870154 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3626 \tTraining Loss: 0.01152602 \tValidation Loss 0.01844030 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3627 \tTraining Loss: 0.01165210 \tValidation Loss 0.01791080 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 3628 \tTraining Loss: 0.01149746 \tValidation Loss 0.01851323 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 3629 \tTraining Loss: 0.01148964 \tValidation Loss 0.01842980 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3630 \tTraining Loss: 0.01149656 \tValidation Loss 0.01908248 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3631 \tTraining Loss: 0.01152621 \tValidation Loss 0.01814777 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3632 \tTraining Loss: 0.01144684 \tValidation Loss 0.01825593 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.588%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3633 \tTraining Loss: 0.01140722 \tValidation Loss 0.01862009 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3634 \tTraining Loss: 0.01152371 \tValidation Loss 0.01869313 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3635 \tTraining Loss: 0.01147556 \tValidation Loss 0.01866877 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3636 \tTraining Loss: 0.01147008 \tValidation Loss 0.01833053 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3637 \tTraining Loss: 0.01155057 \tValidation Loss 0.01856374 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 3638 \tTraining Loss: 0.01148220 \tValidation Loss 0.01865516 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3639 \tTraining Loss: 0.01151159 \tValidation Loss 0.01850749 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3640 \tTraining Loss: 0.01148649 \tValidation Loss 0.01849319 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3641 \tTraining Loss: 0.01143717 \tValidation Loss 0.01823970 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3642 \tTraining Loss: 0.01140409 \tValidation Loss 0.01870381 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3643 \tTraining Loss: 0.01154903 \tValidation Loss 0.01802866 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3644 \tTraining Loss: 0.01145737 \tValidation Loss 0.01848618 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3645 \tTraining Loss: 0.01158201 \tValidation Loss 0.01854548 \tTraining Acuuarcy 42.587% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3646 \tTraining Loss: 0.01150903 \tValidation Loss 0.01844976 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3647 \tTraining Loss: 0.01155043 \tValidation Loss 0.01823421 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 3648 \tTraining Loss: 0.01155821 \tValidation Loss 0.01876650 \tTraining Acuuarcy 42.810% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3649 \tTraining Loss: 0.01149213 \tValidation Loss 0.01829651 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3650 \tTraining Loss: 0.01165883 \tValidation Loss 0.01847582 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3651 \tTraining Loss: 0.01147639 \tValidation Loss 0.01857219 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 3652 \tTraining Loss: 0.01162622 \tValidation Loss 0.01846824 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3653 \tTraining Loss: 0.01147512 \tValidation Loss 0.01827625 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 3654 \tTraining Loss: 0.01148773 \tValidation Loss 0.01875561 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3655 \tTraining Loss: 0.01161041 \tValidation Loss 0.01855845 \tTraining Acuuarcy 42.403% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3656 \tTraining Loss: 0.01152226 \tValidation Loss 0.01791459 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3657 \tTraining Loss: 0.01148484 \tValidation Loss 0.01866328 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3658 \tTraining Loss: 0.01149135 \tValidation Loss 0.01865624 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3659 \tTraining Loss: 0.01138711 \tValidation Loss 0.01877781 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3660 \tTraining Loss: 0.01145076 \tValidation Loss 0.01866070 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 3661 \tTraining Loss: 0.01150112 \tValidation Loss 0.01839742 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 3662 \tTraining Loss: 0.01148533 \tValidation Loss 0.01874720 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 3663 \tTraining Loss: 0.01149465 \tValidation Loss 0.01839614 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 3664 \tTraining Loss: 0.01158228 \tValidation Loss 0.01824333 \tTraining Acuuarcy 42.063% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 3665 \tTraining Loss: 0.01142344 \tValidation Loss 0.01837268 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 3666 \tTraining Loss: 0.01147722 \tValidation Loss 0.01860196 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3667 \tTraining Loss: 0.01145952 \tValidation Loss 0.01831794 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3668 \tTraining Loss: 0.01149917 \tValidation Loss 0.01838390 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3669 \tTraining Loss: 0.01161402 \tValidation Loss 0.01822122 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3670 \tTraining Loss: 0.01154606 \tValidation Loss 0.01803925 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 3671 \tTraining Loss: 0.01161360 \tValidation Loss 0.01833346 \tTraining Acuuarcy 42.225% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3672 \tTraining Loss: 0.01145843 \tValidation Loss 0.01853828 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 3673 \tTraining Loss: 0.01144464 \tValidation Loss 0.01869743 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3674 \tTraining Loss: 0.01152646 \tValidation Loss 0.01851815 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3675 \tTraining Loss: 0.01147821 \tValidation Loss 0.01849907 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3676 \tTraining Loss: 0.01149365 \tValidation Loss 0.01842339 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 3677 \tTraining Loss: 0.01148375 \tValidation Loss 0.01892464 \tTraining Acuuarcy 43.016% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3678 \tTraining Loss: 0.01142786 \tValidation Loss 0.01855317 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 3679 \tTraining Loss: 0.01154485 \tValidation Loss 0.01854032 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 3680 \tTraining Loss: 0.01157057 \tValidation Loss 0.01857897 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3681 \tTraining Loss: 0.01149753 \tValidation Loss 0.01826300 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 3682 \tTraining Loss: 0.01156807 \tValidation Loss 0.01834014 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3683 \tTraining Loss: 0.01145856 \tValidation Loss 0.01864210 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3684 \tTraining Loss: 0.01142470 \tValidation Loss 0.01913999 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3685 \tTraining Loss: 0.01160135 \tValidation Loss 0.01835806 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3686 \tTraining Loss: 0.01147855 \tValidation Loss 0.01846150 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3687 \tTraining Loss: 0.01149039 \tValidation Loss 0.01818509 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3688 \tTraining Loss: 0.01150256 \tValidation Loss 0.01831070 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3689 \tTraining Loss: 0.01141193 \tValidation Loss 0.01882044 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3690 \tTraining Loss: 0.01149397 \tValidation Loss 0.01850478 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 3691 \tTraining Loss: 0.01146869 \tValidation Loss 0.01859823 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3692 \tTraining Loss: 0.01151567 \tValidation Loss 0.01819912 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3693 \tTraining Loss: 0.01147540 \tValidation Loss 0.01876250 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3694 \tTraining Loss: 0.01153516 \tValidation Loss 0.01844431 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3695 \tTraining Loss: 0.01148167 \tValidation Loss 0.01873115 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3696 \tTraining Loss: 0.01147486 \tValidation Loss 0.01868799 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3697 \tTraining Loss: 0.01153505 \tValidation Loss 0.01865820 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 3698 \tTraining Loss: 0.01154816 \tValidation Loss 0.01859117 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3699 \tTraining Loss: 0.01153323 \tValidation Loss 0.01945939 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 18.585%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3700 \tTraining Loss: 0.01148253 \tValidation Loss 0.01858470 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3701 \tTraining Loss: 0.01155631 \tValidation Loss 0.01822863 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3702 \tTraining Loss: 0.01156970 \tValidation Loss 0.01828067 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3703 \tTraining Loss: 0.01143336 \tValidation Loss 0.01883060 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3704 \tTraining Loss: 0.01151282 \tValidation Loss 0.01838905 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3705 \tTraining Loss: 0.01149476 \tValidation Loss 0.01831599 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3706 \tTraining Loss: 0.01151672 \tValidation Loss 0.01852115 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 3707 \tTraining Loss: 0.01148578 \tValidation Loss 0.01920363 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 3708 \tTraining Loss: 0.01148760 \tValidation Loss 0.01831180 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3709 \tTraining Loss: 0.01153309 \tValidation Loss 0.01855809 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 3710 \tTraining Loss: 0.01146205 \tValidation Loss 0.01852570 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3711 \tTraining Loss: 0.01147590 \tValidation Loss 0.01819319 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3712 \tTraining Loss: 0.01156881 \tValidation Loss 0.01897240 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 3713 \tTraining Loss: 0.01157815 \tValidation Loss 0.01836512 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3714 \tTraining Loss: 0.01149068 \tValidation Loss 0.01875680 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3715 \tTraining Loss: 0.01157332 \tValidation Loss 0.01835691 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3716 \tTraining Loss: 0.01146033 \tValidation Loss 0.01866465 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3717 \tTraining Loss: 0.01145471 \tValidation Loss 0.01847235 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3718 \tTraining Loss: 0.01146456 \tValidation Loss 0.01860959 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 3719 \tTraining Loss: 0.01147097 \tValidation Loss 0.01889556 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 3720 \tTraining Loss: 0.01148573 \tValidation Loss 0.01905704 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3721 \tTraining Loss: 0.01144049 \tValidation Loss 0.01851589 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3722 \tTraining Loss: 0.01138439 \tValidation Loss 0.01845687 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 3723 \tTraining Loss: 0.01153228 \tValidation Loss 0.01863572 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3724 \tTraining Loss: 0.01147084 \tValidation Loss 0.01922340 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3725 \tTraining Loss: 0.01146888 \tValidation Loss 0.01806290 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 3726 \tTraining Loss: 0.01150705 \tValidation Loss 0.01806984 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3727 \tTraining Loss: 0.01149544 \tValidation Loss 0.01849127 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 3728 \tTraining Loss: 0.01150796 \tValidation Loss 0.01836862 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3729 \tTraining Loss: 0.01149659 \tValidation Loss 0.01865423 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3730 \tTraining Loss: 0.01156113 \tValidation Loss 0.01880143 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3731 \tTraining Loss: 0.01155195 \tValidation Loss 0.01840637 \tTraining Acuuarcy 42.671% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 3732 \tTraining Loss: 0.01148633 \tValidation Loss 0.01830162 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3733 \tTraining Loss: 0.01150147 \tValidation Loss 0.01803005 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3734 \tTraining Loss: 0.01148389 \tValidation Loss 0.01842444 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3735 \tTraining Loss: 0.01160910 \tValidation Loss 0.01831141 \tTraining Acuuarcy 41.963% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3736 \tTraining Loss: 0.01156531 \tValidation Loss 0.01859146 \tTraining Acuuarcy 42.598% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3737 \tTraining Loss: 0.01147049 \tValidation Loss 0.01860790 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3738 \tTraining Loss: 0.01156662 \tValidation Loss 0.01818974 \tTraining Acuuarcy 42.414% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 3739 \tTraining Loss: 0.01148212 \tValidation Loss 0.01867247 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3740 \tTraining Loss: 0.01149488 \tValidation Loss 0.01820448 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 3741 \tTraining Loss: 0.01156320 \tValidation Loss 0.01860580 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 3742 \tTraining Loss: 0.01146176 \tValidation Loss 0.01824889 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 3743 \tTraining Loss: 0.01152551 \tValidation Loss 0.01855084 \tTraining Acuuarcy 42.398% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3744 \tTraining Loss: 0.01149116 \tValidation Loss 0.01841611 \tTraining Acuuarcy 42.364% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3745 \tTraining Loss: 0.01159355 \tValidation Loss 0.01846272 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3746 \tTraining Loss: 0.01152664 \tValidation Loss 0.01836587 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 3747 \tTraining Loss: 0.01158711 \tValidation Loss 0.01838101 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3748 \tTraining Loss: 0.01155106 \tValidation Loss 0.01797240 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3749 \tTraining Loss: 0.01155408 \tValidation Loss 0.01847770 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3750 \tTraining Loss: 0.01147835 \tValidation Loss 0.01837343 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3751 \tTraining Loss: 0.01154522 \tValidation Loss 0.01867333 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3752 \tTraining Loss: 0.01142617 \tValidation Loss 0.01860892 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3753 \tTraining Loss: 0.01149892 \tValidation Loss 0.01876459 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3754 \tTraining Loss: 0.01147615 \tValidation Loss 0.01823920 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 3755 \tTraining Loss: 0.01151492 \tValidation Loss 0.01812976 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3756 \tTraining Loss: 0.01152667 \tValidation Loss 0.01909440 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 3757 \tTraining Loss: 0.01152378 \tValidation Loss 0.01850867 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3758 \tTraining Loss: 0.01148106 \tValidation Loss 0.01874255 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 3759 \tTraining Loss: 0.01146486 \tValidation Loss 0.01848267 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3760 \tTraining Loss: 0.01148950 \tValidation Loss 0.01843606 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3761 \tTraining Loss: 0.01163575 \tValidation Loss 0.01853403 \tTraining Acuuarcy 42.180% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3762 \tTraining Loss: 0.01154776 \tValidation Loss 0.01838667 \tTraining Acuuarcy 42.548% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3763 \tTraining Loss: 0.01152042 \tValidation Loss 0.01812385 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3764 \tTraining Loss: 0.01150749 \tValidation Loss 0.01903575 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3765 \tTraining Loss: 0.01161537 \tValidation Loss 0.01833604 \tTraining Acuuarcy 42.091% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 3766 \tTraining Loss: 0.01152376 \tValidation Loss 0.01853770 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 18.640%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3767 \tTraining Loss: 0.01148844 \tValidation Loss 0.01811195 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3768 \tTraining Loss: 0.01145401 \tValidation Loss 0.01870867 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3769 \tTraining Loss: 0.01152250 \tValidation Loss 0.01829551 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 3770 \tTraining Loss: 0.01151957 \tValidation Loss 0.01841520 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 3771 \tTraining Loss: 0.01145429 \tValidation Loss 0.01817603 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3772 \tTraining Loss: 0.01153365 \tValidation Loss 0.01869816 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3773 \tTraining Loss: 0.01146821 \tValidation Loss 0.01886037 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3774 \tTraining Loss: 0.01151966 \tValidation Loss 0.01846488 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3775 \tTraining Loss: 0.01154014 \tValidation Loss 0.01833879 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3776 \tTraining Loss: 0.01150653 \tValidation Loss 0.01852587 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 3777 \tTraining Loss: 0.01151946 \tValidation Loss 0.01833904 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3778 \tTraining Loss: 0.01147657 \tValidation Loss 0.01847079 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3779 \tTraining Loss: 0.01143810 \tValidation Loss 0.01872313 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 3780 \tTraining Loss: 0.01149984 \tValidation Loss 0.01825740 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3781 \tTraining Loss: 0.01145719 \tValidation Loss 0.01877766 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3782 \tTraining Loss: 0.01154718 \tValidation Loss 0.01874698 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3783 \tTraining Loss: 0.01143264 \tValidation Loss 0.01810947 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3784 \tTraining Loss: 0.01151379 \tValidation Loss 0.01826633 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3785 \tTraining Loss: 0.01156831 \tValidation Loss 0.01822076 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3786 \tTraining Loss: 0.01155437 \tValidation Loss 0.01827794 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3787 \tTraining Loss: 0.01145926 \tValidation Loss 0.01887473 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 3788 \tTraining Loss: 0.01150822 \tValidation Loss 0.01835296 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3789 \tTraining Loss: 0.01152806 \tValidation Loss 0.01845609 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3790 \tTraining Loss: 0.01147993 \tValidation Loss 0.01895583 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 3791 \tTraining Loss: 0.01157919 \tValidation Loss 0.01816757 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 3792 \tTraining Loss: 0.01150922 \tValidation Loss 0.01815040 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 3793 \tTraining Loss: 0.01146216 \tValidation Loss 0.01886819 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3794 \tTraining Loss: 0.01155353 \tValidation Loss 0.01837213 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3795 \tTraining Loss: 0.01146765 \tValidation Loss 0.01812160 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3796 \tTraining Loss: 0.01151152 \tValidation Loss 0.01840066 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3797 \tTraining Loss: 0.01158902 \tValidation Loss 0.01813550 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 3798 \tTraining Loss: 0.01152535 \tValidation Loss 0.01835264 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3799 \tTraining Loss: 0.01148845 \tValidation Loss 0.01881286 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3800 \tTraining Loss: 0.01150116 \tValidation Loss 0.01841647 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3801 \tTraining Loss: 0.01153790 \tValidation Loss 0.01826466 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 3802 \tTraining Loss: 0.01147872 \tValidation Loss 0.01914053 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3803 \tTraining Loss: 0.01158082 \tValidation Loss 0.01864689 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 3804 \tTraining Loss: 0.01149100 \tValidation Loss 0.01826819 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3805 \tTraining Loss: 0.01149940 \tValidation Loss 0.01829755 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3806 \tTraining Loss: 0.01158547 \tValidation Loss 0.01861824 \tTraining Acuuarcy 42.097% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3807 \tTraining Loss: 0.01150165 \tValidation Loss 0.01862696 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3808 \tTraining Loss: 0.01145809 \tValidation Loss 0.01894507 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 3809 \tTraining Loss: 0.01155167 \tValidation Loss 0.01846681 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 3810 \tTraining Loss: 0.01152588 \tValidation Loss 0.01834595 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 3811 \tTraining Loss: 0.01154372 \tValidation Loss 0.01792272 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3812 \tTraining Loss: 0.01146666 \tValidation Loss 0.01810199 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3813 \tTraining Loss: 0.01152420 \tValidation Loss 0.01846389 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3814 \tTraining Loss: 0.01155914 \tValidation Loss 0.01843878 \tTraining Acuuarcy 42.704% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3815 \tTraining Loss: 0.01149028 \tValidation Loss 0.01842852 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 3816 \tTraining Loss: 0.01148283 \tValidation Loss 0.01834148 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3817 \tTraining Loss: 0.01159046 \tValidation Loss 0.01840013 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3818 \tTraining Loss: 0.01150596 \tValidation Loss 0.01840627 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3819 \tTraining Loss: 0.01147690 \tValidation Loss 0.01929356 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3820 \tTraining Loss: 0.01149955 \tValidation Loss 0.01795651 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3821 \tTraining Loss: 0.01153612 \tValidation Loss 0.01843705 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 3822 \tTraining Loss: 0.01151054 \tValidation Loss 0.01868708 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 3823 \tTraining Loss: 0.01147572 \tValidation Loss 0.01835920 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3824 \tTraining Loss: 0.01152036 \tValidation Loss 0.01854024 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3825 \tTraining Loss: 0.01153617 \tValidation Loss 0.01910345 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3826 \tTraining Loss: 0.01157459 \tValidation Loss 0.01837955 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 3827 \tTraining Loss: 0.01147154 \tValidation Loss 0.01867046 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 3828 \tTraining Loss: 0.01149277 \tValidation Loss 0.01890190 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 3829 \tTraining Loss: 0.01152512 \tValidation Loss 0.01840251 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 3830 \tTraining Loss: 0.01155072 \tValidation Loss 0.01878946 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3831 \tTraining Loss: 0.01151255 \tValidation Loss 0.01848565 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 3832 \tTraining Loss: 0.01152005 \tValidation Loss 0.01817956 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 3833 \tTraining Loss: 0.01148483 \tValidation Loss 0.01838851 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.532%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3834 \tTraining Loss: 0.01148104 \tValidation Loss 0.01832610 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3835 \tTraining Loss: 0.01145176 \tValidation Loss 0.01928211 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 3836 \tTraining Loss: 0.01156864 \tValidation Loss 0.01872754 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3837 \tTraining Loss: 0.01147462 \tValidation Loss 0.01809887 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3838 \tTraining Loss: 0.01151853 \tValidation Loss 0.01864892 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 3839 \tTraining Loss: 0.01150673 \tValidation Loss 0.01822649 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3840 \tTraining Loss: 0.01156326 \tValidation Loss 0.01911165 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3841 \tTraining Loss: 0.01157020 \tValidation Loss 0.01885496 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 3842 \tTraining Loss: 0.01140800 \tValidation Loss 0.01843169 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 3843 \tTraining Loss: 0.01147821 \tValidation Loss 0.01858761 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3844 \tTraining Loss: 0.01153600 \tValidation Loss 0.01919061 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3845 \tTraining Loss: 0.01147530 \tValidation Loss 0.01902531 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 3846 \tTraining Loss: 0.01156773 \tValidation Loss 0.01878960 \tTraining Acuuarcy 42.387% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3847 \tTraining Loss: 0.01157872 \tValidation Loss 0.01821477 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3848 \tTraining Loss: 0.01146184 \tValidation Loss 0.01819586 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3849 \tTraining Loss: 0.01154733 \tValidation Loss 0.01823069 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3850 \tTraining Loss: 0.01148609 \tValidation Loss 0.01858655 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3851 \tTraining Loss: 0.01141483 \tValidation Loss 0.01876228 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3852 \tTraining Loss: 0.01145847 \tValidation Loss 0.01876961 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3853 \tTraining Loss: 0.01146369 \tValidation Loss 0.01835964 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3854 \tTraining Loss: 0.01149691 \tValidation Loss 0.01839402 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3855 \tTraining Loss: 0.01149461 \tValidation Loss 0.01875714 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3856 \tTraining Loss: 0.01157366 \tValidation Loss 0.01877539 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3857 \tTraining Loss: 0.01158548 \tValidation Loss 0.01872679 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3858 \tTraining Loss: 0.01154896 \tValidation Loss 0.01856005 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 3859 \tTraining Loss: 0.01142506 \tValidation Loss 0.01889661 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 3860 \tTraining Loss: 0.01155729 \tValidation Loss 0.01872764 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 3861 \tTraining Loss: 0.01153727 \tValidation Loss 0.01784785 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3862 \tTraining Loss: 0.01163081 \tValidation Loss 0.01884722 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 3863 \tTraining Loss: 0.01146167 \tValidation Loss 0.01826345 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 3864 \tTraining Loss: 0.01154518 \tValidation Loss 0.01863365 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 3865 \tTraining Loss: 0.01154517 \tValidation Loss 0.01829898 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3866 \tTraining Loss: 0.01140907 \tValidation Loss 0.01849479 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 3867 \tTraining Loss: 0.01144670 \tValidation Loss 0.01901652 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3868 \tTraining Loss: 0.01145598 \tValidation Loss 0.01881604 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3869 \tTraining Loss: 0.01142510 \tValidation Loss 0.01813155 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3870 \tTraining Loss: 0.01149736 \tValidation Loss 0.01882399 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3871 \tTraining Loss: 0.01151924 \tValidation Loss 0.01851110 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 3872 \tTraining Loss: 0.01150459 \tValidation Loss 0.01813605 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 3873 \tTraining Loss: 0.01150532 \tValidation Loss 0.01838844 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3874 \tTraining Loss: 0.01152379 \tValidation Loss 0.01889985 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 3875 \tTraining Loss: 0.01147336 \tValidation Loss 0.01848272 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 3876 \tTraining Loss: 0.01153554 \tValidation Loss 0.01817591 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 3877 \tTraining Loss: 0.01160655 \tValidation Loss 0.01828039 \tTraining Acuuarcy 42.492% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 3878 \tTraining Loss: 0.01151851 \tValidation Loss 0.01882027 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3879 \tTraining Loss: 0.01147903 \tValidation Loss 0.01856746 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3880 \tTraining Loss: 0.01147818 \tValidation Loss 0.01870115 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 3881 \tTraining Loss: 0.01147092 \tValidation Loss 0.01886863 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 3882 \tTraining Loss: 0.01151189 \tValidation Loss 0.01825213 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3883 \tTraining Loss: 0.01150151 \tValidation Loss 0.01828601 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3884 \tTraining Loss: 0.01143848 \tValidation Loss 0.01853076 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3885 \tTraining Loss: 0.01157145 \tValidation Loss 0.01827023 \tTraining Acuuarcy 42.303% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 3886 \tTraining Loss: 0.01150218 \tValidation Loss 0.01835641 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3887 \tTraining Loss: 0.01151601 \tValidation Loss 0.01904255 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3888 \tTraining Loss: 0.01147561 \tValidation Loss 0.01834137 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 3889 \tTraining Loss: 0.01145305 \tValidation Loss 0.01821049 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3890 \tTraining Loss: 0.01145085 \tValidation Loss 0.01834558 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3891 \tTraining Loss: 0.01154584 \tValidation Loss 0.01853786 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3892 \tTraining Loss: 0.01141580 \tValidation Loss 0.01856023 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3893 \tTraining Loss: 0.01144167 \tValidation Loss 0.01854846 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3894 \tTraining Loss: 0.01150627 \tValidation Loss 0.01881440 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 3895 \tTraining Loss: 0.01155193 \tValidation Loss 0.01850486 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3896 \tTraining Loss: 0.01148748 \tValidation Loss 0.01834426 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 3897 \tTraining Loss: 0.01148910 \tValidation Loss 0.01820089 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3898 \tTraining Loss: 0.01156157 \tValidation Loss 0.01873945 \tTraining Acuuarcy 42.665% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 3899 \tTraining Loss: 0.01152183 \tValidation Loss 0.01855782 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3900 \tTraining Loss: 0.01147557 \tValidation Loss 0.01895606 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 18.696%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3901 \tTraining Loss: 0.01140322 \tValidation Loss 0.01811296 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 3902 \tTraining Loss: 0.01158193 \tValidation Loss 0.01832495 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3903 \tTraining Loss: 0.01155862 \tValidation Loss 0.01838615 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3904 \tTraining Loss: 0.01146861 \tValidation Loss 0.01820188 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 3905 \tTraining Loss: 0.01145602 \tValidation Loss 0.01879811 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 3906 \tTraining Loss: 0.01149182 \tValidation Loss 0.01856300 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 3907 \tTraining Loss: 0.01146040 \tValidation Loss 0.01849611 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 3908 \tTraining Loss: 0.01147307 \tValidation Loss 0.01870449 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3909 \tTraining Loss: 0.01148993 \tValidation Loss 0.01865161 \tTraining Acuuarcy 42.933% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 3910 \tTraining Loss: 0.01146283 \tValidation Loss 0.01847522 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 3911 \tTraining Loss: 0.01135860 \tValidation Loss 0.01885803 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 3912 \tTraining Loss: 0.01142960 \tValidation Loss 0.01864641 \tTraining Acuuarcy 43.412% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3913 \tTraining Loss: 0.01150980 \tValidation Loss 0.01837636 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3914 \tTraining Loss: 0.01151165 \tValidation Loss 0.01863652 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 3915 \tTraining Loss: 0.01144347 \tValidation Loss 0.01843396 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3916 \tTraining Loss: 0.01158004 \tValidation Loss 0.01881608 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 3917 \tTraining Loss: 0.01151071 \tValidation Loss 0.01836608 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 3918 \tTraining Loss: 0.01147093 \tValidation Loss 0.01850208 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3919 \tTraining Loss: 0.01147179 \tValidation Loss 0.01865578 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3920 \tTraining Loss: 0.01149298 \tValidation Loss 0.01841941 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3921 \tTraining Loss: 0.01154355 \tValidation Loss 0.01855693 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 3922 \tTraining Loss: 0.01148490 \tValidation Loss 0.01812155 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 3923 \tTraining Loss: 0.01147304 \tValidation Loss 0.01826867 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 3924 \tTraining Loss: 0.01150658 \tValidation Loss 0.01918271 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 3925 \tTraining Loss: 0.01148238 \tValidation Loss 0.01815112 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3926 \tTraining Loss: 0.01152774 \tValidation Loss 0.01816035 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 3927 \tTraining Loss: 0.01148894 \tValidation Loss 0.01860505 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3928 \tTraining Loss: 0.01141938 \tValidation Loss 0.01820701 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3929 \tTraining Loss: 0.01140990 \tValidation Loss 0.01841779 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 3930 \tTraining Loss: 0.01155073 \tValidation Loss 0.01833163 \tTraining Acuuarcy 42.933% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 3931 \tTraining Loss: 0.01157513 \tValidation Loss 0.01842371 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 3932 \tTraining Loss: 0.01149725 \tValidation Loss 0.01871645 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3933 \tTraining Loss: 0.01148386 \tValidation Loss 0.01873112 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 3934 \tTraining Loss: 0.01156672 \tValidation Loss 0.01857288 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3935 \tTraining Loss: 0.01151415 \tValidation Loss 0.01861581 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 3936 \tTraining Loss: 0.01152542 \tValidation Loss 0.01833724 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 3937 \tTraining Loss: 0.01144478 \tValidation Loss 0.01867222 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3938 \tTraining Loss: 0.01146917 \tValidation Loss 0.01843071 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3939 \tTraining Loss: 0.01144961 \tValidation Loss 0.01861536 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 3940 \tTraining Loss: 0.01149564 \tValidation Loss 0.01906172 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3941 \tTraining Loss: 0.01154472 \tValidation Loss 0.01803483 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 3942 \tTraining Loss: 0.01145213 \tValidation Loss 0.01838496 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 3943 \tTraining Loss: 0.01156390 \tValidation Loss 0.01835238 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 3944 \tTraining Loss: 0.01145986 \tValidation Loss 0.01822980 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 3945 \tTraining Loss: 0.01145069 \tValidation Loss 0.01842090 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 3946 \tTraining Loss: 0.01157129 \tValidation Loss 0.01885299 \tTraining Acuuarcy 42.348% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 3947 \tTraining Loss: 0.01150765 \tValidation Loss 0.01842725 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 3948 \tTraining Loss: 0.01140156 \tValidation Loss 0.01868152 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 3949 \tTraining Loss: 0.01150479 \tValidation Loss 0.01857003 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 3950 \tTraining Loss: 0.01140683 \tValidation Loss 0.01880747 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3951 \tTraining Loss: 0.01141081 \tValidation Loss 0.01863899 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 3952 \tTraining Loss: 0.01141108 \tValidation Loss 0.01911770 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 3953 \tTraining Loss: 0.01148207 \tValidation Loss 0.01873587 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 3954 \tTraining Loss: 0.01141596 \tValidation Loss 0.01878995 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3955 \tTraining Loss: 0.01155912 \tValidation Loss 0.01861060 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3956 \tTraining Loss: 0.01145719 \tValidation Loss 0.01911704 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3957 \tTraining Loss: 0.01155460 \tValidation Loss 0.01852240 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3958 \tTraining Loss: 0.01152762 \tValidation Loss 0.01822742 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 3959 \tTraining Loss: 0.01147986 \tValidation Loss 0.01848390 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 3960 \tTraining Loss: 0.01145117 \tValidation Loss 0.01845041 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 3961 \tTraining Loss: 0.01144284 \tValidation Loss 0.01865291 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 3962 \tTraining Loss: 0.01154587 \tValidation Loss 0.01790185 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3963 \tTraining Loss: 0.01157931 \tValidation Loss 0.01863403 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3964 \tTraining Loss: 0.01159669 \tValidation Loss 0.01820387 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 3965 \tTraining Loss: 0.01142974 \tValidation Loss 0.01833881 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3966 \tTraining Loss: 0.01144496 \tValidation Loss 0.01875593 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3967 \tTraining Loss: 0.01152098 \tValidation Loss 0.01812384 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.170%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3968 \tTraining Loss: 0.01145503 \tValidation Loss 0.01894669 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 3969 \tTraining Loss: 0.01155618 \tValidation Loss 0.01816943 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 3970 \tTraining Loss: 0.01146566 \tValidation Loss 0.01892760 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 3971 \tTraining Loss: 0.01157738 \tValidation Loss 0.01877265 \tTraining Acuuarcy 42.426% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 3972 \tTraining Loss: 0.01145942 \tValidation Loss 0.01817347 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3973 \tTraining Loss: 0.01140968 \tValidation Loss 0.01837882 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 3974 \tTraining Loss: 0.01153872 \tValidation Loss 0.01855843 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 3975 \tTraining Loss: 0.01141688 \tValidation Loss 0.01874296 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 3976 \tTraining Loss: 0.01147812 \tValidation Loss 0.01895521 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 3977 \tTraining Loss: 0.01146961 \tValidation Loss 0.01868251 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3978 \tTraining Loss: 0.01150460 \tValidation Loss 0.01842512 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3979 \tTraining Loss: 0.01160027 \tValidation Loss 0.01843273 \tTraining Acuuarcy 42.442% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 3980 \tTraining Loss: 0.01155256 \tValidation Loss 0.01832626 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 3981 \tTraining Loss: 0.01148481 \tValidation Loss 0.01879028 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 3982 \tTraining Loss: 0.01155224 \tValidation Loss 0.01866660 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3983 \tTraining Loss: 0.01152055 \tValidation Loss 0.01876150 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 3984 \tTraining Loss: 0.01148091 \tValidation Loss 0.01874766 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 3985 \tTraining Loss: 0.01153121 \tValidation Loss 0.01858889 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 3986 \tTraining Loss: 0.01152181 \tValidation Loss 0.01850656 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 3987 \tTraining Loss: 0.01147919 \tValidation Loss 0.01857294 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 3988 \tTraining Loss: 0.01157852 \tValidation Loss 0.01851120 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 3989 \tTraining Loss: 0.01153495 \tValidation Loss 0.01836723 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 3990 \tTraining Loss: 0.01149477 \tValidation Loss 0.01830485 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 3991 \tTraining Loss: 0.01153653 \tValidation Loss 0.01855878 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3992 \tTraining Loss: 0.01140791 \tValidation Loss 0.01884970 \tTraining Acuuarcy 43.986% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 3993 \tTraining Loss: 0.01146288 \tValidation Loss 0.01862181 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 3994 \tTraining Loss: 0.01149264 \tValidation Loss 0.01873195 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 3995 \tTraining Loss: 0.01154444 \tValidation Loss 0.01823818 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 3996 \tTraining Loss: 0.01152932 \tValidation Loss 0.01861096 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 3997 \tTraining Loss: 0.01149371 \tValidation Loss 0.01824339 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 3998 \tTraining Loss: 0.01145615 \tValidation Loss 0.01875355 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 3999 \tTraining Loss: 0.01142386 \tValidation Loss 0.01884172 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 4000 \tTraining Loss: 0.01147584 \tValidation Loss 0.01845588 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4001 \tTraining Loss: 0.01157607 \tValidation Loss 0.01823027 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 4002 \tTraining Loss: 0.01147489 \tValidation Loss 0.01873153 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 4003 \tTraining Loss: 0.01152524 \tValidation Loss 0.01846294 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4004 \tTraining Loss: 0.01149348 \tValidation Loss 0.01857687 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4005 \tTraining Loss: 0.01146416 \tValidation Loss 0.01835052 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4006 \tTraining Loss: 0.01151355 \tValidation Loss 0.01852974 \tTraining Acuuarcy 42.459% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4007 \tTraining Loss: 0.01150822 \tValidation Loss 0.01839498 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4008 \tTraining Loss: 0.01156806 \tValidation Loss 0.01826588 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4009 \tTraining Loss: 0.01155922 \tValidation Loss 0.01874447 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4010 \tTraining Loss: 0.01142576 \tValidation Loss 0.01837531 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4011 \tTraining Loss: 0.01149103 \tValidation Loss 0.01838144 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 4012 \tTraining Loss: 0.01149094 \tValidation Loss 0.01859991 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 4013 \tTraining Loss: 0.01148434 \tValidation Loss 0.01882949 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4014 \tTraining Loss: 0.01147585 \tValidation Loss 0.01850638 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4015 \tTraining Loss: 0.01154074 \tValidation Loss 0.01833353 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4016 \tTraining Loss: 0.01155649 \tValidation Loss 0.01892113 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4017 \tTraining Loss: 0.01144992 \tValidation Loss 0.01866506 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4018 \tTraining Loss: 0.01147688 \tValidation Loss 0.01900930 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 4019 \tTraining Loss: 0.01149188 \tValidation Loss 0.01871575 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4020 \tTraining Loss: 0.01149058 \tValidation Loss 0.01853144 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4021 \tTraining Loss: 0.01149843 \tValidation Loss 0.01843231 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 4022 \tTraining Loss: 0.01150199 \tValidation Loss 0.01832853 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4023 \tTraining Loss: 0.01144383 \tValidation Loss 0.01854533 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4024 \tTraining Loss: 0.01147498 \tValidation Loss 0.01850334 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 4025 \tTraining Loss: 0.01140678 \tValidation Loss 0.01854853 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4026 \tTraining Loss: 0.01156242 \tValidation Loss 0.01837667 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 4027 \tTraining Loss: 0.01151081 \tValidation Loss 0.01826215 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4028 \tTraining Loss: 0.01143201 \tValidation Loss 0.01860886 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 4029 \tTraining Loss: 0.01143175 \tValidation Loss 0.01832320 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 4030 \tTraining Loss: 0.01146513 \tValidation Loss 0.01847970 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4031 \tTraining Loss: 0.01152253 \tValidation Loss 0.01868235 \tTraining Acuuarcy 42.710% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4032 \tTraining Loss: 0.01154763 \tValidation Loss 0.01866655 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4033 \tTraining Loss: 0.01146772 \tValidation Loss 0.01880275 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4034 \tTraining Loss: 0.01147525 \tValidation Loss 0.01813680 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.170%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4035 \tTraining Loss: 0.01136845 \tValidation Loss 0.01835328 \tTraining Acuuarcy 44.220% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4036 \tTraining Loss: 0.01143556 \tValidation Loss 0.01855679 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 4037 \tTraining Loss: 0.01149070 \tValidation Loss 0.01825075 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4038 \tTraining Loss: 0.01159383 \tValidation Loss 0.01844905 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 4039 \tTraining Loss: 0.01149058 \tValidation Loss 0.01853648 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4040 \tTraining Loss: 0.01147109 \tValidation Loss 0.01814848 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4041 \tTraining Loss: 0.01148364 \tValidation Loss 0.01815632 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4042 \tTraining Loss: 0.01150899 \tValidation Loss 0.01812863 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4043 \tTraining Loss: 0.01144190 \tValidation Loss 0.01821760 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4044 \tTraining Loss: 0.01153672 \tValidation Loss 0.01851858 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 4045 \tTraining Loss: 0.01148539 \tValidation Loss 0.01846990 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 4046 \tTraining Loss: 0.01148351 \tValidation Loss 0.01872771 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4047 \tTraining Loss: 0.01146777 \tValidation Loss 0.01839983 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4048 \tTraining Loss: 0.01148374 \tValidation Loss 0.01811134 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4049 \tTraining Loss: 0.01141809 \tValidation Loss 0.01900987 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 4050 \tTraining Loss: 0.01152144 \tValidation Loss 0.01838077 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4051 \tTraining Loss: 0.01145755 \tValidation Loss 0.01809926 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 4052 \tTraining Loss: 0.01144842 \tValidation Loss 0.01830742 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4053 \tTraining Loss: 0.01151785 \tValidation Loss 0.01833053 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 4054 \tTraining Loss: 0.01150364 \tValidation Loss 0.01902638 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4055 \tTraining Loss: 0.01149199 \tValidation Loss 0.01823198 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4056 \tTraining Loss: 0.01145236 \tValidation Loss 0.01869582 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4057 \tTraining Loss: 0.01157854 \tValidation Loss 0.01833721 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 4058 \tTraining Loss: 0.01146433 \tValidation Loss 0.01851382 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4059 \tTraining Loss: 0.01138019 \tValidation Loss 0.01915045 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 4060 \tTraining Loss: 0.01158978 \tValidation Loss 0.01840246 \tTraining Acuuarcy 42.559% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4061 \tTraining Loss: 0.01153584 \tValidation Loss 0.01823589 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 4062 \tTraining Loss: 0.01147215 \tValidation Loss 0.01837583 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4063 \tTraining Loss: 0.01152650 \tValidation Loss 0.01887814 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4064 \tTraining Loss: 0.01149955 \tValidation Loss 0.01863107 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4065 \tTraining Loss: 0.01154066 \tValidation Loss 0.01881536 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4066 \tTraining Loss: 0.01148054 \tValidation Loss 0.01850684 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4067 \tTraining Loss: 0.01150439 \tValidation Loss 0.01887113 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4068 \tTraining Loss: 0.01152658 \tValidation Loss 0.01851064 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4069 \tTraining Loss: 0.01152192 \tValidation Loss 0.01805008 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4070 \tTraining Loss: 0.01142542 \tValidation Loss 0.01839613 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4071 \tTraining Loss: 0.01147395 \tValidation Loss 0.01854815 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4072 \tTraining Loss: 0.01150092 \tValidation Loss 0.01877060 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4073 \tTraining Loss: 0.01147393 \tValidation Loss 0.01856904 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4074 \tTraining Loss: 0.01150425 \tValidation Loss 0.01873129 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 4075 \tTraining Loss: 0.01150298 \tValidation Loss 0.01868861 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4076 \tTraining Loss: 0.01156326 \tValidation Loss 0.01826317 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 4077 \tTraining Loss: 0.01149666 \tValidation Loss 0.01834875 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4078 \tTraining Loss: 0.01159826 \tValidation Loss 0.01794252 \tTraining Acuuarcy 42.197% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4079 \tTraining Loss: 0.01140933 \tValidation Loss 0.01886114 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 4080 \tTraining Loss: 0.01149205 \tValidation Loss 0.01838823 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4081 \tTraining Loss: 0.01147067 \tValidation Loss 0.01873043 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 4082 \tTraining Loss: 0.01153782 \tValidation Loss 0.01871317 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 4083 \tTraining Loss: 0.01156200 \tValidation Loss 0.01841867 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4084 \tTraining Loss: 0.01153290 \tValidation Loss 0.01830610 \tTraining Acuuarcy 42.331% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4085 \tTraining Loss: 0.01140927 \tValidation Loss 0.01847448 \tTraining Acuuarcy 43.908% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4086 \tTraining Loss: 0.01150829 \tValidation Loss 0.01810321 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4087 \tTraining Loss: 0.01144782 \tValidation Loss 0.01860469 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 4088 \tTraining Loss: 0.01147759 \tValidation Loss 0.01870372 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4089 \tTraining Loss: 0.01153720 \tValidation Loss 0.01858572 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 4090 \tTraining Loss: 0.01148826 \tValidation Loss 0.01852962 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 4091 \tTraining Loss: 0.01145286 \tValidation Loss 0.01834968 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4092 \tTraining Loss: 0.01149416 \tValidation Loss 0.01862504 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 4093 \tTraining Loss: 0.01142377 \tValidation Loss 0.01828015 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4094 \tTraining Loss: 0.01154642 \tValidation Loss 0.01879358 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4095 \tTraining Loss: 0.01148020 \tValidation Loss 0.01849422 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4096 \tTraining Loss: 0.01156144 \tValidation Loss 0.01883677 \tTraining Acuuarcy 42.710% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4097 \tTraining Loss: 0.01154784 \tValidation Loss 0.01809927 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 4098 \tTraining Loss: 0.01148775 \tValidation Loss 0.01885507 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 4099 \tTraining Loss: 0.01154018 \tValidation Loss 0.01866800 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 4100 \tTraining Loss: 0.01143967 \tValidation Loss 0.01812614 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 4101 \tTraining Loss: 0.01151351 \tValidation Loss 0.01886913 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.448%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4102 \tTraining Loss: 0.01153377 \tValidation Loss 0.01821287 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 4103 \tTraining Loss: 0.01149468 \tValidation Loss 0.01865462 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4104 \tTraining Loss: 0.01151048 \tValidation Loss 0.01852139 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4105 \tTraining Loss: 0.01145252 \tValidation Loss 0.01835614 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4106 \tTraining Loss: 0.01150854 \tValidation Loss 0.01809084 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4107 \tTraining Loss: 0.01162844 \tValidation Loss 0.01876247 \tTraining Acuuarcy 42.125% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 4108 \tTraining Loss: 0.01142907 \tValidation Loss 0.01846017 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 4109 \tTraining Loss: 0.01150129 \tValidation Loss 0.01836582 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4110 \tTraining Loss: 0.01150901 \tValidation Loss 0.01854498 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4111 \tTraining Loss: 0.01145499 \tValidation Loss 0.01835123 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4112 \tTraining Loss: 0.01146106 \tValidation Loss 0.01892357 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4113 \tTraining Loss: 0.01151958 \tValidation Loss 0.01820700 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4114 \tTraining Loss: 0.01148537 \tValidation Loss 0.01916905 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4115 \tTraining Loss: 0.01162605 \tValidation Loss 0.01815266 \tTraining Acuuarcy 42.359% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4116 \tTraining Loss: 0.01150957 \tValidation Loss 0.01802192 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4117 \tTraining Loss: 0.01153580 \tValidation Loss 0.01846772 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 4118 \tTraining Loss: 0.01147977 \tValidation Loss 0.01829546 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 4119 \tTraining Loss: 0.01152168 \tValidation Loss 0.01832409 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4120 \tTraining Loss: 0.01146774 \tValidation Loss 0.01878491 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4121 \tTraining Loss: 0.01160316 \tValidation Loss 0.01829466 \tTraining Acuuarcy 42.448% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4122 \tTraining Loss: 0.01146333 \tValidation Loss 0.01854603 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4123 \tTraining Loss: 0.01144793 \tValidation Loss 0.01826497 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 4124 \tTraining Loss: 0.01156340 \tValidation Loss 0.01890115 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 4125 \tTraining Loss: 0.01149679 \tValidation Loss 0.01838906 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4126 \tTraining Loss: 0.01146919 \tValidation Loss 0.01842047 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4127 \tTraining Loss: 0.01158062 \tValidation Loss 0.01805373 \tTraining Acuuarcy 42.325% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4128 \tTraining Loss: 0.01159470 \tValidation Loss 0.01823304 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4129 \tTraining Loss: 0.01144483 \tValidation Loss 0.01852738 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4130 \tTraining Loss: 0.01153148 \tValidation Loss 0.01830277 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4131 \tTraining Loss: 0.01147098 \tValidation Loss 0.01818682 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4132 \tTraining Loss: 0.01157560 \tValidation Loss 0.01856836 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4133 \tTraining Loss: 0.01146062 \tValidation Loss 0.01830203 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 4134 \tTraining Loss: 0.01140719 \tValidation Loss 0.01907515 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 17.637%\n",
      "Epoch: 4135 \tTraining Loss: 0.01148632 \tValidation Loss 0.01890628 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4136 \tTraining Loss: 0.01151776 \tValidation Loss 0.01804419 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4137 \tTraining Loss: 0.01144013 \tValidation Loss 0.01869292 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4138 \tTraining Loss: 0.01140275 \tValidation Loss 0.01850931 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4139 \tTraining Loss: 0.01149380 \tValidation Loss 0.01830708 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4140 \tTraining Loss: 0.01148462 \tValidation Loss 0.01851606 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4141 \tTraining Loss: 0.01154781 \tValidation Loss 0.01843521 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4142 \tTraining Loss: 0.01150867 \tValidation Loss 0.01880233 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 4143 \tTraining Loss: 0.01150358 \tValidation Loss 0.01851769 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4144 \tTraining Loss: 0.01148073 \tValidation Loss 0.01818983 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4145 \tTraining Loss: 0.01140654 \tValidation Loss 0.01833657 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 4146 \tTraining Loss: 0.01154065 \tValidation Loss 0.01836207 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 4147 \tTraining Loss: 0.01140989 \tValidation Loss 0.01829591 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4148 \tTraining Loss: 0.01143089 \tValidation Loss 0.01837330 \tTraining Acuuarcy 44.059% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4149 \tTraining Loss: 0.01146375 \tValidation Loss 0.01865208 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4150 \tTraining Loss: 0.01148116 \tValidation Loss 0.01857992 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 4151 \tTraining Loss: 0.01142014 \tValidation Loss 0.01845231 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4152 \tTraining Loss: 0.01153257 \tValidation Loss 0.01838767 \tTraining Acuuarcy 42.682% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4153 \tTraining Loss: 0.01151025 \tValidation Loss 0.01861287 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4154 \tTraining Loss: 0.01154279 \tValidation Loss 0.01863591 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4155 \tTraining Loss: 0.01137905 \tValidation Loss 0.01865717 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4156 \tTraining Loss: 0.01150314 \tValidation Loss 0.01867734 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4157 \tTraining Loss: 0.01154589 \tValidation Loss 0.01835571 \tTraining Acuuarcy 42.810% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4158 \tTraining Loss: 0.01142043 \tValidation Loss 0.01845623 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4159 \tTraining Loss: 0.01154663 \tValidation Loss 0.01807642 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4160 \tTraining Loss: 0.01153372 \tValidation Loss 0.01859714 \tTraining Acuuarcy 42.665% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4161 \tTraining Loss: 0.01142795 \tValidation Loss 0.01866921 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 4162 \tTraining Loss: 0.01137580 \tValidation Loss 0.01846451 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4163 \tTraining Loss: 0.01151551 \tValidation Loss 0.01870592 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 4164 \tTraining Loss: 0.01149850 \tValidation Loss 0.01790174 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4165 \tTraining Loss: 0.01152631 \tValidation Loss 0.01858492 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4166 \tTraining Loss: 0.01147059 \tValidation Loss 0.01858601 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 4167 \tTraining Loss: 0.01145305 \tValidation Loss 0.01895230 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 4168 \tTraining Loss: 0.01137540 \tValidation Loss 0.01879630 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.811%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4169 \tTraining Loss: 0.01148427 \tValidation Loss 0.01792580 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 4170 \tTraining Loss: 0.01159811 \tValidation Loss 0.01910199 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 4171 \tTraining Loss: 0.01158749 \tValidation Loss 0.01818302 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4172 \tTraining Loss: 0.01154990 \tValidation Loss 0.01836244 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4173 \tTraining Loss: 0.01145205 \tValidation Loss 0.01869677 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4174 \tTraining Loss: 0.01148847 \tValidation Loss 0.01898100 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4175 \tTraining Loss: 0.01152196 \tValidation Loss 0.01834201 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 4176 \tTraining Loss: 0.01146739 \tValidation Loss 0.01851634 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4177 \tTraining Loss: 0.01139235 \tValidation Loss 0.01861685 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4178 \tTraining Loss: 0.01154948 \tValidation Loss 0.01862850 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 4179 \tTraining Loss: 0.01143179 \tValidation Loss 0.01852008 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 4180 \tTraining Loss: 0.01159273 \tValidation Loss 0.01821559 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4181 \tTraining Loss: 0.01145653 \tValidation Loss 0.01873752 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4182 \tTraining Loss: 0.01154841 \tValidation Loss 0.01845085 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 4183 \tTraining Loss: 0.01152566 \tValidation Loss 0.01825297 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 4184 \tTraining Loss: 0.01152499 \tValidation Loss 0.01848329 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4185 \tTraining Loss: 0.01157784 \tValidation Loss 0.01844628 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4186 \tTraining Loss: 0.01139408 \tValidation Loss 0.01865487 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4187 \tTraining Loss: 0.01148457 \tValidation Loss 0.01862483 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4188 \tTraining Loss: 0.01144139 \tValidation Loss 0.01860888 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4189 \tTraining Loss: 0.01143260 \tValidation Loss 0.01832928 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4190 \tTraining Loss: 0.01156410 \tValidation Loss 0.01857316 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4191 \tTraining Loss: 0.01148818 \tValidation Loss 0.01862166 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4192 \tTraining Loss: 0.01150316 \tValidation Loss 0.01849737 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4193 \tTraining Loss: 0.01153944 \tValidation Loss 0.01830532 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4194 \tTraining Loss: 0.01156391 \tValidation Loss 0.01837665 \tTraining Acuuarcy 42.515% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4195 \tTraining Loss: 0.01140690 \tValidation Loss 0.01873494 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4196 \tTraining Loss: 0.01140964 \tValidation Loss 0.01860353 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 4197 \tTraining Loss: 0.01153096 \tValidation Loss 0.01827202 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4198 \tTraining Loss: 0.01150980 \tValidation Loss 0.01849915 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4199 \tTraining Loss: 0.01146582 \tValidation Loss 0.01836420 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4200 \tTraining Loss: 0.01145616 \tValidation Loss 0.01831929 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 4201 \tTraining Loss: 0.01158289 \tValidation Loss 0.01826081 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 4202 \tTraining Loss: 0.01143742 \tValidation Loss 0.01866514 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4203 \tTraining Loss: 0.01155613 \tValidation Loss 0.01869190 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4204 \tTraining Loss: 0.01148940 \tValidation Loss 0.01854261 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4205 \tTraining Loss: 0.01145744 \tValidation Loss 0.01876815 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 4206 \tTraining Loss: 0.01154746 \tValidation Loss 0.01842438 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4207 \tTraining Loss: 0.01146227 \tValidation Loss 0.01824258 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 4208 \tTraining Loss: 0.01148811 \tValidation Loss 0.01867581 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4209 \tTraining Loss: 0.01143952 \tValidation Loss 0.01890195 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4210 \tTraining Loss: 0.01155529 \tValidation Loss 0.01863261 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4211 \tTraining Loss: 0.01145134 \tValidation Loss 0.01892823 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4212 \tTraining Loss: 0.01157719 \tValidation Loss 0.01875968 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 4213 \tTraining Loss: 0.01141642 \tValidation Loss 0.01875855 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4214 \tTraining Loss: 0.01146884 \tValidation Loss 0.01813268 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4215 \tTraining Loss: 0.01150486 \tValidation Loss 0.01879048 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4216 \tTraining Loss: 0.01150084 \tValidation Loss 0.01828409 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4217 \tTraining Loss: 0.01142833 \tValidation Loss 0.01827638 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 4218 \tTraining Loss: 0.01140497 \tValidation Loss 0.01881172 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4219 \tTraining Loss: 0.01152716 \tValidation Loss 0.01820565 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4220 \tTraining Loss: 0.01133460 \tValidation Loss 0.01815888 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4221 \tTraining Loss: 0.01160640 \tValidation Loss 0.01833174 \tTraining Acuuarcy 42.487% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 4222 \tTraining Loss: 0.01151068 \tValidation Loss 0.01868123 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 4223 \tTraining Loss: 0.01144366 \tValidation Loss 0.01879883 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4224 \tTraining Loss: 0.01142223 \tValidation Loss 0.01832319 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4225 \tTraining Loss: 0.01145586 \tValidation Loss 0.01841057 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4226 \tTraining Loss: 0.01146543 \tValidation Loss 0.01833454 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4227 \tTraining Loss: 0.01156042 \tValidation Loss 0.01803172 \tTraining Acuuarcy 42.665% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4228 \tTraining Loss: 0.01147918 \tValidation Loss 0.01882048 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4229 \tTraining Loss: 0.01144714 \tValidation Loss 0.01812643 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4230 \tTraining Loss: 0.01146743 \tValidation Loss 0.01866179 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4231 \tTraining Loss: 0.01141981 \tValidation Loss 0.01862925 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4232 \tTraining Loss: 0.01143978 \tValidation Loss 0.01838377 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4233 \tTraining Loss: 0.01145667 \tValidation Loss 0.01847564 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4234 \tTraining Loss: 0.01152822 \tValidation Loss 0.01850349 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 4235 \tTraining Loss: 0.01157501 \tValidation Loss 0.01864792 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 19.504%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4236 \tTraining Loss: 0.01144269 \tValidation Loss 0.01816297 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4237 \tTraining Loss: 0.01143121 \tValidation Loss 0.01852140 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4238 \tTraining Loss: 0.01149545 \tValidation Loss 0.01812339 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4239 \tTraining Loss: 0.01151774 \tValidation Loss 0.01836733 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 4240 \tTraining Loss: 0.01147905 \tValidation Loss 0.01830199 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4241 \tTraining Loss: 0.01153701 \tValidation Loss 0.01862276 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4242 \tTraining Loss: 0.01141730 \tValidation Loss 0.01889568 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4243 \tTraining Loss: 0.01153907 \tValidation Loss 0.01853757 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4244 \tTraining Loss: 0.01148574 \tValidation Loss 0.01879848 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4245 \tTraining Loss: 0.01151108 \tValidation Loss 0.01857100 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4246 \tTraining Loss: 0.01147968 \tValidation Loss 0.01846234 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4247 \tTraining Loss: 0.01148528 \tValidation Loss 0.01852427 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4248 \tTraining Loss: 0.01145455 \tValidation Loss 0.01826404 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4249 \tTraining Loss: 0.01147918 \tValidation Loss 0.01850392 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4250 \tTraining Loss: 0.01150478 \tValidation Loss 0.01867323 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4251 \tTraining Loss: 0.01145282 \tValidation Loss 0.01830405 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4252 \tTraining Loss: 0.01145943 \tValidation Loss 0.01876817 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4253 \tTraining Loss: 0.01158824 \tValidation Loss 0.01805744 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4254 \tTraining Loss: 0.01153343 \tValidation Loss 0.01864324 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4255 \tTraining Loss: 0.01141881 \tValidation Loss 0.01807720 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4256 \tTraining Loss: 0.01146194 \tValidation Loss 0.01850390 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4257 \tTraining Loss: 0.01143912 \tValidation Loss 0.01836543 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 4258 \tTraining Loss: 0.01155940 \tValidation Loss 0.01909493 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4259 \tTraining Loss: 0.01140889 \tValidation Loss 0.01817625 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4260 \tTraining Loss: 0.01145561 \tValidation Loss 0.01851220 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4261 \tTraining Loss: 0.01149164 \tValidation Loss 0.01868932 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4262 \tTraining Loss: 0.01148971 \tValidation Loss 0.01842341 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4263 \tTraining Loss: 0.01158087 \tValidation Loss 0.01866685 \tTraining Acuuarcy 42.504% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4264 \tTraining Loss: 0.01148844 \tValidation Loss 0.01868256 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4265 \tTraining Loss: 0.01150524 \tValidation Loss 0.01836314 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4266 \tTraining Loss: 0.01147529 \tValidation Loss 0.01891332 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4267 \tTraining Loss: 0.01142772 \tValidation Loss 0.01852355 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 4268 \tTraining Loss: 0.01146406 \tValidation Loss 0.01858650 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4269 \tTraining Loss: 0.01144087 \tValidation Loss 0.01868013 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4270 \tTraining Loss: 0.01146884 \tValidation Loss 0.01862646 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4271 \tTraining Loss: 0.01147009 \tValidation Loss 0.01807618 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4272 \tTraining Loss: 0.01153521 \tValidation Loss 0.01866909 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4273 \tTraining Loss: 0.01144801 \tValidation Loss 0.01898618 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4274 \tTraining Loss: 0.01153119 \tValidation Loss 0.01820519 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4275 \tTraining Loss: 0.01155359 \tValidation Loss 0.01865162 \tTraining Acuuarcy 42.158% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4276 \tTraining Loss: 0.01151154 \tValidation Loss 0.01870171 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4277 \tTraining Loss: 0.01135859 \tValidation Loss 0.01852409 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4278 \tTraining Loss: 0.01150374 \tValidation Loss 0.01871718 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4279 \tTraining Loss: 0.01148763 \tValidation Loss 0.01869148 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4280 \tTraining Loss: 0.01149027 \tValidation Loss 0.01799301 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4281 \tTraining Loss: 0.01147106 \tValidation Loss 0.01861568 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4282 \tTraining Loss: 0.01142525 \tValidation Loss 0.01897365 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4283 \tTraining Loss: 0.01146361 \tValidation Loss 0.01842168 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4284 \tTraining Loss: 0.01146680 \tValidation Loss 0.01883056 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4285 \tTraining Loss: 0.01149191 \tValidation Loss 0.01841162 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 4286 \tTraining Loss: 0.01160050 \tValidation Loss 0.01819695 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 4287 \tTraining Loss: 0.01153812 \tValidation Loss 0.01841241 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 4288 \tTraining Loss: 0.01143749 \tValidation Loss 0.01905714 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 4289 \tTraining Loss: 0.01146868 \tValidation Loss 0.01855404 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4290 \tTraining Loss: 0.01152920 \tValidation Loss 0.01855145 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 4291 \tTraining Loss: 0.01146295 \tValidation Loss 0.01878847 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 4292 \tTraining Loss: 0.01142292 \tValidation Loss 0.01819995 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4293 \tTraining Loss: 0.01147734 \tValidation Loss 0.01860981 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 4294 \tTraining Loss: 0.01167482 \tValidation Loss 0.01830277 \tTraining Acuuarcy 42.314% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4295 \tTraining Loss: 0.01138044 \tValidation Loss 0.01879545 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4296 \tTraining Loss: 0.01144690 \tValidation Loss 0.01863462 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 4297 \tTraining Loss: 0.01136042 \tValidation Loss 0.01867997 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4298 \tTraining Loss: 0.01144653 \tValidation Loss 0.01821196 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 4299 \tTraining Loss: 0.01147725 \tValidation Loss 0.01860072 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 4300 \tTraining Loss: 0.01141476 \tValidation Loss 0.01837467 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 4301 \tTraining Loss: 0.01144085 \tValidation Loss 0.01856382 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4302 \tTraining Loss: 0.01144152 \tValidation Loss 0.01852533 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.950%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4303 \tTraining Loss: 0.01144337 \tValidation Loss 0.01907212 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 4304 \tTraining Loss: 0.01143163 \tValidation Loss 0.01898343 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4305 \tTraining Loss: 0.01145537 \tValidation Loss 0.01893935 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4306 \tTraining Loss: 0.01150743 \tValidation Loss 0.01830438 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 4307 \tTraining Loss: 0.01148163 \tValidation Loss 0.01836067 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4308 \tTraining Loss: 0.01146725 \tValidation Loss 0.01844669 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4309 \tTraining Loss: 0.01149072 \tValidation Loss 0.01905489 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4310 \tTraining Loss: 0.01150704 \tValidation Loss 0.01910933 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4311 \tTraining Loss: 0.01143264 \tValidation Loss 0.01871480 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4312 \tTraining Loss: 0.01147385 \tValidation Loss 0.01878006 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4313 \tTraining Loss: 0.01141405 \tValidation Loss 0.01896755 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4314 \tTraining Loss: 0.01147200 \tValidation Loss 0.01782834 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4315 \tTraining Loss: 0.01155165 \tValidation Loss 0.01834879 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4316 \tTraining Loss: 0.01150534 \tValidation Loss 0.01848935 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4317 \tTraining Loss: 0.01139245 \tValidation Loss 0.01814489 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4318 \tTraining Loss: 0.01151867 \tValidation Loss 0.01856556 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4319 \tTraining Loss: 0.01148957 \tValidation Loss 0.01864045 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4320 \tTraining Loss: 0.01147763 \tValidation Loss 0.01846770 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4321 \tTraining Loss: 0.01144222 \tValidation Loss 0.01846460 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4322 \tTraining Loss: 0.01145453 \tValidation Loss 0.01836358 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 4323 \tTraining Loss: 0.01155491 \tValidation Loss 0.01892619 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4324 \tTraining Loss: 0.01137748 \tValidation Loss 0.01850982 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4325 \tTraining Loss: 0.01141185 \tValidation Loss 0.01844951 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4326 \tTraining Loss: 0.01141726 \tValidation Loss 0.01855406 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 4327 \tTraining Loss: 0.01143012 \tValidation Loss 0.01896176 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4328 \tTraining Loss: 0.01135560 \tValidation Loss 0.01865181 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 4329 \tTraining Loss: 0.01150756 \tValidation Loss 0.01846858 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4330 \tTraining Loss: 0.01137097 \tValidation Loss 0.01915806 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 4331 \tTraining Loss: 0.01154258 \tValidation Loss 0.01848952 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4332 \tTraining Loss: 0.01159078 \tValidation Loss 0.01861350 \tTraining Acuuarcy 42.465% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4333 \tTraining Loss: 0.01144305 \tValidation Loss 0.01871691 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 4334 \tTraining Loss: 0.01152707 \tValidation Loss 0.01859989 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4335 \tTraining Loss: 0.01146138 \tValidation Loss 0.01839028 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 4336 \tTraining Loss: 0.01155529 \tValidation Loss 0.01841729 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4337 \tTraining Loss: 0.01146291 \tValidation Loss 0.01876161 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4338 \tTraining Loss: 0.01147418 \tValidation Loss 0.01843520 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4339 \tTraining Loss: 0.01147062 \tValidation Loss 0.01848590 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4340 \tTraining Loss: 0.01152391 \tValidation Loss 0.01855153 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4341 \tTraining Loss: 0.01156385 \tValidation Loss 0.01825965 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4342 \tTraining Loss: 0.01149106 \tValidation Loss 0.01933869 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4343 \tTraining Loss: 0.01147439 \tValidation Loss 0.01832338 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4344 \tTraining Loss: 0.01143226 \tValidation Loss 0.01823532 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 4345 \tTraining Loss: 0.01141289 \tValidation Loss 0.01849046 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 4346 \tTraining Loss: 0.01160136 \tValidation Loss 0.01833068 \tTraining Acuuarcy 42.576% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 4347 \tTraining Loss: 0.01155253 \tValidation Loss 0.01865017 \tTraining Acuuarcy 42.710% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4348 \tTraining Loss: 0.01146722 \tValidation Loss 0.01832485 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4349 \tTraining Loss: 0.01149793 \tValidation Loss 0.01849642 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 4350 \tTraining Loss: 0.01150990 \tValidation Loss 0.01828765 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 4351 \tTraining Loss: 0.01138531 \tValidation Loss 0.01838885 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 4352 \tTraining Loss: 0.01143441 \tValidation Loss 0.01858300 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 4353 \tTraining Loss: 0.01148900 \tValidation Loss 0.01860921 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4354 \tTraining Loss: 0.01154977 \tValidation Loss 0.01814399 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4355 \tTraining Loss: 0.01150361 \tValidation Loss 0.01846124 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4356 \tTraining Loss: 0.01138924 \tValidation Loss 0.01863149 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4357 \tTraining Loss: 0.01141993 \tValidation Loss 0.01908475 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4358 \tTraining Loss: 0.01147002 \tValidation Loss 0.01877081 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4359 \tTraining Loss: 0.01143819 \tValidation Loss 0.01833189 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4360 \tTraining Loss: 0.01150295 \tValidation Loss 0.01852290 \tTraining Acuuarcy 42.576% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 4361 \tTraining Loss: 0.01149691 \tValidation Loss 0.01832455 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4362 \tTraining Loss: 0.01149051 \tValidation Loss 0.01815140 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4363 \tTraining Loss: 0.01145678 \tValidation Loss 0.01850224 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4364 \tTraining Loss: 0.01152385 \tValidation Loss 0.01880888 \tTraining Acuuarcy 42.448% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4365 \tTraining Loss: 0.01151158 \tValidation Loss 0.01878987 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4366 \tTraining Loss: 0.01142774 \tValidation Loss 0.01860552 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4367 \tTraining Loss: 0.01154044 \tValidation Loss 0.01895139 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 4368 \tTraining Loss: 0.01147510 \tValidation Loss 0.01881538 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4369 \tTraining Loss: 0.01143704 \tValidation Loss 0.01897002 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 18.752%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4370 \tTraining Loss: 0.01142067 \tValidation Loss 0.01849747 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4371 \tTraining Loss: 0.01152408 \tValidation Loss 0.01842335 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4372 \tTraining Loss: 0.01152556 \tValidation Loss 0.01808554 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4373 \tTraining Loss: 0.01145364 \tValidation Loss 0.01808191 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4374 \tTraining Loss: 0.01143527 \tValidation Loss 0.01860699 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4375 \tTraining Loss: 0.01151133 \tValidation Loss 0.01861631 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4376 \tTraining Loss: 0.01143177 \tValidation Loss 0.01852515 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4377 \tTraining Loss: 0.01147324 \tValidation Loss 0.01863013 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4378 \tTraining Loss: 0.01154041 \tValidation Loss 0.01842344 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4379 \tTraining Loss: 0.01139596 \tValidation Loss 0.01880284 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4380 \tTraining Loss: 0.01148273 \tValidation Loss 0.01838035 \tTraining Acuuarcy 42.821% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4381 \tTraining Loss: 0.01142632 \tValidation Loss 0.01929127 \tTraining Acuuarcy 43.412% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4382 \tTraining Loss: 0.01147172 \tValidation Loss 0.01877313 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 4383 \tTraining Loss: 0.01141597 \tValidation Loss 0.01893971 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4384 \tTraining Loss: 0.01145999 \tValidation Loss 0.01856335 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4385 \tTraining Loss: 0.01137541 \tValidation Loss 0.01891417 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 4386 \tTraining Loss: 0.01142583 \tValidation Loss 0.01842287 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 4387 \tTraining Loss: 0.01151489 \tValidation Loss 0.01841404 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4388 \tTraining Loss: 0.01145865 \tValidation Loss 0.01855931 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 4389 \tTraining Loss: 0.01149769 \tValidation Loss 0.01870796 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4390 \tTraining Loss: 0.01152270 \tValidation Loss 0.01810279 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4391 \tTraining Loss: 0.01151052 \tValidation Loss 0.01802226 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4392 \tTraining Loss: 0.01144523 \tValidation Loss 0.01861667 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4393 \tTraining Loss: 0.01151575 \tValidation Loss 0.01925074 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4394 \tTraining Loss: 0.01138998 \tValidation Loss 0.01833426 \tTraining Acuuarcy 44.109% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4395 \tTraining Loss: 0.01157452 \tValidation Loss 0.01815499 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 4396 \tTraining Loss: 0.01151588 \tValidation Loss 0.01833604 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4397 \tTraining Loss: 0.01150752 \tValidation Loss 0.01853729 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4398 \tTraining Loss: 0.01138588 \tValidation Loss 0.01915316 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 4399 \tTraining Loss: 0.01146425 \tValidation Loss 0.01816038 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4400 \tTraining Loss: 0.01149905 \tValidation Loss 0.01872080 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 4401 \tTraining Loss: 0.01150650 \tValidation Loss 0.01845291 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4402 \tTraining Loss: 0.01144194 \tValidation Loss 0.01800279 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 4403 \tTraining Loss: 0.01144660 \tValidation Loss 0.01883128 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4404 \tTraining Loss: 0.01154678 \tValidation Loss 0.01842446 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4405 \tTraining Loss: 0.01145143 \tValidation Loss 0.01869477 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4406 \tTraining Loss: 0.01144331 \tValidation Loss 0.01873530 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 4407 \tTraining Loss: 0.01146684 \tValidation Loss 0.01876907 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4408 \tTraining Loss: 0.01141429 \tValidation Loss 0.01873450 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4409 \tTraining Loss: 0.01140237 \tValidation Loss 0.01890062 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4410 \tTraining Loss: 0.01149617 \tValidation Loss 0.01879739 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4411 \tTraining Loss: 0.01146706 \tValidation Loss 0.01781645 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 4412 \tTraining Loss: 0.01160605 \tValidation Loss 0.01900946 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4413 \tTraining Loss: 0.01141107 \tValidation Loss 0.01899132 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 4414 \tTraining Loss: 0.01153336 \tValidation Loss 0.01852548 \tTraining Acuuarcy 42.821% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 4415 \tTraining Loss: 0.01149388 \tValidation Loss 0.01819921 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4416 \tTraining Loss: 0.01156810 \tValidation Loss 0.01822654 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4417 \tTraining Loss: 0.01144368 \tValidation Loss 0.01832071 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4418 \tTraining Loss: 0.01141629 \tValidation Loss 0.01839065 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 4419 \tTraining Loss: 0.01152102 \tValidation Loss 0.01857376 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4420 \tTraining Loss: 0.01150531 \tValidation Loss 0.01809106 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 4421 \tTraining Loss: 0.01148904 \tValidation Loss 0.01891311 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4422 \tTraining Loss: 0.01151318 \tValidation Loss 0.01831350 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4423 \tTraining Loss: 0.01152845 \tValidation Loss 0.01828521 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4424 \tTraining Loss: 0.01138595 \tValidation Loss 0.01893340 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4425 \tTraining Loss: 0.01151668 \tValidation Loss 0.01842421 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4426 \tTraining Loss: 0.01154982 \tValidation Loss 0.01854907 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4427 \tTraining Loss: 0.01152732 \tValidation Loss 0.01865304 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4428 \tTraining Loss: 0.01139483 \tValidation Loss 0.01849954 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4429 \tTraining Loss: 0.01144726 \tValidation Loss 0.01862681 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 4430 \tTraining Loss: 0.01139332 \tValidation Loss 0.01876280 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 4431 \tTraining Loss: 0.01148934 \tValidation Loss 0.01827027 \tTraining Acuuarcy 42.582% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4432 \tTraining Loss: 0.01148845 \tValidation Loss 0.01874160 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4433 \tTraining Loss: 0.01145831 \tValidation Loss 0.01868784 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4434 \tTraining Loss: 0.01138909 \tValidation Loss 0.01867082 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 4435 \tTraining Loss: 0.01142663 \tValidation Loss 0.01855733 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4436 \tTraining Loss: 0.01151751 \tValidation Loss 0.01892893 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.250%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4437 \tTraining Loss: 0.01137957 \tValidation Loss 0.01924357 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4438 \tTraining Loss: 0.01143924 \tValidation Loss 0.01816556 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4439 \tTraining Loss: 0.01153071 \tValidation Loss 0.01819926 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4440 \tTraining Loss: 0.01146513 \tValidation Loss 0.01845027 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4441 \tTraining Loss: 0.01146462 \tValidation Loss 0.01811679 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4442 \tTraining Loss: 0.01151162 \tValidation Loss 0.01870682 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4443 \tTraining Loss: 0.01149399 \tValidation Loss 0.01862611 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4444 \tTraining Loss: 0.01151621 \tValidation Loss 0.01843131 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4445 \tTraining Loss: 0.01144253 \tValidation Loss 0.01853754 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4446 \tTraining Loss: 0.01144085 \tValidation Loss 0.01821862 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4447 \tTraining Loss: 0.01150463 \tValidation Loss 0.01845274 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4448 \tTraining Loss: 0.01149604 \tValidation Loss 0.01823145 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 4449 \tTraining Loss: 0.01147062 \tValidation Loss 0.01867535 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4450 \tTraining Loss: 0.01153448 \tValidation Loss 0.01907835 \tTraining Acuuarcy 42.704% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 4451 \tTraining Loss: 0.01160421 \tValidation Loss 0.01911225 \tTraining Acuuarcy 42.710% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 4452 \tTraining Loss: 0.01137468 \tValidation Loss 0.01884386 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4453 \tTraining Loss: 0.01150776 \tValidation Loss 0.01852755 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4454 \tTraining Loss: 0.01146884 \tValidation Loss 0.01857336 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4455 \tTraining Loss: 0.01149771 \tValidation Loss 0.01837681 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 4456 \tTraining Loss: 0.01144856 \tValidation Loss 0.01852571 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4457 \tTraining Loss: 0.01150705 \tValidation Loss 0.01899071 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4458 \tTraining Loss: 0.01143001 \tValidation Loss 0.01820611 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4459 \tTraining Loss: 0.01140500 \tValidation Loss 0.01822477 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4460 \tTraining Loss: 0.01153381 \tValidation Loss 0.01848141 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4461 \tTraining Loss: 0.01146977 \tValidation Loss 0.01847219 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4462 \tTraining Loss: 0.01143837 \tValidation Loss 0.01851584 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4463 \tTraining Loss: 0.01148659 \tValidation Loss 0.01808741 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4464 \tTraining Loss: 0.01141505 \tValidation Loss 0.01861047 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4465 \tTraining Loss: 0.01152754 \tValidation Loss 0.01859689 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 4466 \tTraining Loss: 0.01143518 \tValidation Loss 0.01862437 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 4467 \tTraining Loss: 0.01146120 \tValidation Loss 0.01850286 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4468 \tTraining Loss: 0.01139714 \tValidation Loss 0.01911662 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4469 \tTraining Loss: 0.01157783 \tValidation Loss 0.01866855 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4470 \tTraining Loss: 0.01153570 \tValidation Loss 0.01872939 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 4471 \tTraining Loss: 0.01137992 \tValidation Loss 0.01859701 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4472 \tTraining Loss: 0.01139709 \tValidation Loss 0.01853756 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4473 \tTraining Loss: 0.01139179 \tValidation Loss 0.01866899 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4474 \tTraining Loss: 0.01148574 \tValidation Loss 0.01836160 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 4475 \tTraining Loss: 0.01142805 \tValidation Loss 0.01813399 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4476 \tTraining Loss: 0.01144591 \tValidation Loss 0.01834685 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4477 \tTraining Loss: 0.01139116 \tValidation Loss 0.01840703 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4478 \tTraining Loss: 0.01148456 \tValidation Loss 0.01858416 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4479 \tTraining Loss: 0.01138418 \tValidation Loss 0.01865462 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 4480 \tTraining Loss: 0.01157499 \tValidation Loss 0.01841962 \tTraining Acuuarcy 42.537% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4481 \tTraining Loss: 0.01145343 \tValidation Loss 0.01847136 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4482 \tTraining Loss: 0.01149121 \tValidation Loss 0.01863886 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4483 \tTraining Loss: 0.01140942 \tValidation Loss 0.01870006 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4484 \tTraining Loss: 0.01143334 \tValidation Loss 0.01858657 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 4485 \tTraining Loss: 0.01161340 \tValidation Loss 0.01865643 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 4486 \tTraining Loss: 0.01142019 \tValidation Loss 0.01860159 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4487 \tTraining Loss: 0.01155958 \tValidation Loss 0.01799280 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4488 \tTraining Loss: 0.01154037 \tValidation Loss 0.01823838 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4489 \tTraining Loss: 0.01151900 \tValidation Loss 0.01836278 \tTraining Acuuarcy 42.715% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4490 \tTraining Loss: 0.01152860 \tValidation Loss 0.01846835 \tTraining Acuuarcy 42.515% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 4491 \tTraining Loss: 0.01158451 \tValidation Loss 0.01857400 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 4492 \tTraining Loss: 0.01144869 \tValidation Loss 0.01835527 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4493 \tTraining Loss: 0.01143727 \tValidation Loss 0.01811687 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4494 \tTraining Loss: 0.01147741 \tValidation Loss 0.01870541 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4495 \tTraining Loss: 0.01147988 \tValidation Loss 0.01863173 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4496 \tTraining Loss: 0.01151889 \tValidation Loss 0.01861698 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4497 \tTraining Loss: 0.01142536 \tValidation Loss 0.01867798 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4498 \tTraining Loss: 0.01139876 \tValidation Loss 0.01918214 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 4499 \tTraining Loss: 0.01142758 \tValidation Loss 0.01866780 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4500 \tTraining Loss: 0.01152211 \tValidation Loss 0.01857516 \tTraining Acuuarcy 42.442% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4501 \tTraining Loss: 0.01146690 \tValidation Loss 0.01915113 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4502 \tTraining Loss: 0.01144536 \tValidation Loss 0.01871291 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4503 \tTraining Loss: 0.01147932 \tValidation Loss 0.01852012 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.950%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4504 \tTraining Loss: 0.01167911 \tValidation Loss 0.01776730 \tTraining Acuuarcy 42.097% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4505 \tTraining Loss: 0.01155814 \tValidation Loss 0.01833716 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4506 \tTraining Loss: 0.01151035 \tValidation Loss 0.01846176 \tTraining Acuuarcy 42.604% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 4507 \tTraining Loss: 0.01146253 \tValidation Loss 0.01831488 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4508 \tTraining Loss: 0.01147112 \tValidation Loss 0.01884106 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 4509 \tTraining Loss: 0.01152001 \tValidation Loss 0.01835844 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4510 \tTraining Loss: 0.01146797 \tValidation Loss 0.01882168 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4511 \tTraining Loss: 0.01140462 \tValidation Loss 0.01846149 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4512 \tTraining Loss: 0.01137637 \tValidation Loss 0.01846291 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4513 \tTraining Loss: 0.01159479 \tValidation Loss 0.01902312 \tTraining Acuuarcy 42.208% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4514 \tTraining Loss: 0.01142673 \tValidation Loss 0.01889505 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4515 \tTraining Loss: 0.01149090 \tValidation Loss 0.01823626 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 4516 \tTraining Loss: 0.01146637 \tValidation Loss 0.01869710 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4517 \tTraining Loss: 0.01142489 \tValidation Loss 0.01872224 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4518 \tTraining Loss: 0.01146505 \tValidation Loss 0.01869426 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4519 \tTraining Loss: 0.01152087 \tValidation Loss 0.01842172 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4520 \tTraining Loss: 0.01146989 \tValidation Loss 0.01827925 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4521 \tTraining Loss: 0.01150100 \tValidation Loss 0.01849559 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4522 \tTraining Loss: 0.01144824 \tValidation Loss 0.01875853 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4523 \tTraining Loss: 0.01149706 \tValidation Loss 0.01873022 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 4524 \tTraining Loss: 0.01144785 \tValidation Loss 0.01878678 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 4525 \tTraining Loss: 0.01142786 \tValidation Loss 0.01840750 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4526 \tTraining Loss: 0.01151550 \tValidation Loss 0.01873661 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4527 \tTraining Loss: 0.01144863 \tValidation Loss 0.01853107 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4528 \tTraining Loss: 0.01138655 \tValidation Loss 0.01863763 \tTraining Acuuarcy 44.059% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 4529 \tTraining Loss: 0.01150438 \tValidation Loss 0.01802787 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4530 \tTraining Loss: 0.01141313 \tValidation Loss 0.01833381 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4531 \tTraining Loss: 0.01146673 \tValidation Loss 0.01871762 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4532 \tTraining Loss: 0.01145708 \tValidation Loss 0.01918906 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 4533 \tTraining Loss: 0.01154787 \tValidation Loss 0.01877613 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 4534 \tTraining Loss: 0.01151999 \tValidation Loss 0.01897224 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4535 \tTraining Loss: 0.01144919 \tValidation Loss 0.01842053 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4536 \tTraining Loss: 0.01147619 \tValidation Loss 0.01857583 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4537 \tTraining Loss: 0.01157235 \tValidation Loss 0.01844039 \tTraining Acuuarcy 42.258% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4538 \tTraining Loss: 0.01151216 \tValidation Loss 0.01820626 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4539 \tTraining Loss: 0.01162689 \tValidation Loss 0.01846434 \tTraining Acuuarcy 42.242% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 4540 \tTraining Loss: 0.01147834 \tValidation Loss 0.01922794 \tTraining Acuuarcy 42.821% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4541 \tTraining Loss: 0.01154418 \tValidation Loss 0.01870510 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 4542 \tTraining Loss: 0.01137998 \tValidation Loss 0.01845835 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4543 \tTraining Loss: 0.01150213 \tValidation Loss 0.01800177 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4544 \tTraining Loss: 0.01156135 \tValidation Loss 0.01827674 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 4545 \tTraining Loss: 0.01156244 \tValidation Loss 0.01897456 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4546 \tTraining Loss: 0.01147551 \tValidation Loss 0.01882287 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4547 \tTraining Loss: 0.01154230 \tValidation Loss 0.01872670 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 4548 \tTraining Loss: 0.01137839 \tValidation Loss 0.01863452 \tTraining Acuuarcy 44.170% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 4549 \tTraining Loss: 0.01146078 \tValidation Loss 0.01841526 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 4550 \tTraining Loss: 0.01149878 \tValidation Loss 0.01866860 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 4551 \tTraining Loss: 0.01152554 \tValidation Loss 0.01848083 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4552 \tTraining Loss: 0.01148725 \tValidation Loss 0.01857157 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4553 \tTraining Loss: 0.01152190 \tValidation Loss 0.01851776 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4554 \tTraining Loss: 0.01146551 \tValidation Loss 0.01844217 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 4555 \tTraining Loss: 0.01154232 \tValidation Loss 0.01833062 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4556 \tTraining Loss: 0.01143765 \tValidation Loss 0.01835443 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 4557 \tTraining Loss: 0.01153014 \tValidation Loss 0.01865811 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 4558 \tTraining Loss: 0.01156947 \tValidation Loss 0.01857118 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4559 \tTraining Loss: 0.01145408 \tValidation Loss 0.01858161 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4560 \tTraining Loss: 0.01149198 \tValidation Loss 0.01875891 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4561 \tTraining Loss: 0.01148305 \tValidation Loss 0.01889831 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4562 \tTraining Loss: 0.01140054 \tValidation Loss 0.01887878 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 4563 \tTraining Loss: 0.01150477 \tValidation Loss 0.01848389 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4564 \tTraining Loss: 0.01142176 \tValidation Loss 0.01869906 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4565 \tTraining Loss: 0.01146635 \tValidation Loss 0.01841253 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 4566 \tTraining Loss: 0.01139219 \tValidation Loss 0.01846700 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 4567 \tTraining Loss: 0.01140207 \tValidation Loss 0.01805381 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4568 \tTraining Loss: 0.01149089 \tValidation Loss 0.01837935 \tTraining Acuuarcy 42.771% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4569 \tTraining Loss: 0.01146309 \tValidation Loss 0.01871950 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4570 \tTraining Loss: 0.01141037 \tValidation Loss 0.01914794 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 19.030%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4571 \tTraining Loss: 0.01143672 \tValidation Loss 0.01900968 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4572 \tTraining Loss: 0.01146697 \tValidation Loss 0.01870065 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4573 \tTraining Loss: 0.01142015 \tValidation Loss 0.01939363 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4574 \tTraining Loss: 0.01138743 \tValidation Loss 0.01868963 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 4575 \tTraining Loss: 0.01151446 \tValidation Loss 0.01799961 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4576 \tTraining Loss: 0.01142834 \tValidation Loss 0.01878375 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4577 \tTraining Loss: 0.01136454 \tValidation Loss 0.01909225 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4578 \tTraining Loss: 0.01150726 \tValidation Loss 0.01817687 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 4579 \tTraining Loss: 0.01145735 \tValidation Loss 0.01861127 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4580 \tTraining Loss: 0.01153646 \tValidation Loss 0.01853953 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4581 \tTraining Loss: 0.01134548 \tValidation Loss 0.01835422 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4582 \tTraining Loss: 0.01144860 \tValidation Loss 0.01807728 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4583 \tTraining Loss: 0.01144492 \tValidation Loss 0.01847786 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 4584 \tTraining Loss: 0.01149299 \tValidation Loss 0.01865377 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4585 \tTraining Loss: 0.01144588 \tValidation Loss 0.01848544 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 4586 \tTraining Loss: 0.01147001 \tValidation Loss 0.01844755 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4587 \tTraining Loss: 0.01147765 \tValidation Loss 0.01828236 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4588 \tTraining Loss: 0.01142416 \tValidation Loss 0.01854303 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4589 \tTraining Loss: 0.01147740 \tValidation Loss 0.01868232 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4590 \tTraining Loss: 0.01140025 \tValidation Loss 0.01872182 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 4591 \tTraining Loss: 0.01135567 \tValidation Loss 0.01868398 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 4592 \tTraining Loss: 0.01142231 \tValidation Loss 0.01854688 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4593 \tTraining Loss: 0.01144375 \tValidation Loss 0.01837533 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 4594 \tTraining Loss: 0.01147324 \tValidation Loss 0.01875229 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4595 \tTraining Loss: 0.01135298 \tValidation Loss 0.01912358 \tTraining Acuuarcy 44.354% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4596 \tTraining Loss: 0.01147520 \tValidation Loss 0.01879680 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 4597 \tTraining Loss: 0.01145071 \tValidation Loss 0.01923972 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4598 \tTraining Loss: 0.01156327 \tValidation Loss 0.01836607 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4599 \tTraining Loss: 0.01148269 \tValidation Loss 0.01796950 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4600 \tTraining Loss: 0.01148288 \tValidation Loss 0.01881339 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4601 \tTraining Loss: 0.01142793 \tValidation Loss 0.01862352 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 4602 \tTraining Loss: 0.01140397 \tValidation Loss 0.01847155 \tTraining Acuuarcy 44.036% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4603 \tTraining Loss: 0.01149227 \tValidation Loss 0.01882127 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4604 \tTraining Loss: 0.01150047 \tValidation Loss 0.01820528 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4605 \tTraining Loss: 0.01144840 \tValidation Loss 0.01852481 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 4606 \tTraining Loss: 0.01145197 \tValidation Loss 0.01834746 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 4607 \tTraining Loss: 0.01145748 \tValidation Loss 0.01871675 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4608 \tTraining Loss: 0.01141050 \tValidation Loss 0.01814072 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 4609 \tTraining Loss: 0.01152752 \tValidation Loss 0.01797748 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 4610 \tTraining Loss: 0.01152103 \tValidation Loss 0.01833944 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4611 \tTraining Loss: 0.01139704 \tValidation Loss 0.01864289 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4612 \tTraining Loss: 0.01150432 \tValidation Loss 0.01885013 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4613 \tTraining Loss: 0.01141706 \tValidation Loss 0.01891626 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4614 \tTraining Loss: 0.01143493 \tValidation Loss 0.01822353 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 4615 \tTraining Loss: 0.01149050 \tValidation Loss 0.01835423 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4616 \tTraining Loss: 0.01148040 \tValidation Loss 0.01867366 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4617 \tTraining Loss: 0.01143474 \tValidation Loss 0.01903914 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4618 \tTraining Loss: 0.01145896 \tValidation Loss 0.01858191 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4619 \tTraining Loss: 0.01144462 \tValidation Loss 0.01844446 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4620 \tTraining Loss: 0.01133321 \tValidation Loss 0.01903212 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 4621 \tTraining Loss: 0.01140543 \tValidation Loss 0.01876922 \tTraining Acuuarcy 44.098% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 4622 \tTraining Loss: 0.01144059 \tValidation Loss 0.01831529 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 4623 \tTraining Loss: 0.01140211 \tValidation Loss 0.01888391 \tTraining Acuuarcy 44.064% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4624 \tTraining Loss: 0.01147739 \tValidation Loss 0.01862029 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4625 \tTraining Loss: 0.01145559 \tValidation Loss 0.01892374 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4626 \tTraining Loss: 0.01146180 \tValidation Loss 0.01877220 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4627 \tTraining Loss: 0.01148881 \tValidation Loss 0.01860042 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4628 \tTraining Loss: 0.01149802 \tValidation Loss 0.01844992 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4629 \tTraining Loss: 0.01146742 \tValidation Loss 0.01862474 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 4630 \tTraining Loss: 0.01146499 \tValidation Loss 0.01827731 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4631 \tTraining Loss: 0.01153697 \tValidation Loss 0.01872921 \tTraining Acuuarcy 42.933% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4632 \tTraining Loss: 0.01148950 \tValidation Loss 0.01852407 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4633 \tTraining Loss: 0.01146010 \tValidation Loss 0.01862588 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4634 \tTraining Loss: 0.01142923 \tValidation Loss 0.01835649 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4635 \tTraining Loss: 0.01148209 \tValidation Loss 0.01874287 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4636 \tTraining Loss: 0.01151771 \tValidation Loss 0.01853164 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4637 \tTraining Loss: 0.01151748 \tValidation Loss 0.01813356 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 19.114%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4638 \tTraining Loss: 0.01156584 \tValidation Loss 0.01865254 \tTraining Acuuarcy 42.453% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4639 \tTraining Loss: 0.01149223 \tValidation Loss 0.01837510 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4640 \tTraining Loss: 0.01140534 \tValidation Loss 0.01903647 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4641 \tTraining Loss: 0.01146034 \tValidation Loss 0.01845835 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4642 \tTraining Loss: 0.01135367 \tValidation Loss 0.01871775 \tTraining Acuuarcy 44.348% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4643 \tTraining Loss: 0.01140717 \tValidation Loss 0.01842259 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4644 \tTraining Loss: 0.01141702 \tValidation Loss 0.01846081 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 4645 \tTraining Loss: 0.01145170 \tValidation Loss 0.01874563 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 4646 \tTraining Loss: 0.01141322 \tValidation Loss 0.01883065 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4647 \tTraining Loss: 0.01146921 \tValidation Loss 0.01831916 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4648 \tTraining Loss: 0.01142290 \tValidation Loss 0.01887116 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4649 \tTraining Loss: 0.01152659 \tValidation Loss 0.01868982 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4650 \tTraining Loss: 0.01151569 \tValidation Loss 0.01870762 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4651 \tTraining Loss: 0.01155665 \tValidation Loss 0.01825073 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4652 \tTraining Loss: 0.01138360 \tValidation Loss 0.01842829 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4653 \tTraining Loss: 0.01140558 \tValidation Loss 0.01884745 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4654 \tTraining Loss: 0.01147173 \tValidation Loss 0.01849690 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 4655 \tTraining Loss: 0.01145538 \tValidation Loss 0.01910019 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4656 \tTraining Loss: 0.01139215 \tValidation Loss 0.01885036 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4657 \tTraining Loss: 0.01143549 \tValidation Loss 0.01856459 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4658 \tTraining Loss: 0.01143715 \tValidation Loss 0.01849230 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4659 \tTraining Loss: 0.01145177 \tValidation Loss 0.01871793 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4660 \tTraining Loss: 0.01148762 \tValidation Loss 0.01840399 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4661 \tTraining Loss: 0.01139997 \tValidation Loss 0.01910669 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4662 \tTraining Loss: 0.01138157 \tValidation Loss 0.01822277 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4663 \tTraining Loss: 0.01143979 \tValidation Loss 0.01837790 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4664 \tTraining Loss: 0.01148188 \tValidation Loss 0.01898177 \tTraining Acuuarcy 42.615% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 4665 \tTraining Loss: 0.01143441 \tValidation Loss 0.01853192 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 4666 \tTraining Loss: 0.01145175 \tValidation Loss 0.01887078 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4667 \tTraining Loss: 0.01150115 \tValidation Loss 0.01839863 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4668 \tTraining Loss: 0.01147479 \tValidation Loss 0.01852852 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4669 \tTraining Loss: 0.01147685 \tValidation Loss 0.01848137 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 4670 \tTraining Loss: 0.01137559 \tValidation Loss 0.01891701 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4671 \tTraining Loss: 0.01148210 \tValidation Loss 0.01904101 \tTraining Acuuarcy 42.810% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4672 \tTraining Loss: 0.01133847 \tValidation Loss 0.01889865 \tTraining Acuuarcy 44.031% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4673 \tTraining Loss: 0.01146802 \tValidation Loss 0.01872154 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4674 \tTraining Loss: 0.01144069 \tValidation Loss 0.01856656 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4675 \tTraining Loss: 0.01171328 \tValidation Loss 0.01851816 \tTraining Acuuarcy 41.969% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4676 \tTraining Loss: 0.01147497 \tValidation Loss 0.01859031 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4677 \tTraining Loss: 0.01141069 \tValidation Loss 0.01861398 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4678 \tTraining Loss: 0.01145919 \tValidation Loss 0.01862537 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4679 \tTraining Loss: 0.01151235 \tValidation Loss 0.01832320 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4680 \tTraining Loss: 0.01159123 \tValidation Loss 0.01841756 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4681 \tTraining Loss: 0.01147454 \tValidation Loss 0.01840337 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 4682 \tTraining Loss: 0.01155415 \tValidation Loss 0.01854309 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4683 \tTraining Loss: 0.01140625 \tValidation Loss 0.01849940 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4684 \tTraining Loss: 0.01139838 \tValidation Loss 0.01882370 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4685 \tTraining Loss: 0.01144055 \tValidation Loss 0.01820868 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4686 \tTraining Loss: 0.01146312 \tValidation Loss 0.01837072 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4687 \tTraining Loss: 0.01150232 \tValidation Loss 0.01861772 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 4688 \tTraining Loss: 0.01146839 \tValidation Loss 0.01884762 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4689 \tTraining Loss: 0.01156704 \tValidation Loss 0.01842243 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4690 \tTraining Loss: 0.01156648 \tValidation Loss 0.01846998 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 4691 \tTraining Loss: 0.01145973 \tValidation Loss 0.01800828 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 4692 \tTraining Loss: 0.01144741 \tValidation Loss 0.01852961 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4693 \tTraining Loss: 0.01148610 \tValidation Loss 0.01861109 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4694 \tTraining Loss: 0.01149853 \tValidation Loss 0.01823904 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4695 \tTraining Loss: 0.01150297 \tValidation Loss 0.01891702 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4696 \tTraining Loss: 0.01146261 \tValidation Loss 0.01901901 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 4697 \tTraining Loss: 0.01148852 \tValidation Loss 0.01829694 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 4698 \tTraining Loss: 0.01143545 \tValidation Loss 0.01855497 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4699 \tTraining Loss: 0.01144088 \tValidation Loss 0.01867847 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 4700 \tTraining Loss: 0.01143923 \tValidation Loss 0.01854714 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 4701 \tTraining Loss: 0.01151716 \tValidation Loss 0.01854490 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4702 \tTraining Loss: 0.01150898 \tValidation Loss 0.01849856 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 4703 \tTraining Loss: 0.01144149 \tValidation Loss 0.01879758 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4704 \tTraining Loss: 0.01150132 \tValidation Loss 0.01858107 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 19.114%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4705 \tTraining Loss: 0.01147774 \tValidation Loss 0.01851862 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 4706 \tTraining Loss: 0.01146831 \tValidation Loss 0.01866781 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4707 \tTraining Loss: 0.01142533 \tValidation Loss 0.01866466 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4708 \tTraining Loss: 0.01146846 \tValidation Loss 0.01861446 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4709 \tTraining Loss: 0.01148360 \tValidation Loss 0.01845576 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4710 \tTraining Loss: 0.01145129 \tValidation Loss 0.01865180 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4711 \tTraining Loss: 0.01150234 \tValidation Loss 0.01849638 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4712 \tTraining Loss: 0.01144320 \tValidation Loss 0.01843532 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 4713 \tTraining Loss: 0.01146932 \tValidation Loss 0.01947440 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4714 \tTraining Loss: 0.01148711 \tValidation Loss 0.01858506 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4715 \tTraining Loss: 0.01148573 \tValidation Loss 0.01827448 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4716 \tTraining Loss: 0.01144955 \tValidation Loss 0.01905837 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 4717 \tTraining Loss: 0.01146919 \tValidation Loss 0.01844432 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4718 \tTraining Loss: 0.01159637 \tValidation Loss 0.01818227 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 4719 \tTraining Loss: 0.01147903 \tValidation Loss 0.01844558 \tTraining Acuuarcy 42.754% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 4720 \tTraining Loss: 0.01154338 \tValidation Loss 0.01848485 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4721 \tTraining Loss: 0.01142702 \tValidation Loss 0.01794245 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4722 \tTraining Loss: 0.01146079 \tValidation Loss 0.01883325 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 4723 \tTraining Loss: 0.01146109 \tValidation Loss 0.01844398 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4724 \tTraining Loss: 0.01144182 \tValidation Loss 0.01861341 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4725 \tTraining Loss: 0.01137049 \tValidation Loss 0.01857496 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4726 \tTraining Loss: 0.01138367 \tValidation Loss 0.01893327 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 4727 \tTraining Loss: 0.01136494 \tValidation Loss 0.01857700 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4728 \tTraining Loss: 0.01147953 \tValidation Loss 0.01870459 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 4729 \tTraining Loss: 0.01152915 \tValidation Loss 0.01864126 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4730 \tTraining Loss: 0.01144348 \tValidation Loss 0.01861521 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4731 \tTraining Loss: 0.01148066 \tValidation Loss 0.01888302 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4732 \tTraining Loss: 0.01141356 \tValidation Loss 0.01842597 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4733 \tTraining Loss: 0.01153455 \tValidation Loss 0.01819525 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4734 \tTraining Loss: 0.01138015 \tValidation Loss 0.01901268 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 4735 \tTraining Loss: 0.01150887 \tValidation Loss 0.01842056 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4736 \tTraining Loss: 0.01146786 \tValidation Loss 0.01854622 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4737 \tTraining Loss: 0.01141950 \tValidation Loss 0.01884236 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 4738 \tTraining Loss: 0.01154600 \tValidation Loss 0.01841637 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4739 \tTraining Loss: 0.01140715 \tValidation Loss 0.01863032 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4740 \tTraining Loss: 0.01139876 \tValidation Loss 0.01852352 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4741 \tTraining Loss: 0.01140407 \tValidation Loss 0.01829568 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 4742 \tTraining Loss: 0.01153167 \tValidation Loss 0.01833159 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 4743 \tTraining Loss: 0.01146647 \tValidation Loss 0.01832421 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4744 \tTraining Loss: 0.01154453 \tValidation Loss 0.01910680 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4745 \tTraining Loss: 0.01137953 \tValidation Loss 0.01889573 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4746 \tTraining Loss: 0.01146612 \tValidation Loss 0.01848948 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4747 \tTraining Loss: 0.01146876 \tValidation Loss 0.01839200 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 4748 \tTraining Loss: 0.01143008 \tValidation Loss 0.01899233 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4749 \tTraining Loss: 0.01147548 \tValidation Loss 0.01874769 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 4750 \tTraining Loss: 0.01140433 \tValidation Loss 0.01874574 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4751 \tTraining Loss: 0.01141990 \tValidation Loss 0.01871257 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 4752 \tTraining Loss: 0.01145959 \tValidation Loss 0.01851734 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4753 \tTraining Loss: 0.01142226 \tValidation Loss 0.01862802 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 4754 \tTraining Loss: 0.01142688 \tValidation Loss 0.01884998 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4755 \tTraining Loss: 0.01135517 \tValidation Loss 0.01886390 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4756 \tTraining Loss: 0.01139180 \tValidation Loss 0.01834064 \tTraining Acuuarcy 44.098% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4757 \tTraining Loss: 0.01142950 \tValidation Loss 0.01829807 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4758 \tTraining Loss: 0.01142969 \tValidation Loss 0.01820719 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4759 \tTraining Loss: 0.01158697 \tValidation Loss 0.01864173 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 4760 \tTraining Loss: 0.01139347 \tValidation Loss 0.01925756 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 4761 \tTraining Loss: 0.01150560 \tValidation Loss 0.01860694 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4762 \tTraining Loss: 0.01146233 \tValidation Loss 0.01824321 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4763 \tTraining Loss: 0.01146966 \tValidation Loss 0.01877763 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4764 \tTraining Loss: 0.01136792 \tValidation Loss 0.01895254 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4765 \tTraining Loss: 0.01151155 \tValidation Loss 0.01848692 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4766 \tTraining Loss: 0.01140005 \tValidation Loss 0.01893379 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4767 \tTraining Loss: 0.01145786 \tValidation Loss 0.01861372 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 4768 \tTraining Loss: 0.01146908 \tValidation Loss 0.01855279 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 4769 \tTraining Loss: 0.01154873 \tValidation Loss 0.01832811 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4770 \tTraining Loss: 0.01144051 \tValidation Loss 0.01876296 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4771 \tTraining Loss: 0.01137971 \tValidation Loss 0.01876887 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 20.145%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4772 \tTraining Loss: 0.01151611 \tValidation Loss 0.01895790 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 4773 \tTraining Loss: 0.01146078 \tValidation Loss 0.01864325 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4774 \tTraining Loss: 0.01149568 \tValidation Loss 0.01832359 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 4775 \tTraining Loss: 0.01144920 \tValidation Loss 0.01876271 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4776 \tTraining Loss: 0.01155351 \tValidation Loss 0.01851064 \tTraining Acuuarcy 42.838% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 4777 \tTraining Loss: 0.01137795 \tValidation Loss 0.01900873 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 4778 \tTraining Loss: 0.01154903 \tValidation Loss 0.01827340 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4779 \tTraining Loss: 0.01140063 \tValidation Loss 0.01865183 \tTraining Acuuarcy 43.958% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4780 \tTraining Loss: 0.01142159 \tValidation Loss 0.01858480 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 4781 \tTraining Loss: 0.01141890 \tValidation Loss 0.01862174 \tTraining Acuuarcy 44.059% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4782 \tTraining Loss: 0.01146229 \tValidation Loss 0.01846427 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 4783 \tTraining Loss: 0.01138191 \tValidation Loss 0.01852350 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4784 \tTraining Loss: 0.01142055 \tValidation Loss 0.01876587 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 4785 \tTraining Loss: 0.01155199 \tValidation Loss 0.01870729 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 4786 \tTraining Loss: 0.01148063 \tValidation Loss 0.01896431 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4787 \tTraining Loss: 0.01156734 \tValidation Loss 0.01840671 \tTraining Acuuarcy 42.682% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4788 \tTraining Loss: 0.01137066 \tValidation Loss 0.01856526 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4789 \tTraining Loss: 0.01145929 \tValidation Loss 0.01873191 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4790 \tTraining Loss: 0.01148975 \tValidation Loss 0.01869745 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4791 \tTraining Loss: 0.01149076 \tValidation Loss 0.01839655 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4792 \tTraining Loss: 0.01137287 \tValidation Loss 0.01922760 \tTraining Acuuarcy 44.599% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4793 \tTraining Loss: 0.01143214 \tValidation Loss 0.01885829 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4794 \tTraining Loss: 0.01143324 \tValidation Loss 0.01856992 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 4795 \tTraining Loss: 0.01143416 \tValidation Loss 0.01904888 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4796 \tTraining Loss: 0.01155429 \tValidation Loss 0.01842528 \tTraining Acuuarcy 42.604% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4797 \tTraining Loss: 0.01141306 \tValidation Loss 0.01841459 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4798 \tTraining Loss: 0.01144588 \tValidation Loss 0.01867923 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4799 \tTraining Loss: 0.01148401 \tValidation Loss 0.01904479 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4800 \tTraining Loss: 0.01147137 \tValidation Loss 0.01884271 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 4801 \tTraining Loss: 0.01149747 \tValidation Loss 0.01909426 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 4802 \tTraining Loss: 0.01140943 \tValidation Loss 0.01861443 \tTraining Acuuarcy 44.209% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4803 \tTraining Loss: 0.01152392 \tValidation Loss 0.01875171 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4804 \tTraining Loss: 0.01138193 \tValidation Loss 0.01885067 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 4805 \tTraining Loss: 0.01140541 \tValidation Loss 0.01843579 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4806 \tTraining Loss: 0.01144466 \tValidation Loss 0.01882349 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4807 \tTraining Loss: 0.01151235 \tValidation Loss 0.01870066 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 4808 \tTraining Loss: 0.01145071 \tValidation Loss 0.01841271 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4809 \tTraining Loss: 0.01150105 \tValidation Loss 0.01830715 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4810 \tTraining Loss: 0.01147636 \tValidation Loss 0.01888171 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4811 \tTraining Loss: 0.01146624 \tValidation Loss 0.01852716 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4812 \tTraining Loss: 0.01148489 \tValidation Loss 0.01803142 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4813 \tTraining Loss: 0.01140818 \tValidation Loss 0.01874280 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4814 \tTraining Loss: 0.01151291 \tValidation Loss 0.01839076 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 4815 \tTraining Loss: 0.01157894 \tValidation Loss 0.01864464 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4816 \tTraining Loss: 0.01146818 \tValidation Loss 0.01870410 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4817 \tTraining Loss: 0.01140511 \tValidation Loss 0.01866226 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4818 \tTraining Loss: 0.01140214 \tValidation Loss 0.01839657 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4819 \tTraining Loss: 0.01145823 \tValidation Loss 0.01824854 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4820 \tTraining Loss: 0.01146429 \tValidation Loss 0.01877173 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 4821 \tTraining Loss: 0.01149426 \tValidation Loss 0.01887781 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4822 \tTraining Loss: 0.01140902 \tValidation Loss 0.01901826 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 4823 \tTraining Loss: 0.01145219 \tValidation Loss 0.01881300 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 4824 \tTraining Loss: 0.01153830 \tValidation Loss 0.01883651 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 4825 \tTraining Loss: 0.01146133 \tValidation Loss 0.01866330 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4826 \tTraining Loss: 0.01138794 \tValidation Loss 0.01895130 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 4827 \tTraining Loss: 0.01149807 \tValidation Loss 0.01868101 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4828 \tTraining Loss: 0.01146675 \tValidation Loss 0.01879903 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4829 \tTraining Loss: 0.01145166 \tValidation Loss 0.01833580 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4830 \tTraining Loss: 0.01138401 \tValidation Loss 0.01871347 \tTraining Acuuarcy 44.042% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4831 \tTraining Loss: 0.01146629 \tValidation Loss 0.01881494 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4832 \tTraining Loss: 0.01145230 \tValidation Loss 0.01849913 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4833 \tTraining Loss: 0.01137718 \tValidation Loss 0.01849379 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 4834 \tTraining Loss: 0.01140866 \tValidation Loss 0.01820252 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 4835 \tTraining Loss: 0.01136295 \tValidation Loss 0.01961661 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4836 \tTraining Loss: 0.01147721 \tValidation Loss 0.01847773 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4837 \tTraining Loss: 0.01138002 \tValidation Loss 0.01792732 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 4838 \tTraining Loss: 0.01145549 \tValidation Loss 0.01846989 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 19.811%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4839 \tTraining Loss: 0.01143200 \tValidation Loss 0.01857835 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4840 \tTraining Loss: 0.01151179 \tValidation Loss 0.01835161 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 4841 \tTraining Loss: 0.01157029 \tValidation Loss 0.01811035 \tTraining Acuuarcy 42.654% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4842 \tTraining Loss: 0.01143480 \tValidation Loss 0.01877761 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 4843 \tTraining Loss: 0.01153284 \tValidation Loss 0.01889969 \tTraining Acuuarcy 42.598% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4844 \tTraining Loss: 0.01142960 \tValidation Loss 0.01900736 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4845 \tTraining Loss: 0.01146947 \tValidation Loss 0.01807051 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 4846 \tTraining Loss: 0.01152280 \tValidation Loss 0.01852456 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4847 \tTraining Loss: 0.01152485 \tValidation Loss 0.01842844 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4848 \tTraining Loss: 0.01148638 \tValidation Loss 0.01877072 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4849 \tTraining Loss: 0.01145511 \tValidation Loss 0.01847159 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4850 \tTraining Loss: 0.01137081 \tValidation Loss 0.01904529 \tTraining Acuuarcy 44.031% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4851 \tTraining Loss: 0.01151913 \tValidation Loss 0.01856441 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4852 \tTraining Loss: 0.01143564 \tValidation Loss 0.01831051 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 4853 \tTraining Loss: 0.01144731 \tValidation Loss 0.01848679 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4854 \tTraining Loss: 0.01144164 \tValidation Loss 0.01877268 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4855 \tTraining Loss: 0.01146414 \tValidation Loss 0.01860788 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 4856 \tTraining Loss: 0.01149948 \tValidation Loss 0.01871886 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4857 \tTraining Loss: 0.01149879 \tValidation Loss 0.01882562 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4858 \tTraining Loss: 0.01147788 \tValidation Loss 0.01864402 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4859 \tTraining Loss: 0.01140792 \tValidation Loss 0.01841905 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 4860 \tTraining Loss: 0.01152066 \tValidation Loss 0.01853557 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4861 \tTraining Loss: 0.01146259 \tValidation Loss 0.01838701 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 4862 \tTraining Loss: 0.01152655 \tValidation Loss 0.01911561 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4863 \tTraining Loss: 0.01149481 \tValidation Loss 0.01876009 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4864 \tTraining Loss: 0.01155624 \tValidation Loss 0.01877993 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4865 \tTraining Loss: 0.01144858 \tValidation Loss 0.01840408 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4866 \tTraining Loss: 0.01143915 \tValidation Loss 0.01865585 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4867 \tTraining Loss: 0.01152621 \tValidation Loss 0.01847210 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4868 \tTraining Loss: 0.01142891 \tValidation Loss 0.01873268 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 4869 \tTraining Loss: 0.01137796 \tValidation Loss 0.01894076 \tTraining Acuuarcy 44.137% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 4870 \tTraining Loss: 0.01140893 \tValidation Loss 0.01840324 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 4871 \tTraining Loss: 0.01134543 \tValidation Loss 0.01859053 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 4872 \tTraining Loss: 0.01138095 \tValidation Loss 0.01894915 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4873 \tTraining Loss: 0.01143304 \tValidation Loss 0.01879660 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 4874 \tTraining Loss: 0.01149736 \tValidation Loss 0.01870254 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 4875 \tTraining Loss: 0.01151189 \tValidation Loss 0.01906678 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4876 \tTraining Loss: 0.01148986 \tValidation Loss 0.01830006 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4877 \tTraining Loss: 0.01142203 \tValidation Loss 0.01871872 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 21.705%\n",
      "Epoch: 4878 \tTraining Loss: 0.01149977 \tValidation Loss 0.01845430 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4879 \tTraining Loss: 0.01145443 \tValidation Loss 0.01884498 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4880 \tTraining Loss: 0.01149022 \tValidation Loss 0.01822551 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4881 \tTraining Loss: 0.01141459 \tValidation Loss 0.01885822 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 4882 \tTraining Loss: 0.01139079 \tValidation Loss 0.01863085 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4883 \tTraining Loss: 0.01144919 \tValidation Loss 0.01846286 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4884 \tTraining Loss: 0.01147117 \tValidation Loss 0.01818168 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4885 \tTraining Loss: 0.01149132 \tValidation Loss 0.01852316 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4886 \tTraining Loss: 0.01142044 \tValidation Loss 0.01857383 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 4887 \tTraining Loss: 0.01155739 \tValidation Loss 0.01860577 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4888 \tTraining Loss: 0.01152554 \tValidation Loss 0.01820619 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 4889 \tTraining Loss: 0.01139115 \tValidation Loss 0.01922534 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4890 \tTraining Loss: 0.01150826 \tValidation Loss 0.01823516 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 4891 \tTraining Loss: 0.01138529 \tValidation Loss 0.01907756 \tTraining Acuuarcy 44.064% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4892 \tTraining Loss: 0.01150506 \tValidation Loss 0.01810067 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 4893 \tTraining Loss: 0.01145999 \tValidation Loss 0.01856878 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4894 \tTraining Loss: 0.01150636 \tValidation Loss 0.01838851 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4895 \tTraining Loss: 0.01133312 \tValidation Loss 0.01855729 \tTraining Acuuarcy 44.042% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4896 \tTraining Loss: 0.01154202 \tValidation Loss 0.01862898 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 4897 \tTraining Loss: 0.01138486 \tValidation Loss 0.01841512 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4898 \tTraining Loss: 0.01146564 \tValidation Loss 0.01836036 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 4899 \tTraining Loss: 0.01139360 \tValidation Loss 0.01911268 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 4900 \tTraining Loss: 0.01142139 \tValidation Loss 0.01861898 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4901 \tTraining Loss: 0.01143735 \tValidation Loss 0.01853494 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4902 \tTraining Loss: 0.01134898 \tValidation Loss 0.01841556 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4903 \tTraining Loss: 0.01135881 \tValidation Loss 0.01889294 \tTraining Acuuarcy 44.059% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4904 \tTraining Loss: 0.01150530 \tValidation Loss 0.01797411 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4905 \tTraining Loss: 0.01143319 \tValidation Loss 0.01837277 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 18.891%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4906 \tTraining Loss: 0.01139469 \tValidation Loss 0.01845239 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 4907 \tTraining Loss: 0.01139574 \tValidation Loss 0.01877273 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 4908 \tTraining Loss: 0.01135379 \tValidation Loss 0.01872901 \tTraining Acuuarcy 44.120% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4909 \tTraining Loss: 0.01141052 \tValidation Loss 0.01836993 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 4910 \tTraining Loss: 0.01146251 \tValidation Loss 0.01867105 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 4911 \tTraining Loss: 0.01157353 \tValidation Loss 0.01849550 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4912 \tTraining Loss: 0.01151581 \tValidation Loss 0.01800085 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 4913 \tTraining Loss: 0.01138821 \tValidation Loss 0.01833291 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4914 \tTraining Loss: 0.01140395 \tValidation Loss 0.01894883 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4915 \tTraining Loss: 0.01148581 \tValidation Loss 0.01912932 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4916 \tTraining Loss: 0.01136877 \tValidation Loss 0.01889794 \tTraining Acuuarcy 44.036% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 4917 \tTraining Loss: 0.01144422 \tValidation Loss 0.01895211 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 4918 \tTraining Loss: 0.01140873 \tValidation Loss 0.01826986 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4919 \tTraining Loss: 0.01145283 \tValidation Loss 0.01849129 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4920 \tTraining Loss: 0.01148359 \tValidation Loss 0.01841829 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4921 \tTraining Loss: 0.01143912 \tValidation Loss 0.01858803 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 4922 \tTraining Loss: 0.01143607 \tValidation Loss 0.01833725 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 4923 \tTraining Loss: 0.01144163 \tValidation Loss 0.01828220 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4924 \tTraining Loss: 0.01148052 \tValidation Loss 0.01891155 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4925 \tTraining Loss: 0.01145667 \tValidation Loss 0.01841229 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 4926 \tTraining Loss: 0.01140929 \tValidation Loss 0.01885341 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 4927 \tTraining Loss: 0.01133774 \tValidation Loss 0.01873901 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4928 \tTraining Loss: 0.01145153 \tValidation Loss 0.01879196 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 4929 \tTraining Loss: 0.01143177 \tValidation Loss 0.01853291 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 4930 \tTraining Loss: 0.01153524 \tValidation Loss 0.01786279 \tTraining Acuuarcy 42.637% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4931 \tTraining Loss: 0.01147986 \tValidation Loss 0.01823425 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 4932 \tTraining Loss: 0.01147958 \tValidation Loss 0.01826821 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 4933 \tTraining Loss: 0.01146265 \tValidation Loss 0.01864281 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 4934 \tTraining Loss: 0.01151891 \tValidation Loss 0.01815048 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 4935 \tTraining Loss: 0.01147200 \tValidation Loss 0.01879890 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 4936 \tTraining Loss: 0.01147009 \tValidation Loss 0.01873170 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 4937 \tTraining Loss: 0.01140549 \tValidation Loss 0.01863672 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 4938 \tTraining Loss: 0.01137466 \tValidation Loss 0.01841333 \tTraining Acuuarcy 44.410% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 4939 \tTraining Loss: 0.01150767 \tValidation Loss 0.01813969 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4940 \tTraining Loss: 0.01138933 \tValidation Loss 0.01859598 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4941 \tTraining Loss: 0.01142494 \tValidation Loss 0.01862297 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 4942 \tTraining Loss: 0.01135886 \tValidation Loss 0.01864713 \tTraining Acuuarcy 44.176% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 4943 \tTraining Loss: 0.01147690 \tValidation Loss 0.01838205 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 4944 \tTraining Loss: 0.01141371 \tValidation Loss 0.01869077 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4945 \tTraining Loss: 0.01147703 \tValidation Loss 0.01877690 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 4946 \tTraining Loss: 0.01147921 \tValidation Loss 0.01864703 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 4947 \tTraining Loss: 0.01144921 \tValidation Loss 0.01854828 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 4948 \tTraining Loss: 0.01153497 \tValidation Loss 0.01861305 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4949 \tTraining Loss: 0.01141581 \tValidation Loss 0.01903101 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 4950 \tTraining Loss: 0.01149691 \tValidation Loss 0.01840489 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4951 \tTraining Loss: 0.01146825 \tValidation Loss 0.01864709 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 4952 \tTraining Loss: 0.01139992 \tValidation Loss 0.01862789 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 4953 \tTraining Loss: 0.01143030 \tValidation Loss 0.01880936 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 4954 \tTraining Loss: 0.01158306 \tValidation Loss 0.01846165 \tTraining Acuuarcy 42.715% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4955 \tTraining Loss: 0.01150594 \tValidation Loss 0.01848538 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4956 \tTraining Loss: 0.01142550 \tValidation Loss 0.01839329 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4957 \tTraining Loss: 0.01150681 \tValidation Loss 0.01871420 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 4958 \tTraining Loss: 0.01148330 \tValidation Loss 0.01872321 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 4959 \tTraining Loss: 0.01145864 \tValidation Loss 0.01849434 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 4960 \tTraining Loss: 0.01147162 \tValidation Loss 0.01887581 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 4961 \tTraining Loss: 0.01138229 \tValidation Loss 0.01849739 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 4962 \tTraining Loss: 0.01146085 \tValidation Loss 0.01868067 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 4963 \tTraining Loss: 0.01140058 \tValidation Loss 0.01847417 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 4964 \tTraining Loss: 0.01146233 \tValidation Loss 0.01810563 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 4965 \tTraining Loss: 0.01148799 \tValidation Loss 0.01871302 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 4966 \tTraining Loss: 0.01148347 \tValidation Loss 0.01874350 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4967 \tTraining Loss: 0.01149727 \tValidation Loss 0.01872759 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 4968 \tTraining Loss: 0.01145461 \tValidation Loss 0.01863494 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 4969 \tTraining Loss: 0.01153115 \tValidation Loss 0.01826496 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4970 \tTraining Loss: 0.01144405 \tValidation Loss 0.01894185 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 4971 \tTraining Loss: 0.01139627 \tValidation Loss 0.01868636 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4972 \tTraining Loss: 0.01152739 \tValidation Loss 0.01884267 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 18.835%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4973 \tTraining Loss: 0.01149882 \tValidation Loss 0.01897554 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 4974 \tTraining Loss: 0.01135609 \tValidation Loss 0.01931328 \tTraining Acuuarcy 44.293% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 4975 \tTraining Loss: 0.01140265 \tValidation Loss 0.01867137 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 4976 \tTraining Loss: 0.01151257 \tValidation Loss 0.01850495 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4977 \tTraining Loss: 0.01139532 \tValidation Loss 0.01861592 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 4978 \tTraining Loss: 0.01152747 \tValidation Loss 0.01844734 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 4979 \tTraining Loss: 0.01146817 \tValidation Loss 0.01864856 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 4980 \tTraining Loss: 0.01149796 \tValidation Loss 0.01852441 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 4981 \tTraining Loss: 0.01143354 \tValidation Loss 0.01861614 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 4982 \tTraining Loss: 0.01148021 \tValidation Loss 0.01882853 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 4983 \tTraining Loss: 0.01146451 \tValidation Loss 0.01919011 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 4984 \tTraining Loss: 0.01150265 \tValidation Loss 0.01850451 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 4985 \tTraining Loss: 0.01149688 \tValidation Loss 0.01833952 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 4986 \tTraining Loss: 0.01147641 \tValidation Loss 0.01886752 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4987 \tTraining Loss: 0.01148662 \tValidation Loss 0.01852173 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 4988 \tTraining Loss: 0.01145085 \tValidation Loss 0.01847847 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 4989 \tTraining Loss: 0.01145979 \tValidation Loss 0.01866225 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 4990 \tTraining Loss: 0.01147445 \tValidation Loss 0.01834620 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 4991 \tTraining Loss: 0.01149364 \tValidation Loss 0.01850971 \tTraining Acuuarcy 42.793% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 4992 \tTraining Loss: 0.01155380 \tValidation Loss 0.01824763 \tTraining Acuuarcy 42.922% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 4993 \tTraining Loss: 0.01151671 \tValidation Loss 0.01808181 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 4994 \tTraining Loss: 0.01140221 \tValidation Loss 0.01841515 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 4995 \tTraining Loss: 0.01147297 \tValidation Loss 0.01842517 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 4996 \tTraining Loss: 0.01150980 \tValidation Loss 0.01802993 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 4997 \tTraining Loss: 0.01154805 \tValidation Loss 0.01817585 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 4998 \tTraining Loss: 0.01150611 \tValidation Loss 0.01849583 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 4999 \tTraining Loss: 0.01141942 \tValidation Loss 0.01888558 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 5000 \tTraining Loss: 0.01158637 \tValidation Loss 0.01816130 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5001 \tTraining Loss: 0.01144651 \tValidation Loss 0.01825190 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 5002 \tTraining Loss: 0.01151623 \tValidation Loss 0.01829952 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5003 \tTraining Loss: 0.01147663 \tValidation Loss 0.01779729 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5004 \tTraining Loss: 0.01136679 \tValidation Loss 0.01902873 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 5005 \tTraining Loss: 0.01137971 \tValidation Loss 0.01860664 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 5006 \tTraining Loss: 0.01142903 \tValidation Loss 0.01868116 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 5007 \tTraining Loss: 0.01134823 \tValidation Loss 0.01848611 \tTraining Acuuarcy 44.282% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5008 \tTraining Loss: 0.01144210 \tValidation Loss 0.01849659 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5009 \tTraining Loss: 0.01152108 \tValidation Loss 0.01815514 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5010 \tTraining Loss: 0.01150169 \tValidation Loss 0.01822408 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5011 \tTraining Loss: 0.01137320 \tValidation Loss 0.01880720 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5012 \tTraining Loss: 0.01141782 \tValidation Loss 0.01834940 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 5013 \tTraining Loss: 0.01144698 \tValidation Loss 0.01848901 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5014 \tTraining Loss: 0.01156085 \tValidation Loss 0.01846291 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5015 \tTraining Loss: 0.01142068 \tValidation Loss 0.01827756 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 5016 \tTraining Loss: 0.01138461 \tValidation Loss 0.01880005 \tTraining Acuuarcy 43.953% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5017 \tTraining Loss: 0.01146463 \tValidation Loss 0.01835598 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5018 \tTraining Loss: 0.01149949 \tValidation Loss 0.01871800 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5019 \tTraining Loss: 0.01150031 \tValidation Loss 0.01885414 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5020 \tTraining Loss: 0.01145434 \tValidation Loss 0.01860246 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5021 \tTraining Loss: 0.01147891 \tValidation Loss 0.01888548 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 5022 \tTraining Loss: 0.01136037 \tValidation Loss 0.01892528 \tTraining Acuuarcy 44.120% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 5023 \tTraining Loss: 0.01145703 \tValidation Loss 0.01891376 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5024 \tTraining Loss: 0.01149374 \tValidation Loss 0.01876520 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 5025 \tTraining Loss: 0.01141766 \tValidation Loss 0.01879082 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5026 \tTraining Loss: 0.01142830 \tValidation Loss 0.01828860 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5027 \tTraining Loss: 0.01143211 \tValidation Loss 0.01835879 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5028 \tTraining Loss: 0.01143948 \tValidation Loss 0.01863837 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5029 \tTraining Loss: 0.01142997 \tValidation Loss 0.01873564 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5030 \tTraining Loss: 0.01146301 \tValidation Loss 0.01872655 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5031 \tTraining Loss: 0.01151177 \tValidation Loss 0.01884692 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 5032 \tTraining Loss: 0.01145496 \tValidation Loss 0.01835849 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 5033 \tTraining Loss: 0.01150779 \tValidation Loss 0.01873952 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5034 \tTraining Loss: 0.01141364 \tValidation Loss 0.01857697 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5035 \tTraining Loss: 0.01150526 \tValidation Loss 0.01809493 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5036 \tTraining Loss: 0.01148962 \tValidation Loss 0.01795930 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 5037 \tTraining Loss: 0.01134884 \tValidation Loss 0.01840760 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5038 \tTraining Loss: 0.01138432 \tValidation Loss 0.01847372 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5039 \tTraining Loss: 0.01145888 \tValidation Loss 0.01868058 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.532%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5040 \tTraining Loss: 0.01142093 \tValidation Loss 0.01901275 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5041 \tTraining Loss: 0.01142120 \tValidation Loss 0.01875682 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5042 \tTraining Loss: 0.01151754 \tValidation Loss 0.01898429 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5043 \tTraining Loss: 0.01152641 \tValidation Loss 0.01821300 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 5044 \tTraining Loss: 0.01144296 \tValidation Loss 0.01882537 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5045 \tTraining Loss: 0.01132333 \tValidation Loss 0.01907163 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5046 \tTraining Loss: 0.01143949 \tValidation Loss 0.01870375 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 5047 \tTraining Loss: 0.01153720 \tValidation Loss 0.01871555 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5048 \tTraining Loss: 0.01146594 \tValidation Loss 0.01853145 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5049 \tTraining Loss: 0.01146300 \tValidation Loss 0.01846941 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5050 \tTraining Loss: 0.01140685 \tValidation Loss 0.01888460 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 5051 \tTraining Loss: 0.01140918 \tValidation Loss 0.01868514 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5052 \tTraining Loss: 0.01147463 \tValidation Loss 0.01908055 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 5053 \tTraining Loss: 0.01144859 \tValidation Loss 0.01836364 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5054 \tTraining Loss: 0.01138412 \tValidation Loss 0.01853994 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5055 \tTraining Loss: 0.01152397 \tValidation Loss 0.01850011 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5056 \tTraining Loss: 0.01140608 \tValidation Loss 0.01817078 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 5057 \tTraining Loss: 0.01153817 \tValidation Loss 0.01834527 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5058 \tTraining Loss: 0.01132138 \tValidation Loss 0.01856269 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5059 \tTraining Loss: 0.01148681 \tValidation Loss 0.01901941 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5060 \tTraining Loss: 0.01150667 \tValidation Loss 0.01884197 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5061 \tTraining Loss: 0.01144204 \tValidation Loss 0.01873855 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5062 \tTraining Loss: 0.01138142 \tValidation Loss 0.01867331 \tTraining Acuuarcy 43.953% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5063 \tTraining Loss: 0.01145420 \tValidation Loss 0.01854817 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5064 \tTraining Loss: 0.01134920 \tValidation Loss 0.01827319 \tTraining Acuuarcy 44.488% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 5065 \tTraining Loss: 0.01140208 \tValidation Loss 0.01858892 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5066 \tTraining Loss: 0.01148163 \tValidation Loss 0.01844334 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5067 \tTraining Loss: 0.01146916 \tValidation Loss 0.01831141 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 5068 \tTraining Loss: 0.01142336 \tValidation Loss 0.01834217 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5069 \tTraining Loss: 0.01155574 \tValidation Loss 0.01873266 \tTraining Acuuarcy 42.420% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5070 \tTraining Loss: 0.01144380 \tValidation Loss 0.01849453 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5071 \tTraining Loss: 0.01158501 \tValidation Loss 0.01839462 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5072 \tTraining Loss: 0.01153139 \tValidation Loss 0.01877394 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 5073 \tTraining Loss: 0.01153037 \tValidation Loss 0.01856877 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5074 \tTraining Loss: 0.01139349 \tValidation Loss 0.01836582 \tTraining Acuuarcy 44.226% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5075 \tTraining Loss: 0.01147775 \tValidation Loss 0.01884631 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5076 \tTraining Loss: 0.01145201 \tValidation Loss 0.01825362 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5077 \tTraining Loss: 0.01131140 \tValidation Loss 0.01833752 \tTraining Acuuarcy 44.315% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5078 \tTraining Loss: 0.01142420 \tValidation Loss 0.01851509 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5079 \tTraining Loss: 0.01146457 \tValidation Loss 0.01880425 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5080 \tTraining Loss: 0.01148012 \tValidation Loss 0.01827141 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5081 \tTraining Loss: 0.01144811 \tValidation Loss 0.01824241 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5082 \tTraining Loss: 0.01153396 \tValidation Loss 0.01831951 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 5083 \tTraining Loss: 0.01141182 \tValidation Loss 0.01891757 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 5084 \tTraining Loss: 0.01154335 \tValidation Loss 0.01856200 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 5085 \tTraining Loss: 0.01145081 \tValidation Loss 0.01869951 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5086 \tTraining Loss: 0.01155170 \tValidation Loss 0.01854875 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 5087 \tTraining Loss: 0.01137712 \tValidation Loss 0.01833568 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 5088 \tTraining Loss: 0.01149018 \tValidation Loss 0.01877805 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5089 \tTraining Loss: 0.01153734 \tValidation Loss 0.01809469 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 5090 \tTraining Loss: 0.01147365 \tValidation Loss 0.01853889 \tTraining Acuuarcy 42.855% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5091 \tTraining Loss: 0.01142639 \tValidation Loss 0.01853335 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 5092 \tTraining Loss: 0.01142081 \tValidation Loss 0.01832140 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 5093 \tTraining Loss: 0.01138618 \tValidation Loss 0.01877680 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 5094 \tTraining Loss: 0.01146798 \tValidation Loss 0.01851531 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5095 \tTraining Loss: 0.01142352 \tValidation Loss 0.01896218 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5096 \tTraining Loss: 0.01141789 \tValidation Loss 0.01821357 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5097 \tTraining Loss: 0.01144911 \tValidation Loss 0.01853544 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5098 \tTraining Loss: 0.01144881 \tValidation Loss 0.01865901 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5099 \tTraining Loss: 0.01145894 \tValidation Loss 0.01836763 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5100 \tTraining Loss: 0.01142652 \tValidation Loss 0.01824572 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5101 \tTraining Loss: 0.01144462 \tValidation Loss 0.01834545 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 5102 \tTraining Loss: 0.01149567 \tValidation Loss 0.01875841 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5103 \tTraining Loss: 0.01135232 \tValidation Loss 0.01929822 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5104 \tTraining Loss: 0.01146840 \tValidation Loss 0.01847937 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5105 \tTraining Loss: 0.01146409 \tValidation Loss 0.01853237 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5106 \tTraining Loss: 0.01144754 \tValidation Loss 0.01856159 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.114%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5107 \tTraining Loss: 0.01151908 \tValidation Loss 0.01885551 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5108 \tTraining Loss: 0.01137944 \tValidation Loss 0.01846767 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5109 \tTraining Loss: 0.01146388 \tValidation Loss 0.01908690 \tTraining Acuuarcy 43.412% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 5110 \tTraining Loss: 0.01142589 \tValidation Loss 0.01838232 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 5111 \tTraining Loss: 0.01143187 \tValidation Loss 0.01838204 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5112 \tTraining Loss: 0.01147702 \tValidation Loss 0.01827037 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 5113 \tTraining Loss: 0.01140024 \tValidation Loss 0.01829444 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5114 \tTraining Loss: 0.01141421 \tValidation Loss 0.01863889 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5115 \tTraining Loss: 0.01149982 \tValidation Loss 0.01875462 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 5116 \tTraining Loss: 0.01143993 \tValidation Loss 0.01887412 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5117 \tTraining Loss: 0.01152490 \tValidation Loss 0.01835237 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5118 \tTraining Loss: 0.01149144 \tValidation Loss 0.01899315 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5119 \tTraining Loss: 0.01145299 \tValidation Loss 0.01833446 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5120 \tTraining Loss: 0.01146736 \tValidation Loss 0.01870948 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5121 \tTraining Loss: 0.01151782 \tValidation Loss 0.01839383 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5122 \tTraining Loss: 0.01139023 \tValidation Loss 0.01866388 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5123 \tTraining Loss: 0.01143682 \tValidation Loss 0.01881773 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5124 \tTraining Loss: 0.01140178 \tValidation Loss 0.01874031 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5125 \tTraining Loss: 0.01146151 \tValidation Loss 0.01868020 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5126 \tTraining Loss: 0.01149833 \tValidation Loss 0.01824134 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 5127 \tTraining Loss: 0.01144844 \tValidation Loss 0.01828180 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5128 \tTraining Loss: 0.01156368 \tValidation Loss 0.01824987 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5129 \tTraining Loss: 0.01144012 \tValidation Loss 0.01839269 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5130 \tTraining Loss: 0.01141945 \tValidation Loss 0.01818677 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5131 \tTraining Loss: 0.01142102 \tValidation Loss 0.01886447 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5132 \tTraining Loss: 0.01140198 \tValidation Loss 0.01844136 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 5133 \tTraining Loss: 0.01141679 \tValidation Loss 0.01832010 \tTraining Acuuarcy 44.031% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5134 \tTraining Loss: 0.01151598 \tValidation Loss 0.01864685 \tTraining Acuuarcy 42.688% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 5135 \tTraining Loss: 0.01144583 \tValidation Loss 0.01906282 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5136 \tTraining Loss: 0.01153646 \tValidation Loss 0.01830872 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 5137 \tTraining Loss: 0.01149537 \tValidation Loss 0.01897684 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 5138 \tTraining Loss: 0.01140091 \tValidation Loss 0.01927220 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5139 \tTraining Loss: 0.01141730 \tValidation Loss 0.01829949 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 5140 \tTraining Loss: 0.01145089 \tValidation Loss 0.01889070 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5141 \tTraining Loss: 0.01135797 \tValidation Loss 0.01891266 \tTraining Acuuarcy 44.237% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 5142 \tTraining Loss: 0.01142928 \tValidation Loss 0.01850791 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 5143 \tTraining Loss: 0.01149206 \tValidation Loss 0.01807173 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5144 \tTraining Loss: 0.01145468 \tValidation Loss 0.01884403 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 5145 \tTraining Loss: 0.01149321 \tValidation Loss 0.01888259 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5146 \tTraining Loss: 0.01133550 \tValidation Loss 0.01875774 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 5147 \tTraining Loss: 0.01144721 \tValidation Loss 0.01851995 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 5148 \tTraining Loss: 0.01138106 \tValidation Loss 0.01865826 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5149 \tTraining Loss: 0.01149001 \tValidation Loss 0.01884402 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5150 \tTraining Loss: 0.01135407 \tValidation Loss 0.01855268 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5151 \tTraining Loss: 0.01148445 \tValidation Loss 0.01868975 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 5152 \tTraining Loss: 0.01140144 \tValidation Loss 0.01881248 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5153 \tTraining Loss: 0.01142162 \tValidation Loss 0.01877736 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5154 \tTraining Loss: 0.01150812 \tValidation Loss 0.01857462 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5155 \tTraining Loss: 0.01145893 \tValidation Loss 0.01856604 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 5156 \tTraining Loss: 0.01134873 \tValidation Loss 0.01865208 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5157 \tTraining Loss: 0.01143017 \tValidation Loss 0.01871664 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5158 \tTraining Loss: 0.01140041 \tValidation Loss 0.01890091 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5159 \tTraining Loss: 0.01138187 \tValidation Loss 0.01871206 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5160 \tTraining Loss: 0.01143487 \tValidation Loss 0.01846240 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5161 \tTraining Loss: 0.01145198 \tValidation Loss 0.01850606 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5162 \tTraining Loss: 0.01139306 \tValidation Loss 0.01888393 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5163 \tTraining Loss: 0.01158126 \tValidation Loss 0.01859214 \tTraining Acuuarcy 42.515% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5164 \tTraining Loss: 0.01147657 \tValidation Loss 0.01870659 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5165 \tTraining Loss: 0.01145255 \tValidation Loss 0.01900496 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5166 \tTraining Loss: 0.01148118 \tValidation Loss 0.01853020 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5167 \tTraining Loss: 0.01149623 \tValidation Loss 0.01897980 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5168 \tTraining Loss: 0.01141996 \tValidation Loss 0.01870903 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5169 \tTraining Loss: 0.01142363 \tValidation Loss 0.01878602 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 5170 \tTraining Loss: 0.01147368 \tValidation Loss 0.01888910 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5171 \tTraining Loss: 0.01145559 \tValidation Loss 0.01827979 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5172 \tTraining Loss: 0.01139485 \tValidation Loss 0.01884136 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5173 \tTraining Loss: 0.01143207 \tValidation Loss 0.01891102 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.448%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5174 \tTraining Loss: 0.01145409 \tValidation Loss 0.01843772 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5175 \tTraining Loss: 0.01150580 \tValidation Loss 0.01865652 \tTraining Acuuarcy 42.626% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5176 \tTraining Loss: 0.01146987 \tValidation Loss 0.01897961 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5177 \tTraining Loss: 0.01152141 \tValidation Loss 0.01853431 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5178 \tTraining Loss: 0.01146081 \tValidation Loss 0.01871562 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5179 \tTraining Loss: 0.01145099 \tValidation Loss 0.01843324 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 5180 \tTraining Loss: 0.01143985 \tValidation Loss 0.01830862 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 5181 \tTraining Loss: 0.01141185 \tValidation Loss 0.01829998 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5182 \tTraining Loss: 0.01138789 \tValidation Loss 0.01878544 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5183 \tTraining Loss: 0.01139863 \tValidation Loss 0.01849288 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5184 \tTraining Loss: 0.01155400 \tValidation Loss 0.01821201 \tTraining Acuuarcy 42.554% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5185 \tTraining Loss: 0.01149919 \tValidation Loss 0.01870472 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 5186 \tTraining Loss: 0.01142790 \tValidation Loss 0.01878940 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5187 \tTraining Loss: 0.01141362 \tValidation Loss 0.01879551 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5188 \tTraining Loss: 0.01147586 \tValidation Loss 0.01851860 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 5189 \tTraining Loss: 0.01142477 \tValidation Loss 0.01867580 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5190 \tTraining Loss: 0.01159970 \tValidation Loss 0.01787193 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5191 \tTraining Loss: 0.01148091 \tValidation Loss 0.01819512 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5192 \tTraining Loss: 0.01146739 \tValidation Loss 0.01869034 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5193 \tTraining Loss: 0.01139334 \tValidation Loss 0.01883432 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5194 \tTraining Loss: 0.01142098 \tValidation Loss 0.01856800 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5195 \tTraining Loss: 0.01157689 \tValidation Loss 0.01857938 \tTraining Acuuarcy 42.682% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 5196 \tTraining Loss: 0.01144048 \tValidation Loss 0.01858691 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5197 \tTraining Loss: 0.01145477 \tValidation Loss 0.01840294 \tTraining Acuuarcy 42.961% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5198 \tTraining Loss: 0.01154632 \tValidation Loss 0.01869013 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5199 \tTraining Loss: 0.01138155 \tValidation Loss 0.01857439 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5200 \tTraining Loss: 0.01149445 \tValidation Loss 0.01866733 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5201 \tTraining Loss: 0.01133954 \tValidation Loss 0.01849218 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 5202 \tTraining Loss: 0.01144031 \tValidation Loss 0.01842444 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5203 \tTraining Loss: 0.01133701 \tValidation Loss 0.01914922 \tTraining Acuuarcy 44.231% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 5204 \tTraining Loss: 0.01145110 \tValidation Loss 0.01833653 \tTraining Acuuarcy 42.905% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5205 \tTraining Loss: 0.01157341 \tValidation Loss 0.01823176 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 5206 \tTraining Loss: 0.01152096 \tValidation Loss 0.01827376 \tTraining Acuuarcy 42.481% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 5207 \tTraining Loss: 0.01138014 \tValidation Loss 0.01873374 \tTraining Acuuarcy 44.109% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5208 \tTraining Loss: 0.01137339 \tValidation Loss 0.01863815 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5209 \tTraining Loss: 0.01144654 \tValidation Loss 0.01878646 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 5210 \tTraining Loss: 0.01141317 \tValidation Loss 0.01867245 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5211 \tTraining Loss: 0.01140148 \tValidation Loss 0.01838840 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5212 \tTraining Loss: 0.01152433 \tValidation Loss 0.01887800 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5213 \tTraining Loss: 0.01141748 \tValidation Loss 0.01873552 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5214 \tTraining Loss: 0.01132361 \tValidation Loss 0.01826150 \tTraining Acuuarcy 44.187% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5215 \tTraining Loss: 0.01147161 \tValidation Loss 0.01866271 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5216 \tTraining Loss: 0.01144594 \tValidation Loss 0.01894359 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5217 \tTraining Loss: 0.01147231 \tValidation Loss 0.01844604 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5218 \tTraining Loss: 0.01131075 \tValidation Loss 0.01878217 \tTraining Acuuarcy 44.426% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 5219 \tTraining Loss: 0.01136637 \tValidation Loss 0.01858574 \tTraining Acuuarcy 44.120% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5220 \tTraining Loss: 0.01142332 \tValidation Loss 0.01860147 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5221 \tTraining Loss: 0.01140636 \tValidation Loss 0.01869826 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5222 \tTraining Loss: 0.01143799 \tValidation Loss 0.01863345 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5223 \tTraining Loss: 0.01141409 \tValidation Loss 0.01863420 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5224 \tTraining Loss: 0.01145986 \tValidation Loss 0.01834665 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5225 \tTraining Loss: 0.01147273 \tValidation Loss 0.01894069 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 5226 \tTraining Loss: 0.01146001 \tValidation Loss 0.01861617 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5227 \tTraining Loss: 0.01147667 \tValidation Loss 0.01864992 \tTraining Acuuarcy 43.039% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5228 \tTraining Loss: 0.01145308 \tValidation Loss 0.01859339 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 5229 \tTraining Loss: 0.01140091 \tValidation Loss 0.01846135 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5230 \tTraining Loss: 0.01145675 \tValidation Loss 0.01929775 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 5231 \tTraining Loss: 0.01147535 \tValidation Loss 0.01908370 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 5232 \tTraining Loss: 0.01145801 \tValidation Loss 0.01879132 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5233 \tTraining Loss: 0.01145587 \tValidation Loss 0.01868440 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5234 \tTraining Loss: 0.01141188 \tValidation Loss 0.01832564 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5235 \tTraining Loss: 0.01142383 \tValidation Loss 0.01872455 \tTraining Acuuarcy 44.053% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5236 \tTraining Loss: 0.01143328 \tValidation Loss 0.01861789 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5237 \tTraining Loss: 0.01136973 \tValidation Loss 0.01847464 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5238 \tTraining Loss: 0.01144866 \tValidation Loss 0.01806351 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5239 \tTraining Loss: 0.01147468 \tValidation Loss 0.01827440 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5240 \tTraining Loss: 0.01149319 \tValidation Loss 0.01837399 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.476%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5241 \tTraining Loss: 0.01138411 \tValidation Loss 0.01864272 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 5242 \tTraining Loss: 0.01138882 \tValidation Loss 0.01856257 \tTraining Acuuarcy 44.176% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5243 \tTraining Loss: 0.01137865 \tValidation Loss 0.01879381 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5244 \tTraining Loss: 0.01141709 \tValidation Loss 0.01914428 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5245 \tTraining Loss: 0.01139055 \tValidation Loss 0.01827178 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5246 \tTraining Loss: 0.01141632 \tValidation Loss 0.01868551 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5247 \tTraining Loss: 0.01139748 \tValidation Loss 0.01870526 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5248 \tTraining Loss: 0.01136071 \tValidation Loss 0.01904545 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5249 \tTraining Loss: 0.01143949 \tValidation Loss 0.01839922 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5250 \tTraining Loss: 0.01155506 \tValidation Loss 0.01838457 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5251 \tTraining Loss: 0.01146378 \tValidation Loss 0.01831354 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5252 \tTraining Loss: 0.01146531 \tValidation Loss 0.01875734 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5253 \tTraining Loss: 0.01143700 \tValidation Loss 0.01845273 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5254 \tTraining Loss: 0.01143992 \tValidation Loss 0.01835504 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 5255 \tTraining Loss: 0.01148437 \tValidation Loss 0.01801507 \tTraining Acuuarcy 42.810% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 5256 \tTraining Loss: 0.01145962 \tValidation Loss 0.01843568 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 5257 \tTraining Loss: 0.01140813 \tValidation Loss 0.01871095 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5258 \tTraining Loss: 0.01141607 \tValidation Loss 0.01899116 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5259 \tTraining Loss: 0.01140284 \tValidation Loss 0.01913527 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5260 \tTraining Loss: 0.01142697 \tValidation Loss 0.01846437 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5261 \tTraining Loss: 0.01145115 \tValidation Loss 0.01783539 \tTraining Acuuarcy 43.412% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 5262 \tTraining Loss: 0.01143877 \tValidation Loss 0.01777762 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5263 \tTraining Loss: 0.01145675 \tValidation Loss 0.01897807 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5264 \tTraining Loss: 0.01142919 \tValidation Loss 0.01864502 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 5265 \tTraining Loss: 0.01141254 \tValidation Loss 0.01842885 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5266 \tTraining Loss: 0.01147153 \tValidation Loss 0.01889231 \tTraining Acuuarcy 42.871% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5267 \tTraining Loss: 0.01145101 \tValidation Loss 0.01829342 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5268 \tTraining Loss: 0.01141657 \tValidation Loss 0.01887767 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5269 \tTraining Loss: 0.01144615 \tValidation Loss 0.01823560 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5270 \tTraining Loss: 0.01137760 \tValidation Loss 0.01870954 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5271 \tTraining Loss: 0.01136219 \tValidation Loss 0.01865469 \tTraining Acuuarcy 43.908% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5272 \tTraining Loss: 0.01142446 \tValidation Loss 0.01861504 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5273 \tTraining Loss: 0.01152048 \tValidation Loss 0.01858221 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5274 \tTraining Loss: 0.01150690 \tValidation Loss 0.01869241 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5275 \tTraining Loss: 0.01134058 \tValidation Loss 0.01837603 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5276 \tTraining Loss: 0.01138777 \tValidation Loss 0.01881555 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5277 \tTraining Loss: 0.01145418 \tValidation Loss 0.01885792 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5278 \tTraining Loss: 0.01144756 \tValidation Loss 0.01835494 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 5279 \tTraining Loss: 0.01142799 \tValidation Loss 0.01871993 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5280 \tTraining Loss: 0.01156067 \tValidation Loss 0.01863255 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5281 \tTraining Loss: 0.01140956 \tValidation Loss 0.01837343 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5282 \tTraining Loss: 0.01129367 \tValidation Loss 0.01869652 \tTraining Acuuarcy 44.254% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 5283 \tTraining Loss: 0.01147734 \tValidation Loss 0.01859152 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5284 \tTraining Loss: 0.01148849 \tValidation Loss 0.01915350 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5285 \tTraining Loss: 0.01145039 \tValidation Loss 0.01835421 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 5286 \tTraining Loss: 0.01142575 \tValidation Loss 0.01831149 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5287 \tTraining Loss: 0.01142520 \tValidation Loss 0.01844001 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 5288 \tTraining Loss: 0.01145168 \tValidation Loss 0.01890596 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5289 \tTraining Loss: 0.01134070 \tValidation Loss 0.01844016 \tTraining Acuuarcy 44.204% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5290 \tTraining Loss: 0.01152688 \tValidation Loss 0.01849324 \tTraining Acuuarcy 42.292% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5291 \tTraining Loss: 0.01142460 \tValidation Loss 0.01922288 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5292 \tTraining Loss: 0.01144745 \tValidation Loss 0.01860299 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5293 \tTraining Loss: 0.01143627 \tValidation Loss 0.01845842 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 5294 \tTraining Loss: 0.01142179 \tValidation Loss 0.01890332 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 5295 \tTraining Loss: 0.01143100 \tValidation Loss 0.01835214 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5296 \tTraining Loss: 0.01151828 \tValidation Loss 0.01895927 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5297 \tTraining Loss: 0.01133083 \tValidation Loss 0.01889877 \tTraining Acuuarcy 44.220% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5298 \tTraining Loss: 0.01145117 \tValidation Loss 0.01896390 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 5299 \tTraining Loss: 0.01140902 \tValidation Loss 0.01885425 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5300 \tTraining Loss: 0.01142071 \tValidation Loss 0.01896252 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5301 \tTraining Loss: 0.01143152 \tValidation Loss 0.01868907 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 5302 \tTraining Loss: 0.01139366 \tValidation Loss 0.01861901 \tTraining Acuuarcy 44.098% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5303 \tTraining Loss: 0.01137549 \tValidation Loss 0.01880653 \tTraining Acuuarcy 44.270% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5304 \tTraining Loss: 0.01141800 \tValidation Loss 0.01874751 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 5305 \tTraining Loss: 0.01145766 \tValidation Loss 0.01880323 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5306 \tTraining Loss: 0.01149031 \tValidation Loss 0.01811790 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5307 \tTraining Loss: 0.01141906 \tValidation Loss 0.01860535 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 18.529%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5308 \tTraining Loss: 0.01155013 \tValidation Loss 0.01839747 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5309 \tTraining Loss: 0.01149427 \tValidation Loss 0.01862396 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5310 \tTraining Loss: 0.01140080 \tValidation Loss 0.01855902 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 5311 \tTraining Loss: 0.01145989 \tValidation Loss 0.01874973 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5312 \tTraining Loss: 0.01150238 \tValidation Loss 0.01822198 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 5313 \tTraining Loss: 0.01149311 \tValidation Loss 0.01896333 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5314 \tTraining Loss: 0.01151358 \tValidation Loss 0.01859788 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5315 \tTraining Loss: 0.01148198 \tValidation Loss 0.01857355 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5316 \tTraining Loss: 0.01143585 \tValidation Loss 0.01860351 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5317 \tTraining Loss: 0.01143330 \tValidation Loss 0.01821657 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 5318 \tTraining Loss: 0.01150117 \tValidation Loss 0.01887813 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5319 \tTraining Loss: 0.01136194 \tValidation Loss 0.01888030 \tTraining Acuuarcy 44.070% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 5320 \tTraining Loss: 0.01138046 \tValidation Loss 0.01904338 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5321 \tTraining Loss: 0.01146831 \tValidation Loss 0.01857264 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5322 \tTraining Loss: 0.01137139 \tValidation Loss 0.01842492 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5323 \tTraining Loss: 0.01150663 \tValidation Loss 0.01858056 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5324 \tTraining Loss: 0.01153124 \tValidation Loss 0.01842628 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5325 \tTraining Loss: 0.01138823 \tValidation Loss 0.01855569 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 5326 \tTraining Loss: 0.01148153 \tValidation Loss 0.01862648 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 5327 \tTraining Loss: 0.01140098 \tValidation Loss 0.01873677 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5328 \tTraining Loss: 0.01134215 \tValidation Loss 0.01871753 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5329 \tTraining Loss: 0.01137483 \tValidation Loss 0.01835623 \tTraining Acuuarcy 44.226% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5330 \tTraining Loss: 0.01148172 \tValidation Loss 0.01824976 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5331 \tTraining Loss: 0.01136471 \tValidation Loss 0.01843293 \tTraining Acuuarcy 44.176% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5332 \tTraining Loss: 0.01150926 \tValidation Loss 0.01853862 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5333 \tTraining Loss: 0.01149191 \tValidation Loss 0.01900352 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 5334 \tTraining Loss: 0.01149590 \tValidation Loss 0.01841370 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 17.470%\n",
      "Epoch: 5335 \tTraining Loss: 0.01149102 \tValidation Loss 0.01838044 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5336 \tTraining Loss: 0.01142880 \tValidation Loss 0.01829745 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5337 \tTraining Loss: 0.01152484 \tValidation Loss 0.01837501 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 5338 \tTraining Loss: 0.01145953 \tValidation Loss 0.01812583 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5339 \tTraining Loss: 0.01142997 \tValidation Loss 0.01849877 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5340 \tTraining Loss: 0.01142769 \tValidation Loss 0.01854038 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5341 \tTraining Loss: 0.01153870 \tValidation Loss 0.01845241 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5342 \tTraining Loss: 0.01146515 \tValidation Loss 0.01847835 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 5343 \tTraining Loss: 0.01149888 \tValidation Loss 0.01845524 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 5344 \tTraining Loss: 0.01140726 \tValidation Loss 0.01849146 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5345 \tTraining Loss: 0.01137751 \tValidation Loss 0.01890528 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5346 \tTraining Loss: 0.01143859 \tValidation Loss 0.01898553 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5347 \tTraining Loss: 0.01141015 \tValidation Loss 0.01924264 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 5348 \tTraining Loss: 0.01140746 \tValidation Loss 0.01902663 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5349 \tTraining Loss: 0.01165308 \tValidation Loss 0.01842727 \tTraining Acuuarcy 42.286% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 5350 \tTraining Loss: 0.01138962 \tValidation Loss 0.01891061 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 5351 \tTraining Loss: 0.01140297 \tValidation Loss 0.01901076 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5352 \tTraining Loss: 0.01145171 \tValidation Loss 0.01868935 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5353 \tTraining Loss: 0.01138695 \tValidation Loss 0.01868939 \tTraining Acuuarcy 44.070% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5354 \tTraining Loss: 0.01146438 \tValidation Loss 0.01913725 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5355 \tTraining Loss: 0.01146529 \tValidation Loss 0.01844409 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5356 \tTraining Loss: 0.01148093 \tValidation Loss 0.01824641 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5357 \tTraining Loss: 0.01138693 \tValidation Loss 0.01903887 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5358 \tTraining Loss: 0.01141855 \tValidation Loss 0.01832638 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5359 \tTraining Loss: 0.01139245 \tValidation Loss 0.01855113 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 5360 \tTraining Loss: 0.01139431 \tValidation Loss 0.01853415 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5361 \tTraining Loss: 0.01158569 \tValidation Loss 0.01815693 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5362 \tTraining Loss: 0.01139526 \tValidation Loss 0.01887538 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5363 \tTraining Loss: 0.01136589 \tValidation Loss 0.01881613 \tTraining Acuuarcy 44.081% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5364 \tTraining Loss: 0.01137059 \tValidation Loss 0.01847639 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5365 \tTraining Loss: 0.01136706 \tValidation Loss 0.01839303 \tTraining Acuuarcy 44.137% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5366 \tTraining Loss: 0.01139661 \tValidation Loss 0.01871022 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5367 \tTraining Loss: 0.01147190 \tValidation Loss 0.01869547 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 5368 \tTraining Loss: 0.01144492 \tValidation Loss 0.01858590 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5369 \tTraining Loss: 0.01141885 \tValidation Loss 0.01852690 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5370 \tTraining Loss: 0.01150822 \tValidation Loss 0.01842228 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 5371 \tTraining Loss: 0.01129848 \tValidation Loss 0.01853667 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5372 \tTraining Loss: 0.01135013 \tValidation Loss 0.01887119 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 5373 \tTraining Loss: 0.01140859 \tValidation Loss 0.01853345 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5374 \tTraining Loss: 0.01150551 \tValidation Loss 0.01826465 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.114%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5375 \tTraining Loss: 0.01147674 \tValidation Loss 0.01847333 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5376 \tTraining Loss: 0.01145875 \tValidation Loss 0.01851073 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5377 \tTraining Loss: 0.01148742 \tValidation Loss 0.01834330 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5378 \tTraining Loss: 0.01146159 \tValidation Loss 0.01858158 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5379 \tTraining Loss: 0.01140111 \tValidation Loss 0.01845040 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5380 \tTraining Loss: 0.01141180 \tValidation Loss 0.01891606 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5381 \tTraining Loss: 0.01145686 \tValidation Loss 0.01867274 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5382 \tTraining Loss: 0.01144533 \tValidation Loss 0.01853234 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5383 \tTraining Loss: 0.01136800 \tValidation Loss 0.01867774 \tTraining Acuuarcy 43.908% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5384 \tTraining Loss: 0.01149072 \tValidation Loss 0.01899958 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5385 \tTraining Loss: 0.01148055 \tValidation Loss 0.01877255 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5386 \tTraining Loss: 0.01147639 \tValidation Loss 0.01836172 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 5387 \tTraining Loss: 0.01144268 \tValidation Loss 0.01875528 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5388 \tTraining Loss: 0.01140812 \tValidation Loss 0.01934345 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5389 \tTraining Loss: 0.01145951 \tValidation Loss 0.01858958 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5390 \tTraining Loss: 0.01139032 \tValidation Loss 0.01889988 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 5391 \tTraining Loss: 0.01142093 \tValidation Loss 0.01850643 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5392 \tTraining Loss: 0.01145077 \tValidation Loss 0.01845320 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5393 \tTraining Loss: 0.01142797 \tValidation Loss 0.01894322 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5394 \tTraining Loss: 0.01143314 \tValidation Loss 0.01842017 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5395 \tTraining Loss: 0.01134096 \tValidation Loss 0.01887121 \tTraining Acuuarcy 44.181% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5396 \tTraining Loss: 0.01151636 \tValidation Loss 0.01837801 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 5397 \tTraining Loss: 0.01153838 \tValidation Loss 0.01905458 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5398 \tTraining Loss: 0.01147700 \tValidation Loss 0.01820234 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5399 \tTraining Loss: 0.01145462 \tValidation Loss 0.01861759 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5400 \tTraining Loss: 0.01144979 \tValidation Loss 0.01881717 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5401 \tTraining Loss: 0.01138663 \tValidation Loss 0.01876581 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 5402 \tTraining Loss: 0.01138646 \tValidation Loss 0.01877845 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5403 \tTraining Loss: 0.01140481 \tValidation Loss 0.01906402 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5404 \tTraining Loss: 0.01149146 \tValidation Loss 0.01866135 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5405 \tTraining Loss: 0.01134757 \tValidation Loss 0.01912864 \tTraining Acuuarcy 44.209% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5406 \tTraining Loss: 0.01153196 \tValidation Loss 0.01896736 \tTraining Acuuarcy 42.632% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5407 \tTraining Loss: 0.01142615 \tValidation Loss 0.01856316 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5408 \tTraining Loss: 0.01138416 \tValidation Loss 0.01895020 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5409 \tTraining Loss: 0.01142071 \tValidation Loss 0.01877924 \tTraining Acuuarcy 42.827% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5410 \tTraining Loss: 0.01142165 \tValidation Loss 0.01854123 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 5411 \tTraining Loss: 0.01143647 \tValidation Loss 0.01834170 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5412 \tTraining Loss: 0.01147574 \tValidation Loss 0.01863825 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5413 \tTraining Loss: 0.01138967 \tValidation Loss 0.01882629 \tTraining Acuuarcy 44.198% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5414 \tTraining Loss: 0.01144665 \tValidation Loss 0.01795521 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5415 \tTraining Loss: 0.01145756 \tValidation Loss 0.01866916 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5416 \tTraining Loss: 0.01147812 \tValidation Loss 0.01874733 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5417 \tTraining Loss: 0.01138321 \tValidation Loss 0.01848836 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5418 \tTraining Loss: 0.01140070 \tValidation Loss 0.01876576 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5419 \tTraining Loss: 0.01135924 \tValidation Loss 0.01857075 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5420 \tTraining Loss: 0.01147417 \tValidation Loss 0.01869943 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 5421 \tTraining Loss: 0.01148269 \tValidation Loss 0.01856904 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5422 \tTraining Loss: 0.01143275 \tValidation Loss 0.01901263 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5423 \tTraining Loss: 0.01153521 \tValidation Loss 0.01789258 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 5424 \tTraining Loss: 0.01139946 \tValidation Loss 0.01824020 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 5425 \tTraining Loss: 0.01145110 \tValidation Loss 0.01872389 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5426 \tTraining Loss: 0.01142113 \tValidation Loss 0.01887214 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 5427 \tTraining Loss: 0.01147267 \tValidation Loss 0.01861368 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5428 \tTraining Loss: 0.01144348 \tValidation Loss 0.01844179 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 5429 \tTraining Loss: 0.01148253 \tValidation Loss 0.01860705 \tTraining Acuuarcy 42.933% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5430 \tTraining Loss: 0.01143871 \tValidation Loss 0.01860975 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 5431 \tTraining Loss: 0.01146045 \tValidation Loss 0.01866502 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5432 \tTraining Loss: 0.01142320 \tValidation Loss 0.01859304 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5433 \tTraining Loss: 0.01137870 \tValidation Loss 0.01832551 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5434 \tTraining Loss: 0.01142748 \tValidation Loss 0.01845747 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5435 \tTraining Loss: 0.01148843 \tValidation Loss 0.01875025 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5436 \tTraining Loss: 0.01146304 \tValidation Loss 0.01859256 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5437 \tTraining Loss: 0.01146868 \tValidation Loss 0.01890988 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5438 \tTraining Loss: 0.01154630 \tValidation Loss 0.01830432 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 5439 \tTraining Loss: 0.01138704 \tValidation Loss 0.01834498 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5440 \tTraining Loss: 0.01150557 \tValidation Loss 0.01850859 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5441 \tTraining Loss: 0.01137454 \tValidation Loss 0.01906221 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 19.225%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5442 \tTraining Loss: 0.01149609 \tValidation Loss 0.01850810 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5443 \tTraining Loss: 0.01136732 \tValidation Loss 0.01876816 \tTraining Acuuarcy 44.148% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5444 \tTraining Loss: 0.01141532 \tValidation Loss 0.01863279 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5445 \tTraining Loss: 0.01138101 \tValidation Loss 0.01816179 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5446 \tTraining Loss: 0.01141876 \tValidation Loss 0.01871289 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5447 \tTraining Loss: 0.01157785 \tValidation Loss 0.01836885 \tTraining Acuuarcy 42.515% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5448 \tTraining Loss: 0.01142265 \tValidation Loss 0.01855917 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5449 \tTraining Loss: 0.01142851 \tValidation Loss 0.01872108 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 5450 \tTraining Loss: 0.01138102 \tValidation Loss 0.01860661 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 5451 \tTraining Loss: 0.01153166 \tValidation Loss 0.01866189 \tTraining Acuuarcy 42.860% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5452 \tTraining Loss: 0.01141299 \tValidation Loss 0.01842221 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5453 \tTraining Loss: 0.01143022 \tValidation Loss 0.01904909 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5454 \tTraining Loss: 0.01150670 \tValidation Loss 0.01876534 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5455 \tTraining Loss: 0.01147822 \tValidation Loss 0.01827290 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5456 \tTraining Loss: 0.01141165 \tValidation Loss 0.01837887 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 5457 \tTraining Loss: 0.01145293 \tValidation Loss 0.01849121 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 5458 \tTraining Loss: 0.01141076 \tValidation Loss 0.01878629 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5459 \tTraining Loss: 0.01141176 \tValidation Loss 0.01923241 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5460 \tTraining Loss: 0.01137423 \tValidation Loss 0.01898554 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5461 \tTraining Loss: 0.01140415 \tValidation Loss 0.01834435 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 5462 \tTraining Loss: 0.01140436 \tValidation Loss 0.01872379 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 5463 \tTraining Loss: 0.01149112 \tValidation Loss 0.01892145 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5464 \tTraining Loss: 0.01142414 \tValidation Loss 0.01853981 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5465 \tTraining Loss: 0.01149744 \tValidation Loss 0.01854012 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5466 \tTraining Loss: 0.01142168 \tValidation Loss 0.01862557 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 5467 \tTraining Loss: 0.01140859 \tValidation Loss 0.01860157 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5468 \tTraining Loss: 0.01139924 \tValidation Loss 0.01873835 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5469 \tTraining Loss: 0.01156536 \tValidation Loss 0.01854568 \tTraining Acuuarcy 42.593% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5470 \tTraining Loss: 0.01137711 \tValidation Loss 0.01839779 \tTraining Acuuarcy 44.053% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 5471 \tTraining Loss: 0.01147516 \tValidation Loss 0.01866625 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 5472 \tTraining Loss: 0.01149478 \tValidation Loss 0.01819347 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5473 \tTraining Loss: 0.01137193 \tValidation Loss 0.01875360 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5474 \tTraining Loss: 0.01139962 \tValidation Loss 0.01874495 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 5475 \tTraining Loss: 0.01138365 \tValidation Loss 0.01916333 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5476 \tTraining Loss: 0.01154383 \tValidation Loss 0.01903760 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5477 \tTraining Loss: 0.01147059 \tValidation Loss 0.01812980 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5478 \tTraining Loss: 0.01141503 \tValidation Loss 0.01858205 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 5479 \tTraining Loss: 0.01152795 \tValidation Loss 0.01863772 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5480 \tTraining Loss: 0.01136213 \tValidation Loss 0.01913247 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 5481 \tTraining Loss: 0.01135140 \tValidation Loss 0.01869418 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5482 \tTraining Loss: 0.01136554 \tValidation Loss 0.01856549 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5483 \tTraining Loss: 0.01134754 \tValidation Loss 0.01908139 \tTraining Acuuarcy 43.992% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 5484 \tTraining Loss: 0.01145286 \tValidation Loss 0.01915681 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5485 \tTraining Loss: 0.01137584 \tValidation Loss 0.01949768 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5486 \tTraining Loss: 0.01142865 \tValidation Loss 0.01884961 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 5487 \tTraining Loss: 0.01145157 \tValidation Loss 0.01821933 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5488 \tTraining Loss: 0.01149134 \tValidation Loss 0.01822864 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 5489 \tTraining Loss: 0.01142973 \tValidation Loss 0.01854017 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5490 \tTraining Loss: 0.01139676 \tValidation Loss 0.01818726 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 5491 \tTraining Loss: 0.01148953 \tValidation Loss 0.01896285 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5492 \tTraining Loss: 0.01145668 \tValidation Loss 0.01847127 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5493 \tTraining Loss: 0.01149133 \tValidation Loss 0.01809790 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 5494 \tTraining Loss: 0.01142779 \tValidation Loss 0.01852561 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5495 \tTraining Loss: 0.01141494 \tValidation Loss 0.01880147 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5496 \tTraining Loss: 0.01141577 \tValidation Loss 0.01887300 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 5497 \tTraining Loss: 0.01139275 \tValidation Loss 0.01858355 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5498 \tTraining Loss: 0.01148374 \tValidation Loss 0.01881299 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5499 \tTraining Loss: 0.01147613 \tValidation Loss 0.01858073 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5500 \tTraining Loss: 0.01148663 \tValidation Loss 0.01897957 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5501 \tTraining Loss: 0.01136987 \tValidation Loss 0.01874214 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5502 \tTraining Loss: 0.01145670 \tValidation Loss 0.01863473 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5503 \tTraining Loss: 0.01149612 \tValidation Loss 0.01815375 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5504 \tTraining Loss: 0.01142723 \tValidation Loss 0.01847186 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5505 \tTraining Loss: 0.01147617 \tValidation Loss 0.01882067 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5506 \tTraining Loss: 0.01148567 \tValidation Loss 0.01840799 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5507 \tTraining Loss: 0.01137784 \tValidation Loss 0.01873552 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5508 \tTraining Loss: 0.01144520 \tValidation Loss 0.01873873 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.532%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5509 \tTraining Loss: 0.01145393 \tValidation Loss 0.01893532 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 5510 \tTraining Loss: 0.01145139 \tValidation Loss 0.01881494 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 5511 \tTraining Loss: 0.01141698 \tValidation Loss 0.01871971 \tTraining Acuuarcy 44.070% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5512 \tTraining Loss: 0.01146042 \tValidation Loss 0.01906956 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 5513 \tTraining Loss: 0.01141836 \tValidation Loss 0.01853524 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5514 \tTraining Loss: 0.01149482 \tValidation Loss 0.01824348 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5515 \tTraining Loss: 0.01142536 \tValidation Loss 0.01835570 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 5516 \tTraining Loss: 0.01141814 \tValidation Loss 0.01887555 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5517 \tTraining Loss: 0.01141884 \tValidation Loss 0.01860728 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 5518 \tTraining Loss: 0.01140295 \tValidation Loss 0.01863305 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5519 \tTraining Loss: 0.01135504 \tValidation Loss 0.01873090 \tTraining Acuuarcy 44.109% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5520 \tTraining Loss: 0.01134689 \tValidation Loss 0.01875797 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5521 \tTraining Loss: 0.01139844 \tValidation Loss 0.01848553 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 5522 \tTraining Loss: 0.01140604 \tValidation Loss 0.01856269 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5523 \tTraining Loss: 0.01146389 \tValidation Loss 0.01866125 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5524 \tTraining Loss: 0.01138936 \tValidation Loss 0.01870429 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 5525 \tTraining Loss: 0.01151304 \tValidation Loss 0.01872028 \tTraining Acuuarcy 42.699% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 5526 \tTraining Loss: 0.01136885 \tValidation Loss 0.01882560 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5527 \tTraining Loss: 0.01137616 \tValidation Loss 0.01922594 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5528 \tTraining Loss: 0.01143030 \tValidation Loss 0.01893654 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5529 \tTraining Loss: 0.01139407 \tValidation Loss 0.01859814 \tTraining Acuuarcy 44.204% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5530 \tTraining Loss: 0.01144425 \tValidation Loss 0.01894480 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5531 \tTraining Loss: 0.01157919 \tValidation Loss 0.01900507 \tTraining Acuuarcy 42.649% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5532 \tTraining Loss: 0.01143623 \tValidation Loss 0.01889833 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5533 \tTraining Loss: 0.01138870 \tValidation Loss 0.01875482 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5534 \tTraining Loss: 0.01151611 \tValidation Loss 0.01884162 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5535 \tTraining Loss: 0.01153259 \tValidation Loss 0.01828584 \tTraining Acuuarcy 42.849% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5536 \tTraining Loss: 0.01146346 \tValidation Loss 0.01875957 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5537 \tTraining Loss: 0.01148515 \tValidation Loss 0.01879588 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5538 \tTraining Loss: 0.01138297 \tValidation Loss 0.01879922 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5539 \tTraining Loss: 0.01148406 \tValidation Loss 0.01896081 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 5540 \tTraining Loss: 0.01134340 \tValidation Loss 0.01853658 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5541 \tTraining Loss: 0.01144138 \tValidation Loss 0.01866288 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5542 \tTraining Loss: 0.01138680 \tValidation Loss 0.01872011 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5543 \tTraining Loss: 0.01147508 \tValidation Loss 0.01877495 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5544 \tTraining Loss: 0.01140013 \tValidation Loss 0.01927266 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5545 \tTraining Loss: 0.01150022 \tValidation Loss 0.01828967 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5546 \tTraining Loss: 0.01146292 \tValidation Loss 0.01914943 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 5547 \tTraining Loss: 0.01148642 \tValidation Loss 0.01843629 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5548 \tTraining Loss: 0.01145245 \tValidation Loss 0.01964566 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5549 \tTraining Loss: 0.01150361 \tValidation Loss 0.01848804 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5550 \tTraining Loss: 0.01137850 \tValidation Loss 0.01863302 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5551 \tTraining Loss: 0.01152369 \tValidation Loss 0.01858293 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5552 \tTraining Loss: 0.01135178 \tValidation Loss 0.01913031 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5553 \tTraining Loss: 0.01138130 \tValidation Loss 0.01892153 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5554 \tTraining Loss: 0.01134661 \tValidation Loss 0.01895991 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 5555 \tTraining Loss: 0.01140097 \tValidation Loss 0.01865900 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5556 \tTraining Loss: 0.01145790 \tValidation Loss 0.01826300 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 5557 \tTraining Loss: 0.01139983 \tValidation Loss 0.01845600 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5558 \tTraining Loss: 0.01142475 \tValidation Loss 0.01865184 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 5559 \tTraining Loss: 0.01138450 \tValidation Loss 0.01871672 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5560 \tTraining Loss: 0.01144864 \tValidation Loss 0.01838680 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5561 \tTraining Loss: 0.01142970 \tValidation Loss 0.01840911 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5562 \tTraining Loss: 0.01143149 \tValidation Loss 0.01870165 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5563 \tTraining Loss: 0.01151324 \tValidation Loss 0.01901092 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5564 \tTraining Loss: 0.01153699 \tValidation Loss 0.01831797 \tTraining Acuuarcy 42.732% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5565 \tTraining Loss: 0.01142829 \tValidation Loss 0.01835237 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5566 \tTraining Loss: 0.01147404 \tValidation Loss 0.01848017 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5567 \tTraining Loss: 0.01146598 \tValidation Loss 0.01842174 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5568 \tTraining Loss: 0.01143031 \tValidation Loss 0.01845807 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 5569 \tTraining Loss: 0.01147057 \tValidation Loss 0.01869501 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5570 \tTraining Loss: 0.01142038 \tValidation Loss 0.01826615 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5571 \tTraining Loss: 0.01133234 \tValidation Loss 0.01898045 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5572 \tTraining Loss: 0.01141882 \tValidation Loss 0.01932842 \tTraining Acuuarcy 42.955% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5573 \tTraining Loss: 0.01138271 \tValidation Loss 0.01870676 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 5574 \tTraining Loss: 0.01154992 \tValidation Loss 0.01839019 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5575 \tTraining Loss: 0.01147065 \tValidation Loss 0.01832087 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 19.058%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5576 \tTraining Loss: 0.01140583 \tValidation Loss 0.01857877 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5577 \tTraining Loss: 0.01139705 \tValidation Loss 0.01878865 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 5578 \tTraining Loss: 0.01146087 \tValidation Loss 0.01870382 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5579 \tTraining Loss: 0.01129887 \tValidation Loss 0.01865886 \tTraining Acuuarcy 44.371% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5580 \tTraining Loss: 0.01147877 \tValidation Loss 0.01879621 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5581 \tTraining Loss: 0.01153605 \tValidation Loss 0.01846075 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5582 \tTraining Loss: 0.01142459 \tValidation Loss 0.01856978 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5583 \tTraining Loss: 0.01144846 \tValidation Loss 0.01830990 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 5584 \tTraining Loss: 0.01147807 \tValidation Loss 0.01829765 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 5585 \tTraining Loss: 0.01144370 \tValidation Loss 0.01879079 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 5586 \tTraining Loss: 0.01146984 \tValidation Loss 0.01818767 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5587 \tTraining Loss: 0.01136450 \tValidation Loss 0.01848223 \tTraining Acuuarcy 44.254% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5588 \tTraining Loss: 0.01139093 \tValidation Loss 0.01896698 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5589 \tTraining Loss: 0.01145627 \tValidation Loss 0.01858099 \tTraining Acuuarcy 43.412% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5590 \tTraining Loss: 0.01151805 \tValidation Loss 0.01902282 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 5591 \tTraining Loss: 0.01142651 \tValidation Loss 0.01868952 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 5592 \tTraining Loss: 0.01142926 \tValidation Loss 0.01865240 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 5593 \tTraining Loss: 0.01137938 \tValidation Loss 0.01876514 \tTraining Acuuarcy 44.237% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 5594 \tTraining Loss: 0.01140702 \tValidation Loss 0.01847177 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5595 \tTraining Loss: 0.01141607 \tValidation Loss 0.01853968 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5596 \tTraining Loss: 0.01131063 \tValidation Loss 0.01926862 \tTraining Acuuarcy 44.465% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 5597 \tTraining Loss: 0.01143042 \tValidation Loss 0.01859440 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5598 \tTraining Loss: 0.01131667 \tValidation Loss 0.01836860 \tTraining Acuuarcy 43.958% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 5599 \tTraining Loss: 0.01141670 \tValidation Loss 0.01875628 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5600 \tTraining Loss: 0.01141719 \tValidation Loss 0.01870535 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5601 \tTraining Loss: 0.01141271 \tValidation Loss 0.01850866 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 5602 \tTraining Loss: 0.01137531 \tValidation Loss 0.01849649 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5603 \tTraining Loss: 0.01144807 \tValidation Loss 0.01814956 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 5604 \tTraining Loss: 0.01139036 \tValidation Loss 0.01903579 \tTraining Acuuarcy 44.081% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5605 \tTraining Loss: 0.01150005 \tValidation Loss 0.01918468 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5606 \tTraining Loss: 0.01136098 \tValidation Loss 0.01873368 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5607 \tTraining Loss: 0.01142948 \tValidation Loss 0.01886498 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5608 \tTraining Loss: 0.01146477 \tValidation Loss 0.01888638 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5609 \tTraining Loss: 0.01147948 \tValidation Loss 0.01868826 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5610 \tTraining Loss: 0.01141461 \tValidation Loss 0.01806737 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5611 \tTraining Loss: 0.01145911 \tValidation Loss 0.01837980 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5612 \tTraining Loss: 0.01151228 \tValidation Loss 0.01845603 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5613 \tTraining Loss: 0.01150179 \tValidation Loss 0.01837326 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5614 \tTraining Loss: 0.01132672 \tValidation Loss 0.01828896 \tTraining Acuuarcy 44.360% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 5615 \tTraining Loss: 0.01140620 \tValidation Loss 0.01839946 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5616 \tTraining Loss: 0.01144162 \tValidation Loss 0.01831005 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 5617 \tTraining Loss: 0.01137512 \tValidation Loss 0.01898496 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 5618 \tTraining Loss: 0.01139034 \tValidation Loss 0.01829708 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5619 \tTraining Loss: 0.01144041 \tValidation Loss 0.01842878 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5620 \tTraining Loss: 0.01154139 \tValidation Loss 0.01871455 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5621 \tTraining Loss: 0.01137260 \tValidation Loss 0.01894794 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5622 \tTraining Loss: 0.01134982 \tValidation Loss 0.01851137 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 5623 \tTraining Loss: 0.01147069 \tValidation Loss 0.01842382 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5624 \tTraining Loss: 0.01143678 \tValidation Loss 0.01877467 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5625 \tTraining Loss: 0.01139590 \tValidation Loss 0.01882942 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 5626 \tTraining Loss: 0.01140692 \tValidation Loss 0.01851384 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 5627 \tTraining Loss: 0.01141417 \tValidation Loss 0.01902311 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 5628 \tTraining Loss: 0.01144236 \tValidation Loss 0.01907447 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5629 \tTraining Loss: 0.01139129 \tValidation Loss 0.01879495 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5630 \tTraining Loss: 0.01140582 \tValidation Loss 0.01919555 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5631 \tTraining Loss: 0.01137460 \tValidation Loss 0.01859811 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5632 \tTraining Loss: 0.01146193 \tValidation Loss 0.01861079 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5633 \tTraining Loss: 0.01147434 \tValidation Loss 0.01847652 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5634 \tTraining Loss: 0.01149025 \tValidation Loss 0.01868955 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5635 \tTraining Loss: 0.01144463 \tValidation Loss 0.01848412 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5636 \tTraining Loss: 0.01147827 \tValidation Loss 0.01807497 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5637 \tTraining Loss: 0.01143733 \tValidation Loss 0.01837212 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 5638 \tTraining Loss: 0.01142031 \tValidation Loss 0.01883196 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5639 \tTraining Loss: 0.01145842 \tValidation Loss 0.01849625 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5640 \tTraining Loss: 0.01145026 \tValidation Loss 0.01863301 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5641 \tTraining Loss: 0.01146038 \tValidation Loss 0.01872637 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5642 \tTraining Loss: 0.01141527 \tValidation Loss 0.01874275 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.086%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5643 \tTraining Loss: 0.01136538 \tValidation Loss 0.01897531 \tTraining Acuuarcy 44.142% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5644 \tTraining Loss: 0.01139584 \tValidation Loss 0.01923092 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5645 \tTraining Loss: 0.01142056 \tValidation Loss 0.01871315 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 5646 \tTraining Loss: 0.01148078 \tValidation Loss 0.01843431 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5647 \tTraining Loss: 0.01150989 \tValidation Loss 0.01880269 \tTraining Acuuarcy 42.816% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5648 \tTraining Loss: 0.01151142 \tValidation Loss 0.01890902 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5649 \tTraining Loss: 0.01139115 \tValidation Loss 0.01853765 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5650 \tTraining Loss: 0.01145163 \tValidation Loss 0.01859923 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5651 \tTraining Loss: 0.01124976 \tValidation Loss 0.01824858 \tTraining Acuuarcy 44.945% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5652 \tTraining Loss: 0.01144976 \tValidation Loss 0.01840551 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5653 \tTraining Loss: 0.01144047 \tValidation Loss 0.01854752 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5654 \tTraining Loss: 0.01140097 \tValidation Loss 0.01857159 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5655 \tTraining Loss: 0.01140383 \tValidation Loss 0.01878977 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5656 \tTraining Loss: 0.01143016 \tValidation Loss 0.01855212 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 5657 \tTraining Loss: 0.01142035 \tValidation Loss 0.01859636 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5658 \tTraining Loss: 0.01142200 \tValidation Loss 0.01903810 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 5659 \tTraining Loss: 0.01133912 \tValidation Loss 0.01867659 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 5660 \tTraining Loss: 0.01136486 \tValidation Loss 0.01862878 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5661 \tTraining Loss: 0.01142878 \tValidation Loss 0.01850004 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 5662 \tTraining Loss: 0.01147573 \tValidation Loss 0.01861068 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5663 \tTraining Loss: 0.01141205 \tValidation Loss 0.01861502 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5664 \tTraining Loss: 0.01134868 \tValidation Loss 0.01850857 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5665 \tTraining Loss: 0.01151147 \tValidation Loss 0.01859924 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5666 \tTraining Loss: 0.01141773 \tValidation Loss 0.01847827 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5667 \tTraining Loss: 0.01139041 \tValidation Loss 0.01834806 \tTraining Acuuarcy 44.176% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5668 \tTraining Loss: 0.01136027 \tValidation Loss 0.01916152 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 5669 \tTraining Loss: 0.01149038 \tValidation Loss 0.01901932 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5670 \tTraining Loss: 0.01148461 \tValidation Loss 0.01835799 \tTraining Acuuarcy 43.044% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5671 \tTraining Loss: 0.01147862 \tValidation Loss 0.01890385 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5672 \tTraining Loss: 0.01138253 \tValidation Loss 0.01809308 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5673 \tTraining Loss: 0.01145640 \tValidation Loss 0.01883543 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 5674 \tTraining Loss: 0.01139302 \tValidation Loss 0.01890564 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 5675 \tTraining Loss: 0.01145369 \tValidation Loss 0.01845918 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5676 \tTraining Loss: 0.01136162 \tValidation Loss 0.01903664 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5677 \tTraining Loss: 0.01165114 \tValidation Loss 0.01850679 \tTraining Acuuarcy 42.409% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 5678 \tTraining Loss: 0.01149690 \tValidation Loss 0.01854619 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5679 \tTraining Loss: 0.01137117 \tValidation Loss 0.01863717 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5680 \tTraining Loss: 0.01134043 \tValidation Loss 0.01888678 \tTraining Acuuarcy 44.064% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5681 \tTraining Loss: 0.01146727 \tValidation Loss 0.01846578 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5682 \tTraining Loss: 0.01138922 \tValidation Loss 0.01835096 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5683 \tTraining Loss: 0.01135816 \tValidation Loss 0.01855269 \tTraining Acuuarcy 44.332% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 5684 \tTraining Loss: 0.01137037 \tValidation Loss 0.01870864 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5685 \tTraining Loss: 0.01141192 \tValidation Loss 0.01848576 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5686 \tTraining Loss: 0.01142449 \tValidation Loss 0.01853373 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 5687 \tTraining Loss: 0.01135860 \tValidation Loss 0.01856028 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5688 \tTraining Loss: 0.01142846 \tValidation Loss 0.01889718 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5689 \tTraining Loss: 0.01147887 \tValidation Loss 0.01824917 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5690 \tTraining Loss: 0.01149161 \tValidation Loss 0.01830164 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5691 \tTraining Loss: 0.01127417 \tValidation Loss 0.01872814 \tTraining Acuuarcy 44.360% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5692 \tTraining Loss: 0.01143930 \tValidation Loss 0.01855787 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5693 \tTraining Loss: 0.01136720 \tValidation Loss 0.01861430 \tTraining Acuuarcy 44.204% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 5694 \tTraining Loss: 0.01146132 \tValidation Loss 0.01842010 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 5695 \tTraining Loss: 0.01137951 \tValidation Loss 0.01878792 \tTraining Acuuarcy 44.036% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5696 \tTraining Loss: 0.01148011 \tValidation Loss 0.01879894 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5697 \tTraining Loss: 0.01145570 \tValidation Loss 0.01843510 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 5698 \tTraining Loss: 0.01141000 \tValidation Loss 0.01837221 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5699 \tTraining Loss: 0.01142434 \tValidation Loss 0.01855561 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 5700 \tTraining Loss: 0.01130572 \tValidation Loss 0.01849592 \tTraining Acuuarcy 44.544% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5701 \tTraining Loss: 0.01151272 \tValidation Loss 0.01881567 \tTraining Acuuarcy 42.727% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 5702 \tTraining Loss: 0.01138468 \tValidation Loss 0.01870767 \tTraining Acuuarcy 44.159% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5703 \tTraining Loss: 0.01139211 \tValidation Loss 0.01836722 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5704 \tTraining Loss: 0.01145746 \tValidation Loss 0.01838356 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5705 \tTraining Loss: 0.01138706 \tValidation Loss 0.01880805 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 5706 \tTraining Loss: 0.01141672 \tValidation Loss 0.01865741 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5707 \tTraining Loss: 0.01142066 \tValidation Loss 0.01915882 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 5708 \tTraining Loss: 0.01135092 \tValidation Loss 0.01879643 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5709 \tTraining Loss: 0.01144561 \tValidation Loss 0.01920572 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.448%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5710 \tTraining Loss: 0.01137099 \tValidation Loss 0.01888438 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5711 \tTraining Loss: 0.01138780 \tValidation Loss 0.01868142 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 5712 \tTraining Loss: 0.01151703 \tValidation Loss 0.01829152 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5713 \tTraining Loss: 0.01139519 \tValidation Loss 0.01910044 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 5714 \tTraining Loss: 0.01138472 \tValidation Loss 0.01848953 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5715 \tTraining Loss: 0.01149499 \tValidation Loss 0.01825778 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 5716 \tTraining Loss: 0.01144594 \tValidation Loss 0.01860966 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5717 \tTraining Loss: 0.01140499 \tValidation Loss 0.01889017 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5718 \tTraining Loss: 0.01137317 \tValidation Loss 0.01865071 \tTraining Acuuarcy 44.059% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5719 \tTraining Loss: 0.01148149 \tValidation Loss 0.01851490 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5720 \tTraining Loss: 0.01142197 \tValidation Loss 0.01881959 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5721 \tTraining Loss: 0.01143998 \tValidation Loss 0.01870294 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5722 \tTraining Loss: 0.01141444 \tValidation Loss 0.01871229 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5723 \tTraining Loss: 0.01128660 \tValidation Loss 0.01865406 \tTraining Acuuarcy 44.477% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5724 \tTraining Loss: 0.01136659 \tValidation Loss 0.01886525 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 5725 \tTraining Loss: 0.01135505 \tValidation Loss 0.01897660 \tTraining Acuuarcy 44.025% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 5726 \tTraining Loss: 0.01143550 \tValidation Loss 0.01865500 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5727 \tTraining Loss: 0.01142104 \tValidation Loss 0.01846705 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5728 \tTraining Loss: 0.01144235 \tValidation Loss 0.01871840 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 5729 \tTraining Loss: 0.01137829 \tValidation Loss 0.01865716 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5730 \tTraining Loss: 0.01141843 \tValidation Loss 0.01932255 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5731 \tTraining Loss: 0.01143751 \tValidation Loss 0.01857533 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5732 \tTraining Loss: 0.01153806 \tValidation Loss 0.01844995 \tTraining Acuuarcy 42.353% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 5733 \tTraining Loss: 0.01138736 \tValidation Loss 0.01904840 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 5734 \tTraining Loss: 0.01143619 \tValidation Loss 0.01846209 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5735 \tTraining Loss: 0.01131775 \tValidation Loss 0.01890686 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5736 \tTraining Loss: 0.01148830 \tValidation Loss 0.01837540 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 5737 \tTraining Loss: 0.01152140 \tValidation Loss 0.01895224 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5738 \tTraining Loss: 0.01140149 \tValidation Loss 0.01900637 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 5739 \tTraining Loss: 0.01137469 \tValidation Loss 0.01881044 \tTraining Acuuarcy 44.109% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5740 \tTraining Loss: 0.01144714 \tValidation Loss 0.01835688 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5741 \tTraining Loss: 0.01142195 \tValidation Loss 0.01894454 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5742 \tTraining Loss: 0.01136457 \tValidation Loss 0.01835821 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5743 \tTraining Loss: 0.01152496 \tValidation Loss 0.01912441 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 5744 \tTraining Loss: 0.01141944 \tValidation Loss 0.01873517 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 5745 \tTraining Loss: 0.01145448 \tValidation Loss 0.01882199 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5746 \tTraining Loss: 0.01142660 \tValidation Loss 0.01804128 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5747 \tTraining Loss: 0.01142145 \tValidation Loss 0.01878008 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5748 \tTraining Loss: 0.01147201 \tValidation Loss 0.01871620 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5749 \tTraining Loss: 0.01144079 \tValidation Loss 0.01815017 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5750 \tTraining Loss: 0.01133678 \tValidation Loss 0.01848105 \tTraining Acuuarcy 44.226% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 5751 \tTraining Loss: 0.01143719 \tValidation Loss 0.01881046 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5752 \tTraining Loss: 0.01138244 \tValidation Loss 0.01864583 \tTraining Acuuarcy 44.309% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5753 \tTraining Loss: 0.01144515 \tValidation Loss 0.01827678 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5754 \tTraining Loss: 0.01150683 \tValidation Loss 0.01871171 \tTraining Acuuarcy 42.821% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5755 \tTraining Loss: 0.01145280 \tValidation Loss 0.01854764 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 5756 \tTraining Loss: 0.01136897 \tValidation Loss 0.01892199 \tTraining Acuuarcy 44.109% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5757 \tTraining Loss: 0.01152762 \tValidation Loss 0.01849603 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 5758 \tTraining Loss: 0.01142478 \tValidation Loss 0.01858261 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 5759 \tTraining Loss: 0.01136169 \tValidation Loss 0.01866126 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 5760 \tTraining Loss: 0.01141322 \tValidation Loss 0.01866851 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5761 \tTraining Loss: 0.01141332 \tValidation Loss 0.01910335 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 5762 \tTraining Loss: 0.01138951 \tValidation Loss 0.01876873 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5763 \tTraining Loss: 0.01132737 \tValidation Loss 0.01889482 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5764 \tTraining Loss: 0.01139010 \tValidation Loss 0.01863630 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5765 \tTraining Loss: 0.01139330 \tValidation Loss 0.01872110 \tTraining Acuuarcy 44.170% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5766 \tTraining Loss: 0.01149150 \tValidation Loss 0.01846291 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5767 \tTraining Loss: 0.01138577 \tValidation Loss 0.01887944 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5768 \tTraining Loss: 0.01131295 \tValidation Loss 0.01837717 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5769 \tTraining Loss: 0.01140795 \tValidation Loss 0.01910506 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5770 \tTraining Loss: 0.01143974 \tValidation Loss 0.01887805 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5771 \tTraining Loss: 0.01135755 \tValidation Loss 0.01821444 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5772 \tTraining Loss: 0.01144912 \tValidation Loss 0.01844866 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 5773 \tTraining Loss: 0.01132789 \tValidation Loss 0.01886189 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5774 \tTraining Loss: 0.01141791 \tValidation Loss 0.01851822 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5775 \tTraining Loss: 0.01152170 \tValidation Loss 0.01901823 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5776 \tTraining Loss: 0.01147031 \tValidation Loss 0.01889973 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.309%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5777 \tTraining Loss: 0.01145993 \tValidation Loss 0.01852961 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5778 \tTraining Loss: 0.01132377 \tValidation Loss 0.01910160 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5779 \tTraining Loss: 0.01144582 \tValidation Loss 0.01826092 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 5780 \tTraining Loss: 0.01145290 \tValidation Loss 0.01884710 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 5781 \tTraining Loss: 0.01140564 \tValidation Loss 0.01844808 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5782 \tTraining Loss: 0.01133218 \tValidation Loss 0.01873407 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5783 \tTraining Loss: 0.01152447 \tValidation Loss 0.01848174 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5784 \tTraining Loss: 0.01140881 \tValidation Loss 0.01851741 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5785 \tTraining Loss: 0.01140127 \tValidation Loss 0.01889176 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5786 \tTraining Loss: 0.01142727 \tValidation Loss 0.01859445 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 5787 \tTraining Loss: 0.01141123 \tValidation Loss 0.01845489 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5788 \tTraining Loss: 0.01146283 \tValidation Loss 0.01873409 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5789 \tTraining Loss: 0.01134536 \tValidation Loss 0.01887218 \tTraining Acuuarcy 44.053% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5790 \tTraining Loss: 0.01141392 \tValidation Loss 0.01899653 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5791 \tTraining Loss: 0.01141305 \tValidation Loss 0.01902107 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5792 \tTraining Loss: 0.01136043 \tValidation Loss 0.01855965 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 5793 \tTraining Loss: 0.01135971 \tValidation Loss 0.01856551 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 5794 \tTraining Loss: 0.01136464 \tValidation Loss 0.01845486 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 5795 \tTraining Loss: 0.01145210 \tValidation Loss 0.01890087 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5796 \tTraining Loss: 0.01134049 \tValidation Loss 0.01868063 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 5797 \tTraining Loss: 0.01142062 \tValidation Loss 0.01883876 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 5798 \tTraining Loss: 0.01146503 \tValidation Loss 0.01894892 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 5799 \tTraining Loss: 0.01138200 \tValidation Loss 0.01904352 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 5800 \tTraining Loss: 0.01142347 \tValidation Loss 0.01864601 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5801 \tTraining Loss: 0.01137771 \tValidation Loss 0.01865284 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5802 \tTraining Loss: 0.01145158 \tValidation Loss 0.01952024 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5803 \tTraining Loss: 0.01136861 \tValidation Loss 0.01850815 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 5804 \tTraining Loss: 0.01138437 \tValidation Loss 0.01881529 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5805 \tTraining Loss: 0.01149973 \tValidation Loss 0.01894345 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5806 \tTraining Loss: 0.01148090 \tValidation Loss 0.01855186 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 5807 \tTraining Loss: 0.01142187 \tValidation Loss 0.01866974 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5808 \tTraining Loss: 0.01140815 \tValidation Loss 0.01901099 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 5809 \tTraining Loss: 0.01136077 \tValidation Loss 0.01900829 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5810 \tTraining Loss: 0.01141570 \tValidation Loss 0.01867447 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5811 \tTraining Loss: 0.01145244 \tValidation Loss 0.01855155 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5812 \tTraining Loss: 0.01145628 \tValidation Loss 0.01878337 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5813 \tTraining Loss: 0.01149570 \tValidation Loss 0.01850287 \tTraining Acuuarcy 42.810% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 5814 \tTraining Loss: 0.01139894 \tValidation Loss 0.01854301 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5815 \tTraining Loss: 0.01137277 \tValidation Loss 0.01872168 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5816 \tTraining Loss: 0.01140511 \tValidation Loss 0.01861342 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5817 \tTraining Loss: 0.01138440 \tValidation Loss 0.01853035 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5818 \tTraining Loss: 0.01144782 \tValidation Loss 0.01873862 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5819 \tTraining Loss: 0.01140888 \tValidation Loss 0.01877380 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5820 \tTraining Loss: 0.01136668 \tValidation Loss 0.01899112 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 5821 \tTraining Loss: 0.01150079 \tValidation Loss 0.01878463 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 5822 \tTraining Loss: 0.01136029 \tValidation Loss 0.01925900 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 5823 \tTraining Loss: 0.01131082 \tValidation Loss 0.01844998 \tTraining Acuuarcy 44.694% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5824 \tTraining Loss: 0.01143137 \tValidation Loss 0.01834871 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5825 \tTraining Loss: 0.01132561 \tValidation Loss 0.01850866 \tTraining Acuuarcy 44.326% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5826 \tTraining Loss: 0.01144543 \tValidation Loss 0.01853184 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5827 \tTraining Loss: 0.01153494 \tValidation Loss 0.01840616 \tTraining Acuuarcy 42.531% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5828 \tTraining Loss: 0.01143495 \tValidation Loss 0.01852147 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5829 \tTraining Loss: 0.01155507 \tValidation Loss 0.01830246 \tTraining Acuuarcy 42.660% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 5830 \tTraining Loss: 0.01141650 \tValidation Loss 0.01878484 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 5831 \tTraining Loss: 0.01144078 \tValidation Loss 0.01869488 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5832 \tTraining Loss: 0.01139253 \tValidation Loss 0.01863866 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 5833 \tTraining Loss: 0.01143199 \tValidation Loss 0.01892563 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5834 \tTraining Loss: 0.01140438 \tValidation Loss 0.01835627 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5835 \tTraining Loss: 0.01149278 \tValidation Loss 0.01857445 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5836 \tTraining Loss: 0.01140476 \tValidation Loss 0.01865150 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5837 \tTraining Loss: 0.01144819 \tValidation Loss 0.01811902 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5838 \tTraining Loss: 0.01139596 \tValidation Loss 0.01855929 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 5839 \tTraining Loss: 0.01144397 \tValidation Loss 0.01834316 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5840 \tTraining Loss: 0.01126789 \tValidation Loss 0.01897761 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5841 \tTraining Loss: 0.01139623 \tValidation Loss 0.01869332 \tTraining Acuuarcy 44.053% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5842 \tTraining Loss: 0.01141842 \tValidation Loss 0.01876254 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5843 \tTraining Loss: 0.01136096 \tValidation Loss 0.01878165 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.003%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5844 \tTraining Loss: 0.01139024 \tValidation Loss 0.01847410 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5845 \tTraining Loss: 0.01155209 \tValidation Loss 0.01859002 \tTraining Acuuarcy 42.777% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5846 \tTraining Loss: 0.01135469 \tValidation Loss 0.01856134 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 5847 \tTraining Loss: 0.01133133 \tValidation Loss 0.01885131 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5848 \tTraining Loss: 0.01141141 \tValidation Loss 0.01880472 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5849 \tTraining Loss: 0.01145981 \tValidation Loss 0.01942290 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5850 \tTraining Loss: 0.01145987 \tValidation Loss 0.01846492 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 5851 \tTraining Loss: 0.01144162 \tValidation Loss 0.01894221 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5852 \tTraining Loss: 0.01144242 \tValidation Loss 0.01903156 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5853 \tTraining Loss: 0.01143632 \tValidation Loss 0.01872237 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 5854 \tTraining Loss: 0.01149064 \tValidation Loss 0.01859463 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 5855 \tTraining Loss: 0.01137189 \tValidation Loss 0.01837184 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5856 \tTraining Loss: 0.01146463 \tValidation Loss 0.01839088 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5857 \tTraining Loss: 0.01142161 \tValidation Loss 0.01837196 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5858 \tTraining Loss: 0.01165422 \tValidation Loss 0.01867391 \tTraining Acuuarcy 42.610% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5859 \tTraining Loss: 0.01140640 \tValidation Loss 0.01892064 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 5860 \tTraining Loss: 0.01132791 \tValidation Loss 0.01863938 \tTraining Acuuarcy 44.315% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5861 \tTraining Loss: 0.01133228 \tValidation Loss 0.01878983 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5862 \tTraining Loss: 0.01146982 \tValidation Loss 0.01877023 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5863 \tTraining Loss: 0.01145978 \tValidation Loss 0.01829208 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 5864 \tTraining Loss: 0.01135038 \tValidation Loss 0.01886895 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5865 \tTraining Loss: 0.01138629 \tValidation Loss 0.01861168 \tTraining Acuuarcy 44.148% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5866 \tTraining Loss: 0.01148344 \tValidation Loss 0.01829126 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5867 \tTraining Loss: 0.01138228 \tValidation Loss 0.01868950 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5868 \tTraining Loss: 0.01140067 \tValidation Loss 0.01827925 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 5869 \tTraining Loss: 0.01150320 \tValidation Loss 0.01843766 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5870 \tTraining Loss: 0.01137282 \tValidation Loss 0.01860612 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5871 \tTraining Loss: 0.01141769 \tValidation Loss 0.01840378 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5872 \tTraining Loss: 0.01139730 \tValidation Loss 0.01851519 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5873 \tTraining Loss: 0.01145213 \tValidation Loss 0.01882262 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 5874 \tTraining Loss: 0.01139964 \tValidation Loss 0.01839292 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 5875 \tTraining Loss: 0.01141737 \tValidation Loss 0.01848388 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 5876 \tTraining Loss: 0.01143053 \tValidation Loss 0.01840671 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5877 \tTraining Loss: 0.01140747 \tValidation Loss 0.01856150 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 5878 \tTraining Loss: 0.01146589 \tValidation Loss 0.01885383 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5879 \tTraining Loss: 0.01151409 \tValidation Loss 0.01840932 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5880 \tTraining Loss: 0.01142188 \tValidation Loss 0.01833456 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5881 \tTraining Loss: 0.01144679 \tValidation Loss 0.01794126 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5882 \tTraining Loss: 0.01134184 \tValidation Loss 0.01867054 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5883 \tTraining Loss: 0.01150643 \tValidation Loss 0.01892663 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 5884 \tTraining Loss: 0.01136035 \tValidation Loss 0.01849774 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5885 \tTraining Loss: 0.01145483 \tValidation Loss 0.01891161 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5886 \tTraining Loss: 0.01144196 \tValidation Loss 0.01855312 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5887 \tTraining Loss: 0.01151920 \tValidation Loss 0.01826995 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 5888 \tTraining Loss: 0.01143784 \tValidation Loss 0.01870780 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 5889 \tTraining Loss: 0.01158597 \tValidation Loss 0.01881450 \tTraining Acuuarcy 42.665% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5890 \tTraining Loss: 0.01134689 \tValidation Loss 0.01872784 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 5891 \tTraining Loss: 0.01142522 \tValidation Loss 0.01823835 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5892 \tTraining Loss: 0.01139534 \tValidation Loss 0.01840237 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5893 \tTraining Loss: 0.01144042 \tValidation Loss 0.01879196 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 5894 \tTraining Loss: 0.01135154 \tValidation Loss 0.01853913 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5895 \tTraining Loss: 0.01138500 \tValidation Loss 0.01895211 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5896 \tTraining Loss: 0.01137232 \tValidation Loss 0.01870589 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 5897 \tTraining Loss: 0.01141691 \tValidation Loss 0.01872874 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5898 \tTraining Loss: 0.01155814 \tValidation Loss 0.01862502 \tTraining Acuuarcy 42.543% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 5899 \tTraining Loss: 0.01149455 \tValidation Loss 0.01828849 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 5900 \tTraining Loss: 0.01141036 \tValidation Loss 0.01893384 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5901 \tTraining Loss: 0.01147405 \tValidation Loss 0.01878921 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 5902 \tTraining Loss: 0.01146688 \tValidation Loss 0.01885138 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 5903 \tTraining Loss: 0.01141812 \tValidation Loss 0.01835955 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 5904 \tTraining Loss: 0.01141454 \tValidation Loss 0.01882854 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5905 \tTraining Loss: 0.01137708 \tValidation Loss 0.01906569 \tTraining Acuuarcy 44.109% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5906 \tTraining Loss: 0.01147979 \tValidation Loss 0.01837314 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5907 \tTraining Loss: 0.01135625 \tValidation Loss 0.01862594 \tTraining Acuuarcy 44.042% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5908 \tTraining Loss: 0.01141252 \tValidation Loss 0.01844471 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 5909 \tTraining Loss: 0.01148101 \tValidation Loss 0.01827776 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 5910 \tTraining Loss: 0.01136821 \tValidation Loss 0.01883540 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 19.811%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5911 \tTraining Loss: 0.01150960 \tValidation Loss 0.01822751 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5912 \tTraining Loss: 0.01142128 \tValidation Loss 0.01853346 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5913 \tTraining Loss: 0.01135991 \tValidation Loss 0.01859322 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 5914 \tTraining Loss: 0.01141102 \tValidation Loss 0.01948291 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5915 \tTraining Loss: 0.01134166 \tValidation Loss 0.01908598 \tTraining Acuuarcy 44.248% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5916 \tTraining Loss: 0.01142474 \tValidation Loss 0.01830657 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5917 \tTraining Loss: 0.01138832 \tValidation Loss 0.01858154 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 5918 \tTraining Loss: 0.01143212 \tValidation Loss 0.01866649 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 5919 \tTraining Loss: 0.01135008 \tValidation Loss 0.01862115 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5920 \tTraining Loss: 0.01140985 \tValidation Loss 0.01898041 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 5921 \tTraining Loss: 0.01135869 \tValidation Loss 0.01874851 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5922 \tTraining Loss: 0.01141956 \tValidation Loss 0.01872840 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 5923 \tTraining Loss: 0.01143295 \tValidation Loss 0.01900634 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 5924 \tTraining Loss: 0.01140166 \tValidation Loss 0.01896858 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5925 \tTraining Loss: 0.01140704 \tValidation Loss 0.01955589 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 5926 \tTraining Loss: 0.01147456 \tValidation Loss 0.01848248 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5927 \tTraining Loss: 0.01147355 \tValidation Loss 0.01835470 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 5928 \tTraining Loss: 0.01147363 \tValidation Loss 0.01809664 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 5929 \tTraining Loss: 0.01139513 \tValidation Loss 0.01868999 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 5930 \tTraining Loss: 0.01136332 \tValidation Loss 0.01847858 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 5931 \tTraining Loss: 0.01143696 \tValidation Loss 0.01865807 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 5932 \tTraining Loss: 0.01140578 \tValidation Loss 0.01861914 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5933 \tTraining Loss: 0.01139595 \tValidation Loss 0.01843436 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5934 \tTraining Loss: 0.01135675 \tValidation Loss 0.01838846 \tTraining Acuuarcy 44.098% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 5935 \tTraining Loss: 0.01151500 \tValidation Loss 0.01804150 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5936 \tTraining Loss: 0.01144660 \tValidation Loss 0.01865848 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5937 \tTraining Loss: 0.01139501 \tValidation Loss 0.01907957 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 5938 \tTraining Loss: 0.01149183 \tValidation Loss 0.01863599 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5939 \tTraining Loss: 0.01147849 \tValidation Loss 0.01867114 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 5940 \tTraining Loss: 0.01147570 \tValidation Loss 0.01869592 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 5941 \tTraining Loss: 0.01139597 \tValidation Loss 0.01877532 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5942 \tTraining Loss: 0.01139844 \tValidation Loss 0.01913159 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5943 \tTraining Loss: 0.01137576 \tValidation Loss 0.01852991 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5944 \tTraining Loss: 0.01136704 \tValidation Loss 0.01877836 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5945 \tTraining Loss: 0.01142391 \tValidation Loss 0.01949213 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 5946 \tTraining Loss: 0.01146766 \tValidation Loss 0.01862993 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5947 \tTraining Loss: 0.01142488 \tValidation Loss 0.01889511 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5948 \tTraining Loss: 0.01143941 \tValidation Loss 0.01858353 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5949 \tTraining Loss: 0.01145488 \tValidation Loss 0.01863334 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 5950 \tTraining Loss: 0.01140630 \tValidation Loss 0.01879825 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 5951 \tTraining Loss: 0.01145848 \tValidation Loss 0.01840296 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 5952 \tTraining Loss: 0.01163513 \tValidation Loss 0.01850674 \tTraining Acuuarcy 42.058% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 5953 \tTraining Loss: 0.01147208 \tValidation Loss 0.01855127 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5954 \tTraining Loss: 0.01147650 \tValidation Loss 0.01813629 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 5955 \tTraining Loss: 0.01141514 \tValidation Loss 0.01816703 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 5956 \tTraining Loss: 0.01140284 \tValidation Loss 0.01883811 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5957 \tTraining Loss: 0.01133538 \tValidation Loss 0.01902632 \tTraining Acuuarcy 44.092% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 5958 \tTraining Loss: 0.01148262 \tValidation Loss 0.01874012 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 5959 \tTraining Loss: 0.01154565 \tValidation Loss 0.01966421 \tTraining Acuuarcy 42.866% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5960 \tTraining Loss: 0.01132636 \tValidation Loss 0.01879303 \tTraining Acuuarcy 44.231% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 5961 \tTraining Loss: 0.01149666 \tValidation Loss 0.01810778 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5962 \tTraining Loss: 0.01135532 \tValidation Loss 0.01848941 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5963 \tTraining Loss: 0.01137470 \tValidation Loss 0.01833778 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 5964 \tTraining Loss: 0.01147153 \tValidation Loss 0.01881250 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5965 \tTraining Loss: 0.01137210 \tValidation Loss 0.01868015 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5966 \tTraining Loss: 0.01143525 \tValidation Loss 0.01862160 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 5967 \tTraining Loss: 0.01135545 \tValidation Loss 0.01878817 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 5968 \tTraining Loss: 0.01142591 \tValidation Loss 0.01898942 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 5969 \tTraining Loss: 0.01153392 \tValidation Loss 0.01845795 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 5970 \tTraining Loss: 0.01145046 \tValidation Loss 0.01850223 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 5971 \tTraining Loss: 0.01137809 \tValidation Loss 0.01864735 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 5972 \tTraining Loss: 0.01146664 \tValidation Loss 0.01871001 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5973 \tTraining Loss: 0.01134876 \tValidation Loss 0.01829894 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 5974 \tTraining Loss: 0.01133438 \tValidation Loss 0.01883884 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 5975 \tTraining Loss: 0.01138976 \tValidation Loss 0.01907608 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5976 \tTraining Loss: 0.01133742 \tValidation Loss 0.01894329 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 5977 \tTraining Loss: 0.01139483 \tValidation Loss 0.01871609 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 19.003%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5978 \tTraining Loss: 0.01137791 \tValidation Loss 0.01894264 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 5979 \tTraining Loss: 0.01147051 \tValidation Loss 0.01838449 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 5980 \tTraining Loss: 0.01141938 \tValidation Loss 0.01890455 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 5981 \tTraining Loss: 0.01139163 \tValidation Loss 0.01888002 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5982 \tTraining Loss: 0.01140717 \tValidation Loss 0.01854021 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 5983 \tTraining Loss: 0.01138590 \tValidation Loss 0.01868391 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 5984 \tTraining Loss: 0.01139048 \tValidation Loss 0.01917142 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5985 \tTraining Loss: 0.01138977 \tValidation Loss 0.01837463 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5986 \tTraining Loss: 0.01132835 \tValidation Loss 0.01851320 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 5987 \tTraining Loss: 0.01127905 \tValidation Loss 0.01889973 \tTraining Acuuarcy 44.265% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 5988 \tTraining Loss: 0.01144836 \tValidation Loss 0.01902097 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 5989 \tTraining Loss: 0.01138312 \tValidation Loss 0.01920561 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5990 \tTraining Loss: 0.01139831 \tValidation Loss 0.01837992 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 5991 \tTraining Loss: 0.01130706 \tValidation Loss 0.01854789 \tTraining Acuuarcy 44.259% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5992 \tTraining Loss: 0.01131882 \tValidation Loss 0.01930555 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 5993 \tTraining Loss: 0.01141158 \tValidation Loss 0.01917924 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 5994 \tTraining Loss: 0.01140066 \tValidation Loss 0.01939153 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 5995 \tTraining Loss: 0.01138548 \tValidation Loss 0.01935104 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 5996 \tTraining Loss: 0.01148145 \tValidation Loss 0.01853434 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 5997 \tTraining Loss: 0.01149644 \tValidation Loss 0.01868654 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 5998 \tTraining Loss: 0.01133133 \tValidation Loss 0.01902166 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 5999 \tTraining Loss: 0.01141858 \tValidation Loss 0.01868847 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6000 \tTraining Loss: 0.01142902 \tValidation Loss 0.01893211 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6001 \tTraining Loss: 0.01139294 \tValidation Loss 0.01870410 \tTraining Acuuarcy 44.098% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6002 \tTraining Loss: 0.01144796 \tValidation Loss 0.01884724 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6003 \tTraining Loss: 0.01137816 \tValidation Loss 0.01828037 \tTraining Acuuarcy 44.555% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 6004 \tTraining Loss: 0.01145778 \tValidation Loss 0.01869431 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6005 \tTraining Loss: 0.01141416 \tValidation Loss 0.01844282 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6006 \tTraining Loss: 0.01150770 \tValidation Loss 0.01859199 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 6007 \tTraining Loss: 0.01138363 \tValidation Loss 0.01842385 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6008 \tTraining Loss: 0.01139253 \tValidation Loss 0.01837388 \tTraining Acuuarcy 44.187% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6009 \tTraining Loss: 0.01148782 \tValidation Loss 0.01858986 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6010 \tTraining Loss: 0.01134932 \tValidation Loss 0.01901014 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 6011 \tTraining Loss: 0.01140984 \tValidation Loss 0.01890883 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 6012 \tTraining Loss: 0.01135750 \tValidation Loss 0.01847559 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 6013 \tTraining Loss: 0.01145057 \tValidation Loss 0.01900663 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 6014 \tTraining Loss: 0.01140492 \tValidation Loss 0.01853977 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6015 \tTraining Loss: 0.01140045 \tValidation Loss 0.01857447 \tTraining Acuuarcy 44.059% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6016 \tTraining Loss: 0.01135979 \tValidation Loss 0.01850873 \tTraining Acuuarcy 44.237% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6017 \tTraining Loss: 0.01142354 \tValidation Loss 0.01876213 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 6018 \tTraining Loss: 0.01140994 \tValidation Loss 0.01866918 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6019 \tTraining Loss: 0.01138392 \tValidation Loss 0.01865197 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6020 \tTraining Loss: 0.01148910 \tValidation Loss 0.01859835 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 6021 \tTraining Loss: 0.01141929 \tValidation Loss 0.01879258 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6022 \tTraining Loss: 0.01138410 \tValidation Loss 0.01859513 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6023 \tTraining Loss: 0.01147915 \tValidation Loss 0.01825280 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6024 \tTraining Loss: 0.01146328 \tValidation Loss 0.01832770 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6025 \tTraining Loss: 0.01150387 \tValidation Loss 0.01862568 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 6026 \tTraining Loss: 0.01156400 \tValidation Loss 0.01842860 \tTraining Acuuarcy 42.894% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 6027 \tTraining Loss: 0.01136267 \tValidation Loss 0.01892615 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6028 \tTraining Loss: 0.01142243 \tValidation Loss 0.01848604 \tTraining Acuuarcy 44.042% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 6029 \tTraining Loss: 0.01149294 \tValidation Loss 0.01850749 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6030 \tTraining Loss: 0.01130263 \tValidation Loss 0.01906817 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 6031 \tTraining Loss: 0.01139403 \tValidation Loss 0.01875158 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 6032 \tTraining Loss: 0.01157549 \tValidation Loss 0.01860153 \tTraining Acuuarcy 42.571% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6033 \tTraining Loss: 0.01138953 \tValidation Loss 0.01875527 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6034 \tTraining Loss: 0.01139899 \tValidation Loss 0.01828559 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6035 \tTraining Loss: 0.01147763 \tValidation Loss 0.01830639 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6036 \tTraining Loss: 0.01141984 \tValidation Loss 0.01833824 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 6037 \tTraining Loss: 0.01137060 \tValidation Loss 0.01856266 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6038 \tTraining Loss: 0.01134525 \tValidation Loss 0.01847524 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 6039 \tTraining Loss: 0.01141154 \tValidation Loss 0.01872303 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6040 \tTraining Loss: 0.01146352 \tValidation Loss 0.01829287 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6041 \tTraining Loss: 0.01144769 \tValidation Loss 0.01832206 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6042 \tTraining Loss: 0.01138592 \tValidation Loss 0.01867422 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6043 \tTraining Loss: 0.01136309 \tValidation Loss 0.01887930 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6044 \tTraining Loss: 0.01147593 \tValidation Loss 0.01796174 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 20.201%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6045 \tTraining Loss: 0.01152373 \tValidation Loss 0.01879913 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6046 \tTraining Loss: 0.01152302 \tValidation Loss 0.01863577 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 6047 \tTraining Loss: 0.01152095 \tValidation Loss 0.01852733 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6048 \tTraining Loss: 0.01133848 \tValidation Loss 0.01895277 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6049 \tTraining Loss: 0.01148808 \tValidation Loss 0.01881775 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6050 \tTraining Loss: 0.01144404 \tValidation Loss 0.01875866 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6051 \tTraining Loss: 0.01139592 \tValidation Loss 0.01905923 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6052 \tTraining Loss: 0.01147005 \tValidation Loss 0.01845965 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6053 \tTraining Loss: 0.01141441 \tValidation Loss 0.01922160 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 6054 \tTraining Loss: 0.01160199 \tValidation Loss 0.01844110 \tTraining Acuuarcy 42.743% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6055 \tTraining Loss: 0.01190130 \tValidation Loss 0.01859100 \tTraining Acuuarcy 40.926% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 6056 \tTraining Loss: 0.01140825 \tValidation Loss 0.01847877 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6057 \tTraining Loss: 0.01139482 \tValidation Loss 0.01844803 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6058 \tTraining Loss: 0.01144741 \tValidation Loss 0.01840705 \tTraining Acuuarcy 43.412% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6059 \tTraining Loss: 0.01142872 \tValidation Loss 0.01854828 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6060 \tTraining Loss: 0.01129988 \tValidation Loss 0.01855195 \tTraining Acuuarcy 44.365% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6061 \tTraining Loss: 0.01133527 \tValidation Loss 0.01852958 \tTraining Acuuarcy 44.415% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6062 \tTraining Loss: 0.01147451 \tValidation Loss 0.01905559 \tTraining Acuuarcy 43.083% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6063 \tTraining Loss: 0.01144680 \tValidation Loss 0.01898986 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 6064 \tTraining Loss: 0.01138975 \tValidation Loss 0.01824619 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 6065 \tTraining Loss: 0.01144256 \tValidation Loss 0.01861465 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6066 \tTraining Loss: 0.01129043 \tValidation Loss 0.01870400 \tTraining Acuuarcy 44.744% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6067 \tTraining Loss: 0.01138515 \tValidation Loss 0.01863743 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6068 \tTraining Loss: 0.01132362 \tValidation Loss 0.01878959 \tTraining Acuuarcy 44.326% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6069 \tTraining Loss: 0.01146609 \tValidation Loss 0.01894381 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6070 \tTraining Loss: 0.01145403 \tValidation Loss 0.01850280 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6071 \tTraining Loss: 0.01147859 \tValidation Loss 0.01851614 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6072 \tTraining Loss: 0.01146945 \tValidation Loss 0.01892805 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6073 \tTraining Loss: 0.01144067 \tValidation Loss 0.01887433 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 6074 \tTraining Loss: 0.01155530 \tValidation Loss 0.01872193 \tTraining Acuuarcy 42.643% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6075 \tTraining Loss: 0.01140235 \tValidation Loss 0.01862957 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6076 \tTraining Loss: 0.01143465 \tValidation Loss 0.01877128 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 6077 \tTraining Loss: 0.01145959 \tValidation Loss 0.01848943 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6078 \tTraining Loss: 0.01139486 \tValidation Loss 0.01806913 \tTraining Acuuarcy 43.958% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6079 \tTraining Loss: 0.01146049 \tValidation Loss 0.01914428 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 6080 \tTraining Loss: 0.01132183 \tValidation Loss 0.01851956 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 6081 \tTraining Loss: 0.01138286 \tValidation Loss 0.01906860 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 6082 \tTraining Loss: 0.01143521 \tValidation Loss 0.01822331 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 6083 \tTraining Loss: 0.01142925 \tValidation Loss 0.01862722 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 6084 \tTraining Loss: 0.01139649 \tValidation Loss 0.01849851 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6085 \tTraining Loss: 0.01131460 \tValidation Loss 0.01883266 \tTraining Acuuarcy 44.594% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6086 \tTraining Loss: 0.01156821 \tValidation Loss 0.01803606 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 6087 \tTraining Loss: 0.01131564 \tValidation Loss 0.01823456 \tTraining Acuuarcy 44.148% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 6088 \tTraining Loss: 0.01132101 \tValidation Loss 0.01845127 \tTraining Acuuarcy 43.986% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6089 \tTraining Loss: 0.01136142 \tValidation Loss 0.01860947 \tTraining Acuuarcy 44.036% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6090 \tTraining Loss: 0.01144494 \tValidation Loss 0.01827953 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 6091 \tTraining Loss: 0.01129319 \tValidation Loss 0.01883741 \tTraining Acuuarcy 44.672% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 6092 \tTraining Loss: 0.01148216 \tValidation Loss 0.01853742 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6093 \tTraining Loss: 0.01147097 \tValidation Loss 0.01864069 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6094 \tTraining Loss: 0.01144124 \tValidation Loss 0.01847492 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6095 \tTraining Loss: 0.01144343 \tValidation Loss 0.01846431 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6096 \tTraining Loss: 0.01148932 \tValidation Loss 0.01857717 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6097 \tTraining Loss: 0.01146393 \tValidation Loss 0.01845316 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6098 \tTraining Loss: 0.01136468 \tValidation Loss 0.01880845 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 6099 \tTraining Loss: 0.01135081 \tValidation Loss 0.01859083 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6100 \tTraining Loss: 0.01147696 \tValidation Loss 0.01850831 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6101 \tTraining Loss: 0.01144991 \tValidation Loss 0.01859744 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6102 \tTraining Loss: 0.01138805 \tValidation Loss 0.01852658 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6103 \tTraining Loss: 0.01145433 \tValidation Loss 0.01855234 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6104 \tTraining Loss: 0.01140221 \tValidation Loss 0.01852731 \tTraining Acuuarcy 44.137% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6105 \tTraining Loss: 0.01143796 \tValidation Loss 0.01852321 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6106 \tTraining Loss: 0.01143038 \tValidation Loss 0.01885479 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6107 \tTraining Loss: 0.01139181 \tValidation Loss 0.01838427 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6108 \tTraining Loss: 0.01144754 \tValidation Loss 0.01842376 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6109 \tTraining Loss: 0.01138388 \tValidation Loss 0.01855375 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 6110 \tTraining Loss: 0.01136190 \tValidation Loss 0.01890427 \tTraining Acuuarcy 43.908% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 6111 \tTraining Loss: 0.01144135 \tValidation Loss 0.01860003 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 18.947%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6112 \tTraining Loss: 0.01147881 \tValidation Loss 0.01849879 \tTraining Acuuarcy 43.067% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 6113 \tTraining Loss: 0.01152168 \tValidation Loss 0.01857154 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6114 \tTraining Loss: 0.01125780 \tValidation Loss 0.01872418 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6115 \tTraining Loss: 0.01138856 \tValidation Loss 0.01868784 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6116 \tTraining Loss: 0.01141040 \tValidation Loss 0.01850933 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6117 \tTraining Loss: 0.01136265 \tValidation Loss 0.01830031 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 6118 \tTraining Loss: 0.01135541 \tValidation Loss 0.01915719 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6119 \tTraining Loss: 0.01141295 \tValidation Loss 0.01844449 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6120 \tTraining Loss: 0.01139015 \tValidation Loss 0.01867658 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6121 \tTraining Loss: 0.01140701 \tValidation Loss 0.01893747 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6122 \tTraining Loss: 0.01146145 \tValidation Loss 0.01872933 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6123 \tTraining Loss: 0.01151389 \tValidation Loss 0.01888960 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6124 \tTraining Loss: 0.01146558 \tValidation Loss 0.01836295 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6125 \tTraining Loss: 0.01143904 \tValidation Loss 0.01842086 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6126 \tTraining Loss: 0.01138989 \tValidation Loss 0.01859987 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6127 \tTraining Loss: 0.01146093 \tValidation Loss 0.01879518 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6128 \tTraining Loss: 0.01145357 \tValidation Loss 0.01866081 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6129 \tTraining Loss: 0.01150983 \tValidation Loss 0.01829226 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6130 \tTraining Loss: 0.01146316 \tValidation Loss 0.01898729 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 6131 \tTraining Loss: 0.01137174 \tValidation Loss 0.01851249 \tTraining Acuuarcy 43.992% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 6132 \tTraining Loss: 0.01140999 \tValidation Loss 0.01863434 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6133 \tTraining Loss: 0.01166680 \tValidation Loss 0.01829060 \tTraining Acuuarcy 42.097% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6134 \tTraining Loss: 0.01136046 \tValidation Loss 0.01862973 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 6135 \tTraining Loss: 0.01137222 \tValidation Loss 0.01900530 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 6136 \tTraining Loss: 0.01144498 \tValidation Loss 0.01874483 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 6137 \tTraining Loss: 0.01137746 \tValidation Loss 0.01904096 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6138 \tTraining Loss: 0.01138096 \tValidation Loss 0.01874415 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 6139 \tTraining Loss: 0.01141374 \tValidation Loss 0.01883010 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6140 \tTraining Loss: 0.01146404 \tValidation Loss 0.01840307 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6141 \tTraining Loss: 0.01140056 \tValidation Loss 0.01926670 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6142 \tTraining Loss: 0.01135313 \tValidation Loss 0.01875819 \tTraining Acuuarcy 44.248% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6143 \tTraining Loss: 0.01147124 \tValidation Loss 0.01873402 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 6144 \tTraining Loss: 0.01133124 \tValidation Loss 0.01902235 \tTraining Acuuarcy 44.192% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6145 \tTraining Loss: 0.01152348 \tValidation Loss 0.01861645 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6146 \tTraining Loss: 0.01136301 \tValidation Loss 0.01885229 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6147 \tTraining Loss: 0.01141679 \tValidation Loss 0.01861778 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6148 \tTraining Loss: 0.01145800 \tValidation Loss 0.01859914 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6149 \tTraining Loss: 0.01140556 \tValidation Loss 0.01857704 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6150 \tTraining Loss: 0.01147481 \tValidation Loss 0.01866181 \tTraining Acuuarcy 42.877% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6151 \tTraining Loss: 0.01145460 \tValidation Loss 0.01900757 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6152 \tTraining Loss: 0.01149172 \tValidation Loss 0.01897458 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6153 \tTraining Loss: 0.01133657 \tValidation Loss 0.01886017 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6154 \tTraining Loss: 0.01133709 \tValidation Loss 0.01892170 \tTraining Acuuarcy 44.415% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6155 \tTraining Loss: 0.01139860 \tValidation Loss 0.01852118 \tTraining Acuuarcy 44.176% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6156 \tTraining Loss: 0.01147321 \tValidation Loss 0.01830012 \tTraining Acuuarcy 43.033% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6157 \tTraining Loss: 0.01134649 \tValidation Loss 0.01900862 \tTraining Acuuarcy 43.986% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6158 \tTraining Loss: 0.01154771 \tValidation Loss 0.01844686 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6159 \tTraining Loss: 0.01137805 \tValidation Loss 0.01834938 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6160 \tTraining Loss: 0.01134980 \tValidation Loss 0.01890425 \tTraining Acuuarcy 44.070% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6161 \tTraining Loss: 0.01139631 \tValidation Loss 0.01850749 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6162 \tTraining Loss: 0.01137517 \tValidation Loss 0.01864038 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6163 \tTraining Loss: 0.01141621 \tValidation Loss 0.01831517 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6164 \tTraining Loss: 0.01137428 \tValidation Loss 0.01903569 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 6165 \tTraining Loss: 0.01137182 \tValidation Loss 0.01942190 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6166 \tTraining Loss: 0.01139288 \tValidation Loss 0.01878408 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6167 \tTraining Loss: 0.01147672 \tValidation Loss 0.01831120 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6168 \tTraining Loss: 0.01138822 \tValidation Loss 0.01868293 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 6169 \tTraining Loss: 0.01148375 \tValidation Loss 0.01890153 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6170 \tTraining Loss: 0.01135163 \tValidation Loss 0.01862748 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6171 \tTraining Loss: 0.01148612 \tValidation Loss 0.01858633 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6172 \tTraining Loss: 0.01144618 \tValidation Loss 0.01867782 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6173 \tTraining Loss: 0.01139915 \tValidation Loss 0.01844343 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6174 \tTraining Loss: 0.01145407 \tValidation Loss 0.01863360 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6175 \tTraining Loss: 0.01139772 \tValidation Loss 0.01875855 \tTraining Acuuarcy 43.958% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6176 \tTraining Loss: 0.01138903 \tValidation Loss 0.01881055 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6177 \tTraining Loss: 0.01138205 \tValidation Loss 0.01889374 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6178 \tTraining Loss: 0.01143234 \tValidation Loss 0.01847603 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 18.835%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6179 \tTraining Loss: 0.01134658 \tValidation Loss 0.01841801 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6180 \tTraining Loss: 0.01136467 \tValidation Loss 0.01904057 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6181 \tTraining Loss: 0.01139149 \tValidation Loss 0.01864799 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6182 \tTraining Loss: 0.01150246 \tValidation Loss 0.01887686 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 6183 \tTraining Loss: 0.01136363 \tValidation Loss 0.01881032 \tTraining Acuuarcy 44.399% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6184 \tTraining Loss: 0.01143199 \tValidation Loss 0.01851020 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6185 \tTraining Loss: 0.01146536 \tValidation Loss 0.01834835 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6186 \tTraining Loss: 0.01127107 \tValidation Loss 0.01867337 \tTraining Acuuarcy 44.432% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6187 \tTraining Loss: 0.01141184 \tValidation Loss 0.01838795 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6188 \tTraining Loss: 0.01135770 \tValidation Loss 0.01852167 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 6189 \tTraining Loss: 0.01138631 \tValidation Loss 0.01863855 \tTraining Acuuarcy 44.215% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6190 \tTraining Loss: 0.01135245 \tValidation Loss 0.01898666 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6191 \tTraining Loss: 0.01140585 \tValidation Loss 0.01896681 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6192 \tTraining Loss: 0.01140953 \tValidation Loss 0.01873023 \tTraining Acuuarcy 43.992% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6193 \tTraining Loss: 0.01139054 \tValidation Loss 0.01870876 \tTraining Acuuarcy 44.003% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6194 \tTraining Loss: 0.01139303 \tValidation Loss 0.01849150 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6195 \tTraining Loss: 0.01140851 \tValidation Loss 0.01817457 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6196 \tTraining Loss: 0.01143952 \tValidation Loss 0.01876486 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6197 \tTraining Loss: 0.01137064 \tValidation Loss 0.01874897 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6198 \tTraining Loss: 0.01148444 \tValidation Loss 0.01833297 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 6199 \tTraining Loss: 0.01147579 \tValidation Loss 0.01843087 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6200 \tTraining Loss: 0.01153291 \tValidation Loss 0.01848525 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 6201 \tTraining Loss: 0.01137305 \tValidation Loss 0.01902824 \tTraining Acuuarcy 44.081% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6202 \tTraining Loss: 0.01143306 \tValidation Loss 0.01852510 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 6203 \tTraining Loss: 0.01139506 \tValidation Loss 0.01849652 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6204 \tTraining Loss: 0.01138785 \tValidation Loss 0.01882522 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6205 \tTraining Loss: 0.01144923 \tValidation Loss 0.01889882 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 6206 \tTraining Loss: 0.01134137 \tValidation Loss 0.01875745 \tTraining Acuuarcy 44.098% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 6207 \tTraining Loss: 0.01142669 \tValidation Loss 0.01885484 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 6208 \tTraining Loss: 0.01133839 \tValidation Loss 0.01883844 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6209 \tTraining Loss: 0.01144673 \tValidation Loss 0.01880166 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 6210 \tTraining Loss: 0.01136771 \tValidation Loss 0.01879449 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6211 \tTraining Loss: 0.01134209 \tValidation Loss 0.01828140 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6212 \tTraining Loss: 0.01133443 \tValidation Loss 0.01883283 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 6213 \tTraining Loss: 0.01149586 \tValidation Loss 0.01874565 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6214 \tTraining Loss: 0.01133462 \tValidation Loss 0.01918352 \tTraining Acuuarcy 44.237% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 6215 \tTraining Loss: 0.01146120 \tValidation Loss 0.01850153 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6216 \tTraining Loss: 0.01141435 \tValidation Loss 0.01868426 \tTraining Acuuarcy 43.958% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6217 \tTraining Loss: 0.01145865 \tValidation Loss 0.01849193 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 6218 \tTraining Loss: 0.01134444 \tValidation Loss 0.01876669 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6219 \tTraining Loss: 0.01142759 \tValidation Loss 0.01847524 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6220 \tTraining Loss: 0.01139798 \tValidation Loss 0.01821692 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 6221 \tTraining Loss: 0.01152795 \tValidation Loss 0.01889443 \tTraining Acuuarcy 42.966% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6222 \tTraining Loss: 0.01133855 \tValidation Loss 0.01892503 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6223 \tTraining Loss: 0.01133883 \tValidation Loss 0.01916403 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6224 \tTraining Loss: 0.01145540 \tValidation Loss 0.01830113 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6225 \tTraining Loss: 0.01134677 \tValidation Loss 0.01874298 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6226 \tTraining Loss: 0.01150677 \tValidation Loss 0.01893048 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 6227 \tTraining Loss: 0.01133355 \tValidation Loss 0.01840885 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6228 \tTraining Loss: 0.01141159 \tValidation Loss 0.01843497 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6229 \tTraining Loss: 0.01142929 \tValidation Loss 0.01898147 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 6230 \tTraining Loss: 0.01135461 \tValidation Loss 0.01862435 \tTraining Acuuarcy 44.053% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6231 \tTraining Loss: 0.01140057 \tValidation Loss 0.01879002 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6232 \tTraining Loss: 0.01142872 \tValidation Loss 0.01834772 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6233 \tTraining Loss: 0.01143593 \tValidation Loss 0.01886651 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6234 \tTraining Loss: 0.01132434 \tValidation Loss 0.01845758 \tTraining Acuuarcy 44.415% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6235 \tTraining Loss: 0.01144959 \tValidation Loss 0.01835705 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6236 \tTraining Loss: 0.01134316 \tValidation Loss 0.01870897 \tTraining Acuuarcy 44.337% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 6237 \tTraining Loss: 0.01147210 \tValidation Loss 0.01857091 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 6238 \tTraining Loss: 0.01141918 \tValidation Loss 0.01884847 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6239 \tTraining Loss: 0.01137483 \tValidation Loss 0.01893816 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6240 \tTraining Loss: 0.01143011 \tValidation Loss 0.01857013 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6241 \tTraining Loss: 0.01137316 \tValidation Loss 0.01865243 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6242 \tTraining Loss: 0.01149627 \tValidation Loss 0.01870875 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6243 \tTraining Loss: 0.01143697 \tValidation Loss 0.01842351 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6244 \tTraining Loss: 0.01137501 \tValidation Loss 0.01855895 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6245 \tTraining Loss: 0.01135436 \tValidation Loss 0.01897392 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 19.476%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6246 \tTraining Loss: 0.01137650 \tValidation Loss 0.01845373 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6247 \tTraining Loss: 0.01138814 \tValidation Loss 0.01862717 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6248 \tTraining Loss: 0.01136872 \tValidation Loss 0.01816991 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6249 \tTraining Loss: 0.01145439 \tValidation Loss 0.01836161 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 6250 \tTraining Loss: 0.01154335 \tValidation Loss 0.01872150 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 6251 \tTraining Loss: 0.01139160 \tValidation Loss 0.01875048 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6252 \tTraining Loss: 0.01138260 \tValidation Loss 0.01902534 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6253 \tTraining Loss: 0.01139956 \tValidation Loss 0.01903389 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6254 \tTraining Loss: 0.01138360 \tValidation Loss 0.01869928 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6255 \tTraining Loss: 0.01132964 \tValidation Loss 0.01920622 \tTraining Acuuarcy 44.198% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 6256 \tTraining Loss: 0.01131799 \tValidation Loss 0.01928197 \tTraining Acuuarcy 44.438% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6257 \tTraining Loss: 0.01139779 \tValidation Loss 0.01899021 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6258 \tTraining Loss: 0.01135052 \tValidation Loss 0.01864902 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6259 \tTraining Loss: 0.01162360 \tValidation Loss 0.01836041 \tTraining Acuuarcy 42.509% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6260 \tTraining Loss: 0.01146658 \tValidation Loss 0.01823198 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6261 \tTraining Loss: 0.01134130 \tValidation Loss 0.01870797 \tTraining Acuuarcy 44.410% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6262 \tTraining Loss: 0.01136614 \tValidation Loss 0.01935843 \tTraining Acuuarcy 44.042% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6263 \tTraining Loss: 0.01137671 \tValidation Loss 0.01916678 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 6264 \tTraining Loss: 0.01148776 \tValidation Loss 0.01880179 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 6265 \tTraining Loss: 0.01152429 \tValidation Loss 0.01821075 \tTraining Acuuarcy 42.782% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6266 \tTraining Loss: 0.01145673 \tValidation Loss 0.01839884 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6267 \tTraining Loss: 0.01136961 \tValidation Loss 0.01904497 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 6268 \tTraining Loss: 0.01137764 \tValidation Loss 0.01918282 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 6269 \tTraining Loss: 0.01138944 \tValidation Loss 0.01834652 \tTraining Acuuarcy 44.443% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 6270 \tTraining Loss: 0.01162379 \tValidation Loss 0.01861981 \tTraining Acuuarcy 42.359% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6271 \tTraining Loss: 0.01137759 \tValidation Loss 0.01882337 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6272 \tTraining Loss: 0.01138888 \tValidation Loss 0.01811206 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6273 \tTraining Loss: 0.01140978 \tValidation Loss 0.01867214 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6274 \tTraining Loss: 0.01149231 \tValidation Loss 0.01863659 \tTraining Acuuarcy 43.050% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6275 \tTraining Loss: 0.01141327 \tValidation Loss 0.01868460 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6276 \tTraining Loss: 0.01140870 \tValidation Loss 0.01884289 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6277 \tTraining Loss: 0.01140651 \tValidation Loss 0.01847754 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 6278 \tTraining Loss: 0.01142386 \tValidation Loss 0.01860374 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6279 \tTraining Loss: 0.01149211 \tValidation Loss 0.01877707 \tTraining Acuuarcy 43.000% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6280 \tTraining Loss: 0.01139974 \tValidation Loss 0.01890734 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6281 \tTraining Loss: 0.01145889 \tValidation Loss 0.01861893 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6282 \tTraining Loss: 0.01146652 \tValidation Loss 0.01833713 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6283 \tTraining Loss: 0.01135998 \tValidation Loss 0.01851547 \tTraining Acuuarcy 44.326% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6284 \tTraining Loss: 0.01132352 \tValidation Loss 0.01854627 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6285 \tTraining Loss: 0.01142771 \tValidation Loss 0.01894919 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6286 \tTraining Loss: 0.01147022 \tValidation Loss 0.01901686 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 6287 \tTraining Loss: 0.01138133 \tValidation Loss 0.01863687 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 6288 \tTraining Loss: 0.01144092 \tValidation Loss 0.01854029 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6289 \tTraining Loss: 0.01137556 \tValidation Loss 0.01878711 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6290 \tTraining Loss: 0.01137984 \tValidation Loss 0.01885058 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 6291 \tTraining Loss: 0.01154049 \tValidation Loss 0.01846702 \tTraining Acuuarcy 42.704% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 6292 \tTraining Loss: 0.01135837 \tValidation Loss 0.01873810 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6293 \tTraining Loss: 0.01137228 \tValidation Loss 0.01825203 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 6294 \tTraining Loss: 0.01150123 \tValidation Loss 0.01817468 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6295 \tTraining Loss: 0.01146339 \tValidation Loss 0.01854424 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6296 \tTraining Loss: 0.01137013 \tValidation Loss 0.01918886 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6297 \tTraining Loss: 0.01143195 \tValidation Loss 0.01844710 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6298 \tTraining Loss: 0.01146361 \tValidation Loss 0.01854388 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6299 \tTraining Loss: 0.01142980 \tValidation Loss 0.01851918 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6300 \tTraining Loss: 0.01138473 \tValidation Loss 0.01836062 \tTraining Acuuarcy 43.958% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6301 \tTraining Loss: 0.01145085 \tValidation Loss 0.01874482 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6302 \tTraining Loss: 0.01136838 \tValidation Loss 0.01829867 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 6303 \tTraining Loss: 0.01132597 \tValidation Loss 0.01858861 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6304 \tTraining Loss: 0.01139717 \tValidation Loss 0.01849867 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6305 \tTraining Loss: 0.01148748 \tValidation Loss 0.01884706 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6306 \tTraining Loss: 0.01137538 \tValidation Loss 0.01869190 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6307 \tTraining Loss: 0.01136094 \tValidation Loss 0.01882531 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6308 \tTraining Loss: 0.01141720 \tValidation Loss 0.01881320 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6309 \tTraining Loss: 0.01145373 \tValidation Loss 0.01899242 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6310 \tTraining Loss: 0.01150014 \tValidation Loss 0.01892152 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 6311 \tTraining Loss: 0.01145720 \tValidation Loss 0.01871958 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6312 \tTraining Loss: 0.01140512 \tValidation Loss 0.01874558 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 18.975%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6313 \tTraining Loss: 0.01139012 \tValidation Loss 0.01894225 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6314 \tTraining Loss: 0.01148014 \tValidation Loss 0.01899982 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6315 \tTraining Loss: 0.01136388 \tValidation Loss 0.01876055 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6316 \tTraining Loss: 0.01129345 \tValidation Loss 0.01914003 \tTraining Acuuarcy 44.393% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 6317 \tTraining Loss: 0.01135163 \tValidation Loss 0.01921485 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6318 \tTraining Loss: 0.01134537 \tValidation Loss 0.01844076 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 6319 \tTraining Loss: 0.01149158 \tValidation Loss 0.01850690 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6320 \tTraining Loss: 0.01134330 \tValidation Loss 0.01883882 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6321 \tTraining Loss: 0.01143453 \tValidation Loss 0.01835304 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6322 \tTraining Loss: 0.01143291 \tValidation Loss 0.01843615 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6323 \tTraining Loss: 0.01148222 \tValidation Loss 0.01853670 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6324 \tTraining Loss: 0.01143174 \tValidation Loss 0.01863623 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6325 \tTraining Loss: 0.01136553 \tValidation Loss 0.01819465 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 6326 \tTraining Loss: 0.01135191 \tValidation Loss 0.01849306 \tTraining Acuuarcy 44.059% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6327 \tTraining Loss: 0.01133884 \tValidation Loss 0.01882144 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 6328 \tTraining Loss: 0.01132728 \tValidation Loss 0.01875275 \tTraining Acuuarcy 44.399% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6329 \tTraining Loss: 0.01138320 \tValidation Loss 0.01877973 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 6330 \tTraining Loss: 0.01134906 \tValidation Loss 0.01902071 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 6331 \tTraining Loss: 0.01144718 \tValidation Loss 0.01927801 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 6332 \tTraining Loss: 0.01142184 \tValidation Loss 0.01845068 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 6333 \tTraining Loss: 0.01141152 \tValidation Loss 0.01858042 \tTraining Acuuarcy 44.148% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6334 \tTraining Loss: 0.01143444 \tValidation Loss 0.01912888 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6335 \tTraining Loss: 0.01139520 \tValidation Loss 0.01852579 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6336 \tTraining Loss: 0.01145027 \tValidation Loss 0.01857606 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6337 \tTraining Loss: 0.01133144 \tValidation Loss 0.01843780 \tTraining Acuuarcy 44.560% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6338 \tTraining Loss: 0.01149369 \tValidation Loss 0.01899755 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6339 \tTraining Loss: 0.01140779 \tValidation Loss 0.01890474 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6340 \tTraining Loss: 0.01137402 \tValidation Loss 0.01838706 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 6341 \tTraining Loss: 0.01139650 \tValidation Loss 0.01914781 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6342 \tTraining Loss: 0.01132556 \tValidation Loss 0.01909069 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6343 \tTraining Loss: 0.01150112 \tValidation Loss 0.01878654 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6344 \tTraining Loss: 0.01133646 \tValidation Loss 0.01885072 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6345 \tTraining Loss: 0.01136321 \tValidation Loss 0.01866412 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6346 \tTraining Loss: 0.01133842 \tValidation Loss 0.01862985 \tTraining Acuuarcy 44.354% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6347 \tTraining Loss: 0.01140571 \tValidation Loss 0.01859141 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6348 \tTraining Loss: 0.01138330 \tValidation Loss 0.01826418 \tTraining Acuuarcy 44.170% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6349 \tTraining Loss: 0.01142352 \tValidation Loss 0.01910768 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 6350 \tTraining Loss: 0.01135829 \tValidation Loss 0.01894572 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 6351 \tTraining Loss: 0.01140660 \tValidation Loss 0.01827882 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 6352 \tTraining Loss: 0.01149131 \tValidation Loss 0.01853876 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 6353 \tTraining Loss: 0.01136171 \tValidation Loss 0.01840388 \tTraining Acuuarcy 44.148% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6354 \tTraining Loss: 0.01143245 \tValidation Loss 0.01869239 \tTraining Acuuarcy 43.958% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6355 \tTraining Loss: 0.01143172 \tValidation Loss 0.01896324 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6356 \tTraining Loss: 0.01140928 \tValidation Loss 0.01856283 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6357 \tTraining Loss: 0.01133047 \tValidation Loss 0.01898795 \tTraining Acuuarcy 44.248% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6358 \tTraining Loss: 0.01134165 \tValidation Loss 0.01867032 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6359 \tTraining Loss: 0.01152519 \tValidation Loss 0.01885874 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6360 \tTraining Loss: 0.01144963 \tValidation Loss 0.01855538 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6361 \tTraining Loss: 0.01149250 \tValidation Loss 0.01884526 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6362 \tTraining Loss: 0.01142492 \tValidation Loss 0.01856354 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 6363 \tTraining Loss: 0.01133947 \tValidation Loss 0.01887775 \tTraining Acuuarcy 44.165% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6364 \tTraining Loss: 0.01139225 \tValidation Loss 0.01858302 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6365 \tTraining Loss: 0.01139103 \tValidation Loss 0.01872083 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6366 \tTraining Loss: 0.01142536 \tValidation Loss 0.01859607 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6367 \tTraining Loss: 0.01141283 \tValidation Loss 0.01853883 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 6368 \tTraining Loss: 0.01140980 \tValidation Loss 0.01804887 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 6369 \tTraining Loss: 0.01150860 \tValidation Loss 0.01924516 \tTraining Acuuarcy 42.766% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6370 \tTraining Loss: 0.01141739 \tValidation Loss 0.01837525 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6371 \tTraining Loss: 0.01137716 \tValidation Loss 0.01847374 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6372 \tTraining Loss: 0.01149382 \tValidation Loss 0.01871201 \tTraining Acuuarcy 42.565% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6373 \tTraining Loss: 0.01139199 \tValidation Loss 0.01875407 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6374 \tTraining Loss: 0.01138464 \tValidation Loss 0.01897334 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6375 \tTraining Loss: 0.01140574 \tValidation Loss 0.01876652 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 6376 \tTraining Loss: 0.01146812 \tValidation Loss 0.01902903 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6377 \tTraining Loss: 0.01143774 \tValidation Loss 0.01895712 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6378 \tTraining Loss: 0.01139220 \tValidation Loss 0.01893966 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6379 \tTraining Loss: 0.01133716 \tValidation Loss 0.01911490 \tTraining Acuuarcy 44.465% \tValidation Acuuarcy 18.696%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6380 \tTraining Loss: 0.01145991 \tValidation Loss 0.01851630 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6381 \tTraining Loss: 0.01147647 \tValidation Loss 0.01890179 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6382 \tTraining Loss: 0.01145025 \tValidation Loss 0.01827335 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 6383 \tTraining Loss: 0.01137089 \tValidation Loss 0.01864834 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 6384 \tTraining Loss: 0.01142799 \tValidation Loss 0.01840526 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6385 \tTraining Loss: 0.01146742 \tValidation Loss 0.01893259 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6386 \tTraining Loss: 0.01142304 \tValidation Loss 0.01819089 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6387 \tTraining Loss: 0.01135383 \tValidation Loss 0.01863107 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6388 \tTraining Loss: 0.01150009 \tValidation Loss 0.01865906 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 6389 \tTraining Loss: 0.01151735 \tValidation Loss 0.01841208 \tTraining Acuuarcy 42.749% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6390 \tTraining Loss: 0.01133253 \tValidation Loss 0.01894038 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6391 \tTraining Loss: 0.01147669 \tValidation Loss 0.01878976 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6392 \tTraining Loss: 0.01142664 \tValidation Loss 0.01877252 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 17.581%\n",
      "Epoch: 6393 \tTraining Loss: 0.01140241 \tValidation Loss 0.01847667 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6394 \tTraining Loss: 0.01143266 \tValidation Loss 0.01871077 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6395 \tTraining Loss: 0.01143841 \tValidation Loss 0.01851734 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6396 \tTraining Loss: 0.01139334 \tValidation Loss 0.01864195 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6397 \tTraining Loss: 0.01132255 \tValidation Loss 0.01890720 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6398 \tTraining Loss: 0.01137123 \tValidation Loss 0.01854960 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6399 \tTraining Loss: 0.01136008 \tValidation Loss 0.01856784 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 6400 \tTraining Loss: 0.01144993 \tValidation Loss 0.01870540 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 6401 \tTraining Loss: 0.01136445 \tValidation Loss 0.01856504 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 6402 \tTraining Loss: 0.01133979 \tValidation Loss 0.01851651 \tTraining Acuuarcy 44.220% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 6403 \tTraining Loss: 0.01153205 \tValidation Loss 0.01840181 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6404 \tTraining Loss: 0.01146111 \tValidation Loss 0.01863025 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6405 \tTraining Loss: 0.01142565 \tValidation Loss 0.01882269 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6406 \tTraining Loss: 0.01134345 \tValidation Loss 0.01870417 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6407 \tTraining Loss: 0.01147442 \tValidation Loss 0.01859563 \tTraining Acuuarcy 43.150% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6408 \tTraining Loss: 0.01133201 \tValidation Loss 0.01872724 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6409 \tTraining Loss: 0.01142429 \tValidation Loss 0.01867641 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6410 \tTraining Loss: 0.01149648 \tValidation Loss 0.01858635 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6411 \tTraining Loss: 0.01128848 \tValidation Loss 0.01889586 \tTraining Acuuarcy 44.354% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6412 \tTraining Loss: 0.01140916 \tValidation Loss 0.01889238 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6413 \tTraining Loss: 0.01135346 \tValidation Loss 0.01889517 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6414 \tTraining Loss: 0.01133083 \tValidation Loss 0.01869232 \tTraining Acuuarcy 44.142% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 6415 \tTraining Loss: 0.01138477 \tValidation Loss 0.01886333 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6416 \tTraining Loss: 0.01142375 \tValidation Loss 0.01813745 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6417 \tTraining Loss: 0.01149931 \tValidation Loss 0.01833349 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6418 \tTraining Loss: 0.01150124 \tValidation Loss 0.01857299 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6419 \tTraining Loss: 0.01137042 \tValidation Loss 0.01891897 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6420 \tTraining Loss: 0.01138949 \tValidation Loss 0.01886357 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 6421 \tTraining Loss: 0.01138866 \tValidation Loss 0.01846919 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6422 \tTraining Loss: 0.01144893 \tValidation Loss 0.01796955 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 6423 \tTraining Loss: 0.01143417 \tValidation Loss 0.01867487 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6424 \tTraining Loss: 0.01129186 \tValidation Loss 0.01914151 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 6425 \tTraining Loss: 0.01142530 \tValidation Loss 0.01822999 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6426 \tTraining Loss: 0.01140482 \tValidation Loss 0.01876721 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6427 \tTraining Loss: 0.01131140 \tValidation Loss 0.01913694 \tTraining Acuuarcy 44.516% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6428 \tTraining Loss: 0.01136449 \tValidation Loss 0.01869565 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6429 \tTraining Loss: 0.01142407 \tValidation Loss 0.01850139 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6430 \tTraining Loss: 0.01136828 \tValidation Loss 0.01837738 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6431 \tTraining Loss: 0.01139091 \tValidation Loss 0.01928714 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6432 \tTraining Loss: 0.01128909 \tValidation Loss 0.01867829 \tTraining Acuuarcy 44.438% \tValidation Acuuarcy 17.777%\n",
      "Epoch: 6433 \tTraining Loss: 0.01149078 \tValidation Loss 0.01820436 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 6434 \tTraining Loss: 0.01138981 \tValidation Loss 0.01859241 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6435 \tTraining Loss: 0.01134262 \tValidation Loss 0.01908065 \tTraining Acuuarcy 44.170% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 6436 \tTraining Loss: 0.01137370 \tValidation Loss 0.01872069 \tTraining Acuuarcy 44.109% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6437 \tTraining Loss: 0.01137292 \tValidation Loss 0.01855846 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6438 \tTraining Loss: 0.01138243 \tValidation Loss 0.01912209 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6439 \tTraining Loss: 0.01137680 \tValidation Loss 0.01916237 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6440 \tTraining Loss: 0.01136192 \tValidation Loss 0.01944221 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6441 \tTraining Loss: 0.01137409 \tValidation Loss 0.01877711 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 6442 \tTraining Loss: 0.01146938 \tValidation Loss 0.01889875 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 6443 \tTraining Loss: 0.01144629 \tValidation Loss 0.01813063 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6444 \tTraining Loss: 0.01146515 \tValidation Loss 0.01821425 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6445 \tTraining Loss: 0.01152157 \tValidation Loss 0.01846194 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6446 \tTraining Loss: 0.01142604 \tValidation Loss 0.01924141 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.615%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6447 \tTraining Loss: 0.01144343 \tValidation Loss 0.01876328 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6448 \tTraining Loss: 0.01150252 \tValidation Loss 0.01923519 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6449 \tTraining Loss: 0.01143336 \tValidation Loss 0.01865248 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 6450 \tTraining Loss: 0.01143389 \tValidation Loss 0.01824630 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6451 \tTraining Loss: 0.01140708 \tValidation Loss 0.01848818 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6452 \tTraining Loss: 0.01131000 \tValidation Loss 0.01896022 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6453 \tTraining Loss: 0.01147815 \tValidation Loss 0.01820373 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 6454 \tTraining Loss: 0.01142375 \tValidation Loss 0.01890863 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6455 \tTraining Loss: 0.01155287 \tValidation Loss 0.01842219 \tTraining Acuuarcy 42.760% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 6456 \tTraining Loss: 0.01140949 \tValidation Loss 0.01882488 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6457 \tTraining Loss: 0.01133903 \tValidation Loss 0.01889816 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6458 \tTraining Loss: 0.01150901 \tValidation Loss 0.01836497 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6459 \tTraining Loss: 0.01147634 \tValidation Loss 0.01844286 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6460 \tTraining Loss: 0.01144915 \tValidation Loss 0.01876680 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6461 \tTraining Loss: 0.01141051 \tValidation Loss 0.01837505 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6462 \tTraining Loss: 0.01145423 \tValidation Loss 0.01821846 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 6463 \tTraining Loss: 0.01139041 \tValidation Loss 0.01841885 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6464 \tTraining Loss: 0.01138642 \tValidation Loss 0.01870841 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 6465 \tTraining Loss: 0.01139928 \tValidation Loss 0.01841192 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6466 \tTraining Loss: 0.01140312 \tValidation Loss 0.01850949 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6467 \tTraining Loss: 0.01142548 \tValidation Loss 0.01873881 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6468 \tTraining Loss: 0.01153897 \tValidation Loss 0.01849457 \tTraining Acuuarcy 42.621% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 6469 \tTraining Loss: 0.01136875 \tValidation Loss 0.01839288 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6470 \tTraining Loss: 0.01145666 \tValidation Loss 0.01895854 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 6471 \tTraining Loss: 0.01140459 \tValidation Loss 0.01888416 \tTraining Acuuarcy 44.204% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6472 \tTraining Loss: 0.01140725 \tValidation Loss 0.01854002 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6473 \tTraining Loss: 0.01143279 \tValidation Loss 0.01874110 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 6474 \tTraining Loss: 0.01138730 \tValidation Loss 0.01893556 \tTraining Acuuarcy 44.204% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6475 \tTraining Loss: 0.01142422 \tValidation Loss 0.01896048 \tTraining Acuuarcy 43.412% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6476 \tTraining Loss: 0.01145358 \tValidation Loss 0.01873511 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6477 \tTraining Loss: 0.01137063 \tValidation Loss 0.01862003 \tTraining Acuuarcy 44.215% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6478 \tTraining Loss: 0.01152653 \tValidation Loss 0.01871830 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6479 \tTraining Loss: 0.01127071 \tValidation Loss 0.01951654 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6480 \tTraining Loss: 0.01145866 \tValidation Loss 0.01886863 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6481 \tTraining Loss: 0.01135444 \tValidation Loss 0.01877688 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 6482 \tTraining Loss: 0.01130518 \tValidation Loss 0.01952440 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 6483 \tTraining Loss: 0.01137191 \tValidation Loss 0.01925046 \tTraining Acuuarcy 44.209% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6484 \tTraining Loss: 0.01134598 \tValidation Loss 0.01877581 \tTraining Acuuarcy 44.432% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 6485 \tTraining Loss: 0.01146197 \tValidation Loss 0.01873085 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6486 \tTraining Loss: 0.01142085 \tValidation Loss 0.01847058 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6487 \tTraining Loss: 0.01144576 \tValidation Loss 0.01832198 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 6488 \tTraining Loss: 0.01139711 \tValidation Loss 0.01855759 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6489 \tTraining Loss: 0.01140448 \tValidation Loss 0.01854745 \tTraining Acuuarcy 43.908% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 6490 \tTraining Loss: 0.01145913 \tValidation Loss 0.01878933 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6491 \tTraining Loss: 0.01144279 \tValidation Loss 0.01883628 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6492 \tTraining Loss: 0.01138652 \tValidation Loss 0.01907971 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 6493 \tTraining Loss: 0.01131373 \tValidation Loss 0.01861260 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 6494 \tTraining Loss: 0.01136843 \tValidation Loss 0.01873461 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6495 \tTraining Loss: 0.01138607 \tValidation Loss 0.01870470 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6496 \tTraining Loss: 0.01146619 \tValidation Loss 0.01872908 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6497 \tTraining Loss: 0.01144151 \tValidation Loss 0.01855666 \tTraining Acuuarcy 43.078% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6498 \tTraining Loss: 0.01151148 \tValidation Loss 0.01862197 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6499 \tTraining Loss: 0.01149003 \tValidation Loss 0.01812909 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6500 \tTraining Loss: 0.01142254 \tValidation Loss 0.01883577 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6501 \tTraining Loss: 0.01134415 \tValidation Loss 0.01845981 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6502 \tTraining Loss: 0.01142192 \tValidation Loss 0.01822920 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 6503 \tTraining Loss: 0.01137998 \tValidation Loss 0.01869769 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6504 \tTraining Loss: 0.01142052 \tValidation Loss 0.01838567 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 6505 \tTraining Loss: 0.01143258 \tValidation Loss 0.01820973 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6506 \tTraining Loss: 0.01138924 \tValidation Loss 0.01839882 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6507 \tTraining Loss: 0.01134644 \tValidation Loss 0.01875421 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 6508 \tTraining Loss: 0.01140151 \tValidation Loss 0.01852360 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6509 \tTraining Loss: 0.01146202 \tValidation Loss 0.01833997 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6510 \tTraining Loss: 0.01135268 \tValidation Loss 0.01882377 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6511 \tTraining Loss: 0.01129192 \tValidation Loss 0.01863713 \tTraining Acuuarcy 44.215% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6512 \tTraining Loss: 0.01142519 \tValidation Loss 0.01859332 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6513 \tTraining Loss: 0.01148903 \tValidation Loss 0.01839259 \tTraining Acuuarcy 43.005% \tValidation Acuuarcy 19.755%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6514 \tTraining Loss: 0.01149949 \tValidation Loss 0.01850519 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6515 \tTraining Loss: 0.01146509 \tValidation Loss 0.01906571 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6516 \tTraining Loss: 0.01146344 \tValidation Loss 0.01853992 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 6517 \tTraining Loss: 0.01140920 \tValidation Loss 0.01877926 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6518 \tTraining Loss: 0.01143504 \tValidation Loss 0.01818876 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6519 \tTraining Loss: 0.01139645 \tValidation Loss 0.01868494 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6520 \tTraining Loss: 0.01144319 \tValidation Loss 0.01829370 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 6521 \tTraining Loss: 0.01139435 \tValidation Loss 0.01893006 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6522 \tTraining Loss: 0.01138513 \tValidation Loss 0.01853121 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6523 \tTraining Loss: 0.01142427 \tValidation Loss 0.01889860 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 6524 \tTraining Loss: 0.01140180 \tValidation Loss 0.01868267 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6525 \tTraining Loss: 0.01140693 \tValidation Loss 0.01859212 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6526 \tTraining Loss: 0.01137864 \tValidation Loss 0.01871000 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 6527 \tTraining Loss: 0.01140625 \tValidation Loss 0.01833100 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 6528 \tTraining Loss: 0.01132387 \tValidation Loss 0.01877533 \tTraining Acuuarcy 44.387% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 6529 \tTraining Loss: 0.01135263 \tValidation Loss 0.01863763 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6530 \tTraining Loss: 0.01134392 \tValidation Loss 0.01901792 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6531 \tTraining Loss: 0.01138447 \tValidation Loss 0.01884270 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 6532 \tTraining Loss: 0.01141427 \tValidation Loss 0.01876022 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6533 \tTraining Loss: 0.01135706 \tValidation Loss 0.01861678 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6534 \tTraining Loss: 0.01135871 \tValidation Loss 0.01896082 \tTraining Acuuarcy 44.449% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6535 \tTraining Loss: 0.01135615 \tValidation Loss 0.01837582 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6536 \tTraining Loss: 0.01143247 \tValidation Loss 0.01894678 \tTraining Acuuarcy 44.209% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 6537 \tTraining Loss: 0.01143233 \tValidation Loss 0.01824264 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6538 \tTraining Loss: 0.01140418 \tValidation Loss 0.01871754 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6539 \tTraining Loss: 0.01141069 \tValidation Loss 0.01849081 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 6540 \tTraining Loss: 0.01142117 \tValidation Loss 0.01867812 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6541 \tTraining Loss: 0.01135317 \tValidation Loss 0.01836099 \tTraining Acuuarcy 44.248% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6542 \tTraining Loss: 0.01136354 \tValidation Loss 0.01869645 \tTraining Acuuarcy 44.354% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6543 \tTraining Loss: 0.01138661 \tValidation Loss 0.01848574 \tTraining Acuuarcy 44.165% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6544 \tTraining Loss: 0.01147235 \tValidation Loss 0.01852834 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6545 \tTraining Loss: 0.01137812 \tValidation Loss 0.01851493 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6546 \tTraining Loss: 0.01130699 \tValidation Loss 0.01865543 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6547 \tTraining Loss: 0.01136787 \tValidation Loss 0.01893218 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 6548 \tTraining Loss: 0.01150010 \tValidation Loss 0.01857425 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6549 \tTraining Loss: 0.01138030 \tValidation Loss 0.01862314 \tTraining Acuuarcy 44.176% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 6550 \tTraining Loss: 0.01137175 \tValidation Loss 0.01885133 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6551 \tTraining Loss: 0.01142059 \tValidation Loss 0.01846816 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6552 \tTraining Loss: 0.01142052 \tValidation Loss 0.01869220 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6553 \tTraining Loss: 0.01139974 \tValidation Loss 0.01924469 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 17.442%\n",
      "Epoch: 6554 \tTraining Loss: 0.01149715 \tValidation Loss 0.01848391 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 6555 \tTraining Loss: 0.01138098 \tValidation Loss 0.01842563 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 6556 \tTraining Loss: 0.01135407 \tValidation Loss 0.01873135 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6557 \tTraining Loss: 0.01136300 \tValidation Loss 0.01848755 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6558 \tTraining Loss: 0.01123635 \tValidation Loss 0.01898569 \tTraining Acuuarcy 44.577% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6559 \tTraining Loss: 0.01140997 \tValidation Loss 0.01851616 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 6560 \tTraining Loss: 0.01145279 \tValidation Loss 0.01878155 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6561 \tTraining Loss: 0.01138384 \tValidation Loss 0.01856940 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6562 \tTraining Loss: 0.01143813 \tValidation Loss 0.01875948 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6563 \tTraining Loss: 0.01144323 \tValidation Loss 0.01871872 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6564 \tTraining Loss: 0.01133994 \tValidation Loss 0.01899424 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6565 \tTraining Loss: 0.01141978 \tValidation Loss 0.01939458 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6566 \tTraining Loss: 0.01140185 \tValidation Loss 0.01900631 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6567 \tTraining Loss: 0.01135171 \tValidation Loss 0.01864240 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6568 \tTraining Loss: 0.01148359 \tValidation Loss 0.01833351 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6569 \tTraining Loss: 0.01133730 \tValidation Loss 0.01888395 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6570 \tTraining Loss: 0.01137465 \tValidation Loss 0.01859954 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6571 \tTraining Loss: 0.01136794 \tValidation Loss 0.01891270 \tTraining Acuuarcy 44.265% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6572 \tTraining Loss: 0.01141831 \tValidation Loss 0.01913862 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6573 \tTraining Loss: 0.01133300 \tValidation Loss 0.01864329 \tTraining Acuuarcy 44.521% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 6574 \tTraining Loss: 0.01136488 \tValidation Loss 0.01845671 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6575 \tTraining Loss: 0.01138369 \tValidation Loss 0.01878723 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6576 \tTraining Loss: 0.01142824 \tValidation Loss 0.01854866 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6577 \tTraining Loss: 0.01142467 \tValidation Loss 0.01871829 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 6578 \tTraining Loss: 0.01143697 \tValidation Loss 0.01836452 \tTraining Acuuarcy 43.178% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6579 \tTraining Loss: 0.01146892 \tValidation Loss 0.01839036 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6580 \tTraining Loss: 0.01141518 \tValidation Loss 0.01882812 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 18.640%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6581 \tTraining Loss: 0.01136421 \tValidation Loss 0.01834541 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6582 \tTraining Loss: 0.01140579 \tValidation Loss 0.01892045 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6583 \tTraining Loss: 0.01142147 \tValidation Loss 0.01852746 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6584 \tTraining Loss: 0.01143196 \tValidation Loss 0.01845114 \tTraining Acuuarcy 43.156% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6585 \tTraining Loss: 0.01135984 \tValidation Loss 0.01909742 \tTraining Acuuarcy 44.421% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 6586 \tTraining Loss: 0.01140713 \tValidation Loss 0.01856109 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6587 \tTraining Loss: 0.01148210 \tValidation Loss 0.01846436 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6588 \tTraining Loss: 0.01136112 \tValidation Loss 0.01859372 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 6589 \tTraining Loss: 0.01143437 \tValidation Loss 0.01833029 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 6590 \tTraining Loss: 0.01141492 \tValidation Loss 0.01891389 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6591 \tTraining Loss: 0.01134020 \tValidation Loss 0.01878484 \tTraining Acuuarcy 44.053% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6592 \tTraining Loss: 0.01139292 \tValidation Loss 0.01907180 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6593 \tTraining Loss: 0.01132364 \tValidation Loss 0.01886185 \tTraining Acuuarcy 44.326% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6594 \tTraining Loss: 0.01149715 \tValidation Loss 0.01858175 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6595 \tTraining Loss: 0.01145107 \tValidation Loss 0.01878315 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6596 \tTraining Loss: 0.01143965 \tValidation Loss 0.01872301 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 6597 \tTraining Loss: 0.01139593 \tValidation Loss 0.01871292 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6598 \tTraining Loss: 0.01140939 \tValidation Loss 0.01884557 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6599 \tTraining Loss: 0.01143621 \tValidation Loss 0.01848146 \tTraining Acuuarcy 43.011% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6600 \tTraining Loss: 0.01152919 \tValidation Loss 0.01893745 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6601 \tTraining Loss: 0.01136791 \tValidation Loss 0.01895891 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 6602 \tTraining Loss: 0.01139132 \tValidation Loss 0.01854613 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6603 \tTraining Loss: 0.01133273 \tValidation Loss 0.01897347 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6604 \tTraining Loss: 0.01145259 \tValidation Loss 0.01867483 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 6605 \tTraining Loss: 0.01138491 \tValidation Loss 0.01892874 \tTraining Acuuarcy 44.042% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6606 \tTraining Loss: 0.01139182 \tValidation Loss 0.01889102 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 6607 \tTraining Loss: 0.01137620 \tValidation Loss 0.01868373 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6608 \tTraining Loss: 0.01144576 \tValidation Loss 0.01888953 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6609 \tTraining Loss: 0.01136890 \tValidation Loss 0.01871446 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6610 \tTraining Loss: 0.01150023 \tValidation Loss 0.01857415 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6611 \tTraining Loss: 0.01144063 \tValidation Loss 0.01897965 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6612 \tTraining Loss: 0.01135907 \tValidation Loss 0.01871786 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6613 \tTraining Loss: 0.01138382 \tValidation Loss 0.01844564 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6614 \tTraining Loss: 0.01130282 \tValidation Loss 0.01884975 \tTraining Acuuarcy 44.872% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6615 \tTraining Loss: 0.01142902 \tValidation Loss 0.01845262 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6616 \tTraining Loss: 0.01140083 \tValidation Loss 0.01793654 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 6617 \tTraining Loss: 0.01138944 \tValidation Loss 0.01850356 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6618 \tTraining Loss: 0.01136600 \tValidation Loss 0.01865476 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6619 \tTraining Loss: 0.01140362 \tValidation Loss 0.01902696 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6620 \tTraining Loss: 0.01146835 \tValidation Loss 0.01816476 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6621 \tTraining Loss: 0.01143217 \tValidation Loss 0.01864728 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 6622 \tTraining Loss: 0.01155638 \tValidation Loss 0.01872013 \tTraining Acuuarcy 42.916% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6623 \tTraining Loss: 0.01153702 \tValidation Loss 0.01840800 \tTraining Acuuarcy 42.832% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6624 \tTraining Loss: 0.01140606 \tValidation Loss 0.01864478 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6625 \tTraining Loss: 0.01163398 \tValidation Loss 0.01869476 \tTraining Acuuarcy 42.108% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6626 \tTraining Loss: 0.01133067 \tValidation Loss 0.01902089 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6627 \tTraining Loss: 0.01140363 \tValidation Loss 0.01870711 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6628 \tTraining Loss: 0.01141251 \tValidation Loss 0.01906745 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6629 \tTraining Loss: 0.01151081 \tValidation Loss 0.01829777 \tTraining Acuuarcy 43.145% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6630 \tTraining Loss: 0.01139379 \tValidation Loss 0.01856960 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6631 \tTraining Loss: 0.01134665 \tValidation Loss 0.01846953 \tTraining Acuuarcy 44.215% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6632 \tTraining Loss: 0.01134748 \tValidation Loss 0.01870238 \tTraining Acuuarcy 44.259% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 6633 \tTraining Loss: 0.01133787 \tValidation Loss 0.01857912 \tTraining Acuuarcy 44.454% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 6634 \tTraining Loss: 0.01140831 \tValidation Loss 0.01869046 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6635 \tTraining Loss: 0.01150766 \tValidation Loss 0.01816934 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6636 \tTraining Loss: 0.01141682 \tValidation Loss 0.01836116 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6637 \tTraining Loss: 0.01129513 \tValidation Loss 0.01918166 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6638 \tTraining Loss: 0.01139148 \tValidation Loss 0.01859203 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 6639 \tTraining Loss: 0.01135643 \tValidation Loss 0.01868555 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6640 \tTraining Loss: 0.01146042 \tValidation Loss 0.01870032 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6641 \tTraining Loss: 0.01136249 \tValidation Loss 0.01843978 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6642 \tTraining Loss: 0.01142504 \tValidation Loss 0.01891336 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 6643 \tTraining Loss: 0.01140334 \tValidation Loss 0.01852629 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 6644 \tTraining Loss: 0.01138949 \tValidation Loss 0.01868430 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6645 \tTraining Loss: 0.01145965 \tValidation Loss 0.01848342 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6646 \tTraining Loss: 0.01134978 \tValidation Loss 0.01859900 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 6647 \tTraining Loss: 0.01144331 \tValidation Loss 0.01864478 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 19.114%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6648 \tTraining Loss: 0.01137469 \tValidation Loss 0.01886139 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6649 \tTraining Loss: 0.01141242 \tValidation Loss 0.01814211 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6650 \tTraining Loss: 0.01138110 \tValidation Loss 0.01905624 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6651 \tTraining Loss: 0.01144292 \tValidation Loss 0.01876266 \tTraining Acuuarcy 43.223% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6652 \tTraining Loss: 0.01138474 \tValidation Loss 0.01805899 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6653 \tTraining Loss: 0.01129749 \tValidation Loss 0.01891440 \tTraining Acuuarcy 44.243% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6654 \tTraining Loss: 0.01148167 \tValidation Loss 0.01841778 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6655 \tTraining Loss: 0.01147256 \tValidation Loss 0.01883695 \tTraining Acuuarcy 43.022% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6656 \tTraining Loss: 0.01137638 \tValidation Loss 0.01852716 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6657 \tTraining Loss: 0.01140300 \tValidation Loss 0.01870992 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6658 \tTraining Loss: 0.01139002 \tValidation Loss 0.01860858 \tTraining Acuuarcy 44.064% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 6659 \tTraining Loss: 0.01147154 \tValidation Loss 0.01843960 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6660 \tTraining Loss: 0.01140678 \tValidation Loss 0.01881043 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6661 \tTraining Loss: 0.01148307 \tValidation Loss 0.01884504 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6662 \tTraining Loss: 0.01142969 \tValidation Loss 0.01827618 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6663 \tTraining Loss: 0.01142256 \tValidation Loss 0.01871730 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6664 \tTraining Loss: 0.01143008 \tValidation Loss 0.01868612 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6665 \tTraining Loss: 0.01135118 \tValidation Loss 0.01873192 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6666 \tTraining Loss: 0.01142506 \tValidation Loss 0.01866490 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6667 \tTraining Loss: 0.01140081 \tValidation Loss 0.01860012 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 6668 \tTraining Loss: 0.01134096 \tValidation Loss 0.01881930 \tTraining Acuuarcy 44.477% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6669 \tTraining Loss: 0.01142936 \tValidation Loss 0.01878629 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6670 \tTraining Loss: 0.01152741 \tValidation Loss 0.01832493 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 6671 \tTraining Loss: 0.01145111 \tValidation Loss 0.01839057 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 6672 \tTraining Loss: 0.01145394 \tValidation Loss 0.01910182 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6673 \tTraining Loss: 0.01136010 \tValidation Loss 0.01865972 \tTraining Acuuarcy 44.059% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 6674 \tTraining Loss: 0.01138732 \tValidation Loss 0.01870580 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6675 \tTraining Loss: 0.01148130 \tValidation Loss 0.01903765 \tTraining Acuuarcy 43.323% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 6676 \tTraining Loss: 0.01144541 \tValidation Loss 0.01887058 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6677 \tTraining Loss: 0.01138463 \tValidation Loss 0.01887609 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 6678 \tTraining Loss: 0.01145026 \tValidation Loss 0.01907746 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 6679 \tTraining Loss: 0.01137564 \tValidation Loss 0.01822293 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6680 \tTraining Loss: 0.01155976 \tValidation Loss 0.01823940 \tTraining Acuuarcy 42.927% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 6681 \tTraining Loss: 0.01142892 \tValidation Loss 0.01884603 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6682 \tTraining Loss: 0.01142456 \tValidation Loss 0.01886700 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6683 \tTraining Loss: 0.01140673 \tValidation Loss 0.01866751 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6684 \tTraining Loss: 0.01131949 \tValidation Loss 0.01880765 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6685 \tTraining Loss: 0.01145841 \tValidation Loss 0.01857120 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6686 \tTraining Loss: 0.01147334 \tValidation Loss 0.01832531 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6687 \tTraining Loss: 0.01145675 \tValidation Loss 0.01857426 \tTraining Acuuarcy 43.161% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6688 \tTraining Loss: 0.01140341 \tValidation Loss 0.01888149 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6689 \tTraining Loss: 0.01139966 \tValidation Loss 0.01918545 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6690 \tTraining Loss: 0.01138632 \tValidation Loss 0.01867876 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 6691 \tTraining Loss: 0.01141677 \tValidation Loss 0.01833710 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6692 \tTraining Loss: 0.01141792 \tValidation Loss 0.01899487 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 6693 \tTraining Loss: 0.01135894 \tValidation Loss 0.01833949 \tTraining Acuuarcy 44.025% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6694 \tTraining Loss: 0.01135174 \tValidation Loss 0.01912722 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6695 \tTraining Loss: 0.01146801 \tValidation Loss 0.01864275 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6696 \tTraining Loss: 0.01139086 \tValidation Loss 0.01866658 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6697 \tTraining Loss: 0.01135571 \tValidation Loss 0.01875637 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 6698 \tTraining Loss: 0.01138610 \tValidation Loss 0.01866806 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6699 \tTraining Loss: 0.01146725 \tValidation Loss 0.01882694 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 6700 \tTraining Loss: 0.01144112 \tValidation Loss 0.01847073 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6701 \tTraining Loss: 0.01134665 \tValidation Loss 0.01861301 \tTraining Acuuarcy 44.031% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6702 \tTraining Loss: 0.01141404 \tValidation Loss 0.01909560 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 17.386%\n",
      "Epoch: 6703 \tTraining Loss: 0.01145296 \tValidation Loss 0.01843121 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 6704 \tTraining Loss: 0.01143438 \tValidation Loss 0.01832488 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 6705 \tTraining Loss: 0.01145314 \tValidation Loss 0.01929167 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6706 \tTraining Loss: 0.01127186 \tValidation Loss 0.01943473 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 6707 \tTraining Loss: 0.01138109 \tValidation Loss 0.01930633 \tTraining Acuuarcy 44.070% \tValidation Acuuarcy 17.303%\n",
      "Epoch: 6708 \tTraining Loss: 0.01139737 \tValidation Loss 0.01890507 \tTraining Acuuarcy 44.159% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6709 \tTraining Loss: 0.01142457 \tValidation Loss 0.01811818 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 6710 \tTraining Loss: 0.01135641 \tValidation Loss 0.01860204 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 6711 \tTraining Loss: 0.01129701 \tValidation Loss 0.01877904 \tTraining Acuuarcy 44.560% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6712 \tTraining Loss: 0.01137294 \tValidation Loss 0.01867854 \tTraining Acuuarcy 44.198% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 6713 \tTraining Loss: 0.01127718 \tValidation Loss 0.01881349 \tTraining Acuuarcy 44.884% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 6714 \tTraining Loss: 0.01139717 \tValidation Loss 0.01867569 \tTraining Acuuarcy 44.287% \tValidation Acuuarcy 19.504%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6715 \tTraining Loss: 0.01140005 \tValidation Loss 0.01843855 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6716 \tTraining Loss: 0.01158195 \tValidation Loss 0.01848729 \tTraining Acuuarcy 42.676% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 6717 \tTraining Loss: 0.01137418 \tValidation Loss 0.01856866 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6718 \tTraining Loss: 0.01142171 \tValidation Loss 0.01844264 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6719 \tTraining Loss: 0.01142104 \tValidation Loss 0.01918690 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 6720 \tTraining Loss: 0.01145968 \tValidation Loss 0.01876444 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 6721 \tTraining Loss: 0.01141772 \tValidation Loss 0.01857754 \tTraining Acuuarcy 43.953% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6722 \tTraining Loss: 0.01130005 \tValidation Loss 0.01891332 \tTraining Acuuarcy 43.986% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6723 \tTraining Loss: 0.01140421 \tValidation Loss 0.01848160 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6724 \tTraining Loss: 0.01145500 \tValidation Loss 0.01879795 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6725 \tTraining Loss: 0.01140082 \tValidation Loss 0.01875874 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6726 \tTraining Loss: 0.01137172 \tValidation Loss 0.01808028 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 6727 \tTraining Loss: 0.01137023 \tValidation Loss 0.01883530 \tTraining Acuuarcy 44.031% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6728 \tTraining Loss: 0.01153200 \tValidation Loss 0.01777937 \tTraining Acuuarcy 42.933% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 6729 \tTraining Loss: 0.01152289 \tValidation Loss 0.01865729 \tTraining Acuuarcy 42.938% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6730 \tTraining Loss: 0.01138234 \tValidation Loss 0.01863939 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6731 \tTraining Loss: 0.01137730 \tValidation Loss 0.01851842 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 6732 \tTraining Loss: 0.01138731 \tValidation Loss 0.01837041 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6733 \tTraining Loss: 0.01140417 \tValidation Loss 0.01812387 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6734 \tTraining Loss: 0.01139312 \tValidation Loss 0.01834271 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6735 \tTraining Loss: 0.01134675 \tValidation Loss 0.01846109 \tTraining Acuuarcy 44.421% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 6736 \tTraining Loss: 0.01137968 \tValidation Loss 0.01876902 \tTraining Acuuarcy 44.137% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6737 \tTraining Loss: 0.01141844 \tValidation Loss 0.01855296 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 6738 \tTraining Loss: 0.01134898 \tValidation Loss 0.01918353 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6739 \tTraining Loss: 0.01136079 \tValidation Loss 0.01861331 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 6740 \tTraining Loss: 0.01137869 \tValidation Loss 0.01882774 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6741 \tTraining Loss: 0.01142118 \tValidation Loss 0.01844999 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6742 \tTraining Loss: 0.01140808 \tValidation Loss 0.01882809 \tTraining Acuuarcy 43.451% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 6743 \tTraining Loss: 0.01144700 \tValidation Loss 0.01846041 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6744 \tTraining Loss: 0.01149049 \tValidation Loss 0.01914704 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6745 \tTraining Loss: 0.01141363 \tValidation Loss 0.01861909 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6746 \tTraining Loss: 0.01145215 \tValidation Loss 0.01850236 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6747 \tTraining Loss: 0.01150879 \tValidation Loss 0.01797923 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 6748 \tTraining Loss: 0.01132837 \tValidation Loss 0.01889481 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 6749 \tTraining Loss: 0.01138437 \tValidation Loss 0.01821565 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 6750 \tTraining Loss: 0.01140100 \tValidation Loss 0.01857247 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 6751 \tTraining Loss: 0.01142440 \tValidation Loss 0.01862470 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6752 \tTraining Loss: 0.01138102 \tValidation Loss 0.01875464 \tTraining Acuuarcy 44.025% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6753 \tTraining Loss: 0.01145437 \tValidation Loss 0.01843902 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 6754 \tTraining Loss: 0.01148399 \tValidation Loss 0.01806645 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6755 \tTraining Loss: 0.01148720 \tValidation Loss 0.01871642 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 6756 \tTraining Loss: 0.01137136 \tValidation Loss 0.01880380 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 6757 \tTraining Loss: 0.01133100 \tValidation Loss 0.01865776 \tTraining Acuuarcy 44.042% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 6758 \tTraining Loss: 0.01136038 \tValidation Loss 0.01851863 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6759 \tTraining Loss: 0.01139699 \tValidation Loss 0.01884768 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 6760 \tTraining Loss: 0.01127243 \tValidation Loss 0.01890805 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6761 \tTraining Loss: 0.01144228 \tValidation Loss 0.01848920 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 6762 \tTraining Loss: 0.01144470 \tValidation Loss 0.01896284 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 6763 \tTraining Loss: 0.01131274 \tValidation Loss 0.01891315 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6764 \tTraining Loss: 0.01141963 \tValidation Loss 0.01940815 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6765 \tTraining Loss: 0.01127989 \tValidation Loss 0.01877753 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 6766 \tTraining Loss: 0.01146280 \tValidation Loss 0.01874571 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6767 \tTraining Loss: 0.01138627 \tValidation Loss 0.01896467 \tTraining Acuuarcy 44.070% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 6768 \tTraining Loss: 0.01138739 \tValidation Loss 0.01846080 \tTraining Acuuarcy 44.404% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 6769 \tTraining Loss: 0.01133083 \tValidation Loss 0.01854850 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6770 \tTraining Loss: 0.01134881 \tValidation Loss 0.01870211 \tTraining Acuuarcy 44.298% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6771 \tTraining Loss: 0.01152279 \tValidation Loss 0.01846369 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6772 \tTraining Loss: 0.01140684 \tValidation Loss 0.01851157 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 6773 \tTraining Loss: 0.01147281 \tValidation Loss 0.01859954 \tTraining Acuuarcy 43.111% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6774 \tTraining Loss: 0.01138524 \tValidation Loss 0.01849200 \tTraining Acuuarcy 44.165% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 6775 \tTraining Loss: 0.01140489 \tValidation Loss 0.01868504 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 6776 \tTraining Loss: 0.01137106 \tValidation Loss 0.01872988 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6777 \tTraining Loss: 0.01133787 \tValidation Loss 0.01843510 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6778 \tTraining Loss: 0.01144274 \tValidation Loss 0.01850007 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6779 \tTraining Loss: 0.01149088 \tValidation Loss 0.01861063 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6780 \tTraining Loss: 0.01142723 \tValidation Loss 0.01889782 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 6781 \tTraining Loss: 0.01143705 \tValidation Loss 0.01894137 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 18.807%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6782 \tTraining Loss: 0.01135941 \tValidation Loss 0.01893688 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6783 \tTraining Loss: 0.01144713 \tValidation Loss 0.01852953 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6784 \tTraining Loss: 0.01153869 \tValidation Loss 0.01889148 \tTraining Acuuarcy 42.805% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 6785 \tTraining Loss: 0.01136049 \tValidation Loss 0.01883163 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 6786 \tTraining Loss: 0.01148009 \tValidation Loss 0.01850534 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6787 \tTraining Loss: 0.01148465 \tValidation Loss 0.01942257 \tTraining Acuuarcy 43.061% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6788 \tTraining Loss: 0.01137478 \tValidation Loss 0.01859367 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6789 \tTraining Loss: 0.01137031 \tValidation Loss 0.01851782 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 6790 \tTraining Loss: 0.01139645 \tValidation Loss 0.01846129 \tTraining Acuuarcy 43.958% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 6791 \tTraining Loss: 0.01131158 \tValidation Loss 0.01846399 \tTraining Acuuarcy 44.432% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 6792 \tTraining Loss: 0.01135645 \tValidation Loss 0.01902107 \tTraining Acuuarcy 44.293% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6793 \tTraining Loss: 0.01131850 \tValidation Loss 0.01920950 \tTraining Acuuarcy 44.231% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6794 \tTraining Loss: 0.01142419 \tValidation Loss 0.01867232 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6795 \tTraining Loss: 0.01144382 \tValidation Loss 0.01854032 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 6796 \tTraining Loss: 0.01138996 \tValidation Loss 0.01862373 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6797 \tTraining Loss: 0.01145358 \tValidation Loss 0.01876533 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6798 \tTraining Loss: 0.01140278 \tValidation Loss 0.01837699 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6799 \tTraining Loss: 0.01132653 \tValidation Loss 0.01911791 \tTraining Acuuarcy 44.243% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6800 \tTraining Loss: 0.01128265 \tValidation Loss 0.01818502 \tTraining Acuuarcy 44.215% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 6801 \tTraining Loss: 0.01138943 \tValidation Loss 0.01862809 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6802 \tTraining Loss: 0.01144277 \tValidation Loss 0.01866530 \tTraining Acuuarcy 43.262% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 6803 \tTraining Loss: 0.01141443 \tValidation Loss 0.01891011 \tTraining Acuuarcy 43.184% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6804 \tTraining Loss: 0.01138838 \tValidation Loss 0.01878746 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6805 \tTraining Loss: 0.01141339 \tValidation Loss 0.01847009 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6806 \tTraining Loss: 0.01137223 \tValidation Loss 0.01893813 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 6807 \tTraining Loss: 0.01148528 \tValidation Loss 0.01848671 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6808 \tTraining Loss: 0.01134712 \tValidation Loss 0.01896979 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 6809 \tTraining Loss: 0.01134477 \tValidation Loss 0.01878181 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 6810 \tTraining Loss: 0.01139126 \tValidation Loss 0.01862619 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6811 \tTraining Loss: 0.01137961 \tValidation Loss 0.01846889 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6812 \tTraining Loss: 0.01136284 \tValidation Loss 0.01843236 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 6813 \tTraining Loss: 0.01147393 \tValidation Loss 0.01852313 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 6814 \tTraining Loss: 0.01137088 \tValidation Loss 0.01827706 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 6815 \tTraining Loss: 0.01140234 \tValidation Loss 0.01865975 \tTraining Acuuarcy 44.064% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6816 \tTraining Loss: 0.01140714 \tValidation Loss 0.01854746 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6817 \tTraining Loss: 0.01132015 \tValidation Loss 0.01891443 \tTraining Acuuarcy 44.042% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6818 \tTraining Loss: 0.01139128 \tValidation Loss 0.01907017 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 6819 \tTraining Loss: 0.01140341 \tValidation Loss 0.01876600 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6820 \tTraining Loss: 0.01134565 \tValidation Loss 0.01895382 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 6821 \tTraining Loss: 0.01138096 \tValidation Loss 0.01889985 \tTraining Acuuarcy 44.187% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 6822 \tTraining Loss: 0.01140986 \tValidation Loss 0.01847506 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6823 \tTraining Loss: 0.01127760 \tValidation Loss 0.01941925 \tTraining Acuuarcy 44.750% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6824 \tTraining Loss: 0.01141565 \tValidation Loss 0.01881221 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6825 \tTraining Loss: 0.01147262 \tValidation Loss 0.01898647 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 6826 \tTraining Loss: 0.01144251 \tValidation Loss 0.01818898 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 6827 \tTraining Loss: 0.01134826 \tValidation Loss 0.01909878 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6828 \tTraining Loss: 0.01140807 \tValidation Loss 0.01859178 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6829 \tTraining Loss: 0.01146222 \tValidation Loss 0.01838897 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6830 \tTraining Loss: 0.01136402 \tValidation Loss 0.01910581 \tTraining Acuuarcy 44.248% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6831 \tTraining Loss: 0.01143356 \tValidation Loss 0.01870840 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 6832 \tTraining Loss: 0.01134838 \tValidation Loss 0.01903873 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 6833 \tTraining Loss: 0.01138841 \tValidation Loss 0.01839610 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6834 \tTraining Loss: 0.01146376 \tValidation Loss 0.01853590 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 6835 \tTraining Loss: 0.01147313 \tValidation Loss 0.01819139 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 6836 \tTraining Loss: 0.01136105 \tValidation Loss 0.01878934 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6837 \tTraining Loss: 0.01138397 \tValidation Loss 0.01835713 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 6838 \tTraining Loss: 0.01130428 \tValidation Loss 0.01847413 \tTraining Acuuarcy 44.298% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6839 \tTraining Loss: 0.01135947 \tValidation Loss 0.01856924 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6840 \tTraining Loss: 0.01138021 \tValidation Loss 0.01892297 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 6841 \tTraining Loss: 0.01139883 \tValidation Loss 0.01898222 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6842 \tTraining Loss: 0.01139860 \tValidation Loss 0.01892692 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 6843 \tTraining Loss: 0.01139190 \tValidation Loss 0.01872853 \tTraining Acuuarcy 44.153% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6844 \tTraining Loss: 0.01138750 \tValidation Loss 0.01886205 \tTraining Acuuarcy 44.254% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 6845 \tTraining Loss: 0.01134600 \tValidation Loss 0.01859630 \tTraining Acuuarcy 44.165% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6846 \tTraining Loss: 0.01147212 \tValidation Loss 0.01839307 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6847 \tTraining Loss: 0.01141761 \tValidation Loss 0.01864481 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6848 \tTraining Loss: 0.01131287 \tValidation Loss 0.01925566 \tTraining Acuuarcy 44.298% \tValidation Acuuarcy 19.337%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6849 \tTraining Loss: 0.01144687 \tValidation Loss 0.01874600 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6850 \tTraining Loss: 0.01137375 \tValidation Loss 0.01862198 \tTraining Acuuarcy 43.808% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 6851 \tTraining Loss: 0.01149913 \tValidation Loss 0.01893345 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 6852 \tTraining Loss: 0.01142790 \tValidation Loss 0.01854036 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 6853 \tTraining Loss: 0.01141535 \tValidation Loss 0.01873148 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6854 \tTraining Loss: 0.01126958 \tValidation Loss 0.01872246 \tTraining Acuuarcy 44.204% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6855 \tTraining Loss: 0.01143940 \tValidation Loss 0.01855935 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 6856 \tTraining Loss: 0.01144444 \tValidation Loss 0.01844341 \tTraining Acuuarcy 43.368% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6857 \tTraining Loss: 0.01147794 \tValidation Loss 0.01837458 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 17.554%\n",
      "Epoch: 6858 \tTraining Loss: 0.01131755 \tValidation Loss 0.01904244 \tTraining Acuuarcy 44.583% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6859 \tTraining Loss: 0.01138007 \tValidation Loss 0.01883971 \tTraining Acuuarcy 43.953% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6860 \tTraining Loss: 0.01135603 \tValidation Loss 0.01850832 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6861 \tTraining Loss: 0.01143502 \tValidation Loss 0.01886142 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6862 \tTraining Loss: 0.01148045 \tValidation Loss 0.01871806 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 6863 \tTraining Loss: 0.01133108 \tValidation Loss 0.01866696 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6864 \tTraining Loss: 0.01142106 \tValidation Loss 0.01844913 \tTraining Acuuarcy 42.899% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6865 \tTraining Loss: 0.01146678 \tValidation Loss 0.01866037 \tTraining Acuuarcy 43.373% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 6866 \tTraining Loss: 0.01140750 \tValidation Loss 0.01891946 \tTraining Acuuarcy 43.908% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6867 \tTraining Loss: 0.01132859 \tValidation Loss 0.01862772 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6868 \tTraining Loss: 0.01137918 \tValidation Loss 0.01911868 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 6869 \tTraining Loss: 0.01133578 \tValidation Loss 0.01870142 \tTraining Acuuarcy 44.326% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 6870 \tTraining Loss: 0.01139692 \tValidation Loss 0.01916333 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 6871 \tTraining Loss: 0.01140277 \tValidation Loss 0.01869046 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 17.693%\n",
      "Epoch: 6872 \tTraining Loss: 0.01151569 \tValidation Loss 0.01863654 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6873 \tTraining Loss: 0.01135055 \tValidation Loss 0.01861474 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 6874 \tTraining Loss: 0.01148140 \tValidation Loss 0.01870455 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6875 \tTraining Loss: 0.01146193 \tValidation Loss 0.01880978 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6876 \tTraining Loss: 0.01123717 \tValidation Loss 0.01822220 \tTraining Acuuarcy 44.499% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6877 \tTraining Loss: 0.01136662 \tValidation Loss 0.01839485 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6878 \tTraining Loss: 0.01136014 \tValidation Loss 0.01845639 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 6879 \tTraining Loss: 0.01137127 \tValidation Loss 0.01896719 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6880 \tTraining Loss: 0.01132073 \tValidation Loss 0.01826800 \tTraining Acuuarcy 44.304% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 6881 \tTraining Loss: 0.01141488 \tValidation Loss 0.01900073 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6882 \tTraining Loss: 0.01144446 \tValidation Loss 0.01869902 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6883 \tTraining Loss: 0.01147406 \tValidation Loss 0.01814615 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 6884 \tTraining Loss: 0.01138790 \tValidation Loss 0.01861013 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6885 \tTraining Loss: 0.01142353 \tValidation Loss 0.01884256 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6886 \tTraining Loss: 0.01138275 \tValidation Loss 0.01832699 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 6887 \tTraining Loss: 0.01142983 \tValidation Loss 0.01861277 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6888 \tTraining Loss: 0.01126580 \tValidation Loss 0.01888151 \tTraining Acuuarcy 44.761% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6889 \tTraining Loss: 0.01141496 \tValidation Loss 0.01904979 \tTraining Acuuarcy 44.081% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6890 \tTraining Loss: 0.01146429 \tValidation Loss 0.01840723 \tTraining Acuuarcy 43.234% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6891 \tTraining Loss: 0.01136233 \tValidation Loss 0.01823689 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 6892 \tTraining Loss: 0.01129161 \tValidation Loss 0.01884098 \tTraining Acuuarcy 44.181% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 6893 \tTraining Loss: 0.01136882 \tValidation Loss 0.01920979 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6894 \tTraining Loss: 0.01147541 \tValidation Loss 0.01876239 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 6895 \tTraining Loss: 0.01141269 \tValidation Loss 0.01848251 \tTraining Acuuarcy 43.295% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6896 \tTraining Loss: 0.01138483 \tValidation Loss 0.01851254 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 6897 \tTraining Loss: 0.01143899 \tValidation Loss 0.01895133 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 6898 \tTraining Loss: 0.01140625 \tValidation Loss 0.01881817 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6899 \tTraining Loss: 0.01146422 \tValidation Loss 0.01879804 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 6900 \tTraining Loss: 0.01140590 \tValidation Loss 0.01881483 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6901 \tTraining Loss: 0.01148126 \tValidation Loss 0.01836168 \tTraining Acuuarcy 43.106% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 6902 \tTraining Loss: 0.01132494 \tValidation Loss 0.01828657 \tTraining Acuuarcy 44.231% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 6903 \tTraining Loss: 0.01144349 \tValidation Loss 0.01868978 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6904 \tTraining Loss: 0.01144725 \tValidation Loss 0.01850759 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6905 \tTraining Loss: 0.01149839 \tValidation Loss 0.01808009 \tTraining Acuuarcy 42.788% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 6906 \tTraining Loss: 0.01134219 \tValidation Loss 0.01850691 \tTraining Acuuarcy 44.181% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 6907 \tTraining Loss: 0.01135652 \tValidation Loss 0.01870385 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 6908 \tTraining Loss: 0.01137675 \tValidation Loss 0.01883120 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6909 \tTraining Loss: 0.01145933 \tValidation Loss 0.01865582 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6910 \tTraining Loss: 0.01139751 \tValidation Loss 0.01869160 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6911 \tTraining Loss: 0.01130767 \tValidation Loss 0.01885916 \tTraining Acuuarcy 44.059% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6912 \tTraining Loss: 0.01143335 \tValidation Loss 0.01872494 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6913 \tTraining Loss: 0.01138185 \tValidation Loss 0.01871187 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6914 \tTraining Loss: 0.01136290 \tValidation Loss 0.01870179 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 6915 \tTraining Loss: 0.01142292 \tValidation Loss 0.01862397 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 20.284%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6916 \tTraining Loss: 0.01137585 \tValidation Loss 0.01873392 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 6917 \tTraining Loss: 0.01138313 \tValidation Loss 0.01864063 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6918 \tTraining Loss: 0.01134524 \tValidation Loss 0.01905167 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6919 \tTraining Loss: 0.01135081 \tValidation Loss 0.01869961 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6920 \tTraining Loss: 0.01145442 \tValidation Loss 0.01880620 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 6921 \tTraining Loss: 0.01136527 \tValidation Loss 0.01883197 \tTraining Acuuarcy 44.114% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 6922 \tTraining Loss: 0.01136496 \tValidation Loss 0.01865627 \tTraining Acuuarcy 44.036% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6923 \tTraining Loss: 0.01150398 \tValidation Loss 0.01859295 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6924 \tTraining Loss: 0.01127615 \tValidation Loss 0.01911754 \tTraining Acuuarcy 44.332% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6925 \tTraining Loss: 0.01149314 \tValidation Loss 0.01820779 \tTraining Acuuarcy 42.983% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 6926 \tTraining Loss: 0.01139355 \tValidation Loss 0.01900137 \tTraining Acuuarcy 43.880% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6927 \tTraining Loss: 0.01142425 \tValidation Loss 0.01859054 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 6928 \tTraining Loss: 0.01136834 \tValidation Loss 0.01849596 \tTraining Acuuarcy 44.031% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 6929 \tTraining Loss: 0.01136496 \tValidation Loss 0.01940857 \tTraining Acuuarcy 44.493% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 6930 \tTraining Loss: 0.01138204 \tValidation Loss 0.01891995 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 6931 \tTraining Loss: 0.01139073 \tValidation Loss 0.01931558 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6932 \tTraining Loss: 0.01130169 \tValidation Loss 0.01898353 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 6933 \tTraining Loss: 0.01136189 \tValidation Loss 0.01874976 \tTraining Acuuarcy 44.153% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 6934 \tTraining Loss: 0.01145801 \tValidation Loss 0.01820739 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 6935 \tTraining Loss: 0.01145075 \tValidation Loss 0.01894634 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 6936 \tTraining Loss: 0.01132544 \tValidation Loss 0.01873317 \tTraining Acuuarcy 44.248% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 6937 \tTraining Loss: 0.01142153 \tValidation Loss 0.01839182 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6938 \tTraining Loss: 0.01142547 \tValidation Loss 0.01888141 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6939 \tTraining Loss: 0.01133081 \tValidation Loss 0.01867891 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 6940 \tTraining Loss: 0.01151974 \tValidation Loss 0.01859538 \tTraining Acuuarcy 43.267% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 6941 \tTraining Loss: 0.01133435 \tValidation Loss 0.01837717 \tTraining Acuuarcy 44.070% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 6942 \tTraining Loss: 0.01138971 \tValidation Loss 0.01895875 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 6943 \tTraining Loss: 0.01143130 \tValidation Loss 0.01914107 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6944 \tTraining Loss: 0.01144603 \tValidation Loss 0.01868168 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 6945 \tTraining Loss: 0.01139348 \tValidation Loss 0.01831334 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6946 \tTraining Loss: 0.01136776 \tValidation Loss 0.01910746 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 6947 \tTraining Loss: 0.01136315 \tValidation Loss 0.01896140 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 6948 \tTraining Loss: 0.01135832 \tValidation Loss 0.01848578 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 6949 \tTraining Loss: 0.01140435 \tValidation Loss 0.01872077 \tTraining Acuuarcy 44.003% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6950 \tTraining Loss: 0.01143409 \tValidation Loss 0.01872384 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6951 \tTraining Loss: 0.01143195 \tValidation Loss 0.01848858 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6952 \tTraining Loss: 0.01144286 \tValidation Loss 0.01839591 \tTraining Acuuarcy 43.312% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6953 \tTraining Loss: 0.01133809 \tValidation Loss 0.01892609 \tTraining Acuuarcy 44.298% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6954 \tTraining Loss: 0.01133412 \tValidation Loss 0.01874471 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 6955 \tTraining Loss: 0.01136572 \tValidation Loss 0.01907476 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 6956 \tTraining Loss: 0.01140499 \tValidation Loss 0.01871754 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 6957 \tTraining Loss: 0.01140689 \tValidation Loss 0.01879122 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 6958 \tTraining Loss: 0.01136074 \tValidation Loss 0.01850908 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 6959 \tTraining Loss: 0.01142831 \tValidation Loss 0.01847155 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 6960 \tTraining Loss: 0.01137543 \tValidation Loss 0.01907917 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 6961 \tTraining Loss: 0.01157097 \tValidation Loss 0.01852601 \tTraining Acuuarcy 42.297% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6962 \tTraining Loss: 0.01126669 \tValidation Loss 0.01907754 \tTraining Acuuarcy 44.566% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 6963 \tTraining Loss: 0.01132301 \tValidation Loss 0.01861142 \tTraining Acuuarcy 44.148% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6964 \tTraining Loss: 0.01137690 \tValidation Loss 0.01875098 \tTraining Acuuarcy 44.360% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6965 \tTraining Loss: 0.01132742 \tValidation Loss 0.01881336 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 17.721%\n",
      "Epoch: 6966 \tTraining Loss: 0.01138336 \tValidation Loss 0.01907306 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 6967 \tTraining Loss: 0.01135610 \tValidation Loss 0.01893697 \tTraining Acuuarcy 44.231% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 6968 \tTraining Loss: 0.01141896 \tValidation Loss 0.01839872 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 6969 \tTraining Loss: 0.01135088 \tValidation Loss 0.01897147 \tTraining Acuuarcy 44.170% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6970 \tTraining Loss: 0.01147791 \tValidation Loss 0.01841041 \tTraining Acuuarcy 43.117% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 6971 \tTraining Loss: 0.01144291 \tValidation Loss 0.01830057 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 6972 \tTraining Loss: 0.01138526 \tValidation Loss 0.01871178 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 6973 \tTraining Loss: 0.01144830 \tValidation Loss 0.01952125 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6974 \tTraining Loss: 0.01139903 \tValidation Loss 0.01868947 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 6975 \tTraining Loss: 0.01139177 \tValidation Loss 0.01888674 \tTraining Acuuarcy 43.524% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6976 \tTraining Loss: 0.01134869 \tValidation Loss 0.01854337 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6977 \tTraining Loss: 0.01143097 \tValidation Loss 0.01861753 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 6978 \tTraining Loss: 0.01142970 \tValidation Loss 0.01890022 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6979 \tTraining Loss: 0.01133953 \tValidation Loss 0.01878339 \tTraining Acuuarcy 44.360% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6980 \tTraining Loss: 0.01140027 \tValidation Loss 0.01902611 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 6981 \tTraining Loss: 0.01142449 \tValidation Loss 0.01891626 \tTraining Acuuarcy 42.972% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 6982 \tTraining Loss: 0.01136803 \tValidation Loss 0.01858705 \tTraining Acuuarcy 44.410% \tValidation Acuuarcy 19.225%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6983 \tTraining Loss: 0.01137024 \tValidation Loss 0.01849883 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 6984 \tTraining Loss: 0.01137967 \tValidation Loss 0.01913540 \tTraining Acuuarcy 43.457% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6985 \tTraining Loss: 0.01146466 \tValidation Loss 0.01872532 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 6986 \tTraining Loss: 0.01140666 \tValidation Loss 0.01916250 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6987 \tTraining Loss: 0.01141769 \tValidation Loss 0.01846652 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 6988 \tTraining Loss: 0.01139590 \tValidation Loss 0.01869885 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 6989 \tTraining Loss: 0.01134117 \tValidation Loss 0.01899125 \tTraining Acuuarcy 44.694% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 6990 \tTraining Loss: 0.01139371 \tValidation Loss 0.01854269 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 6991 \tTraining Loss: 0.01142772 \tValidation Loss 0.01907836 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 6992 \tTraining Loss: 0.01133165 \tValidation Loss 0.01886025 \tTraining Acuuarcy 44.387% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 6993 \tTraining Loss: 0.01137062 \tValidation Loss 0.01913722 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 6994 \tTraining Loss: 0.01140070 \tValidation Loss 0.01900124 \tTraining Acuuarcy 43.802% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 6995 \tTraining Loss: 0.01129137 \tValidation Loss 0.01881026 \tTraining Acuuarcy 44.343% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 6996 \tTraining Loss: 0.01138809 \tValidation Loss 0.01863695 \tTraining Acuuarcy 44.081% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 6997 \tTraining Loss: 0.01138379 \tValidation Loss 0.01838720 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 6998 \tTraining Loss: 0.01140839 \tValidation Loss 0.01863956 \tTraining Acuuarcy 43.518% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 6999 \tTraining Loss: 0.01137896 \tValidation Loss 0.01875495 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7000 \tTraining Loss: 0.01138164 \tValidation Loss 0.01871380 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 7001 \tTraining Loss: 0.01135776 \tValidation Loss 0.01859317 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 7002 \tTraining Loss: 0.01138425 \tValidation Loss 0.01887989 \tTraining Acuuarcy 44.326% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7003 \tTraining Loss: 0.01132330 \tValidation Loss 0.01870287 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7004 \tTraining Loss: 0.01147177 \tValidation Loss 0.01861830 \tTraining Acuuarcy 44.081% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 7005 \tTraining Loss: 0.01142477 \tValidation Loss 0.01881006 \tTraining Acuuarcy 44.254% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 7006 \tTraining Loss: 0.01148668 \tValidation Loss 0.01836278 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 7007 \tTraining Loss: 0.01143860 \tValidation Loss 0.01801690 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7008 \tTraining Loss: 0.01150551 \tValidation Loss 0.01865065 \tTraining Acuuarcy 42.933% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 7009 \tTraining Loss: 0.01133662 \tValidation Loss 0.01884383 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 7010 \tTraining Loss: 0.01129402 \tValidation Loss 0.01875735 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 7011 \tTraining Loss: 0.01132029 \tValidation Loss 0.01920233 \tTraining Acuuarcy 44.120% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 7012 \tTraining Loss: 0.01130414 \tValidation Loss 0.01868538 \tTraining Acuuarcy 43.908% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 7013 \tTraining Loss: 0.01144938 \tValidation Loss 0.01870880 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 7014 \tTraining Loss: 0.01133302 \tValidation Loss 0.01879417 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 7015 \tTraining Loss: 0.01130992 \tValidation Loss 0.01894045 \tTraining Acuuarcy 44.332% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 7016 \tTraining Loss: 0.01148282 \tValidation Loss 0.01859777 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7017 \tTraining Loss: 0.01145269 \tValidation Loss 0.01849324 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7018 \tTraining Loss: 0.01131900 \tValidation Loss 0.01851112 \tTraining Acuuarcy 44.148% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7019 \tTraining Loss: 0.01139416 \tValidation Loss 0.01837388 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 7020 \tTraining Loss: 0.01140832 \tValidation Loss 0.01844065 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7021 \tTraining Loss: 0.01137060 \tValidation Loss 0.01898329 \tTraining Acuuarcy 44.371% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7022 \tTraining Loss: 0.01146297 \tValidation Loss 0.01810898 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7023 \tTraining Loss: 0.01137702 \tValidation Loss 0.01850743 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7024 \tTraining Loss: 0.01134140 \tValidation Loss 0.01901150 \tTraining Acuuarcy 44.326% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 7025 \tTraining Loss: 0.01132207 \tValidation Loss 0.01889938 \tTraining Acuuarcy 44.343% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 7026 \tTraining Loss: 0.01142308 \tValidation Loss 0.01870968 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7027 \tTraining Loss: 0.01144669 \tValidation Loss 0.01832626 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7028 \tTraining Loss: 0.01142926 \tValidation Loss 0.01850175 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 7029 \tTraining Loss: 0.01127686 \tValidation Loss 0.01910818 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7030 \tTraining Loss: 0.01141749 \tValidation Loss 0.01892791 \tTraining Acuuarcy 43.055% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7031 \tTraining Loss: 0.01135636 \tValidation Loss 0.01835683 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 7032 \tTraining Loss: 0.01136464 \tValidation Loss 0.01863870 \tTraining Acuuarcy 43.986% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 7033 \tTraining Loss: 0.01132814 \tValidation Loss 0.01896482 \tTraining Acuuarcy 44.243% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 7034 \tTraining Loss: 0.01140564 \tValidation Loss 0.01906571 \tTraining Acuuarcy 44.070% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 7035 \tTraining Loss: 0.01135394 \tValidation Loss 0.01863062 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7036 \tTraining Loss: 0.01145964 \tValidation Loss 0.01865571 \tTraining Acuuarcy 43.239% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 7037 \tTraining Loss: 0.01138649 \tValidation Loss 0.01895422 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 7038 \tTraining Loss: 0.01140694 \tValidation Loss 0.01860223 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 7039 \tTraining Loss: 0.01147021 \tValidation Loss 0.01867443 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 7040 \tTraining Loss: 0.01137042 \tValidation Loss 0.01813417 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 7041 \tTraining Loss: 0.01142806 \tValidation Loss 0.01849241 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 7042 \tTraining Loss: 0.01141884 \tValidation Loss 0.01877977 \tTraining Acuuarcy 43.340% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7043 \tTraining Loss: 0.01134788 \tValidation Loss 0.01859731 \tTraining Acuuarcy 44.120% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 7044 \tTraining Loss: 0.01142982 \tValidation Loss 0.01875288 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 7045 \tTraining Loss: 0.01129020 \tValidation Loss 0.01865641 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 7046 \tTraining Loss: 0.01144060 \tValidation Loss 0.01876093 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7047 \tTraining Loss: 0.01141557 \tValidation Loss 0.01883607 \tTraining Acuuarcy 44.187% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 7048 \tTraining Loss: 0.01139476 \tValidation Loss 0.01830951 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 7049 \tTraining Loss: 0.01135808 \tValidation Loss 0.01874705 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 18.640%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7050 \tTraining Loss: 0.01143792 \tValidation Loss 0.01883100 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7051 \tTraining Loss: 0.01149535 \tValidation Loss 0.01844564 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7052 \tTraining Loss: 0.01134986 \tValidation Loss 0.01831832 \tTraining Acuuarcy 43.618% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7053 \tTraining Loss: 0.01144073 \tValidation Loss 0.01892701 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 7054 \tTraining Loss: 0.01139295 \tValidation Loss 0.01874066 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7055 \tTraining Loss: 0.01132070 \tValidation Loss 0.01898372 \tTraining Acuuarcy 44.192% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 7056 \tTraining Loss: 0.01136446 \tValidation Loss 0.01835916 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 7057 \tTraining Loss: 0.01142936 \tValidation Loss 0.01849123 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 7058 \tTraining Loss: 0.01135851 \tValidation Loss 0.01863794 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7059 \tTraining Loss: 0.01145912 \tValidation Loss 0.01897320 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7060 \tTraining Loss: 0.01131242 \tValidation Loss 0.01848306 \tTraining Acuuarcy 44.025% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7061 \tTraining Loss: 0.01141654 \tValidation Loss 0.01903174 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7062 \tTraining Loss: 0.01135053 \tValidation Loss 0.01921683 \tTraining Acuuarcy 44.031% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 7063 \tTraining Loss: 0.01133554 \tValidation Loss 0.01856340 \tTraining Acuuarcy 44.098% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 7064 \tTraining Loss: 0.01142081 \tValidation Loss 0.01891169 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7065 \tTraining Loss: 0.01139246 \tValidation Loss 0.01843284 \tTraining Acuuarcy 43.379% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7066 \tTraining Loss: 0.01143778 \tValidation Loss 0.01911311 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7067 \tTraining Loss: 0.01144133 \tValidation Loss 0.01868666 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 7068 \tTraining Loss: 0.01142165 \tValidation Loss 0.01886286 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 7069 \tTraining Loss: 0.01141423 \tValidation Loss 0.01849757 \tTraining Acuuarcy 44.231% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7070 \tTraining Loss: 0.01143107 \tValidation Loss 0.01864944 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7071 \tTraining Loss: 0.01130063 \tValidation Loss 0.01879951 \tTraining Acuuarcy 44.237% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 7072 \tTraining Loss: 0.01146059 \tValidation Loss 0.01861377 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7073 \tTraining Loss: 0.01139530 \tValidation Loss 0.01916163 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7074 \tTraining Loss: 0.01141435 \tValidation Loss 0.01889180 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 7075 \tTraining Loss: 0.01135308 \tValidation Loss 0.01901135 \tTraining Acuuarcy 44.165% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 7076 \tTraining Loss: 0.01144475 \tValidation Loss 0.01835989 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 7077 \tTraining Loss: 0.01138915 \tValidation Loss 0.01879026 \tTraining Acuuarcy 43.986% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 7078 \tTraining Loss: 0.01138216 \tValidation Loss 0.01864992 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7079 \tTraining Loss: 0.01141270 \tValidation Loss 0.01850867 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 7080 \tTraining Loss: 0.01141110 \tValidation Loss 0.01828139 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 7081 \tTraining Loss: 0.01146262 \tValidation Loss 0.01915233 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7082 \tTraining Loss: 0.01136447 \tValidation Loss 0.01907129 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7083 \tTraining Loss: 0.01137484 \tValidation Loss 0.01879951 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 7084 \tTraining Loss: 0.01148421 \tValidation Loss 0.01843460 \tTraining Acuuarcy 43.228% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 7085 \tTraining Loss: 0.01147339 \tValidation Loss 0.01846959 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 7086 \tTraining Loss: 0.01140137 \tValidation Loss 0.01880832 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7087 \tTraining Loss: 0.01135555 \tValidation Loss 0.01852532 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 7088 \tTraining Loss: 0.01142063 \tValidation Loss 0.01835322 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 7089 \tTraining Loss: 0.01126978 \tValidation Loss 0.01872935 \tTraining Acuuarcy 44.666% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 7090 \tTraining Loss: 0.01138751 \tValidation Loss 0.01907137 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 7091 \tTraining Loss: 0.01148646 \tValidation Loss 0.01856898 \tTraining Acuuarcy 42.950% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 7092 \tTraining Loss: 0.01136021 \tValidation Loss 0.01899455 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 7093 \tTraining Loss: 0.01138863 \tValidation Loss 0.01886423 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 7094 \tTraining Loss: 0.01142590 \tValidation Loss 0.01898571 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7095 \tTraining Loss: 0.01139000 \tValidation Loss 0.01846964 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 7096 \tTraining Loss: 0.01130511 \tValidation Loss 0.01844009 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7097 \tTraining Loss: 0.01136903 \tValidation Loss 0.01856369 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7098 \tTraining Loss: 0.01143436 \tValidation Loss 0.01801577 \tTraining Acuuarcy 43.886% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 7099 \tTraining Loss: 0.01138556 \tValidation Loss 0.01893400 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 7100 \tTraining Loss: 0.01139412 \tValidation Loss 0.01914256 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 7101 \tTraining Loss: 0.01143117 \tValidation Loss 0.01920760 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7102 \tTraining Loss: 0.01136918 \tValidation Loss 0.01901825 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 7103 \tTraining Loss: 0.01145352 \tValidation Loss 0.01845409 \tTraining Acuuarcy 43.206% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 7104 \tTraining Loss: 0.01125540 \tValidation Loss 0.01910457 \tTraining Acuuarcy 44.555% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 7105 \tTraining Loss: 0.01146987 \tValidation Loss 0.01886254 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 7106 \tTraining Loss: 0.01144770 \tValidation Loss 0.01892613 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 7107 \tTraining Loss: 0.01141836 \tValidation Loss 0.01833365 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 7108 \tTraining Loss: 0.01148927 \tValidation Loss 0.01824302 \tTraining Acuuarcy 43.016% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7109 \tTraining Loss: 0.01148614 \tValidation Loss 0.01863157 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7110 \tTraining Loss: 0.01124190 \tValidation Loss 0.01922451 \tTraining Acuuarcy 44.622% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7111 \tTraining Loss: 0.01141911 \tValidation Loss 0.01846376 \tTraining Acuuarcy 43.602% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7112 \tTraining Loss: 0.01140900 \tValidation Loss 0.01911937 \tTraining Acuuarcy 43.607% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 7113 \tTraining Loss: 0.01150720 \tValidation Loss 0.01819998 \tTraining Acuuarcy 43.089% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7114 \tTraining Loss: 0.01143351 \tValidation Loss 0.01902018 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7115 \tTraining Loss: 0.01130658 \tValidation Loss 0.01870875 \tTraining Acuuarcy 44.192% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 7116 \tTraining Loss: 0.01136512 \tValidation Loss 0.01822525 \tTraining Acuuarcy 44.142% \tValidation Acuuarcy 19.727%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7117 \tTraining Loss: 0.01143461 \tValidation Loss 0.01863587 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 7118 \tTraining Loss: 0.01132968 \tValidation Loss 0.01856487 \tTraining Acuuarcy 44.209% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 7119 \tTraining Loss: 0.01133615 \tValidation Loss 0.01869377 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 7120 \tTraining Loss: 0.01144356 \tValidation Loss 0.01858405 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 7121 \tTraining Loss: 0.01149328 \tValidation Loss 0.01858434 \tTraining Acuuarcy 43.172% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7122 \tTraining Loss: 0.01136723 \tValidation Loss 0.01845448 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 7123 \tTraining Loss: 0.01138487 \tValidation Loss 0.01863821 \tTraining Acuuarcy 44.042% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7124 \tTraining Loss: 0.01127878 \tValidation Loss 0.01846710 \tTraining Acuuarcy 44.348% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7125 \tTraining Loss: 0.01139152 \tValidation Loss 0.01914321 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 7126 \tTraining Loss: 0.01228362 \tValidation Loss 0.01815414 \tTraining Acuuarcy 38.959% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 7127 \tTraining Loss: 0.01140206 \tValidation Loss 0.01862010 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 7128 \tTraining Loss: 0.01139904 \tValidation Loss 0.01839343 \tTraining Acuuarcy 44.265% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 7129 \tTraining Loss: 0.01135237 \tValidation Loss 0.01876899 \tTraining Acuuarcy 44.053% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 7130 \tTraining Loss: 0.01135796 \tValidation Loss 0.01889340 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7131 \tTraining Loss: 0.01129071 \tValidation Loss 0.01829233 \tTraining Acuuarcy 44.482% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7132 \tTraining Loss: 0.01139098 \tValidation Loss 0.01881171 \tTraining Acuuarcy 44.181% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 7133 \tTraining Loss: 0.01148377 \tValidation Loss 0.01837468 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 7134 \tTraining Loss: 0.01145960 \tValidation Loss 0.01875497 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 7135 \tTraining Loss: 0.01147916 \tValidation Loss 0.01862395 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 17.832%\n",
      "Epoch: 7136 \tTraining Loss: 0.01147562 \tValidation Loss 0.01881617 \tTraining Acuuarcy 43.139% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 7137 \tTraining Loss: 0.01137109 \tValidation Loss 0.01836305 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 7138 \tTraining Loss: 0.01139860 \tValidation Loss 0.01844530 \tTraining Acuuarcy 43.919% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7139 \tTraining Loss: 0.01146966 \tValidation Loss 0.01834014 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 7140 \tTraining Loss: 0.01146313 \tValidation Loss 0.01867996 \tTraining Acuuarcy 42.994% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7141 \tTraining Loss: 0.01141849 \tValidation Loss 0.01879528 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 7142 \tTraining Loss: 0.01141701 \tValidation Loss 0.01917664 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7143 \tTraining Loss: 0.01144268 \tValidation Loss 0.01886576 \tTraining Acuuarcy 43.100% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7144 \tTraining Loss: 0.01138807 \tValidation Loss 0.01862529 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 7145 \tTraining Loss: 0.01136410 \tValidation Loss 0.01813191 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 7146 \tTraining Loss: 0.01134578 \tValidation Loss 0.01882960 \tTraining Acuuarcy 44.243% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 7147 \tTraining Loss: 0.01141979 \tValidation Loss 0.01839512 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 7148 \tTraining Loss: 0.01142995 \tValidation Loss 0.01847569 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 7149 \tTraining Loss: 0.01137106 \tValidation Loss 0.01867011 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 7150 \tTraining Loss: 0.01144739 \tValidation Loss 0.01879606 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 7151 \tTraining Loss: 0.01143476 \tValidation Loss 0.01842554 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 7152 \tTraining Loss: 0.01143725 \tValidation Loss 0.01854871 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7153 \tTraining Loss: 0.01142491 \tValidation Loss 0.01846365 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7154 \tTraining Loss: 0.01143740 \tValidation Loss 0.01873545 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 7155 \tTraining Loss: 0.01137839 \tValidation Loss 0.01832773 \tTraining Acuuarcy 44.120% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 7156 \tTraining Loss: 0.01143623 \tValidation Loss 0.01897748 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7157 \tTraining Loss: 0.01134216 \tValidation Loss 0.01879183 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7158 \tTraining Loss: 0.01132775 \tValidation Loss 0.01906640 \tTraining Acuuarcy 43.852% \tValidation Acuuarcy 17.498%\n",
      "Epoch: 7159 \tTraining Loss: 0.01135981 \tValidation Loss 0.01890775 \tTraining Acuuarcy 44.337% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7160 \tTraining Loss: 0.01137713 \tValidation Loss 0.01857023 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 7161 \tTraining Loss: 0.01141801 \tValidation Loss 0.01846078 \tTraining Acuuarcy 43.496% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 7162 \tTraining Loss: 0.01138049 \tValidation Loss 0.01866229 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 7163 \tTraining Loss: 0.01138684 \tValidation Loss 0.01863260 \tTraining Acuuarcy 43.674% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7164 \tTraining Loss: 0.01135440 \tValidation Loss 0.01846296 \tTraining Acuuarcy 43.752% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7165 \tTraining Loss: 0.01143066 \tValidation Loss 0.01983180 \tTraining Acuuarcy 43.557% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 7166 \tTraining Loss: 0.01145950 \tValidation Loss 0.01908124 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 7167 \tTraining Loss: 0.01139419 \tValidation Loss 0.01913018 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 7168 \tTraining Loss: 0.01147220 \tValidation Loss 0.01817895 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 7169 \tTraining Loss: 0.01152745 \tValidation Loss 0.01866912 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 7170 \tTraining Loss: 0.01145211 \tValidation Loss 0.01864205 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7171 \tTraining Loss: 0.01135774 \tValidation Loss 0.01881197 \tTraining Acuuarcy 44.321% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 7172 \tTraining Loss: 0.01139322 \tValidation Loss 0.01888038 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 7173 \tTraining Loss: 0.01142785 \tValidation Loss 0.01858155 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 7174 \tTraining Loss: 0.01138781 \tValidation Loss 0.01882963 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7175 \tTraining Loss: 0.01134198 \tValidation Loss 0.01889071 \tTraining Acuuarcy 44.153% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 7176 \tTraining Loss: 0.01139903 \tValidation Loss 0.01845025 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 7177 \tTraining Loss: 0.01156992 \tValidation Loss 0.01826186 \tTraining Acuuarcy 42.844% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 7178 \tTraining Loss: 0.01129179 \tValidation Loss 0.01841183 \tTraining Acuuarcy 44.566% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 7179 \tTraining Loss: 0.01132944 \tValidation Loss 0.01905429 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 7180 \tTraining Loss: 0.01138156 \tValidation Loss 0.01889351 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7181 \tTraining Loss: 0.01138728 \tValidation Loss 0.01893704 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7182 \tTraining Loss: 0.01133241 \tValidation Loss 0.01881501 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 7183 \tTraining Loss: 0.01138170 \tValidation Loss 0.01856525 \tTraining Acuuarcy 44.176% \tValidation Acuuarcy 18.417%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7184 \tTraining Loss: 0.01131717 \tValidation Loss 0.01898912 \tTraining Acuuarcy 44.159% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7185 \tTraining Loss: 0.01133514 \tValidation Loss 0.01866194 \tTraining Acuuarcy 43.986% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 7186 \tTraining Loss: 0.01141773 \tValidation Loss 0.01843979 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 7187 \tTraining Loss: 0.01132554 \tValidation Loss 0.01869554 \tTraining Acuuarcy 44.421% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 7188 \tTraining Loss: 0.01140372 \tValidation Loss 0.01860716 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 7189 \tTraining Loss: 0.01154138 \tValidation Loss 0.01828569 \tTraining Acuuarcy 42.977% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7190 \tTraining Loss: 0.01142316 \tValidation Loss 0.01813337 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7191 \tTraining Loss: 0.01127476 \tValidation Loss 0.01881339 \tTraining Acuuarcy 44.460% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 7192 \tTraining Loss: 0.01130371 \tValidation Loss 0.01908758 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7193 \tTraining Loss: 0.01142556 \tValidation Loss 0.01868998 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7194 \tTraining Loss: 0.01132118 \tValidation Loss 0.01894131 \tTraining Acuuarcy 44.833% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 7195 \tTraining Loss: 0.01148761 \tValidation Loss 0.01854754 \tTraining Acuuarcy 43.217% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7196 \tTraining Loss: 0.01138160 \tValidation Loss 0.01863515 \tTraining Acuuarcy 43.797% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 7197 \tTraining Loss: 0.01137747 \tValidation Loss 0.01871943 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 7198 \tTraining Loss: 0.01140049 \tValidation Loss 0.01830429 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 7199 \tTraining Loss: 0.01137887 \tValidation Loss 0.01858017 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 7200 \tTraining Loss: 0.01131669 \tValidation Loss 0.01909816 \tTraining Acuuarcy 43.869% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 7201 \tTraining Loss: 0.01137704 \tValidation Loss 0.01849434 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 7202 \tTraining Loss: 0.01138552 \tValidation Loss 0.01838556 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 7203 \tTraining Loss: 0.01143996 \tValidation Loss 0.01811933 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 7204 \tTraining Loss: 0.01144887 \tValidation Loss 0.01848369 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7205 \tTraining Loss: 0.01141007 \tValidation Loss 0.01853035 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7206 \tTraining Loss: 0.01135217 \tValidation Loss 0.01868717 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7207 \tTraining Loss: 0.01137553 \tValidation Loss 0.01881310 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7208 \tTraining Loss: 0.01144593 \tValidation Loss 0.01861385 \tTraining Acuuarcy 43.356% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 7209 \tTraining Loss: 0.01145938 \tValidation Loss 0.01846195 \tTraining Acuuarcy 43.329% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7210 \tTraining Loss: 0.01141883 \tValidation Loss 0.01848369 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7211 \tTraining Loss: 0.01132648 \tValidation Loss 0.01830798 \tTraining Acuuarcy 44.309% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7212 \tTraining Loss: 0.01137017 \tValidation Loss 0.01857007 \tTraining Acuuarcy 44.003% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7213 \tTraining Loss: 0.01133275 \tValidation Loss 0.01880099 \tTraining Acuuarcy 44.209% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7214 \tTraining Loss: 0.01142164 \tValidation Loss 0.01848537 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 7215 \tTraining Loss: 0.01145618 \tValidation Loss 0.01872561 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7216 \tTraining Loss: 0.01146047 \tValidation Loss 0.01837199 \tTraining Acuuarcy 43.016% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 7217 \tTraining Loss: 0.01146106 \tValidation Loss 0.01841461 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7218 \tTraining Loss: 0.01129278 \tValidation Loss 0.01881447 \tTraining Acuuarcy 44.438% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 7219 \tTraining Loss: 0.01131292 \tValidation Loss 0.01881984 \tTraining Acuuarcy 44.460% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7220 \tTraining Loss: 0.01144612 \tValidation Loss 0.01871504 \tTraining Acuuarcy 43.284% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7221 \tTraining Loss: 0.01129748 \tValidation Loss 0.01873818 \tTraining Acuuarcy 44.282% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7222 \tTraining Loss: 0.01136015 \tValidation Loss 0.01851478 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7223 \tTraining Loss: 0.01138423 \tValidation Loss 0.01915399 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7224 \tTraining Loss: 0.01154066 \tValidation Loss 0.01846175 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 7225 \tTraining Loss: 0.01134713 \tValidation Loss 0.01903424 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 7226 \tTraining Loss: 0.01136398 \tValidation Loss 0.01896926 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 7227 \tTraining Loss: 0.01142350 \tValidation Loss 0.01833860 \tTraining Acuuarcy 43.830% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7228 \tTraining Loss: 0.01133390 \tValidation Loss 0.01874494 \tTraining Acuuarcy 44.109% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 7229 \tTraining Loss: 0.01131830 \tValidation Loss 0.01866599 \tTraining Acuuarcy 44.187% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 7230 \tTraining Loss: 0.01134538 \tValidation Loss 0.01927680 \tTraining Acuuarcy 44.549% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 7231 \tTraining Loss: 0.01142060 \tValidation Loss 0.01874408 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7232 \tTraining Loss: 0.01151650 \tValidation Loss 0.01848219 \tTraining Acuuarcy 43.028% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7233 \tTraining Loss: 0.01131859 \tValidation Loss 0.01870627 \tTraining Acuuarcy 44.003% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7234 \tTraining Loss: 0.01133463 \tValidation Loss 0.01880125 \tTraining Acuuarcy 44.577% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7235 \tTraining Loss: 0.01137356 \tValidation Loss 0.01938831 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7236 \tTraining Loss: 0.01141846 \tValidation Loss 0.01898959 \tTraining Acuuarcy 43.429% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 7237 \tTraining Loss: 0.01139082 \tValidation Loss 0.01880384 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 7238 \tTraining Loss: 0.01155883 \tValidation Loss 0.01862536 \tTraining Acuuarcy 42.944% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7239 \tTraining Loss: 0.01150673 \tValidation Loss 0.01841476 \tTraining Acuuarcy 42.693% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 7240 \tTraining Loss: 0.01129620 \tValidation Loss 0.01871764 \tTraining Acuuarcy 44.248% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7241 \tTraining Loss: 0.01141001 \tValidation Loss 0.01912957 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 7242 \tTraining Loss: 0.01130947 \tValidation Loss 0.01901255 \tTraining Acuuarcy 44.415% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 7243 \tTraining Loss: 0.01138116 \tValidation Loss 0.01867006 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 7244 \tTraining Loss: 0.01135317 \tValidation Loss 0.01838098 \tTraining Acuuarcy 44.231% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 7245 \tTraining Loss: 0.01137986 \tValidation Loss 0.01864300 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7246 \tTraining Loss: 0.01145580 \tValidation Loss 0.01851060 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7247 \tTraining Loss: 0.01136282 \tValidation Loss 0.01826620 \tTraining Acuuarcy 44.443% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7248 \tTraining Loss: 0.01144378 \tValidation Loss 0.01823291 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 7249 \tTraining Loss: 0.01125498 \tValidation Loss 0.01906066 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 7250 \tTraining Loss: 0.01145628 \tValidation Loss 0.01907017 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 18.278%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7251 \tTraining Loss: 0.01136018 \tValidation Loss 0.01852640 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 7252 \tTraining Loss: 0.01141797 \tValidation Loss 0.01854121 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 7253 \tTraining Loss: 0.01145507 \tValidation Loss 0.01854284 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 7254 \tTraining Loss: 0.01149304 \tValidation Loss 0.01862143 \tTraining Acuuarcy 43.122% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 7255 \tTraining Loss: 0.01144944 \tValidation Loss 0.01807332 \tTraining Acuuarcy 43.390% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7256 \tTraining Loss: 0.01142440 \tValidation Loss 0.01854427 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7257 \tTraining Loss: 0.01134522 \tValidation Loss 0.01867811 \tTraining Acuuarcy 43.763% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 7258 \tTraining Loss: 0.01145660 \tValidation Loss 0.01857439 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 7259 \tTraining Loss: 0.01141433 \tValidation Loss 0.01883989 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 7260 \tTraining Loss: 0.01142398 \tValidation Loss 0.01838668 \tTraining Acuuarcy 43.211% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 7261 \tTraining Loss: 0.01143522 \tValidation Loss 0.01872998 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 7262 \tTraining Loss: 0.01140872 \tValidation Loss 0.01899525 \tTraining Acuuarcy 43.925% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7263 \tTraining Loss: 0.01136318 \tValidation Loss 0.01875456 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 7264 \tTraining Loss: 0.01146400 \tValidation Loss 0.01873820 \tTraining Acuuarcy 43.735% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 7265 \tTraining Loss: 0.01130216 \tValidation Loss 0.01919639 \tTraining Acuuarcy 44.204% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 7266 \tTraining Loss: 0.01140141 \tValidation Loss 0.01846582 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7267 \tTraining Loss: 0.01140997 \tValidation Loss 0.01862299 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7268 \tTraining Loss: 0.01134957 \tValidation Loss 0.01913996 \tTraining Acuuarcy 44.165% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 7269 \tTraining Loss: 0.01139967 \tValidation Loss 0.01888976 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 7270 \tTraining Loss: 0.01136097 \tValidation Loss 0.01875185 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 7271 \tTraining Loss: 0.01144797 \tValidation Loss 0.01890837 \tTraining Acuuarcy 43.167% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 7272 \tTraining Loss: 0.01143982 \tValidation Loss 0.01853002 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 7273 \tTraining Loss: 0.01137480 \tValidation Loss 0.01925925 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7274 \tTraining Loss: 0.01140509 \tValidation Loss 0.01871521 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 7275 \tTraining Loss: 0.01153457 \tValidation Loss 0.01877416 \tTraining Acuuarcy 42.738% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 7276 \tTraining Loss: 0.01133385 \tValidation Loss 0.01897014 \tTraining Acuuarcy 44.109% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 7277 \tTraining Loss: 0.01149057 \tValidation Loss 0.01830004 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 7278 \tTraining Loss: 0.01139539 \tValidation Loss 0.01879392 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 7279 \tTraining Loss: 0.01136101 \tValidation Loss 0.01851332 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 7280 \tTraining Loss: 0.01132226 \tValidation Loss 0.01882372 \tTraining Acuuarcy 43.992% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 7281 \tTraining Loss: 0.01137413 \tValidation Loss 0.01884060 \tTraining Acuuarcy 43.713% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7282 \tTraining Loss: 0.01138116 \tValidation Loss 0.01914598 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 7283 \tTraining Loss: 0.01139675 \tValidation Loss 0.01875077 \tTraining Acuuarcy 43.668% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 7284 \tTraining Loss: 0.01145288 \tValidation Loss 0.01887787 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 7285 \tTraining Loss: 0.01143900 \tValidation Loss 0.01847548 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 7286 \tTraining Loss: 0.01133061 \tValidation Loss 0.01875851 \tTraining Acuuarcy 44.092% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7287 \tTraining Loss: 0.01139314 \tValidation Loss 0.01867285 \tTraining Acuuarcy 43.908% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 7288 \tTraining Loss: 0.01136285 \tValidation Loss 0.01923145 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 7289 \tTraining Loss: 0.01143266 \tValidation Loss 0.01882544 \tTraining Acuuarcy 43.535% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 7290 \tTraining Loss: 0.01147074 \tValidation Loss 0.01849486 \tTraining Acuuarcy 43.579% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7291 \tTraining Loss: 0.01138224 \tValidation Loss 0.01870275 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7292 \tTraining Loss: 0.01132207 \tValidation Loss 0.01864378 \tTraining Acuuarcy 44.544% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7293 \tTraining Loss: 0.01146779 \tValidation Loss 0.01878188 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7294 \tTraining Loss: 0.01137881 \tValidation Loss 0.01887340 \tTraining Acuuarcy 44.081% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 7295 \tTraining Loss: 0.01145583 \tValidation Loss 0.01844833 \tTraining Acuuarcy 43.195% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 7296 \tTraining Loss: 0.01135842 \tValidation Loss 0.01871459 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7297 \tTraining Loss: 0.01130754 \tValidation Loss 0.01831442 \tTraining Acuuarcy 44.332% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 7298 \tTraining Loss: 0.01141934 \tValidation Loss 0.01907309 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 7299 \tTraining Loss: 0.01135890 \tValidation Loss 0.01897623 \tTraining Acuuarcy 43.418% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 7300 \tTraining Loss: 0.01137696 \tValidation Loss 0.01933247 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 7301 \tTraining Loss: 0.01147003 \tValidation Loss 0.01887742 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7302 \tTraining Loss: 0.01144359 \tValidation Loss 0.01832929 \tTraining Acuuarcy 43.652% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7303 \tTraining Loss: 0.01139031 \tValidation Loss 0.01863284 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 7304 \tTraining Loss: 0.01136184 \tValidation Loss 0.01869463 \tTraining Acuuarcy 44.465% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7305 \tTraining Loss: 0.01144583 \tValidation Loss 0.01879611 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 7306 \tTraining Loss: 0.01128946 \tValidation Loss 0.01897934 \tTraining Acuuarcy 44.532% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 7307 \tTraining Loss: 0.01141237 \tValidation Loss 0.01850793 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 7308 \tTraining Loss: 0.01153246 \tValidation Loss 0.01830974 \tTraining Acuuarcy 42.777% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 7309 \tTraining Loss: 0.01148692 \tValidation Loss 0.01888504 \tTraining Acuuarcy 43.345% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 7310 \tTraining Loss: 0.01141756 \tValidation Loss 0.01892325 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7311 \tTraining Loss: 0.01144845 \tValidation Loss 0.01835771 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 7312 \tTraining Loss: 0.01137854 \tValidation Loss 0.01885036 \tTraining Acuuarcy 43.981% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 7313 \tTraining Loss: 0.01132287 \tValidation Loss 0.01843048 \tTraining Acuuarcy 44.047% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7314 \tTraining Loss: 0.01143695 \tValidation Loss 0.01882991 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 7315 \tTraining Loss: 0.01137359 \tValidation Loss 0.01874410 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 7316 \tTraining Loss: 0.01142753 \tValidation Loss 0.01865258 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 7317 \tTraining Loss: 0.01145944 \tValidation Loss 0.01886395 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 18.250%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7318 \tTraining Loss: 0.01138879 \tValidation Loss 0.01851838 \tTraining Acuuarcy 43.958% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 7319 \tTraining Loss: 0.01135262 \tValidation Loss 0.01911233 \tTraining Acuuarcy 43.858% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7320 \tTraining Loss: 0.01131626 \tValidation Loss 0.01871586 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 7321 \tTraining Loss: 0.01144139 \tValidation Loss 0.01868852 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 7322 \tTraining Loss: 0.01145483 \tValidation Loss 0.01845676 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 7323 \tTraining Loss: 0.01138952 \tValidation Loss 0.01899781 \tTraining Acuuarcy 43.362% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 7324 \tTraining Loss: 0.01143118 \tValidation Loss 0.01848959 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 7325 \tTraining Loss: 0.01141296 \tValidation Loss 0.01917624 \tTraining Acuuarcy 43.986% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7326 \tTraining Loss: 0.01135549 \tValidation Loss 0.01884596 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 7327 \tTraining Loss: 0.01140282 \tValidation Loss 0.01844924 \tTraining Acuuarcy 43.529% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 7328 \tTraining Loss: 0.01137471 \tValidation Loss 0.01846711 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 7329 \tTraining Loss: 0.01128505 \tValidation Loss 0.01883805 \tTraining Acuuarcy 44.137% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 7330 \tTraining Loss: 0.01151758 \tValidation Loss 0.01832740 \tTraining Acuuarcy 43.200% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 7331 \tTraining Loss: 0.01133153 \tValidation Loss 0.01886406 \tTraining Acuuarcy 43.992% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 7332 \tTraining Loss: 0.01132114 \tValidation Loss 0.01872728 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 7333 \tTraining Loss: 0.01141736 \tValidation Loss 0.01864473 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7334 \tTraining Loss: 0.01132470 \tValidation Loss 0.01877088 \tTraining Acuuarcy 44.599% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7335 \tTraining Loss: 0.01141020 \tValidation Loss 0.01859365 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7336 \tTraining Loss: 0.01141249 \tValidation Loss 0.01840920 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 7337 \tTraining Loss: 0.01136663 \tValidation Loss 0.01858247 \tTraining Acuuarcy 43.836% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 7338 \tTraining Loss: 0.01139204 \tValidation Loss 0.01883047 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 7339 \tTraining Loss: 0.01143129 \tValidation Loss 0.01873636 \tTraining Acuuarcy 43.462% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 7340 \tTraining Loss: 0.01146170 \tValidation Loss 0.01870195 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7341 \tTraining Loss: 0.01140938 \tValidation Loss 0.01841121 \tTraining Acuuarcy 44.243% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 7342 \tTraining Loss: 0.01133189 \tValidation Loss 0.01927399 \tTraining Acuuarcy 44.137% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 7343 \tTraining Loss: 0.01138364 \tValidation Loss 0.01873459 \tTraining Acuuarcy 43.986% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7344 \tTraining Loss: 0.01139134 \tValidation Loss 0.01869647 \tTraining Acuuarcy 43.351% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7345 \tTraining Loss: 0.01129341 \tValidation Loss 0.01837518 \tTraining Acuuarcy 44.443% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 7346 \tTraining Loss: 0.01139317 \tValidation Loss 0.01846423 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 7347 \tTraining Loss: 0.01136422 \tValidation Loss 0.01832292 \tTraining Acuuarcy 43.702% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7348 \tTraining Loss: 0.01130848 \tValidation Loss 0.01850201 \tTraining Acuuarcy 44.449% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 7349 \tTraining Loss: 0.01139527 \tValidation Loss 0.01929471 \tTraining Acuuarcy 44.181% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7350 \tTraining Loss: 0.01138225 \tValidation Loss 0.01847642 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 7351 \tTraining Loss: 0.01146035 \tValidation Loss 0.01873921 \tTraining Acuuarcy 43.334% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 7352 \tTraining Loss: 0.01144256 \tValidation Loss 0.01824324 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7353 \tTraining Loss: 0.01132920 \tValidation Loss 0.01882278 \tTraining Acuuarcy 44.103% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7354 \tTraining Loss: 0.01146958 \tValidation Loss 0.01851451 \tTraining Acuuarcy 43.133% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 7355 \tTraining Loss: 0.01148515 \tValidation Loss 0.01907519 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7356 \tTraining Loss: 0.01132396 \tValidation Loss 0.01879583 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 7357 \tTraining Loss: 0.01152075 \tValidation Loss 0.01864707 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7358 \tTraining Loss: 0.01143239 \tValidation Loss 0.01872441 \tTraining Acuuarcy 43.407% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 7359 \tTraining Loss: 0.01132390 \tValidation Loss 0.01857772 \tTraining Acuuarcy 44.477% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7360 \tTraining Loss: 0.01131533 \tValidation Loss 0.01893652 \tTraining Acuuarcy 44.081% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7361 \tTraining Loss: 0.01145835 \tValidation Loss 0.01863550 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7362 \tTraining Loss: 0.01149442 \tValidation Loss 0.01888058 \tTraining Acuuarcy 43.094% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 7363 \tTraining Loss: 0.01137727 \tValidation Loss 0.01826512 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7364 \tTraining Loss: 0.01140147 \tValidation Loss 0.01837181 \tTraining Acuuarcy 43.741% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 7365 \tTraining Loss: 0.01136966 \tValidation Loss 0.01859101 \tTraining Acuuarcy 43.747% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7366 \tTraining Loss: 0.01139633 \tValidation Loss 0.01889856 \tTraining Acuuarcy 43.864% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 7367 \tTraining Loss: 0.01129685 \tValidation Loss 0.01855311 \tTraining Acuuarcy 44.198% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7368 \tTraining Loss: 0.01136975 \tValidation Loss 0.01867402 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 7369 \tTraining Loss: 0.01138202 \tValidation Loss 0.01829519 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7370 \tTraining Loss: 0.01137827 \tValidation Loss 0.01879865 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 17.749%\n",
      "Epoch: 7371 \tTraining Loss: 0.01133677 \tValidation Loss 0.01900032 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7372 \tTraining Loss: 0.01136541 \tValidation Loss 0.01850597 \tTraining Acuuarcy 43.930% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7373 \tTraining Loss: 0.01142517 \tValidation Loss 0.01936051 \tTraining Acuuarcy 43.540% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7374 \tTraining Loss: 0.01125945 \tValidation Loss 0.01864405 \tTraining Acuuarcy 44.672% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7375 \tTraining Loss: 0.01138317 \tValidation Loss 0.01864877 \tTraining Acuuarcy 43.423% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7376 \tTraining Loss: 0.01138243 \tValidation Loss 0.01929099 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 7377 \tTraining Loss: 0.01149394 \tValidation Loss 0.01870996 \tTraining Acuuarcy 43.306% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 7378 \tTraining Loss: 0.01133211 \tValidation Loss 0.01871098 \tTraining Acuuarcy 44.231% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 7379 \tTraining Loss: 0.01131285 \tValidation Loss 0.01839930 \tTraining Acuuarcy 44.488% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7380 \tTraining Loss: 0.01128558 \tValidation Loss 0.01865412 \tTraining Acuuarcy 44.075% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 7381 \tTraining Loss: 0.01140712 \tValidation Loss 0.01876918 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 7382 \tTraining Loss: 0.01136367 \tValidation Loss 0.01874550 \tTraining Acuuarcy 43.997% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7383 \tTraining Loss: 0.01140183 \tValidation Loss 0.01879531 \tTraining Acuuarcy 44.220% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 7384 \tTraining Loss: 0.01130799 \tValidation Loss 0.01890377 \tTraining Acuuarcy 44.376% \tValidation Acuuarcy 18.501%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7385 \tTraining Loss: 0.01134696 \tValidation Loss 0.01874296 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7386 \tTraining Loss: 0.01138443 \tValidation Loss 0.01906359 \tTraining Acuuarcy 44.293% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 7387 \tTraining Loss: 0.01139645 \tValidation Loss 0.01879539 \tTraining Acuuarcy 44.176% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 7388 \tTraining Loss: 0.01137130 \tValidation Loss 0.01854660 \tTraining Acuuarcy 44.070% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 7389 \tTraining Loss: 0.01136744 \tValidation Loss 0.01952779 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 17.860%\n",
      "Epoch: 7390 \tTraining Loss: 0.01142998 \tValidation Loss 0.01846723 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 7391 \tTraining Loss: 0.01146804 \tValidation Loss 0.01860631 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 7392 \tTraining Loss: 0.01133495 \tValidation Loss 0.01876854 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7393 \tTraining Loss: 0.01144334 \tValidation Loss 0.01850869 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7394 \tTraining Loss: 0.01138189 \tValidation Loss 0.01887497 \tTraining Acuuarcy 43.964% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7395 \tTraining Loss: 0.01142006 \tValidation Loss 0.01830827 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 7396 \tTraining Loss: 0.01143397 \tValidation Loss 0.01873733 \tTraining Acuuarcy 43.072% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7397 \tTraining Loss: 0.01137445 \tValidation Loss 0.01904448 \tTraining Acuuarcy 44.298% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 7398 \tTraining Loss: 0.01127937 \tValidation Loss 0.01836572 \tTraining Acuuarcy 44.393% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7399 \tTraining Loss: 0.01141144 \tValidation Loss 0.01852881 \tTraining Acuuarcy 44.092% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 7400 \tTraining Loss: 0.01140425 \tValidation Loss 0.01853492 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7401 \tTraining Loss: 0.01143554 \tValidation Loss 0.01836697 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 7402 \tTraining Loss: 0.01143758 \tValidation Loss 0.01845650 \tTraining Acuuarcy 43.401% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 7403 \tTraining Loss: 0.01135602 \tValidation Loss 0.01897906 \tTraining Acuuarcy 44.426% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7404 \tTraining Loss: 0.01142644 \tValidation Loss 0.01834591 \tTraining Acuuarcy 42.989% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 7405 \tTraining Loss: 0.01148338 \tValidation Loss 0.01844386 \tTraining Acuuarcy 43.501% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7406 \tTraining Loss: 0.01134763 \tValidation Loss 0.01871391 \tTraining Acuuarcy 43.646% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 7407 \tTraining Loss: 0.01135279 \tValidation Loss 0.01902548 \tTraining Acuuarcy 44.098% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7408 \tTraining Loss: 0.01137641 \tValidation Loss 0.01883762 \tTraining Acuuarcy 43.708% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 7409 \tTraining Loss: 0.01131659 \tValidation Loss 0.01904487 \tTraining Acuuarcy 44.276% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7410 \tTraining Loss: 0.01151442 \tValidation Loss 0.01895772 \tTraining Acuuarcy 42.910% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 7411 \tTraining Loss: 0.01134851 \tValidation Loss 0.01883179 \tTraining Acuuarcy 44.332% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7412 \tTraining Loss: 0.01137462 \tValidation Loss 0.01851983 \tTraining Acuuarcy 44.343% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7413 \tTraining Loss: 0.01127314 \tValidation Loss 0.01891937 \tTraining Acuuarcy 44.220% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7414 \tTraining Loss: 0.01140379 \tValidation Loss 0.01906705 \tTraining Acuuarcy 43.685% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 7415 \tTraining Loss: 0.01149691 \tValidation Loss 0.01853406 \tTraining Acuuarcy 43.245% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 7416 \tTraining Loss: 0.01133807 \tValidation Loss 0.01899459 \tTraining Acuuarcy 44.165% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 7417 \tTraining Loss: 0.01152620 \tValidation Loss 0.01878111 \tTraining Acuuarcy 42.883% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 7418 \tTraining Loss: 0.01148594 \tValidation Loss 0.01869687 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 7419 \tTraining Loss: 0.01141311 \tValidation Loss 0.01854563 \tTraining Acuuarcy 43.189% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7420 \tTraining Loss: 0.01137313 \tValidation Loss 0.01894127 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7421 \tTraining Loss: 0.01147024 \tValidation Loss 0.01866458 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7422 \tTraining Loss: 0.01141249 \tValidation Loss 0.01861001 \tTraining Acuuarcy 43.479% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7423 \tTraining Loss: 0.01132551 \tValidation Loss 0.01922716 \tTraining Acuuarcy 44.192% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 7424 \tTraining Loss: 0.01145138 \tValidation Loss 0.01858279 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 7425 \tTraining Loss: 0.01161362 \tValidation Loss 0.01884558 \tTraining Acuuarcy 42.520% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 7426 \tTraining Loss: 0.01136061 \tValidation Loss 0.01881027 \tTraining Acuuarcy 44.025% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 7427 \tTraining Loss: 0.01136646 \tValidation Loss 0.01880292 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7428 \tTraining Loss: 0.01134575 \tValidation Loss 0.01868966 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 18.055%\n",
      "Epoch: 7429 \tTraining Loss: 0.01133887 \tValidation Loss 0.01848908 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7430 \tTraining Loss: 0.01137550 \tValidation Loss 0.01843167 \tTraining Acuuarcy 44.126% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 7431 \tTraining Loss: 0.01142740 \tValidation Loss 0.01836093 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 7432 \tTraining Loss: 0.01136663 \tValidation Loss 0.01887676 \tTraining Acuuarcy 43.691% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 7433 \tTraining Loss: 0.01140262 \tValidation Loss 0.01846798 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7434 \tTraining Loss: 0.01141430 \tValidation Loss 0.01874933 \tTraining Acuuarcy 43.273% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 7435 \tTraining Loss: 0.01133731 \tValidation Loss 0.01820846 \tTraining Acuuarcy 44.159% \tValidation Acuuarcy 17.916%\n",
      "Epoch: 7436 \tTraining Loss: 0.01142174 \tValidation Loss 0.01866224 \tTraining Acuuarcy 44.237% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 7437 \tTraining Loss: 0.01134024 \tValidation Loss 0.01885377 \tTraining Acuuarcy 43.953% \tValidation Acuuarcy 17.609%\n",
      "Epoch: 7438 \tTraining Loss: 0.01138762 \tValidation Loss 0.01861812 \tTraining Acuuarcy 43.635% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 7439 \tTraining Loss: 0.01134288 \tValidation Loss 0.01886202 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7440 \tTraining Loss: 0.01149753 \tValidation Loss 0.01854056 \tTraining Acuuarcy 43.289% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7441 \tTraining Loss: 0.01141073 \tValidation Loss 0.01900836 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 7442 \tTraining Loss: 0.01134254 \tValidation Loss 0.01894259 \tTraining Acuuarcy 44.142% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7443 \tTraining Loss: 0.01143859 \tValidation Loss 0.01878352 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7444 \tTraining Loss: 0.01140907 \tValidation Loss 0.01829065 \tTraining Acuuarcy 43.657% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 7445 \tTraining Loss: 0.01130028 \tValidation Loss 0.01862194 \tTraining Acuuarcy 44.371% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7446 \tTraining Loss: 0.01140243 \tValidation Loss 0.01891289 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7447 \tTraining Loss: 0.01146331 \tValidation Loss 0.01824453 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 7448 \tTraining Loss: 0.01150640 \tValidation Loss 0.01874521 \tTraining Acuuarcy 43.317% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7449 \tTraining Loss: 0.01145415 \tValidation Loss 0.01902714 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 7450 \tTraining Loss: 0.01139115 \tValidation Loss 0.01884764 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 7451 \tTraining Loss: 0.01134481 \tValidation Loss 0.01913980 \tTraining Acuuarcy 44.204% \tValidation Acuuarcy 19.615%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7452 \tTraining Loss: 0.01128051 \tValidation Loss 0.01882026 \tTraining Acuuarcy 44.694% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 7453 \tTraining Loss: 0.01133739 \tValidation Loss 0.01893783 \tTraining Acuuarcy 44.610% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 7454 \tTraining Loss: 0.01136294 \tValidation Loss 0.01909705 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7455 \tTraining Loss: 0.01132136 \tValidation Loss 0.01952322 \tTraining Acuuarcy 44.064% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 7456 \tTraining Loss: 0.01143298 \tValidation Loss 0.01847460 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 7457 \tTraining Loss: 0.01134791 \tValidation Loss 0.01876351 \tTraining Acuuarcy 43.841% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 7458 \tTraining Loss: 0.01140008 \tValidation Loss 0.01847405 \tTraining Acuuarcy 43.891% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 7459 \tTraining Loss: 0.01144243 \tValidation Loss 0.01873489 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7460 \tTraining Loss: 0.01147779 \tValidation Loss 0.01952491 \tTraining Acuuarcy 43.128% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7461 \tTraining Loss: 0.01141073 \tValidation Loss 0.01935232 \tTraining Acuuarcy 43.791% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 7462 \tTraining Loss: 0.01141050 \tValidation Loss 0.01853740 \tTraining Acuuarcy 43.395% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 7463 \tTraining Loss: 0.01148775 \tValidation Loss 0.01852808 \tTraining Acuuarcy 43.473% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 7464 \tTraining Loss: 0.01149061 \tValidation Loss 0.01840149 \tTraining Acuuarcy 43.256% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 7465 \tTraining Loss: 0.01137492 \tValidation Loss 0.01882415 \tTraining Acuuarcy 44.031% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 7466 \tTraining Loss: 0.01128500 \tValidation Loss 0.01895309 \tTraining Acuuarcy 44.025% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7467 \tTraining Loss: 0.01143818 \tValidation Loss 0.01832049 \tTraining Acuuarcy 43.825% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 7468 \tTraining Loss: 0.01142464 \tValidation Loss 0.01820400 \tTraining Acuuarcy 43.485% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 7469 \tTraining Loss: 0.01128859 \tValidation Loss 0.01886961 \tTraining Acuuarcy 44.270% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 7470 \tTraining Loss: 0.01142863 \tValidation Loss 0.01877818 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 7471 \tTraining Loss: 0.01142837 \tValidation Loss 0.01855222 \tTraining Acuuarcy 43.719% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7472 \tTraining Loss: 0.01129878 \tValidation Loss 0.01905616 \tTraining Acuuarcy 44.432% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 7473 \tTraining Loss: 0.01139806 \tValidation Loss 0.01856633 \tTraining Acuuarcy 43.758% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 7474 \tTraining Loss: 0.01139817 \tValidation Loss 0.01923185 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7475 \tTraining Loss: 0.01137137 \tValidation Loss 0.01892131 \tTraining Acuuarcy 44.025% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 7476 \tTraining Loss: 0.01138333 \tValidation Loss 0.01846498 \tTraining Acuuarcy 43.730% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 7477 \tTraining Loss: 0.01142194 \tValidation Loss 0.01850216 \tTraining Acuuarcy 43.563% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 7478 \tTraining Loss: 0.01135406 \tValidation Loss 0.01913277 \tTraining Acuuarcy 43.914% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 7479 \tTraining Loss: 0.01148236 \tValidation Loss 0.01871560 \tTraining Acuuarcy 43.551% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 7480 \tTraining Loss: 0.01139252 \tValidation Loss 0.01888444 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7481 \tTraining Loss: 0.01139643 \tValidation Loss 0.01821226 \tTraining Acuuarcy 43.696% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 7482 \tTraining Loss: 0.01134839 \tValidation Loss 0.01857412 \tTraining Acuuarcy 44.064% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7483 \tTraining Loss: 0.01147502 \tValidation Loss 0.01899353 \tTraining Acuuarcy 43.440% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 7484 \tTraining Loss: 0.01139726 \tValidation Loss 0.01847500 \tTraining Acuuarcy 43.947% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 7485 \tTraining Loss: 0.01155693 \tValidation Loss 0.01785531 \tTraining Acuuarcy 42.888% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7486 \tTraining Loss: 0.01147067 \tValidation Loss 0.01848599 \tTraining Acuuarcy 43.278% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 7487 \tTraining Loss: 0.01140421 \tValidation Loss 0.01879395 \tTraining Acuuarcy 43.490% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 7488 \tTraining Loss: 0.01132044 \tValidation Loss 0.01844220 \tTraining Acuuarcy 44.449% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 7489 \tTraining Loss: 0.01141480 \tValidation Loss 0.01904496 \tTraining Acuuarcy 43.629% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 7490 \tTraining Loss: 0.01137588 \tValidation Loss 0.01916068 \tTraining Acuuarcy 44.020% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 7491 \tTraining Loss: 0.01137453 \tValidation Loss 0.01917759 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7492 \tTraining Loss: 0.01138132 \tValidation Loss 0.01886838 \tTraining Acuuarcy 43.897% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 7493 \tTraining Loss: 0.01137272 \tValidation Loss 0.01841097 \tTraining Acuuarcy 43.819% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 7494 \tTraining Loss: 0.01126144 \tValidation Loss 0.01868552 \tTraining Acuuarcy 44.382% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 7495 \tTraining Loss: 0.01142963 \tValidation Loss 0.01852715 \tTraining Acuuarcy 43.585% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7496 \tTraining Loss: 0.01143775 \tValidation Loss 0.01863950 \tTraining Acuuarcy 43.301% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 7497 \tTraining Loss: 0.01134253 \tValidation Loss 0.01865761 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 7498 \tTraining Loss: 0.01139034 \tValidation Loss 0.01862566 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 7499 \tTraining Loss: 0.01136101 \tValidation Loss 0.01902203 \tTraining Acuuarcy 43.680% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 7500 \tTraining Loss: 0.01140549 \tValidation Loss 0.01851194 \tTraining Acuuarcy 43.507% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7501 \tTraining Loss: 0.01130774 \tValidation Loss 0.01859376 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 7502 \tTraining Loss: 0.01135996 \tValidation Loss 0.01908283 \tTraining Acuuarcy 44.298% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 7503 \tTraining Loss: 0.01130285 \tValidation Loss 0.01890546 \tTraining Acuuarcy 44.521% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7504 \tTraining Loss: 0.01137463 \tValidation Loss 0.01880886 \tTraining Acuuarcy 43.663% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 7505 \tTraining Loss: 0.01139438 \tValidation Loss 0.01907275 \tTraining Acuuarcy 43.975% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 7506 \tTraining Loss: 0.01132286 \tValidation Loss 0.01883357 \tTraining Acuuarcy 43.942% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7507 \tTraining Loss: 0.01140161 \tValidation Loss 0.01869574 \tTraining Acuuarcy 43.624% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 7508 \tTraining Loss: 0.01136997 \tValidation Loss 0.01879874 \tTraining Acuuarcy 44.092% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 7509 \tTraining Loss: 0.01133487 \tValidation Loss 0.01837186 \tTraining Acuuarcy 43.774% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 7510 \tTraining Loss: 0.01145950 \tValidation Loss 0.01884654 \tTraining Acuuarcy 42.799% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 7511 \tTraining Loss: 0.01143638 \tValidation Loss 0.01856007 \tTraining Acuuarcy 43.512% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7512 \tTraining Loss: 0.01128350 \tValidation Loss 0.01827805 \tTraining Acuuarcy 44.332% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 7513 \tTraining Loss: 0.01143359 \tValidation Loss 0.01845084 \tTraining Acuuarcy 43.384% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 7514 \tTraining Loss: 0.01129171 \tValidation Loss 0.01912291 \tTraining Acuuarcy 44.716% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 7515 \tTraining Loss: 0.01138748 \tValidation Loss 0.01846181 \tTraining Acuuarcy 43.780% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 7516 \tTraining Loss: 0.01143418 \tValidation Loss 0.01894889 \tTraining Acuuarcy 43.568% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 7517 \tTraining Loss: 0.01141002 \tValidation Loss 0.01892490 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 7518 \tTraining Loss: 0.01140599 \tValidation Loss 0.01867404 \tTraining Acuuarcy 43.903% \tValidation Acuuarcy 19.086%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7519 \tTraining Loss: 0.01140692 \tValidation Loss 0.01835830 \tTraining Acuuarcy 43.641% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 7520 \tTraining Loss: 0.01143584 \tValidation Loss 0.01870523 \tTraining Acuuarcy 43.434% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 7521 \tTraining Loss: 0.01136336 \tValidation Loss 0.01875578 \tTraining Acuuarcy 43.590% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 7522 \tTraining Loss: 0.01141166 \tValidation Loss 0.01872574 \tTraining Acuuarcy 43.546% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 7523 \tTraining Loss: 0.01135732 \tValidation Loss 0.01873055 \tTraining Acuuarcy 43.786% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 7524 \tTraining Loss: 0.01134442 \tValidation Loss 0.01829135 \tTraining Acuuarcy 44.254% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 7525 \tTraining Loss: 0.01142940 \tValidation Loss 0.01846503 \tTraining Acuuarcy 43.875% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 7526 \tTraining Loss: 0.01142984 \tValidation Loss 0.01889929 \tTraining Acuuarcy 43.613% \tValidation Acuuarcy 17.999%\n",
      "Epoch: 7527 \tTraining Loss: 0.01137791 \tValidation Loss 0.01912839 \tTraining Acuuarcy 43.769% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 7528 \tTraining Loss: 0.01144566 \tValidation Loss 0.01879807 \tTraining Acuuarcy 44.087% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 7529 \tTraining Loss: 0.01141688 \tValidation Loss 0.01868968 \tTraining Acuuarcy 43.446% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 7530 \tTraining Loss: 0.01140293 \tValidation Loss 0.01844785 \tTraining Acuuarcy 43.412% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 7531 \tTraining Loss: 0.01140117 \tValidation Loss 0.01865179 \tTraining Acuuarcy 43.936% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 7532 \tTraining Loss: 0.01136972 \tValidation Loss 0.01883466 \tTraining Acuuarcy 43.813% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 7533 \tTraining Loss: 0.01131276 \tValidation Loss 0.01932792 \tTraining Acuuarcy 43.992% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 7534 \tTraining Loss: 0.01125733 \tValidation Loss 0.01937677 \tTraining Acuuarcy 44.248% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 7535 \tTraining Loss: 0.01137469 \tValidation Loss 0.01859875 \tTraining Acuuarcy 43.596% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 7536 \tTraining Loss: 0.01135303 \tValidation Loss 0.01880540 \tTraining Acuuarcy 43.574% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 7537 \tTraining Loss: 0.01138704 \tValidation Loss 0.01870215 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 7538 \tTraining Loss: 0.01140531 \tValidation Loss 0.01855052 \tTraining Acuuarcy 43.847% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 7539 \tTraining Loss: 0.01143367 \tValidation Loss 0.01893626 \tTraining Acuuarcy 43.468% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 7540 \tTraining Loss: 0.01142223 \tValidation Loss 0.01852626 \tTraining Acuuarcy 43.250% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 7541 \tTraining Loss: 0.01139151 \tValidation Loss 0.01852357 \tTraining Acuuarcy 43.969% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 7542 \tTraining Loss: 0.01135040 \tValidation Loss 0.01886499 \tTraining Acuuarcy 44.187% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 7543 \tTraining Loss: 0.01133597 \tValidation Loss 0.01864568 \tTraining Acuuarcy 44.131% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 7544 \tTraining Loss: 0.01132430 \tValidation Loss 0.01915418 \tTraining Acuuarcy 44.014% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 7545 \tTraining Loss: 0.01136599 \tValidation Loss 0.01866088 \tTraining Acuuarcy 43.724% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 7546 \tTraining Loss: 0.01136684 \tValidation Loss 0.01877789 \tTraining Acuuarcy 44.008% \tValidation Acuuarcy 18.891%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 113\u001b[0m\n\u001b[0;32m    111\u001b[0m criterion\u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m    112\u001b[0m optmizer\u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m lr)\n\u001b[1;32m--> 113\u001b[0m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptmizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m, in \u001b[0;36mTrain\u001b[1;34m(epochs, train_loader, val_loader, criterion, optmizer, device)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Train the model  #\u001b[39;00m\n\u001b[0;32m     34\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     36\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     37\u001b[0m     optmizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anaconda\\Project by me\\Deep-Emotion-master\\data_loaders.py:39\u001b[0m, in \u001b[0;36mPlain_Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     36\u001b[0m lables \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(lables)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform :\n\u001b[1;32m---> 39\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img,lables\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"CUDA is not available. Make sure you have a CUDA-enabled GPU.\")\n",
    "\n",
    "def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        # Train the model  #\n",
    "        net.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optmizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        #validate the model#\n",
    "        net.eval()\n",
    "        for data,labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            val_outputs = net(data)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs,1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss =  validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
    "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
    "\n",
    "    torch.save(net.state_dict(),'deep_emotion-{}-{}-{}.pt'.format(epochs,batchsize,lr))\n",
    "    print(\"===================================Training Finished===================================\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"Configuration of setup and training process\")\n",
    "#     parser.add_argument('-s', '--setup', type=bool, help='setup the dataset for the first time')\n",
    "#     parser.add_argument('-d', '--data', type=str,required= True,\n",
    "#                                help='data folder that contains data files that downloaded from kaggle (train.csv and test.csv)')\n",
    "#     parser.add_argument('-hparams', '--hyperparams', type=bool,\n",
    "#                                help='True when changing the hyperparameters e.g (batch size, LR, num. of epochs)')\n",
    "#     parser.add_argument('-e', '--epochs', type= int, help= 'number of epochs')\n",
    "#     parser.add_argument('-lr', '--learning_rate', type= float, help= 'value of learning rate')\n",
    "#     parser.add_argument('-bs', '--batch_size', type= int, help= 'training/validation batch size')\n",
    "#     parser.add_argument('-t', '--train', type=bool, help='True when training')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if args.setup :\n",
    "#         generate_dataset = Generate_data(args.data)\n",
    "#         generate_dataset.split_test()\n",
    "#         generate_dataset.save_images('train')\n",
    "#         generate_dataset.save_images('test')\n",
    "#         generate_dataset.save_images('val')\n",
    "\n",
    "#     if args.hyperparams:\n",
    "#         epochs = args.epochs\n",
    "#         lr = args.learning_rate\n",
    "#         batchsize = args.batch_size\n",
    "#     else :\n",
    "epochs = 10000\n",
    "lr = 0.005\n",
    "batchsize = 128\n",
    "\n",
    "#     if args.train:\n",
    "net = Deep_Emotion()\n",
    "net.to(device)\n",
    "print(\"Model archticture: \", net)\n",
    "traincsv_file = 'data'+'/'+'train.csv'\n",
    "validationcsv_file = 'data'+'/'+'val.csv'\n",
    "train_img_dir = 'data'+'/'+'train/'\n",
    "validation_img_dir = 'data'+'/'+'val/'\n",
    "\n",
    "transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
    "validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
    "train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optmizer= optim.Adam(net.parameters(),lr= lr)\n",
    "Train(epochs, train_loader, val_loader, criterion, optmizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc6edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'Speaktrum_by_SOVA_Latest-Trained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88b4995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep_Emotion(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
       "  (localization): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_loc): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net= Deep_Emotion()\n",
    "net.load_state_dict(torch.load('Speaktrum_by_SOVA_Latest-Trained.pt'))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa499e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b53f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
