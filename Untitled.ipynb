{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac840b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archticture:  Deep_Emotion(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "===================================Start Training===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4277: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4215: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.01435154 \tValidation Loss 0.01483585 \tTraining Acuuarcy 24.250% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 2 \tTraining Loss: 0.01425282 \tValidation Loss 0.01464794 \tTraining Acuuarcy 25.020% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 3 \tTraining Loss: 0.01424262 \tValidation Loss 0.01473584 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 4 \tTraining Loss: 0.01423277 \tValidation Loss 0.01476651 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 5 \tTraining Loss: 0.01423895 \tValidation Loss 0.01473681 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 6 \tTraining Loss: 0.01421977 \tValidation Loss 0.01475337 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 7 \tTraining Loss: 0.01421535 \tValidation Loss 0.01476265 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 8 \tTraining Loss: 0.01421078 \tValidation Loss 0.01476519 \tTraining Acuuarcy 25.053% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 9 \tTraining Loss: 0.01421848 \tValidation Loss 0.01474483 \tTraining Acuuarcy 25.053% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 10 \tTraining Loss: 0.01420791 \tValidation Loss 0.01478988 \tTraining Acuuarcy 25.053% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 11 \tTraining Loss: 0.01420782 \tValidation Loss 0.01492249 \tTraining Acuuarcy 25.036% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 12 \tTraining Loss: 0.01420299 \tValidation Loss 0.01473270 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 13 \tTraining Loss: 0.01419891 \tValidation Loss 0.01466590 \tTraining Acuuarcy 25.042% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 14 \tTraining Loss: 0.01421145 \tValidation Loss 0.01470074 \tTraining Acuuarcy 25.053% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 15 \tTraining Loss: 0.01419726 \tValidation Loss 0.01476207 \tTraining Acuuarcy 25.059% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 16 \tTraining Loss: 0.01419543 \tValidation Loss 0.01483063 \tTraining Acuuarcy 25.086% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 17 \tTraining Loss: 0.01418658 \tValidation Loss 0.01473997 \tTraining Acuuarcy 25.025% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 18 \tTraining Loss: 0.01418812 \tValidation Loss 0.01477868 \tTraining Acuuarcy 25.064% \tValidation Acuuarcy 25.077%\n",
      "Epoch: 19 \tTraining Loss: 0.01418577 \tValidation Loss 0.01480267 \tTraining Acuuarcy 25.053% \tValidation Acuuarcy 25.104%\n",
      "Epoch: 20 \tTraining Loss: 0.01418169 \tValidation Loss 0.01474606 \tTraining Acuuarcy 25.053% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 21 \tTraining Loss: 0.01418097 \tValidation Loss 0.01472492 \tTraining Acuuarcy 24.997% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 22 \tTraining Loss: 0.01417999 \tValidation Loss 0.01483429 \tTraining Acuuarcy 24.947% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 23 \tTraining Loss: 0.01417363 \tValidation Loss 0.01475203 \tTraining Acuuarcy 25.036% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 24 \tTraining Loss: 0.01416798 \tValidation Loss 0.01485314 \tTraining Acuuarcy 25.036% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 25 \tTraining Loss: 0.01416965 \tValidation Loss 0.01478367 \tTraining Acuuarcy 25.036% \tValidation Acuuarcy 24.714%\n",
      "Epoch: 26 \tTraining Loss: 0.01417372 \tValidation Loss 0.01477292 \tTraining Acuuarcy 25.209% \tValidation Acuuarcy 24.826%\n",
      "Epoch: 27 \tTraining Loss: 0.01417441 \tValidation Loss 0.01475361 \tTraining Acuuarcy 25.092% \tValidation Acuuarcy 25.021%\n",
      "Epoch: 28 \tTraining Loss: 0.01415506 \tValidation Loss 0.01476444 \tTraining Acuuarcy 25.170% \tValidation Acuuarcy 25.077%\n",
      "Epoch: 29 \tTraining Loss: 0.01416075 \tValidation Loss 0.01481814 \tTraining Acuuarcy 24.958% \tValidation Acuuarcy 24.937%\n",
      "Epoch: 30 \tTraining Loss: 0.01416417 \tValidation Loss 0.01471758 \tTraining Acuuarcy 25.025% \tValidation Acuuarcy 25.049%\n",
      "Epoch: 31 \tTraining Loss: 0.01414607 \tValidation Loss 0.01476336 \tTraining Acuuarcy 25.215% \tValidation Acuuarcy 25.021%\n",
      "Epoch: 32 \tTraining Loss: 0.01414740 \tValidation Loss 0.01485244 \tTraining Acuuarcy 25.070% \tValidation Acuuarcy 24.714%\n",
      "Epoch: 33 \tTraining Loss: 0.01414706 \tValidation Loss 0.01483592 \tTraining Acuuarcy 25.265% \tValidation Acuuarcy 25.049%\n",
      "Epoch: 34 \tTraining Loss: 0.01414667 \tValidation Loss 0.01479554 \tTraining Acuuarcy 25.131% \tValidation Acuuarcy 24.213%\n",
      "Epoch: 35 \tTraining Loss: 0.01414663 \tValidation Loss 0.01504020 \tTraining Acuuarcy 24.980% \tValidation Acuuarcy 24.603%\n",
      "Epoch: 36 \tTraining Loss: 0.01412292 \tValidation Loss 0.01483858 \tTraining Acuuarcy 25.109% \tValidation Acuuarcy 25.049%\n",
      "Epoch: 37 \tTraining Loss: 0.01412274 \tValidation Loss 0.01479164 \tTraining Acuuarcy 25.254% \tValidation Acuuarcy 24.659%\n",
      "Epoch: 38 \tTraining Loss: 0.01412175 \tValidation Loss 0.01486729 \tTraining Acuuarcy 25.070% \tValidation Acuuarcy 23.349%\n",
      "Epoch: 39 \tTraining Loss: 0.01410987 \tValidation Loss 0.01482361 \tTraining Acuuarcy 25.254% \tValidation Acuuarcy 24.519%\n",
      "Epoch: 40 \tTraining Loss: 0.01410419 \tValidation Loss 0.01489412 \tTraining Acuuarcy 25.248% \tValidation Acuuarcy 24.185%\n",
      "Epoch: 41 \tTraining Loss: 0.01407256 \tValidation Loss 0.01479952 \tTraining Acuuarcy 25.287% \tValidation Acuuarcy 23.516%\n",
      "Epoch: 42 \tTraining Loss: 0.01407546 \tValidation Loss 0.01489318 \tTraining Acuuarcy 25.376% \tValidation Acuuarcy 24.129%\n",
      "Epoch: 43 \tTraining Loss: 0.01408993 \tValidation Loss 0.01489834 \tTraining Acuuarcy 25.242% \tValidation Acuuarcy 23.656%\n",
      "Epoch: 44 \tTraining Loss: 0.01406948 \tValidation Loss 0.01495952 \tTraining Acuuarcy 25.287% \tValidation Acuuarcy 24.269%\n",
      "Epoch: 45 \tTraining Loss: 0.01406274 \tValidation Loss 0.01494010 \tTraining Acuuarcy 25.493% \tValidation Acuuarcy 23.990%\n",
      "Epoch: 46 \tTraining Loss: 0.01401891 \tValidation Loss 0.01499045 \tTraining Acuuarcy 25.566% \tValidation Acuuarcy 22.235%\n",
      "Epoch: 47 \tTraining Loss: 0.01401096 \tValidation Loss 0.01505672 \tTraining Acuuarcy 25.822% \tValidation Acuuarcy 23.182%\n",
      "Epoch: 48 \tTraining Loss: 0.01402453 \tValidation Loss 0.01500564 \tTraining Acuuarcy 25.577% \tValidation Acuuarcy 23.628%\n",
      "Epoch: 49 \tTraining Loss: 0.01399510 \tValidation Loss 0.01491130 \tTraining Acuuarcy 25.967% \tValidation Acuuarcy 22.235%\n",
      "Epoch: 50 \tTraining Loss: 0.01399104 \tValidation Loss 0.01511507 \tTraining Acuuarcy 25.761% \tValidation Acuuarcy 22.179%\n",
      "Epoch: 51 \tTraining Loss: 0.01397961 \tValidation Loss 0.01506601 \tTraining Acuuarcy 26.324% \tValidation Acuuarcy 22.959%\n",
      "Epoch: 52 \tTraining Loss: 0.01395198 \tValidation Loss 0.01516254 \tTraining Acuuarcy 26.413% \tValidation Acuuarcy 21.789%\n",
      "Epoch: 53 \tTraining Loss: 0.01393709 \tValidation Loss 0.01493146 \tTraining Acuuarcy 26.235% \tValidation Acuuarcy 22.151%\n",
      "Epoch: 54 \tTraining Loss: 0.01395148 \tValidation Loss 0.01501964 \tTraining Acuuarcy 26.602% \tValidation Acuuarcy 22.680%\n",
      "Epoch: 55 \tTraining Loss: 0.01390681 \tValidation Loss 0.01516774 \tTraining Acuuarcy 26.669% \tValidation Acuuarcy 22.179%\n",
      "Epoch: 56 \tTraining Loss: 0.01388641 \tValidation Loss 0.01507681 \tTraining Acuuarcy 26.993% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 57 \tTraining Loss: 0.01389006 \tValidation Loss 0.01523224 \tTraining Acuuarcy 26.625% \tValidation Acuuarcy 23.210%\n",
      "Epoch: 58 \tTraining Loss: 0.01386591 \tValidation Loss 0.01513640 \tTraining Acuuarcy 26.502% \tValidation Acuuarcy 22.875%\n",
      "Epoch: 59 \tTraining Loss: 0.01382929 \tValidation Loss 0.01514823 \tTraining Acuuarcy 26.909% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 60 \tTraining Loss: 0.01380015 \tValidation Loss 0.01530858 \tTraining Acuuarcy 27.645% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 61 \tTraining Loss: 0.01383425 \tValidation Loss 0.01515051 \tTraining Acuuarcy 26.892% \tValidation Acuuarcy 23.544%\n",
      "Epoch: 62 \tTraining Loss: 0.01379126 \tValidation Loss 0.01519820 \tTraining Acuuarcy 27.260% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 63 \tTraining Loss: 0.01376665 \tValidation Loss 0.01527885 \tTraining Acuuarcy 27.489% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 64 \tTraining Loss: 0.01376551 \tValidation Loss 0.01522462 \tTraining Acuuarcy 27.059% \tValidation Acuuarcy 22.597%\n",
      "Epoch: 65 \tTraining Loss: 0.01374880 \tValidation Loss 0.01532642 \tTraining Acuuarcy 28.102% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 66 \tTraining Loss: 0.01374482 \tValidation Loss 0.01541725 \tTraining Acuuarcy 27.622% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 67 \tTraining Loss: 0.01372918 \tValidation Loss 0.01540549 \tTraining Acuuarcy 27.728% \tValidation Acuuarcy 21.594%\n",
      "Epoch: 68 \tTraining Loss: 0.01371068 \tValidation Loss 0.01534552 \tTraining Acuuarcy 27.622% \tValidation Acuuarcy 22.374%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 \tTraining Loss: 0.01372191 \tValidation Loss 0.01521548 \tTraining Acuuarcy 27.572% \tValidation Acuuarcy 22.485%\n",
      "Epoch: 70 \tTraining Loss: 0.01370287 \tValidation Loss 0.01550523 \tTraining Acuuarcy 27.739% \tValidation Acuuarcy 21.900%\n",
      "Epoch: 71 \tTraining Loss: 0.01365932 \tValidation Loss 0.01549642 \tTraining Acuuarcy 28.063% \tValidation Acuuarcy 22.680%\n",
      "Epoch: 72 \tTraining Loss: 0.01363931 \tValidation Loss 0.01519855 \tTraining Acuuarcy 28.737% \tValidation Acuuarcy 22.736%\n",
      "Epoch: 73 \tTraining Loss: 0.01363458 \tValidation Loss 0.01528234 \tTraining Acuuarcy 28.442% \tValidation Acuuarcy 22.708%\n",
      "Epoch: 74 \tTraining Loss: 0.01365025 \tValidation Loss 0.01533955 \tTraining Acuuarcy 28.102% \tValidation Acuuarcy 21.510%\n",
      "Epoch: 75 \tTraining Loss: 0.01356224 \tValidation Loss 0.01556959 \tTraining Acuuarcy 28.821% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 76 \tTraining Loss: 0.01360358 \tValidation Loss 0.01538111 \tTraining Acuuarcy 28.230% \tValidation Acuuarcy 21.789%\n",
      "Epoch: 77 \tTraining Loss: 0.01361929 \tValidation Loss 0.01549850 \tTraining Acuuarcy 28.347% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 78 \tTraining Loss: 0.01362401 \tValidation Loss 0.01539242 \tTraining Acuuarcy 28.258% \tValidation Acuuarcy 22.012%\n",
      "Epoch: 79 \tTraining Loss: 0.01354425 \tValidation Loss 0.01546661 \tTraining Acuuarcy 28.865% \tValidation Acuuarcy 21.900%\n",
      "Epoch: 80 \tTraining Loss: 0.01354758 \tValidation Loss 0.01551116 \tTraining Acuuarcy 29.467% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 81 \tTraining Loss: 0.01353889 \tValidation Loss 0.01572950 \tTraining Acuuarcy 29.389% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 82 \tTraining Loss: 0.01356461 \tValidation Loss 0.01539975 \tTraining Acuuarcy 28.943% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 83 \tTraining Loss: 0.01352472 \tValidation Loss 0.01561460 \tTraining Acuuarcy 29.367% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 84 \tTraining Loss: 0.01352837 \tValidation Loss 0.01574046 \tTraining Acuuarcy 29.495% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 85 \tTraining Loss: 0.01352309 \tValidation Loss 0.01574132 \tTraining Acuuarcy 29.428% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 86 \tTraining Loss: 0.01351016 \tValidation Loss 0.01541124 \tTraining Acuuarcy 29.345% \tValidation Acuuarcy 22.151%\n",
      "Epoch: 87 \tTraining Loss: 0.01348312 \tValidation Loss 0.01556245 \tTraining Acuuarcy 29.228% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 88 \tTraining Loss: 0.01349119 \tValidation Loss 0.01559375 \tTraining Acuuarcy 29.835% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 89 \tTraining Loss: 0.01346761 \tValidation Loss 0.01566056 \tTraining Acuuarcy 29.450% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 90 \tTraining Loss: 0.01349211 \tValidation Loss 0.01559427 \tTraining Acuuarcy 29.673% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 91 \tTraining Loss: 0.01346729 \tValidation Loss 0.01554015 \tTraining Acuuarcy 29.517% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 92 \tTraining Loss: 0.01347208 \tValidation Loss 0.01562071 \tTraining Acuuarcy 29.406% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 93 \tTraining Loss: 0.01344940 \tValidation Loss 0.01564372 \tTraining Acuuarcy 30.013% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 94 \tTraining Loss: 0.01346240 \tValidation Loss 0.01562293 \tTraining Acuuarcy 29.540% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 95 \tTraining Loss: 0.01345421 \tValidation Loss 0.01563157 \tTraining Acuuarcy 29.941% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 96 \tTraining Loss: 0.01339898 \tValidation Loss 0.01581300 \tTraining Acuuarcy 30.181% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 97 \tTraining Loss: 0.01343491 \tValidation Loss 0.01584680 \tTraining Acuuarcy 29.997% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 98 \tTraining Loss: 0.01343368 \tValidation Loss 0.01586729 \tTraining Acuuarcy 29.902% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 99 \tTraining Loss: 0.01340535 \tValidation Loss 0.01569979 \tTraining Acuuarcy 30.398% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 100 \tTraining Loss: 0.01343764 \tValidation Loss 0.01576500 \tTraining Acuuarcy 29.751% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 101 \tTraining Loss: 0.01340575 \tValidation Loss 0.01573295 \tTraining Acuuarcy 30.682% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 102 \tTraining Loss: 0.01340702 \tValidation Loss 0.01568567 \tTraining Acuuarcy 30.348% \tValidation Acuuarcy 22.123%\n",
      "Epoch: 103 \tTraining Loss: 0.01342446 \tValidation Loss 0.01563002 \tTraining Acuuarcy 29.623% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 104 \tTraining Loss: 0.01339195 \tValidation Loss 0.01567236 \tTraining Acuuarcy 30.214% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 105 \tTraining Loss: 0.01338236 \tValidation Loss 0.01574003 \tTraining Acuuarcy 30.303% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 106 \tTraining Loss: 0.01337486 \tValidation Loss 0.01555457 \tTraining Acuuarcy 30.342% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 107 \tTraining Loss: 0.01333061 \tValidation Loss 0.01570940 \tTraining Acuuarcy 30.671% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 108 \tTraining Loss: 0.01335530 \tValidation Loss 0.01578223 \tTraining Acuuarcy 30.404% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 109 \tTraining Loss: 0.01336087 \tValidation Loss 0.01570091 \tTraining Acuuarcy 30.521% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 110 \tTraining Loss: 0.01336266 \tValidation Loss 0.01590477 \tTraining Acuuarcy 30.420% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 111 \tTraining Loss: 0.01335208 \tValidation Loss 0.01568956 \tTraining Acuuarcy 30.688% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 112 \tTraining Loss: 0.01334571 \tValidation Loss 0.01555401 \tTraining Acuuarcy 30.872% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 113 \tTraining Loss: 0.01333717 \tValidation Loss 0.01599583 \tTraining Acuuarcy 30.504% \tValidation Acuuarcy 22.179%\n",
      "Epoch: 114 \tTraining Loss: 0.01338813 \tValidation Loss 0.01554212 \tTraining Acuuarcy 30.197% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 115 \tTraining Loss: 0.01332697 \tValidation Loss 0.01588375 \tTraining Acuuarcy 30.576% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 116 \tTraining Loss: 0.01332529 \tValidation Loss 0.01606597 \tTraining Acuuarcy 31.173% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 117 \tTraining Loss: 0.01332347 \tValidation Loss 0.01584093 \tTraining Acuuarcy 30.727% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 118 \tTraining Loss: 0.01329669 \tValidation Loss 0.01586744 \tTraining Acuuarcy 30.682% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 119 \tTraining Loss: 0.01329851 \tValidation Loss 0.01593055 \tTraining Acuuarcy 30.415% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 120 \tTraining Loss: 0.01336561 \tValidation Loss 0.01588738 \tTraining Acuuarcy 30.342% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 121 \tTraining Loss: 0.01331808 \tValidation Loss 0.01576742 \tTraining Acuuarcy 30.704% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 122 \tTraining Loss: 0.01328855 \tValidation Loss 0.01588430 \tTraining Acuuarcy 31.083% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 123 \tTraining Loss: 0.01330383 \tValidation Loss 0.01625923 \tTraining Acuuarcy 30.788% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 124 \tTraining Loss: 0.01329081 \tValidation Loss 0.01588200 \tTraining Acuuarcy 31.245% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 125 \tTraining Loss: 0.01325333 \tValidation Loss 0.01590038 \tTraining Acuuarcy 31.050% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 126 \tTraining Loss: 0.01329640 \tValidation Loss 0.01576734 \tTraining Acuuarcy 31.111% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 127 \tTraining Loss: 0.01328816 \tValidation Loss 0.01584844 \tTraining Acuuarcy 30.822% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 128 \tTraining Loss: 0.01327744 \tValidation Loss 0.01583905 \tTraining Acuuarcy 31.195% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 129 \tTraining Loss: 0.01327328 \tValidation Loss 0.01594694 \tTraining Acuuarcy 30.632% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 130 \tTraining Loss: 0.01326238 \tValidation Loss 0.01589808 \tTraining Acuuarcy 31.295% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 131 \tTraining Loss: 0.01326487 \tValidation Loss 0.01597051 \tTraining Acuuarcy 31.083% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 132 \tTraining Loss: 0.01322515 \tValidation Loss 0.01610132 \tTraining Acuuarcy 31.535% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 133 \tTraining Loss: 0.01321835 \tValidation Loss 0.01589033 \tTraining Acuuarcy 31.691% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 134 \tTraining Loss: 0.01324201 \tValidation Loss 0.01594468 \tTraining Acuuarcy 31.189% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 135 \tTraining Loss: 0.01328672 \tValidation Loss 0.01569232 \tTraining Acuuarcy 31.111% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 136 \tTraining Loss: 0.01327498 \tValidation Loss 0.01580002 \tTraining Acuuarcy 31.284% \tValidation Acuuarcy 19.699%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137 \tTraining Loss: 0.01321997 \tValidation Loss 0.01584192 \tTraining Acuuarcy 31.524% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 138 \tTraining Loss: 0.01325650 \tValidation Loss 0.01592120 \tTraining Acuuarcy 31.033% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 139 \tTraining Loss: 0.01322333 \tValidation Loss 0.01597426 \tTraining Acuuarcy 31.050% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 140 \tTraining Loss: 0.01325507 \tValidation Loss 0.01607505 \tTraining Acuuarcy 31.763% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 141 \tTraining Loss: 0.01323214 \tValidation Loss 0.01577486 \tTraining Acuuarcy 31.290% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 142 \tTraining Loss: 0.01325384 \tValidation Loss 0.01614321 \tTraining Acuuarcy 31.240% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 143 \tTraining Loss: 0.01318843 \tValidation Loss 0.01612738 \tTraining Acuuarcy 31.563% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 144 \tTraining Loss: 0.01321114 \tValidation Loss 0.01579926 \tTraining Acuuarcy 31.485% \tValidation Acuuarcy 21.845%\n",
      "Epoch: 145 \tTraining Loss: 0.01320498 \tValidation Loss 0.01583318 \tTraining Acuuarcy 31.808% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 146 \tTraining Loss: 0.01318973 \tValidation Loss 0.01603860 \tTraining Acuuarcy 31.808% \tValidation Acuuarcy 22.346%\n",
      "Epoch: 147 \tTraining Loss: 0.01315463 \tValidation Loss 0.01591631 \tTraining Acuuarcy 31.836% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 148 \tTraining Loss: 0.01314722 \tValidation Loss 0.01586169 \tTraining Acuuarcy 32.037% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 149 \tTraining Loss: 0.01322419 \tValidation Loss 0.01611817 \tTraining Acuuarcy 31.624% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 150 \tTraining Loss: 0.01317815 \tValidation Loss 0.01591609 \tTraining Acuuarcy 31.674% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 151 \tTraining Loss: 0.01316223 \tValidation Loss 0.01605408 \tTraining Acuuarcy 31.942% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 152 \tTraining Loss: 0.01316777 \tValidation Loss 0.01604045 \tTraining Acuuarcy 31.763% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 153 \tTraining Loss: 0.01318603 \tValidation Loss 0.01600552 \tTraining Acuuarcy 32.254% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 154 \tTraining Loss: 0.01317024 \tValidation Loss 0.01607883 \tTraining Acuuarcy 31.607% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 155 \tTraining Loss: 0.01316581 \tValidation Loss 0.01577167 \tTraining Acuuarcy 31.786% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 156 \tTraining Loss: 0.01315815 \tValidation Loss 0.01612478 \tTraining Acuuarcy 31.931% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 157 \tTraining Loss: 0.01317306 \tValidation Loss 0.01634402 \tTraining Acuuarcy 31.490% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 158 \tTraining Loss: 0.01315423 \tValidation Loss 0.01612320 \tTraining Acuuarcy 31.847% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 159 \tTraining Loss: 0.01315453 \tValidation Loss 0.01597288 \tTraining Acuuarcy 32.092% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 160 \tTraining Loss: 0.01312922 \tValidation Loss 0.01609577 \tTraining Acuuarcy 31.936% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 161 \tTraining Loss: 0.01316493 \tValidation Loss 0.01604606 \tTraining Acuuarcy 31.947% \tValidation Acuuarcy 21.204%\n",
      "Epoch: 162 \tTraining Loss: 0.01314591 \tValidation Loss 0.01587740 \tTraining Acuuarcy 32.271% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 163 \tTraining Loss: 0.01311316 \tValidation Loss 0.01606386 \tTraining Acuuarcy 32.538% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 164 \tTraining Loss: 0.01315935 \tValidation Loss 0.01592241 \tTraining Acuuarcy 32.232% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 165 \tTraining Loss: 0.01315739 \tValidation Loss 0.01599787 \tTraining Acuuarcy 31.925% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 166 \tTraining Loss: 0.01313368 \tValidation Loss 0.01598622 \tTraining Acuuarcy 32.410% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 167 \tTraining Loss: 0.01312386 \tValidation Loss 0.01608460 \tTraining Acuuarcy 32.560% \tValidation Acuuarcy 21.649%\n",
      "Epoch: 168 \tTraining Loss: 0.01309373 \tValidation Loss 0.01609558 \tTraining Acuuarcy 32.304% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 169 \tTraining Loss: 0.01313900 \tValidation Loss 0.01611345 \tTraining Acuuarcy 32.349% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 170 \tTraining Loss: 0.01308616 \tValidation Loss 0.01602374 \tTraining Acuuarcy 32.566% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 171 \tTraining Loss: 0.01309134 \tValidation Loss 0.01615639 \tTraining Acuuarcy 32.449% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 172 \tTraining Loss: 0.01308957 \tValidation Loss 0.01607476 \tTraining Acuuarcy 32.477% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 173 \tTraining Loss: 0.01311228 \tValidation Loss 0.01602783 \tTraining Acuuarcy 32.510% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 174 \tTraining Loss: 0.01310380 \tValidation Loss 0.01611190 \tTraining Acuuarcy 32.594% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 175 \tTraining Loss: 0.01310019 \tValidation Loss 0.01626782 \tTraining Acuuarcy 32.733% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 176 \tTraining Loss: 0.01311289 \tValidation Loss 0.01602183 \tTraining Acuuarcy 32.488% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 177 \tTraining Loss: 0.01313790 \tValidation Loss 0.01603235 \tTraining Acuuarcy 32.421% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 178 \tTraining Loss: 0.01309586 \tValidation Loss 0.01631143 \tTraining Acuuarcy 32.482% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 179 \tTraining Loss: 0.01309060 \tValidation Loss 0.01631171 \tTraining Acuuarcy 32.639% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 180 \tTraining Loss: 0.01305762 \tValidation Loss 0.01614902 \tTraining Acuuarcy 32.990% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 181 \tTraining Loss: 0.01307786 \tValidation Loss 0.01604087 \tTraining Acuuarcy 32.555% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 182 \tTraining Loss: 0.01305694 \tValidation Loss 0.01614828 \tTraining Acuuarcy 32.934% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 183 \tTraining Loss: 0.01307750 \tValidation Loss 0.01614470 \tTraining Acuuarcy 32.728% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 184 \tTraining Loss: 0.01305893 \tValidation Loss 0.01634206 \tTraining Acuuarcy 32.471% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 185 \tTraining Loss: 0.01301838 \tValidation Loss 0.01632444 \tTraining Acuuarcy 32.834% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 186 \tTraining Loss: 0.01310058 \tValidation Loss 0.01610751 \tTraining Acuuarcy 32.488% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 187 \tTraining Loss: 0.01307752 \tValidation Loss 0.01601318 \tTraining Acuuarcy 32.806% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 188 \tTraining Loss: 0.01308367 \tValidation Loss 0.01672633 \tTraining Acuuarcy 32.912% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 189 \tTraining Loss: 0.01310161 \tValidation Loss 0.01597292 \tTraining Acuuarcy 32.198% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 190 \tTraining Loss: 0.01305425 \tValidation Loss 0.01625643 \tTraining Acuuarcy 32.928% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 191 \tTraining Loss: 0.01304543 \tValidation Loss 0.01640707 \tTraining Acuuarcy 32.990% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 192 \tTraining Loss: 0.01309443 \tValidation Loss 0.01614425 \tTraining Acuuarcy 32.315% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 193 \tTraining Loss: 0.01302354 \tValidation Loss 0.01598796 \tTraining Acuuarcy 32.750% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 194 \tTraining Loss: 0.01305574 \tValidation Loss 0.01610663 \tTraining Acuuarcy 32.639% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 195 \tTraining Loss: 0.01301553 \tValidation Loss 0.01613262 \tTraining Acuuarcy 32.778% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 196 \tTraining Loss: 0.01305534 \tValidation Loss 0.01616766 \tTraining Acuuarcy 32.772% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 197 \tTraining Loss: 0.01302897 \tValidation Loss 0.01629546 \tTraining Acuuarcy 32.711% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 198 \tTraining Loss: 0.01306602 \tValidation Loss 0.01602156 \tTraining Acuuarcy 32.717% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 199 \tTraining Loss: 0.01301543 \tValidation Loss 0.01627663 \tTraining Acuuarcy 33.073% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 200 \tTraining Loss: 0.01302179 \tValidation Loss 0.01600854 \tTraining Acuuarcy 32.900% \tValidation Acuuarcy 21.761%\n",
      "Epoch: 201 \tTraining Loss: 0.01305354 \tValidation Loss 0.01600207 \tTraining Acuuarcy 32.616% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 202 \tTraining Loss: 0.01300136 \tValidation Loss 0.01596477 \tTraining Acuuarcy 33.135% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 203 \tTraining Loss: 0.01303123 \tValidation Loss 0.01613435 \tTraining Acuuarcy 32.594% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 204 \tTraining Loss: 0.01306789 \tValidation Loss 0.01630681 \tTraining Acuuarcy 32.767% \tValidation Acuuarcy 20.033%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205 \tTraining Loss: 0.01304996 \tValidation Loss 0.01626994 \tTraining Acuuarcy 32.577% \tValidation Acuuarcy 21.482%\n",
      "Epoch: 206 \tTraining Loss: 0.01300032 \tValidation Loss 0.01617190 \tTraining Acuuarcy 33.430% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 207 \tTraining Loss: 0.01301729 \tValidation Loss 0.01598470 \tTraining Acuuarcy 33.062% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 208 \tTraining Loss: 0.01300821 \tValidation Loss 0.01605085 \tTraining Acuuarcy 33.157% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 209 \tTraining Loss: 0.01297496 \tValidation Loss 0.01633999 \tTraining Acuuarcy 33.090% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 210 \tTraining Loss: 0.01298509 \tValidation Loss 0.01627250 \tTraining Acuuarcy 33.475% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 211 \tTraining Loss: 0.01299634 \tValidation Loss 0.01619699 \tTraining Acuuarcy 32.728% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 212 \tTraining Loss: 0.01301078 \tValidation Loss 0.01607370 \tTraining Acuuarcy 33.185% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 213 \tTraining Loss: 0.01299219 \tValidation Loss 0.01680899 \tTraining Acuuarcy 33.525% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 214 \tTraining Loss: 0.01300532 \tValidation Loss 0.01674498 \tTraining Acuuarcy 33.374% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 215 \tTraining Loss: 0.01300913 \tValidation Loss 0.01611281 \tTraining Acuuarcy 33.101% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 216 \tTraining Loss: 0.01301203 \tValidation Loss 0.01604807 \tTraining Acuuarcy 33.380% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 217 \tTraining Loss: 0.01308530 \tValidation Loss 0.01592331 \tTraining Acuuarcy 32.800% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 218 \tTraining Loss: 0.01299275 \tValidation Loss 0.01609068 \tTraining Acuuarcy 33.051% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 219 \tTraining Loss: 0.01302607 \tValidation Loss 0.01605608 \tTraining Acuuarcy 33.068% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 220 \tTraining Loss: 0.01296417 \tValidation Loss 0.01633453 \tTraining Acuuarcy 33.157% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 221 \tTraining Loss: 0.01297652 \tValidation Loss 0.01630550 \tTraining Acuuarcy 33.240% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 222 \tTraining Loss: 0.01299366 \tValidation Loss 0.01658148 \tTraining Acuuarcy 33.508% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 223 \tTraining Loss: 0.01302972 \tValidation Loss 0.01604705 \tTraining Acuuarcy 33.001% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 224 \tTraining Loss: 0.01293578 \tValidation Loss 0.01646867 \tTraining Acuuarcy 33.848% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 225 \tTraining Loss: 0.01303859 \tValidation Loss 0.01614268 \tTraining Acuuarcy 33.001% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 226 \tTraining Loss: 0.01296756 \tValidation Loss 0.01645242 \tTraining Acuuarcy 33.508% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 227 \tTraining Loss: 0.01303101 \tValidation Loss 0.01603635 \tTraining Acuuarcy 32.917% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 228 \tTraining Loss: 0.01296986 \tValidation Loss 0.01638867 \tTraining Acuuarcy 33.536% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 229 \tTraining Loss: 0.01295757 \tValidation Loss 0.01660800 \tTraining Acuuarcy 33.291% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 230 \tTraining Loss: 0.01299781 \tValidation Loss 0.01622700 \tTraining Acuuarcy 33.363% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 231 \tTraining Loss: 0.01297236 \tValidation Loss 0.01606197 \tTraining Acuuarcy 33.374% \tValidation Acuuarcy 21.538%\n",
      "Epoch: 232 \tTraining Loss: 0.01290035 \tValidation Loss 0.01638317 \tTraining Acuuarcy 33.647% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 233 \tTraining Loss: 0.01299549 \tValidation Loss 0.01622550 \tTraining Acuuarcy 33.675% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 234 \tTraining Loss: 0.01298912 \tValidation Loss 0.01630524 \tTraining Acuuarcy 33.252% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 235 \tTraining Loss: 0.01293863 \tValidation Loss 0.01632892 \tTraining Acuuarcy 33.547% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 236 \tTraining Loss: 0.01294675 \tValidation Loss 0.01638689 \tTraining Acuuarcy 33.525% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 237 \tTraining Loss: 0.01293130 \tValidation Loss 0.01640797 \tTraining Acuuarcy 33.809% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 238 \tTraining Loss: 0.01298285 \tValidation Loss 0.01598400 \tTraining Acuuarcy 33.090% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 239 \tTraining Loss: 0.01295577 \tValidation Loss 0.01602413 \tTraining Acuuarcy 33.826% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 240 \tTraining Loss: 0.01295175 \tValidation Loss 0.01638776 \tTraining Acuuarcy 33.870% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 241 \tTraining Loss: 0.01295967 \tValidation Loss 0.01625926 \tTraining Acuuarcy 33.854% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 242 \tTraining Loss: 0.01290962 \tValidation Loss 0.01625271 \tTraining Acuuarcy 33.463% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 243 \tTraining Loss: 0.01290172 \tValidation Loss 0.01621086 \tTraining Acuuarcy 34.361% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 244 \tTraining Loss: 0.01292096 \tValidation Loss 0.01610325 \tTraining Acuuarcy 33.764% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 245 \tTraining Loss: 0.01293966 \tValidation Loss 0.01640728 \tTraining Acuuarcy 33.363% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 246 \tTraining Loss: 0.01296911 \tValidation Loss 0.01640277 \tTraining Acuuarcy 33.508% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 247 \tTraining Loss: 0.01291394 \tValidation Loss 0.01651163 \tTraining Acuuarcy 33.664% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 248 \tTraining Loss: 0.01295612 \tValidation Loss 0.01633674 \tTraining Acuuarcy 32.978% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 249 \tTraining Loss: 0.01293178 \tValidation Loss 0.01620561 \tTraining Acuuarcy 33.636% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 250 \tTraining Loss: 0.01297297 \tValidation Loss 0.01625585 \tTraining Acuuarcy 33.068% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 251 \tTraining Loss: 0.01300840 \tValidation Loss 0.01621364 \tTraining Acuuarcy 33.402% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 252 \tTraining Loss: 0.01294284 \tValidation Loss 0.01623507 \tTraining Acuuarcy 33.257% \tValidation Acuuarcy 21.677%\n",
      "Epoch: 253 \tTraining Loss: 0.01296598 \tValidation Loss 0.01650219 \tTraining Acuuarcy 33.508% \tValidation Acuuarcy 21.148%\n",
      "Epoch: 254 \tTraining Loss: 0.01296879 \tValidation Loss 0.01634195 \tTraining Acuuarcy 33.547% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 255 \tTraining Loss: 0.01293376 \tValidation Loss 0.01620242 \tTraining Acuuarcy 33.413% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 256 \tTraining Loss: 0.01293170 \tValidation Loss 0.01636248 \tTraining Acuuarcy 33.530% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 257 \tTraining Loss: 0.01297066 \tValidation Loss 0.01615598 \tTraining Acuuarcy 33.692% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 258 \tTraining Loss: 0.01294662 \tValidation Loss 0.01623753 \tTraining Acuuarcy 33.586% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 259 \tTraining Loss: 0.01298426 \tValidation Loss 0.01661361 \tTraining Acuuarcy 33.268% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 260 \tTraining Loss: 0.01292312 \tValidation Loss 0.01630899 \tTraining Acuuarcy 33.959% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 261 \tTraining Loss: 0.01290841 \tValidation Loss 0.01624074 \tTraining Acuuarcy 33.948% \tValidation Acuuarcy 21.622%\n",
      "Epoch: 262 \tTraining Loss: 0.01295990 \tValidation Loss 0.01631432 \tTraining Acuuarcy 33.497% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 263 \tTraining Loss: 0.01291739 \tValidation Loss 0.01614884 \tTraining Acuuarcy 33.703% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 264 \tTraining Loss: 0.01294405 \tValidation Loss 0.01618274 \tTraining Acuuarcy 33.436% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 265 \tTraining Loss: 0.01292730 \tValidation Loss 0.01638731 \tTraining Acuuarcy 33.781% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 266 \tTraining Loss: 0.01295136 \tValidation Loss 0.01611624 \tTraining Acuuarcy 33.012% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 267 \tTraining Loss: 0.01289572 \tValidation Loss 0.01625027 \tTraining Acuuarcy 34.561% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 268 \tTraining Loss: 0.01292018 \tValidation Loss 0.01632260 \tTraining Acuuarcy 33.536% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 269 \tTraining Loss: 0.01290642 \tValidation Loss 0.01612117 \tTraining Acuuarcy 34.154% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 270 \tTraining Loss: 0.01292847 \tValidation Loss 0.01632674 \tTraining Acuuarcy 33.993% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 271 \tTraining Loss: 0.01290512 \tValidation Loss 0.01620660 \tTraining Acuuarcy 33.887% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 272 \tTraining Loss: 0.01294649 \tValidation Loss 0.01631092 \tTraining Acuuarcy 33.592% \tValidation Acuuarcy 20.284%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273 \tTraining Loss: 0.01293703 \tValidation Loss 0.01616326 \tTraining Acuuarcy 33.597% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 274 \tTraining Loss: 0.01288184 \tValidation Loss 0.01634635 \tTraining Acuuarcy 33.959% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 275 \tTraining Loss: 0.01294882 \tValidation Loss 0.01596949 \tTraining Acuuarcy 33.697% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 276 \tTraining Loss: 0.01290103 \tValidation Loss 0.01677658 \tTraining Acuuarcy 33.759% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 277 \tTraining Loss: 0.01283435 \tValidation Loss 0.01652635 \tTraining Acuuarcy 34.076% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 278 \tTraining Loss: 0.01291677 \tValidation Loss 0.01616128 \tTraining Acuuarcy 33.775% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 279 \tTraining Loss: 0.01290207 \tValidation Loss 0.01620744 \tTraining Acuuarcy 33.837% \tValidation Acuuarcy 21.064%\n",
      "Epoch: 280 \tTraining Loss: 0.01292972 \tValidation Loss 0.01627472 \tTraining Acuuarcy 33.826% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 281 \tTraining Loss: 0.01287935 \tValidation Loss 0.01668081 \tTraining Acuuarcy 33.764% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 282 \tTraining Loss: 0.01286730 \tValidation Loss 0.01617760 \tTraining Acuuarcy 34.205% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 283 \tTraining Loss: 0.01285630 \tValidation Loss 0.01644894 \tTraining Acuuarcy 33.753% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 284 \tTraining Loss: 0.01291944 \tValidation Loss 0.01632613 \tTraining Acuuarcy 33.876% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 285 \tTraining Loss: 0.01285482 \tValidation Loss 0.01633218 \tTraining Acuuarcy 34.149% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 286 \tTraining Loss: 0.01288264 \tValidation Loss 0.01641332 \tTraining Acuuarcy 34.199% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 287 \tTraining Loss: 0.01289028 \tValidation Loss 0.01650805 \tTraining Acuuarcy 34.138% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 288 \tTraining Loss: 0.01288050 \tValidation Loss 0.01622147 \tTraining Acuuarcy 33.854% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 289 \tTraining Loss: 0.01289870 \tValidation Loss 0.01654901 \tTraining Acuuarcy 33.279% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 290 \tTraining Loss: 0.01289706 \tValidation Loss 0.01653245 \tTraining Acuuarcy 34.093% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 291 \tTraining Loss: 0.01292158 \tValidation Loss 0.01629557 \tTraining Acuuarcy 33.396% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 292 \tTraining Loss: 0.01290116 \tValidation Loss 0.01650807 \tTraining Acuuarcy 33.798% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 293 \tTraining Loss: 0.01286698 \tValidation Loss 0.01625448 \tTraining Acuuarcy 34.132% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 294 \tTraining Loss: 0.01291020 \tValidation Loss 0.01641861 \tTraining Acuuarcy 34.104% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 295 \tTraining Loss: 0.01288495 \tValidation Loss 0.01663322 \tTraining Acuuarcy 34.049% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 296 \tTraining Loss: 0.01291635 \tValidation Loss 0.01631174 \tTraining Acuuarcy 33.748% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 297 \tTraining Loss: 0.01291361 \tValidation Loss 0.01638885 \tTraining Acuuarcy 34.154% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 298 \tTraining Loss: 0.01291958 \tValidation Loss 0.01623507 \tTraining Acuuarcy 33.792% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 299 \tTraining Loss: 0.01288485 \tValidation Loss 0.01634230 \tTraining Acuuarcy 34.110% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 300 \tTraining Loss: 0.01283867 \tValidation Loss 0.01660450 \tTraining Acuuarcy 34.422% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 301 \tTraining Loss: 0.01288405 \tValidation Loss 0.01652705 \tTraining Acuuarcy 34.043% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 302 \tTraining Loss: 0.01293473 \tValidation Loss 0.01650404 \tTraining Acuuarcy 33.798% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 303 \tTraining Loss: 0.01285102 \tValidation Loss 0.01652017 \tTraining Acuuarcy 34.099% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 304 \tTraining Loss: 0.01285750 \tValidation Loss 0.01634459 \tTraining Acuuarcy 34.121% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 305 \tTraining Loss: 0.01289418 \tValidation Loss 0.01652757 \tTraining Acuuarcy 33.870% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 306 \tTraining Loss: 0.01282281 \tValidation Loss 0.01650680 \tTraining Acuuarcy 34.444% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 307 \tTraining Loss: 0.01290261 \tValidation Loss 0.01637750 \tTraining Acuuarcy 33.787% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 308 \tTraining Loss: 0.01288780 \tValidation Loss 0.01634272 \tTraining Acuuarcy 34.272% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 309 \tTraining Loss: 0.01286880 \tValidation Loss 0.01657941 \tTraining Acuuarcy 34.115% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 310 \tTraining Loss: 0.01285053 \tValidation Loss 0.01626692 \tTraining Acuuarcy 34.522% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 311 \tTraining Loss: 0.01286857 \tValidation Loss 0.01639141 \tTraining Acuuarcy 33.954% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 312 \tTraining Loss: 0.01286313 \tValidation Loss 0.01625757 \tTraining Acuuarcy 33.887% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 313 \tTraining Loss: 0.01286598 \tValidation Loss 0.01632118 \tTraining Acuuarcy 34.188% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 314 \tTraining Loss: 0.01290008 \tValidation Loss 0.01643616 \tTraining Acuuarcy 34.099% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 315 \tTraining Loss: 0.01283596 \tValidation Loss 0.01637852 \tTraining Acuuarcy 34.210% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 316 \tTraining Loss: 0.01291545 \tValidation Loss 0.01653261 \tTraining Acuuarcy 34.004% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 317 \tTraining Loss: 0.01288876 \tValidation Loss 0.01628760 \tTraining Acuuarcy 33.932% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 318 \tTraining Loss: 0.01290001 \tValidation Loss 0.01630377 \tTraining Acuuarcy 34.182% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 319 \tTraining Loss: 0.01287289 \tValidation Loss 0.01634919 \tTraining Acuuarcy 34.138% \tValidation Acuuarcy 21.733%\n",
      "Epoch: 320 \tTraining Loss: 0.01285125 \tValidation Loss 0.01672095 \tTraining Acuuarcy 33.954% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 321 \tTraining Loss: 0.01284681 \tValidation Loss 0.01633342 \tTraining Acuuarcy 34.004% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 322 \tTraining Loss: 0.01294079 \tValidation Loss 0.01644495 \tTraining Acuuarcy 33.346% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 323 \tTraining Loss: 0.01285969 \tValidation Loss 0.01627540 \tTraining Acuuarcy 34.545% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 324 \tTraining Loss: 0.01282035 \tValidation Loss 0.01631865 \tTraining Acuuarcy 34.294% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 325 \tTraining Loss: 0.01285951 \tValidation Loss 0.01657749 \tTraining Acuuarcy 34.422% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 326 \tTraining Loss: 0.01285101 \tValidation Loss 0.01642809 \tTraining Acuuarcy 33.932% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 327 \tTraining Loss: 0.01279415 \tValidation Loss 0.01640345 \tTraining Acuuarcy 34.322% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 328 \tTraining Loss: 0.01283953 \tValidation Loss 0.01670804 \tTraining Acuuarcy 34.166% \tValidation Acuuarcy 21.176%\n",
      "Epoch: 329 \tTraining Loss: 0.01290703 \tValidation Loss 0.01618779 \tTraining Acuuarcy 33.798% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 330 \tTraining Loss: 0.01277235 \tValidation Loss 0.01643734 \tTraining Acuuarcy 34.773% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 331 \tTraining Loss: 0.01288912 \tValidation Loss 0.01647297 \tTraining Acuuarcy 33.937% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 332 \tTraining Loss: 0.01288341 \tValidation Loss 0.01649116 \tTraining Acuuarcy 34.060% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 333 \tTraining Loss: 0.01279045 \tValidation Loss 0.01647484 \tTraining Acuuarcy 34.918% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 334 \tTraining Loss: 0.01284424 \tValidation Loss 0.01652631 \tTraining Acuuarcy 34.099% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 335 \tTraining Loss: 0.01280141 \tValidation Loss 0.01638460 \tTraining Acuuarcy 34.712% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 336 \tTraining Loss: 0.01278317 \tValidation Loss 0.01639097 \tTraining Acuuarcy 34.701% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 337 \tTraining Loss: 0.01279910 \tValidation Loss 0.01659949 \tTraining Acuuarcy 34.628% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 338 \tTraining Loss: 0.01291452 \tValidation Loss 0.01657389 \tTraining Acuuarcy 33.670% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 339 \tTraining Loss: 0.01276958 \tValidation Loss 0.01679173 \tTraining Acuuarcy 35.069% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 340 \tTraining Loss: 0.01287187 \tValidation Loss 0.01640845 \tTraining Acuuarcy 34.182% \tValidation Acuuarcy 20.925%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 341 \tTraining Loss: 0.01283056 \tValidation Loss 0.01632709 \tTraining Acuuarcy 34.701% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 342 \tTraining Loss: 0.01288364 \tValidation Loss 0.01645122 \tTraining Acuuarcy 33.798% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 343 \tTraining Loss: 0.01286734 \tValidation Loss 0.01647144 \tTraining Acuuarcy 34.115% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 344 \tTraining Loss: 0.01286639 \tValidation Loss 0.01653827 \tTraining Acuuarcy 33.893% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 345 \tTraining Loss: 0.01285416 \tValidation Loss 0.01669831 \tTraining Acuuarcy 34.182% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 346 \tTraining Loss: 0.01285294 \tValidation Loss 0.01630676 \tTraining Acuuarcy 34.227% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 347 \tTraining Loss: 0.01281924 \tValidation Loss 0.01619435 \tTraining Acuuarcy 34.645% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 348 \tTraining Loss: 0.01284041 \tValidation Loss 0.01622278 \tTraining Acuuarcy 34.606% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 349 \tTraining Loss: 0.01282800 \tValidation Loss 0.01624212 \tTraining Acuuarcy 34.812% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 350 \tTraining Loss: 0.01282912 \tValidation Loss 0.01648081 \tTraining Acuuarcy 34.673% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 351 \tTraining Loss: 0.01285620 \tValidation Loss 0.01645602 \tTraining Acuuarcy 34.450% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 352 \tTraining Loss: 0.01277333 \tValidation Loss 0.01651017 \tTraining Acuuarcy 34.511% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 353 \tTraining Loss: 0.01288360 \tValidation Loss 0.01638538 \tTraining Acuuarcy 33.909% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 354 \tTraining Loss: 0.01287649 \tValidation Loss 0.01653101 \tTraining Acuuarcy 34.210% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 355 \tTraining Loss: 0.01283540 \tValidation Loss 0.01644516 \tTraining Acuuarcy 34.701% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 356 \tTraining Loss: 0.01282892 \tValidation Loss 0.01632040 \tTraining Acuuarcy 34.316% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 357 \tTraining Loss: 0.01285166 \tValidation Loss 0.01637460 \tTraining Acuuarcy 34.138% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 358 \tTraining Loss: 0.01284365 \tValidation Loss 0.01642034 \tTraining Acuuarcy 34.015% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 359 \tTraining Loss: 0.01278634 \tValidation Loss 0.01628084 \tTraining Acuuarcy 34.327% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 360 \tTraining Loss: 0.01281218 \tValidation Loss 0.01690880 \tTraining Acuuarcy 34.561% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 361 \tTraining Loss: 0.01283313 \tValidation Loss 0.01672954 \tTraining Acuuarcy 34.455% \tValidation Acuuarcy 21.259%\n",
      "Epoch: 362 \tTraining Loss: 0.01275838 \tValidation Loss 0.01655538 \tTraining Acuuarcy 34.896% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 363 \tTraining Loss: 0.01283582 \tValidation Loss 0.01649305 \tTraining Acuuarcy 34.121% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 364 \tTraining Loss: 0.01278027 \tValidation Loss 0.01648216 \tTraining Acuuarcy 35.080% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 365 \tTraining Loss: 0.01281624 \tValidation Loss 0.01673125 \tTraining Acuuarcy 34.584% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 366 \tTraining Loss: 0.01280665 \tValidation Loss 0.01631003 \tTraining Acuuarcy 34.511% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 367 \tTraining Loss: 0.01280779 \tValidation Loss 0.01658848 \tTraining Acuuarcy 34.467% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 368 \tTraining Loss: 0.01283072 \tValidation Loss 0.01641527 \tTraining Acuuarcy 33.965% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 369 \tTraining Loss: 0.01287937 \tValidation Loss 0.01653277 \tTraining Acuuarcy 34.338% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 370 \tTraining Loss: 0.01282011 \tValidation Loss 0.01638473 \tTraining Acuuarcy 34.283% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 371 \tTraining Loss: 0.01281463 \tValidation Loss 0.01647631 \tTraining Acuuarcy 34.483% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 372 \tTraining Loss: 0.01283060 \tValidation Loss 0.01644300 \tTraining Acuuarcy 34.684% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 373 \tTraining Loss: 0.01278402 \tValidation Loss 0.01654566 \tTraining Acuuarcy 34.639% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 374 \tTraining Loss: 0.01285290 \tValidation Loss 0.01653554 \tTraining Acuuarcy 34.294% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 375 \tTraining Loss: 0.01276411 \tValidation Loss 0.01652084 \tTraining Acuuarcy 34.260% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 376 \tTraining Loss: 0.01280886 \tValidation Loss 0.01643705 \tTraining Acuuarcy 34.422% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 377 \tTraining Loss: 0.01283534 \tValidation Loss 0.01631882 \tTraining Acuuarcy 34.439% \tValidation Acuuarcy 21.371%\n",
      "Epoch: 378 \tTraining Loss: 0.01278217 \tValidation Loss 0.01652419 \tTraining Acuuarcy 34.968% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 379 \tTraining Loss: 0.01272627 \tValidation Loss 0.01694998 \tTraining Acuuarcy 34.745% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 380 \tTraining Loss: 0.01276849 \tValidation Loss 0.01660411 \tTraining Acuuarcy 34.834% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 381 \tTraining Loss: 0.01282314 \tValidation Loss 0.01639538 \tTraining Acuuarcy 34.550% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 382 \tTraining Loss: 0.01281175 \tValidation Loss 0.01622461 \tTraining Acuuarcy 34.299% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 383 \tTraining Loss: 0.01281336 \tValidation Loss 0.01648173 \tTraining Acuuarcy 34.595% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 384 \tTraining Loss: 0.01278012 \tValidation Loss 0.01649014 \tTraining Acuuarcy 34.912% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 385 \tTraining Loss: 0.01273170 \tValidation Loss 0.01677333 \tTraining Acuuarcy 35.052% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 386 \tTraining Loss: 0.01276185 \tValidation Loss 0.01650936 \tTraining Acuuarcy 34.690% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 387 \tTraining Loss: 0.01283523 \tValidation Loss 0.01694796 \tTraining Acuuarcy 34.678% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 388 \tTraining Loss: 0.01278303 \tValidation Loss 0.01647335 \tTraining Acuuarcy 34.912% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 389 \tTraining Loss: 0.01280411 \tValidation Loss 0.01639713 \tTraining Acuuarcy 34.533% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 390 \tTraining Loss: 0.01281332 \tValidation Loss 0.01646149 \tTraining Acuuarcy 34.344% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 391 \tTraining Loss: 0.01275544 \tValidation Loss 0.01640404 \tTraining Acuuarcy 34.734% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 392 \tTraining Loss: 0.01276037 \tValidation Loss 0.01656068 \tTraining Acuuarcy 35.314% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 393 \tTraining Loss: 0.01279215 \tValidation Loss 0.01631265 \tTraining Acuuarcy 34.338% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 394 \tTraining Loss: 0.01282807 \tValidation Loss 0.01655483 \tTraining Acuuarcy 34.166% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 395 \tTraining Loss: 0.01274983 \tValidation Loss 0.01673286 \tTraining Acuuarcy 34.912% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 396 \tTraining Loss: 0.01274722 \tValidation Loss 0.01621963 \tTraining Acuuarcy 35.325% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 397 \tTraining Loss: 0.01278007 \tValidation Loss 0.01664937 \tTraining Acuuarcy 34.834% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 398 \tTraining Loss: 0.01277255 \tValidation Loss 0.01670589 \tTraining Acuuarcy 34.667% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 399 \tTraining Loss: 0.01274523 \tValidation Loss 0.01642640 \tTraining Acuuarcy 35.069% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 400 \tTraining Loss: 0.01277514 \tValidation Loss 0.01648841 \tTraining Acuuarcy 34.957% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 401 \tTraining Loss: 0.01279632 \tValidation Loss 0.01664645 \tTraining Acuuarcy 34.935% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 402 \tTraining Loss: 0.01282178 \tValidation Loss 0.01676845 \tTraining Acuuarcy 34.556% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 403 \tTraining Loss: 0.01277770 \tValidation Loss 0.01673429 \tTraining Acuuarcy 35.063% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 404 \tTraining Loss: 0.01274448 \tValidation Loss 0.01659029 \tTraining Acuuarcy 35.052% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 405 \tTraining Loss: 0.01278317 \tValidation Loss 0.01652345 \tTraining Acuuarcy 34.522% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 406 \tTraining Loss: 0.01284196 \tValidation Loss 0.01648528 \tTraining Acuuarcy 34.160% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 407 \tTraining Loss: 0.01278848 \tValidation Loss 0.01651391 \tTraining Acuuarcy 34.600% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 408 \tTraining Loss: 0.01280702 \tValidation Loss 0.01664302 \tTraining Acuuarcy 34.244% \tValidation Acuuarcy 18.891%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 409 \tTraining Loss: 0.01274582 \tValidation Loss 0.01662913 \tTraining Acuuarcy 35.113% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 410 \tTraining Loss: 0.01277624 \tValidation Loss 0.01652954 \tTraining Acuuarcy 34.940% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 411 \tTraining Loss: 0.01283419 \tValidation Loss 0.01631707 \tTraining Acuuarcy 34.617% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 412 \tTraining Loss: 0.01281728 \tValidation Loss 0.01643306 \tTraining Acuuarcy 34.873% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 413 \tTraining Loss: 0.01280640 \tValidation Loss 0.01648160 \tTraining Acuuarcy 34.511% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 414 \tTraining Loss: 0.01282015 \tValidation Loss 0.01630789 \tTraining Acuuarcy 34.171% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 415 \tTraining Loss: 0.01276330 \tValidation Loss 0.01649450 \tTraining Acuuarcy 35.319% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 416 \tTraining Loss: 0.01276161 \tValidation Loss 0.01649846 \tTraining Acuuarcy 35.018% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 417 \tTraining Loss: 0.01279805 \tValidation Loss 0.01666311 \tTraining Acuuarcy 34.678% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 418 \tTraining Loss: 0.01279797 \tValidation Loss 0.01678515 \tTraining Acuuarcy 34.667% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 419 \tTraining Loss: 0.01275722 \tValidation Loss 0.01681780 \tTraining Acuuarcy 34.991% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 420 \tTraining Loss: 0.01271584 \tValidation Loss 0.01652943 \tTraining Acuuarcy 35.080% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 421 \tTraining Loss: 0.01273497 \tValidation Loss 0.01660235 \tTraining Acuuarcy 34.578% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 422 \tTraining Loss: 0.01277398 \tValidation Loss 0.01663797 \tTraining Acuuarcy 34.600% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 423 \tTraining Loss: 0.01277317 \tValidation Loss 0.01652754 \tTraining Acuuarcy 34.929% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 424 \tTraining Loss: 0.01277028 \tValidation Loss 0.01646411 \tTraining Acuuarcy 35.119% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 425 \tTraining Loss: 0.01279777 \tValidation Loss 0.01668859 \tTraining Acuuarcy 34.773% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 426 \tTraining Loss: 0.01267077 \tValidation Loss 0.01642499 \tTraining Acuuarcy 35.381% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 427 \tTraining Loss: 0.01271833 \tValidation Loss 0.01646381 \tTraining Acuuarcy 35.046% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 428 \tTraining Loss: 0.01277749 \tValidation Loss 0.01633944 \tTraining Acuuarcy 35.096% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 429 \tTraining Loss: 0.01277954 \tValidation Loss 0.01657244 \tTraining Acuuarcy 35.113% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 430 \tTraining Loss: 0.01278460 \tValidation Loss 0.01656703 \tTraining Acuuarcy 34.439% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 431 \tTraining Loss: 0.01282814 \tValidation Loss 0.01635773 \tTraining Acuuarcy 34.483% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 432 \tTraining Loss: 0.01281418 \tValidation Loss 0.01658984 \tTraining Acuuarcy 34.751% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 433 \tTraining Loss: 0.01275247 \tValidation Loss 0.01645574 \tTraining Acuuarcy 34.974% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 434 \tTraining Loss: 0.01278238 \tValidation Loss 0.01632280 \tTraining Acuuarcy 34.890% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 435 \tTraining Loss: 0.01274713 \tValidation Loss 0.01653093 \tTraining Acuuarcy 35.342% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 436 \tTraining Loss: 0.01274434 \tValidation Loss 0.01686800 \tTraining Acuuarcy 34.991% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 437 \tTraining Loss: 0.01277098 \tValidation Loss 0.01672953 \tTraining Acuuarcy 34.639% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 438 \tTraining Loss: 0.01271861 \tValidation Loss 0.01665121 \tTraining Acuuarcy 35.108% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 439 \tTraining Loss: 0.01273628 \tValidation Loss 0.01673590 \tTraining Acuuarcy 35.459% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 440 \tTraining Loss: 0.01267403 \tValidation Loss 0.01672818 \tTraining Acuuarcy 35.069% \tValidation Acuuarcy 21.454%\n",
      "Epoch: 441 \tTraining Loss: 0.01280997 \tValidation Loss 0.01645351 \tTraining Acuuarcy 34.829% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 442 \tTraining Loss: 0.01273902 \tValidation Loss 0.01662700 \tTraining Acuuarcy 35.163% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 443 \tTraining Loss: 0.01279278 \tValidation Loss 0.01676192 \tTraining Acuuarcy 34.712% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 444 \tTraining Loss: 0.01276844 \tValidation Loss 0.01672015 \tTraining Acuuarcy 34.734% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 445 \tTraining Loss: 0.01277162 \tValidation Loss 0.01668345 \tTraining Acuuarcy 34.801% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 446 \tTraining Loss: 0.01272300 \tValidation Loss 0.01673826 \tTraining Acuuarcy 35.130% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 447 \tTraining Loss: 0.01277614 \tValidation Loss 0.01652489 \tTraining Acuuarcy 34.773% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 448 \tTraining Loss: 0.01274226 \tValidation Loss 0.01652739 \tTraining Acuuarcy 35.024% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 449 \tTraining Loss: 0.01271363 \tValidation Loss 0.01645351 \tTraining Acuuarcy 35.336% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 450 \tTraining Loss: 0.01277438 \tValidation Loss 0.01661064 \tTraining Acuuarcy 34.550% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 451 \tTraining Loss: 0.01272111 \tValidation Loss 0.01639073 \tTraining Acuuarcy 35.319% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 452 \tTraining Loss: 0.01277101 \tValidation Loss 0.01642745 \tTraining Acuuarcy 34.751% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 453 \tTraining Loss: 0.01275986 \tValidation Loss 0.01652276 \tTraining Acuuarcy 34.729% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 454 \tTraining Loss: 0.01278392 \tValidation Loss 0.01641816 \tTraining Acuuarcy 34.377% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 455 \tTraining Loss: 0.01280525 \tValidation Loss 0.01666730 \tTraining Acuuarcy 34.467% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 456 \tTraining Loss: 0.01272976 \tValidation Loss 0.01644831 \tTraining Acuuarcy 35.291% \tValidation Acuuarcy 20.981%\n",
      "Epoch: 457 \tTraining Loss: 0.01270497 \tValidation Loss 0.01673303 \tTraining Acuuarcy 34.996% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 458 \tTraining Loss: 0.01273168 \tValidation Loss 0.01662002 \tTraining Acuuarcy 34.952% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 459 \tTraining Loss: 0.01273597 \tValidation Loss 0.01653430 \tTraining Acuuarcy 35.102% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 460 \tTraining Loss: 0.01270595 \tValidation Loss 0.01661199 \tTraining Acuuarcy 35.877% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 461 \tTraining Loss: 0.01273938 \tValidation Loss 0.01650986 \tTraining Acuuarcy 35.130% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 462 \tTraining Loss: 0.01273062 \tValidation Loss 0.01663260 \tTraining Acuuarcy 35.141% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 463 \tTraining Loss: 0.01277340 \tValidation Loss 0.01653025 \tTraining Acuuarcy 35.147% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 464 \tTraining Loss: 0.01272150 \tValidation Loss 0.01651213 \tTraining Acuuarcy 35.470% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 465 \tTraining Loss: 0.01272679 \tValidation Loss 0.01665158 \tTraining Acuuarcy 34.974% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 466 \tTraining Loss: 0.01278650 \tValidation Loss 0.01657224 \tTraining Acuuarcy 34.885% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 467 \tTraining Loss: 0.01274047 \tValidation Loss 0.01668750 \tTraining Acuuarcy 34.952% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 468 \tTraining Loss: 0.01278689 \tValidation Loss 0.01678843 \tTraining Acuuarcy 34.896% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 469 \tTraining Loss: 0.01272299 \tValidation Loss 0.01653679 \tTraining Acuuarcy 34.851% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 470 \tTraining Loss: 0.01271248 \tValidation Loss 0.01642637 \tTraining Acuuarcy 35.174% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 471 \tTraining Loss: 0.01282459 \tValidation Loss 0.01646904 \tTraining Acuuarcy 34.717% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 472 \tTraining Loss: 0.01269487 \tValidation Loss 0.01678093 \tTraining Acuuarcy 35.514% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 473 \tTraining Loss: 0.01271833 \tValidation Loss 0.01653671 \tTraining Acuuarcy 34.991% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 474 \tTraining Loss: 0.01273166 \tValidation Loss 0.01664616 \tTraining Acuuarcy 34.717% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 475 \tTraining Loss: 0.01276547 \tValidation Loss 0.01664899 \tTraining Acuuarcy 34.885% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 476 \tTraining Loss: 0.01269733 \tValidation Loss 0.01656078 \tTraining Acuuarcy 35.370% \tValidation Acuuarcy 19.448%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477 \tTraining Loss: 0.01268604 \tValidation Loss 0.01684961 \tTraining Acuuarcy 35.135% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 478 \tTraining Loss: 0.01276298 \tValidation Loss 0.01663783 \tTraining Acuuarcy 34.818% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 479 \tTraining Loss: 0.01271110 \tValidation Loss 0.01663578 \tTraining Acuuarcy 35.130% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 480 \tTraining Loss: 0.01270679 \tValidation Loss 0.01652762 \tTraining Acuuarcy 35.436% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 481 \tTraining Loss: 0.01270239 \tValidation Loss 0.01664544 \tTraining Acuuarcy 35.018% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 482 \tTraining Loss: 0.01273670 \tValidation Loss 0.01646606 \tTraining Acuuarcy 35.063% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 483 \tTraining Loss: 0.01276734 \tValidation Loss 0.01676630 \tTraining Acuuarcy 34.617% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 484 \tTraining Loss: 0.01274868 \tValidation Loss 0.01646931 \tTraining Acuuarcy 35.069% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 485 \tTraining Loss: 0.01269612 \tValidation Loss 0.01663996 \tTraining Acuuarcy 35.169% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 486 \tTraining Loss: 0.01274716 \tValidation Loss 0.01655816 \tTraining Acuuarcy 35.236% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 487 \tTraining Loss: 0.01277204 \tValidation Loss 0.01662428 \tTraining Acuuarcy 34.684% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 488 \tTraining Loss: 0.01276054 \tValidation Loss 0.01652381 \tTraining Acuuarcy 35.074% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 489 \tTraining Loss: 0.01273189 \tValidation Loss 0.01663512 \tTraining Acuuarcy 35.626% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 490 \tTraining Loss: 0.01276008 \tValidation Loss 0.01644629 \tTraining Acuuarcy 34.795% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 491 \tTraining Loss: 0.01276562 \tValidation Loss 0.01667193 \tTraining Acuuarcy 34.522% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 492 \tTraining Loss: 0.01266945 \tValidation Loss 0.01684647 \tTraining Acuuarcy 35.308% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 493 \tTraining Loss: 0.01279657 \tValidation Loss 0.01671336 \tTraining Acuuarcy 34.840% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 494 \tTraining Loss: 0.01270092 \tValidation Loss 0.01649908 \tTraining Acuuarcy 35.431% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 495 \tTraining Loss: 0.01269093 \tValidation Loss 0.01692339 \tTraining Acuuarcy 34.974% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 496 \tTraining Loss: 0.01273378 \tValidation Loss 0.01663907 \tTraining Acuuarcy 35.353% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 497 \tTraining Loss: 0.01269284 \tValidation Loss 0.01664276 \tTraining Acuuarcy 35.431% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 498 \tTraining Loss: 0.01272565 \tValidation Loss 0.01652316 \tTraining Acuuarcy 35.124% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 499 \tTraining Loss: 0.01267982 \tValidation Loss 0.01646375 \tTraining Acuuarcy 35.615% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 500 \tTraining Loss: 0.01273351 \tValidation Loss 0.01690808 \tTraining Acuuarcy 35.074% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 501 \tTraining Loss: 0.01271861 \tValidation Loss 0.01678887 \tTraining Acuuarcy 34.979% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 502 \tTraining Loss: 0.01274044 \tValidation Loss 0.01660157 \tTraining Acuuarcy 34.901% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 503 \tTraining Loss: 0.01277273 \tValidation Loss 0.01647575 \tTraining Acuuarcy 34.751% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 504 \tTraining Loss: 0.01269461 \tValidation Loss 0.01640998 \tTraining Acuuarcy 34.957% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 505 \tTraining Loss: 0.01272443 \tValidation Loss 0.01626458 \tTraining Acuuarcy 34.868% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 506 \tTraining Loss: 0.01273477 \tValidation Loss 0.01645194 \tTraining Acuuarcy 34.818% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 507 \tTraining Loss: 0.01275758 \tValidation Loss 0.01663038 \tTraining Acuuarcy 35.225% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 508 \tTraining Loss: 0.01272505 \tValidation Loss 0.01697516 \tTraining Acuuarcy 34.968% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 509 \tTraining Loss: 0.01272591 \tValidation Loss 0.01669913 \tTraining Acuuarcy 34.907% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 510 \tTraining Loss: 0.01266759 \tValidation Loss 0.01658653 \tTraining Acuuarcy 35.241% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 511 \tTraining Loss: 0.01265379 \tValidation Loss 0.01684512 \tTraining Acuuarcy 35.358% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 512 \tTraining Loss: 0.01267564 \tValidation Loss 0.01680292 \tTraining Acuuarcy 35.247% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 513 \tTraining Loss: 0.01269102 \tValidation Loss 0.01649699 \tTraining Acuuarcy 34.807% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 514 \tTraining Loss: 0.01273427 \tValidation Loss 0.01638807 \tTraining Acuuarcy 34.684% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 515 \tTraining Loss: 0.01271788 \tValidation Loss 0.01627978 \tTraining Acuuarcy 35.113% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 516 \tTraining Loss: 0.01270221 \tValidation Loss 0.01643017 \tTraining Acuuarcy 35.325% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 517 \tTraining Loss: 0.01271625 \tValidation Loss 0.01641376 \tTraining Acuuarcy 35.163% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 518 \tTraining Loss: 0.01269232 \tValidation Loss 0.01672941 \tTraining Acuuarcy 35.141% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 519 \tTraining Loss: 0.01271816 \tValidation Loss 0.01652274 \tTraining Acuuarcy 34.929% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 520 \tTraining Loss: 0.01272090 \tValidation Loss 0.01658012 \tTraining Acuuarcy 34.600% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 521 \tTraining Loss: 0.01267287 \tValidation Loss 0.01673782 \tTraining Acuuarcy 35.698% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 522 \tTraining Loss: 0.01267471 \tValidation Loss 0.01671586 \tTraining Acuuarcy 35.319% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 523 \tTraining Loss: 0.01270043 \tValidation Loss 0.01666611 \tTraining Acuuarcy 35.866% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 524 \tTraining Loss: 0.01270453 \tValidation Loss 0.01670323 \tTraining Acuuarcy 35.598% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 525 \tTraining Loss: 0.01276067 \tValidation Loss 0.01658763 \tTraining Acuuarcy 35.108% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 526 \tTraining Loss: 0.01267447 \tValidation Loss 0.01652468 \tTraining Acuuarcy 35.286% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 527 \tTraining Loss: 0.01270072 \tValidation Loss 0.01649867 \tTraining Acuuarcy 35.765% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 528 \tTraining Loss: 0.01267637 \tValidation Loss 0.01689021 \tTraining Acuuarcy 35.531% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 529 \tTraining Loss: 0.01270512 \tValidation Loss 0.01660804 \tTraining Acuuarcy 35.442% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 530 \tTraining Loss: 0.01268551 \tValidation Loss 0.01694120 \tTraining Acuuarcy 35.035% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 531 \tTraining Loss: 0.01270446 \tValidation Loss 0.01667331 \tTraining Acuuarcy 35.230% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 532 \tTraining Loss: 0.01273842 \tValidation Loss 0.01663861 \tTraining Acuuarcy 34.628% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 533 \tTraining Loss: 0.01263145 \tValidation Loss 0.01652741 \tTraining Acuuarcy 35.693% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 534 \tTraining Loss: 0.01274556 \tValidation Loss 0.01699531 \tTraining Acuuarcy 34.991% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 535 \tTraining Loss: 0.01275773 \tValidation Loss 0.01679908 \tTraining Acuuarcy 34.795% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 536 \tTraining Loss: 0.01264937 \tValidation Loss 0.01660824 \tTraining Acuuarcy 35.637% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 537 \tTraining Loss: 0.01261326 \tValidation Loss 0.01685423 \tTraining Acuuarcy 35.916% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 538 \tTraining Loss: 0.01269719 \tValidation Loss 0.01670218 \tTraining Acuuarcy 34.968% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 539 \tTraining Loss: 0.01273800 \tValidation Loss 0.01634295 \tTraining Acuuarcy 34.740% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 540 \tTraining Loss: 0.01270683 \tValidation Loss 0.01643868 \tTraining Acuuarcy 35.291% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 541 \tTraining Loss: 0.01274555 \tValidation Loss 0.01699583 \tTraining Acuuarcy 35.336% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 542 \tTraining Loss: 0.01264590 \tValidation Loss 0.01679226 \tTraining Acuuarcy 35.955% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 543 \tTraining Loss: 0.01269923 \tValidation Loss 0.01682826 \tTraining Acuuarcy 35.163% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 544 \tTraining Loss: 0.01268705 \tValidation Loss 0.01649005 \tTraining Acuuarcy 35.113% \tValidation Acuuarcy 19.950%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 545 \tTraining Loss: 0.01267632 \tValidation Loss 0.01649424 \tTraining Acuuarcy 35.598% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 546 \tTraining Loss: 0.01268014 \tValidation Loss 0.01676329 \tTraining Acuuarcy 35.587% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 547 \tTraining Loss: 0.01272415 \tValidation Loss 0.01644618 \tTraining Acuuarcy 34.996% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 548 \tTraining Loss: 0.01268881 \tValidation Loss 0.01662825 \tTraining Acuuarcy 35.074% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 549 \tTraining Loss: 0.01270549 \tValidation Loss 0.01707303 \tTraining Acuuarcy 35.397% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 550 \tTraining Loss: 0.01267579 \tValidation Loss 0.01703061 \tTraining Acuuarcy 35.241% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 551 \tTraining Loss: 0.01269558 \tValidation Loss 0.01673850 \tTraining Acuuarcy 35.609% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 552 \tTraining Loss: 0.01272087 \tValidation Loss 0.01649663 \tTraining Acuuarcy 35.241% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 553 \tTraining Loss: 0.01266031 \tValidation Loss 0.01659458 \tTraining Acuuarcy 35.481% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 554 \tTraining Loss: 0.01271015 \tValidation Loss 0.01670808 \tTraining Acuuarcy 35.280% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 555 \tTraining Loss: 0.01267883 \tValidation Loss 0.01674116 \tTraining Acuuarcy 35.286% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 556 \tTraining Loss: 0.01268532 \tValidation Loss 0.01658760 \tTraining Acuuarcy 35.715% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 557 \tTraining Loss: 0.01276916 \tValidation Loss 0.01629598 \tTraining Acuuarcy 35.018% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 558 \tTraining Loss: 0.01268977 \tValidation Loss 0.01686868 \tTraining Acuuarcy 35.520% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 559 \tTraining Loss: 0.01266527 \tValidation Loss 0.01670917 \tTraining Acuuarcy 35.631% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 560 \tTraining Loss: 0.01273925 \tValidation Loss 0.01645194 \tTraining Acuuarcy 35.024% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 561 \tTraining Loss: 0.01266244 \tValidation Loss 0.01662943 \tTraining Acuuarcy 35.431% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 562 \tTraining Loss: 0.01269038 \tValidation Loss 0.01653422 \tTraining Acuuarcy 35.710% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 563 \tTraining Loss: 0.01269048 \tValidation Loss 0.01694899 \tTraining Acuuarcy 35.514% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 564 \tTraining Loss: 0.01272219 \tValidation Loss 0.01686933 \tTraining Acuuarcy 35.085% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 565 \tTraining Loss: 0.01269601 \tValidation Loss 0.01674351 \tTraining Acuuarcy 35.013% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 566 \tTraining Loss: 0.01271021 \tValidation Loss 0.01676811 \tTraining Acuuarcy 35.592% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 567 \tTraining Loss: 0.01267322 \tValidation Loss 0.01640520 \tTraining Acuuarcy 35.252% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 568 \tTraining Loss: 0.01269724 \tValidation Loss 0.01651005 \tTraining Acuuarcy 35.542% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 569 \tTraining Loss: 0.01266980 \tValidation Loss 0.01646544 \tTraining Acuuarcy 35.565% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 570 \tTraining Loss: 0.01267942 \tValidation Loss 0.01642919 \tTraining Acuuarcy 35.370% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 571 \tTraining Loss: 0.01271786 \tValidation Loss 0.01663325 \tTraining Acuuarcy 35.069% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 572 \tTraining Loss: 0.01266899 \tValidation Loss 0.01670109 \tTraining Acuuarcy 35.676% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 573 \tTraining Loss: 0.01267906 \tValidation Loss 0.01663151 \tTraining Acuuarcy 34.996% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 574 \tTraining Loss: 0.01272053 \tValidation Loss 0.01656958 \tTraining Acuuarcy 34.873% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 575 \tTraining Loss: 0.01272237 \tValidation Loss 0.01669078 \tTraining Acuuarcy 35.091% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 576 \tTraining Loss: 0.01273120 \tValidation Loss 0.01656759 \tTraining Acuuarcy 35.085% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 577 \tTraining Loss: 0.01271471 \tValidation Loss 0.01688431 \tTraining Acuuarcy 35.626% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 578 \tTraining Loss: 0.01265220 \tValidation Loss 0.01668497 \tTraining Acuuarcy 35.141% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 579 \tTraining Loss: 0.01275738 \tValidation Loss 0.01675108 \tTraining Acuuarcy 34.991% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 580 \tTraining Loss: 0.01269748 \tValidation Loss 0.01682299 \tTraining Acuuarcy 35.370% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 581 \tTraining Loss: 0.01268343 \tValidation Loss 0.01641228 \tTraining Acuuarcy 35.581% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 582 \tTraining Loss: 0.01263792 \tValidation Loss 0.01664162 \tTraining Acuuarcy 35.938% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 583 \tTraining Loss: 0.01266438 \tValidation Loss 0.01674218 \tTraining Acuuarcy 35.576% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 584 \tTraining Loss: 0.01262484 \tValidation Loss 0.01648132 \tTraining Acuuarcy 35.676% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 585 \tTraining Loss: 0.01265552 \tValidation Loss 0.01696912 \tTraining Acuuarcy 35.631% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 586 \tTraining Loss: 0.01267258 \tValidation Loss 0.01657109 \tTraining Acuuarcy 35.821% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 587 \tTraining Loss: 0.01268184 \tValidation Loss 0.01669320 \tTraining Acuuarcy 35.353% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 588 \tTraining Loss: 0.01272550 \tValidation Loss 0.01670843 \tTraining Acuuarcy 34.862% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 589 \tTraining Loss: 0.01265196 \tValidation Loss 0.01689057 \tTraining Acuuarcy 35.425% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 590 \tTraining Loss: 0.01270266 \tValidation Loss 0.01673268 \tTraining Acuuarcy 35.754% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 591 \tTraining Loss: 0.01265493 \tValidation Loss 0.01659230 \tTraining Acuuarcy 35.576% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 592 \tTraining Loss: 0.01264770 \tValidation Loss 0.01663326 \tTraining Acuuarcy 35.743% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 593 \tTraining Loss: 0.01264351 \tValidation Loss 0.01707533 \tTraining Acuuarcy 35.392% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 594 \tTraining Loss: 0.01270868 \tValidation Loss 0.01641410 \tTraining Acuuarcy 35.531% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 595 \tTraining Loss: 0.01271648 \tValidation Loss 0.01637751 \tTraining Acuuarcy 35.124% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 596 \tTraining Loss: 0.01270272 \tValidation Loss 0.01656121 \tTraining Acuuarcy 35.158% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 597 \tTraining Loss: 0.01274291 \tValidation Loss 0.01675922 \tTraining Acuuarcy 35.247% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 598 \tTraining Loss: 0.01263137 \tValidation Loss 0.01652714 \tTraining Acuuarcy 36.027% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 599 \tTraining Loss: 0.01262116 \tValidation Loss 0.01656036 \tTraining Acuuarcy 35.988% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 600 \tTraining Loss: 0.01266505 \tValidation Loss 0.01665371 \tTraining Acuuarcy 35.565% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 601 \tTraining Loss: 0.01274417 \tValidation Loss 0.01681561 \tTraining Acuuarcy 34.929% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 602 \tTraining Loss: 0.01274027 \tValidation Loss 0.01663259 \tTraining Acuuarcy 35.057% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 603 \tTraining Loss: 0.01270587 \tValidation Loss 0.01699000 \tTraining Acuuarcy 35.409% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 604 \tTraining Loss: 0.01262895 \tValidation Loss 0.01668371 \tTraining Acuuarcy 35.319% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 605 \tTraining Loss: 0.01267556 \tValidation Loss 0.01671812 \tTraining Acuuarcy 35.219% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 606 \tTraining Loss: 0.01260496 \tValidation Loss 0.01706954 \tTraining Acuuarcy 35.698% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 607 \tTraining Loss: 0.01268386 \tValidation Loss 0.01674621 \tTraining Acuuarcy 35.420% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 608 \tTraining Loss: 0.01266799 \tValidation Loss 0.01662339 \tTraining Acuuarcy 35.659% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 609 \tTraining Loss: 0.01266686 \tValidation Loss 0.01672962 \tTraining Acuuarcy 35.475% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 610 \tTraining Loss: 0.01269172 \tValidation Loss 0.01664006 \tTraining Acuuarcy 35.258% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 611 \tTraining Loss: 0.01266675 \tValidation Loss 0.01692312 \tTraining Acuuarcy 35.687% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 612 \tTraining Loss: 0.01265025 \tValidation Loss 0.01673470 \tTraining Acuuarcy 35.620% \tValidation Acuuarcy 19.476%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 613 \tTraining Loss: 0.01264632 \tValidation Loss 0.01690268 \tTraining Acuuarcy 35.810% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 614 \tTraining Loss: 0.01270306 \tValidation Loss 0.01674938 \tTraining Acuuarcy 35.264% \tValidation Acuuarcy 21.037%\n",
      "Epoch: 615 \tTraining Loss: 0.01261675 \tValidation Loss 0.01676556 \tTraining Acuuarcy 36.038% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 616 \tTraining Loss: 0.01266640 \tValidation Loss 0.01691573 \tTraining Acuuarcy 35.654% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 617 \tTraining Loss: 0.01267873 \tValidation Loss 0.01690302 \tTraining Acuuarcy 35.358% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 618 \tTraining Loss: 0.01267223 \tValidation Loss 0.01670878 \tTraining Acuuarcy 35.548% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 619 \tTraining Loss: 0.01263424 \tValidation Loss 0.01643438 \tTraining Acuuarcy 35.871% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 620 \tTraining Loss: 0.01270161 \tValidation Loss 0.01636060 \tTraining Acuuarcy 35.520% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 621 \tTraining Loss: 0.01268863 \tValidation Loss 0.01669809 \tTraining Acuuarcy 35.431% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 622 \tTraining Loss: 0.01263345 \tValidation Loss 0.01681184 \tTraining Acuuarcy 35.347% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 623 \tTraining Loss: 0.01259150 \tValidation Loss 0.01659072 \tTraining Acuuarcy 36.328% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 624 \tTraining Loss: 0.01265429 \tValidation Loss 0.01666425 \tTraining Acuuarcy 35.487% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 625 \tTraining Loss: 0.01266549 \tValidation Loss 0.01680941 \tTraining Acuuarcy 35.492% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 626 \tTraining Loss: 0.01267144 \tValidation Loss 0.01662771 \tTraining Acuuarcy 35.643% \tValidation Acuuarcy 21.315%\n",
      "Epoch: 627 \tTraining Loss: 0.01265027 \tValidation Loss 0.01655339 \tTraining Acuuarcy 35.793% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 628 \tTraining Loss: 0.01265909 \tValidation Loss 0.01667130 \tTraining Acuuarcy 35.670% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 629 \tTraining Loss: 0.01262351 \tValidation Loss 0.01649786 \tTraining Acuuarcy 35.821% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 630 \tTraining Loss: 0.01265753 \tValidation Loss 0.01660250 \tTraining Acuuarcy 35.860% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 631 \tTraining Loss: 0.01271472 \tValidation Loss 0.01660786 \tTraining Acuuarcy 35.347% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 632 \tTraining Loss: 0.01266878 \tValidation Loss 0.01662801 \tTraining Acuuarcy 35.905% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 633 \tTraining Loss: 0.01263722 \tValidation Loss 0.01667595 \tTraining Acuuarcy 36.211% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 634 \tTraining Loss: 0.01260306 \tValidation Loss 0.01707048 \tTraining Acuuarcy 36.217% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 635 \tTraining Loss: 0.01266854 \tValidation Loss 0.01687960 \tTraining Acuuarcy 35.604% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 636 \tTraining Loss: 0.01264593 \tValidation Loss 0.01704193 \tTraining Acuuarcy 35.815% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 637 \tTraining Loss: 0.01270290 \tValidation Loss 0.01663333 \tTraining Acuuarcy 35.553% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 638 \tTraining Loss: 0.01264602 \tValidation Loss 0.01646364 \tTraining Acuuarcy 36.122% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 639 \tTraining Loss: 0.01264422 \tValidation Loss 0.01678386 \tTraining Acuuarcy 35.587% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 640 \tTraining Loss: 0.01256572 \tValidation Loss 0.01656960 \tTraining Acuuarcy 36.345% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 641 \tTraining Loss: 0.01265977 \tValidation Loss 0.01668130 \tTraining Acuuarcy 35.905% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 642 \tTraining Loss: 0.01265301 \tValidation Loss 0.01673314 \tTraining Acuuarcy 35.932% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 643 \tTraining Loss: 0.01263352 \tValidation Loss 0.01659197 \tTraining Acuuarcy 35.944% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 644 \tTraining Loss: 0.01266079 \tValidation Loss 0.01668359 \tTraining Acuuarcy 35.325% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 645 \tTraining Loss: 0.01263898 \tValidation Loss 0.01658543 \tTraining Acuuarcy 35.576% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 646 \tTraining Loss: 0.01268194 \tValidation Loss 0.01676156 \tTraining Acuuarcy 35.487% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 647 \tTraining Loss: 0.01262526 \tValidation Loss 0.01680513 \tTraining Acuuarcy 35.537% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 648 \tTraining Loss: 0.01260603 \tValidation Loss 0.01680555 \tTraining Acuuarcy 35.927% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 649 \tTraining Loss: 0.01263468 \tValidation Loss 0.01665915 \tTraining Acuuarcy 35.548% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 650 \tTraining Loss: 0.01261322 \tValidation Loss 0.01664188 \tTraining Acuuarcy 36.267% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 651 \tTraining Loss: 0.01262097 \tValidation Loss 0.01664001 \tTraining Acuuarcy 36.005% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 652 \tTraining Loss: 0.01261078 \tValidation Loss 0.01672415 \tTraining Acuuarcy 35.815% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 653 \tTraining Loss: 0.01262157 \tValidation Loss 0.01687709 \tTraining Acuuarcy 35.659% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 654 \tTraining Loss: 0.01268922 \tValidation Loss 0.01656001 \tTraining Acuuarcy 35.565% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 655 \tTraining Loss: 0.01268191 \tValidation Loss 0.01663252 \tTraining Acuuarcy 35.771% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 656 \tTraining Loss: 0.01262976 \tValidation Loss 0.01681744 \tTraining Acuuarcy 35.637% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 657 \tTraining Loss: 0.01265226 \tValidation Loss 0.01660817 \tTraining Acuuarcy 35.570% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 658 \tTraining Loss: 0.01267867 \tValidation Loss 0.01674805 \tTraining Acuuarcy 35.487% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 659 \tTraining Loss: 0.01265945 \tValidation Loss 0.01665837 \tTraining Acuuarcy 35.810% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 660 \tTraining Loss: 0.01264618 \tValidation Loss 0.01657132 \tTraining Acuuarcy 35.592% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 661 \tTraining Loss: 0.01260229 \tValidation Loss 0.01664840 \tTraining Acuuarcy 35.520% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 662 \tTraining Loss: 0.01264229 \tValidation Loss 0.01679881 \tTraining Acuuarcy 35.436% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 663 \tTraining Loss: 0.01266175 \tValidation Loss 0.01679273 \tTraining Acuuarcy 35.882% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 664 \tTraining Loss: 0.01263667 \tValidation Loss 0.01669770 \tTraining Acuuarcy 35.631% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 665 \tTraining Loss: 0.01265162 \tValidation Loss 0.01662610 \tTraining Acuuarcy 35.860% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 666 \tTraining Loss: 0.01264611 \tValidation Loss 0.01662647 \tTraining Acuuarcy 35.520% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 667 \tTraining Loss: 0.01264385 \tValidation Loss 0.01678264 \tTraining Acuuarcy 35.548% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 668 \tTraining Loss: 0.01266811 \tValidation Loss 0.01661629 \tTraining Acuuarcy 35.225% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 669 \tTraining Loss: 0.01259891 \tValidation Loss 0.01680613 \tTraining Acuuarcy 36.278% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 670 \tTraining Loss: 0.01263224 \tValidation Loss 0.01664108 \tTraining Acuuarcy 35.849% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 671 \tTraining Loss: 0.01260920 \tValidation Loss 0.01704978 \tTraining Acuuarcy 36.066% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 672 \tTraining Loss: 0.01264079 \tValidation Loss 0.01678533 \tTraining Acuuarcy 35.381% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 673 \tTraining Loss: 0.01264784 \tValidation Loss 0.01677448 \tTraining Acuuarcy 35.370% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 674 \tTraining Loss: 0.01263507 \tValidation Loss 0.01679790 \tTraining Acuuarcy 35.866% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 675 \tTraining Loss: 0.01271629 \tValidation Loss 0.01698266 \tTraining Acuuarcy 35.213% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 676 \tTraining Loss: 0.01259123 \tValidation Loss 0.01675491 \tTraining Acuuarcy 35.877% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 677 \tTraining Loss: 0.01262739 \tValidation Loss 0.01654062 \tTraining Acuuarcy 35.665% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 678 \tTraining Loss: 0.01267485 \tValidation Loss 0.01703891 \tTraining Acuuarcy 35.670% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 679 \tTraining Loss: 0.01261294 \tValidation Loss 0.01673306 \tTraining Acuuarcy 36.116% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 680 \tTraining Loss: 0.01266253 \tValidation Loss 0.01674950 \tTraining Acuuarcy 35.264% \tValidation Acuuarcy 20.674%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 681 \tTraining Loss: 0.01259906 \tValidation Loss 0.01679133 \tTraining Acuuarcy 36.222% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 682 \tTraining Loss: 0.01260112 \tValidation Loss 0.01701557 \tTraining Acuuarcy 36.150% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 683 \tTraining Loss: 0.01265286 \tValidation Loss 0.01653681 \tTraining Acuuarcy 35.693% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 684 \tTraining Loss: 0.01264079 \tValidation Loss 0.01663028 \tTraining Acuuarcy 35.737% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 685 \tTraining Loss: 0.01270591 \tValidation Loss 0.01660759 \tTraining Acuuarcy 35.252% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 686 \tTraining Loss: 0.01264688 \tValidation Loss 0.01680839 \tTraining Acuuarcy 35.481% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 687 \tTraining Loss: 0.01258565 \tValidation Loss 0.01681539 \tTraining Acuuarcy 36.049% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 688 \tTraining Loss: 0.01262245 \tValidation Loss 0.01667703 \tTraining Acuuarcy 36.250% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 689 \tTraining Loss: 0.01264119 \tValidation Loss 0.01658373 \tTraining Acuuarcy 35.921% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 690 \tTraining Loss: 0.01260760 \tValidation Loss 0.01665811 \tTraining Acuuarcy 35.710% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 691 \tTraining Loss: 0.01259036 \tValidation Loss 0.01691787 \tTraining Acuuarcy 35.665% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 692 \tTraining Loss: 0.01259510 \tValidation Loss 0.01696220 \tTraining Acuuarcy 36.791% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 693 \tTraining Loss: 0.01263225 \tValidation Loss 0.01670146 \tTraining Acuuarcy 35.955% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 694 \tTraining Loss: 0.01264182 \tValidation Loss 0.01678115 \tTraining Acuuarcy 35.598% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 695 \tTraining Loss: 0.01260856 \tValidation Loss 0.01671639 \tTraining Acuuarcy 36.077% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 696 \tTraining Loss: 0.01262975 \tValidation Loss 0.01670087 \tTraining Acuuarcy 36.384% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 697 \tTraining Loss: 0.01263937 \tValidation Loss 0.01680897 \tTraining Acuuarcy 35.877% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 698 \tTraining Loss: 0.01262047 \tValidation Loss 0.01667429 \tTraining Acuuarcy 36.178% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 699 \tTraining Loss: 0.01262633 \tValidation Loss 0.01698490 \tTraining Acuuarcy 35.944% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 700 \tTraining Loss: 0.01260162 \tValidation Loss 0.01698497 \tTraining Acuuarcy 36.334% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 701 \tTraining Loss: 0.01263096 \tValidation Loss 0.01687781 \tTraining Acuuarcy 36.066% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 702 \tTraining Loss: 0.01260365 \tValidation Loss 0.01669910 \tTraining Acuuarcy 35.698% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 703 \tTraining Loss: 0.01265763 \tValidation Loss 0.01655038 \tTraining Acuuarcy 35.503% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 704 \tTraining Loss: 0.01265175 \tValidation Loss 0.01677045 \tTraining Acuuarcy 35.698% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 705 \tTraining Loss: 0.01263516 \tValidation Loss 0.01667229 \tTraining Acuuarcy 36.211% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 706 \tTraining Loss: 0.01265245 \tValidation Loss 0.01671115 \tTraining Acuuarcy 35.815% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 707 \tTraining Loss: 0.01268188 \tValidation Loss 0.01673277 \tTraining Acuuarcy 35.938% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 708 \tTraining Loss: 0.01262866 \tValidation Loss 0.01685404 \tTraining Acuuarcy 35.615% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 709 \tTraining Loss: 0.01261187 \tValidation Loss 0.01680144 \tTraining Acuuarcy 36.178% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 710 \tTraining Loss: 0.01261772 \tValidation Loss 0.01687567 \tTraining Acuuarcy 35.715% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 711 \tTraining Loss: 0.01257993 \tValidation Loss 0.01697849 \tTraining Acuuarcy 36.729% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 712 \tTraining Loss: 0.01262853 \tValidation Loss 0.01675212 \tTraining Acuuarcy 36.362% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 713 \tTraining Loss: 0.01265343 \tValidation Loss 0.01700589 \tTraining Acuuarcy 35.737% \tValidation Acuuarcy 21.343%\n",
      "Epoch: 714 \tTraining Loss: 0.01260243 \tValidation Loss 0.01689400 \tTraining Acuuarcy 35.994% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 715 \tTraining Loss: 0.01265052 \tValidation Loss 0.01641521 \tTraining Acuuarcy 35.431% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 716 \tTraining Loss: 0.01266817 \tValidation Loss 0.01695104 \tTraining Acuuarcy 35.191% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 717 \tTraining Loss: 0.01260352 \tValidation Loss 0.01660644 \tTraining Acuuarcy 35.810% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 718 \tTraining Loss: 0.01264101 \tValidation Loss 0.01683175 \tTraining Acuuarcy 35.609% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 719 \tTraining Loss: 0.01251363 \tValidation Loss 0.01676978 \tTraining Acuuarcy 36.651% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 720 \tTraining Loss: 0.01257393 \tValidation Loss 0.01690306 \tTraining Acuuarcy 36.250% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 721 \tTraining Loss: 0.01259565 \tValidation Loss 0.01693249 \tTraining Acuuarcy 35.905% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 722 \tTraining Loss: 0.01269655 \tValidation Loss 0.01672576 \tTraining Acuuarcy 35.498% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 723 \tTraining Loss: 0.01255775 \tValidation Loss 0.01681601 \tTraining Acuuarcy 36.362% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 724 \tTraining Loss: 0.01265090 \tValidation Loss 0.01664215 \tTraining Acuuarcy 35.910% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 725 \tTraining Loss: 0.01266374 \tValidation Loss 0.01659007 \tTraining Acuuarcy 35.570% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 726 \tTraining Loss: 0.01265047 \tValidation Loss 0.01665129 \tTraining Acuuarcy 35.698% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 727 \tTraining Loss: 0.01256566 \tValidation Loss 0.01678800 \tTraining Acuuarcy 36.841% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 728 \tTraining Loss: 0.01260115 \tValidation Loss 0.01688357 \tTraining Acuuarcy 36.406% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 729 \tTraining Loss: 0.01265893 \tValidation Loss 0.01707935 \tTraining Acuuarcy 35.882% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 730 \tTraining Loss: 0.01261219 \tValidation Loss 0.01697671 \tTraining Acuuarcy 35.888% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 731 \tTraining Loss: 0.01257072 \tValidation Loss 0.01681497 \tTraining Acuuarcy 36.323% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 732 \tTraining Loss: 0.01260396 \tValidation Loss 0.01680603 \tTraining Acuuarcy 36.027% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 733 \tTraining Loss: 0.01255484 \tValidation Loss 0.01714136 \tTraining Acuuarcy 36.395% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 734 \tTraining Loss: 0.01259117 \tValidation Loss 0.01701951 \tTraining Acuuarcy 36.010% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 735 \tTraining Loss: 0.01262072 \tValidation Loss 0.01672397 \tTraining Acuuarcy 35.782% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 736 \tTraining Loss: 0.01258838 \tValidation Loss 0.01705509 \tTraining Acuuarcy 36.133% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 737 \tTraining Loss: 0.01252012 \tValidation Loss 0.01689814 \tTraining Acuuarcy 36.384% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 738 \tTraining Loss: 0.01262485 \tValidation Loss 0.01712836 \tTraining Acuuarcy 35.776% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 739 \tTraining Loss: 0.01263439 \tValidation Loss 0.01695118 \tTraining Acuuarcy 35.531% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 740 \tTraining Loss: 0.01261031 \tValidation Loss 0.01672874 \tTraining Acuuarcy 36.311% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 741 \tTraining Loss: 0.01258639 \tValidation Loss 0.01686508 \tTraining Acuuarcy 35.765% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 742 \tTraining Loss: 0.01256095 \tValidation Loss 0.01675160 \tTraining Acuuarcy 36.083% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 743 \tTraining Loss: 0.01264094 \tValidation Loss 0.01676670 \tTraining Acuuarcy 36.038% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 744 \tTraining Loss: 0.01263110 \tValidation Loss 0.01691348 \tTraining Acuuarcy 35.749% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 745 \tTraining Loss: 0.01265042 \tValidation Loss 0.01729514 \tTraining Acuuarcy 36.178% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 746 \tTraining Loss: 0.01260085 \tValidation Loss 0.01689059 \tTraining Acuuarcy 35.882% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 747 \tTraining Loss: 0.01265233 \tValidation Loss 0.01663635 \tTraining Acuuarcy 36.016% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 748 \tTraining Loss: 0.01257219 \tValidation Loss 0.01683061 \tTraining Acuuarcy 35.643% \tValidation Acuuarcy 19.393%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 749 \tTraining Loss: 0.01257517 \tValidation Loss 0.01676163 \tTraining Acuuarcy 36.473% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 750 \tTraining Loss: 0.01252973 \tValidation Loss 0.01678739 \tTraining Acuuarcy 36.685% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 751 \tTraining Loss: 0.01261600 \tValidation Loss 0.01656290 \tTraining Acuuarcy 35.927% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 752 \tTraining Loss: 0.01263710 \tValidation Loss 0.01663206 \tTraining Acuuarcy 35.481% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 753 \tTraining Loss: 0.01260794 \tValidation Loss 0.01702788 \tTraining Acuuarcy 36.155% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 754 \tTraining Loss: 0.01261983 \tValidation Loss 0.01677380 \tTraining Acuuarcy 36.038% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 755 \tTraining Loss: 0.01261816 \tValidation Loss 0.01659595 \tTraining Acuuarcy 35.977% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 756 \tTraining Loss: 0.01259950 \tValidation Loss 0.01721067 \tTraining Acuuarcy 35.631% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 757 \tTraining Loss: 0.01256468 \tValidation Loss 0.01696268 \tTraining Acuuarcy 36.350% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 758 \tTraining Loss: 0.01257532 \tValidation Loss 0.01698415 \tTraining Acuuarcy 36.222% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 759 \tTraining Loss: 0.01252368 \tValidation Loss 0.01680670 \tTraining Acuuarcy 36.217% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 760 \tTraining Loss: 0.01265664 \tValidation Loss 0.01679363 \tTraining Acuuarcy 35.955% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 761 \tTraining Loss: 0.01267524 \tValidation Loss 0.01667187 \tTraining Acuuarcy 35.403% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 762 \tTraining Loss: 0.01264410 \tValidation Loss 0.01693589 \tTraining Acuuarcy 35.509% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 763 \tTraining Loss: 0.01258437 \tValidation Loss 0.01691309 \tTraining Acuuarcy 36.233% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 764 \tTraining Loss: 0.01254023 \tValidation Loss 0.01716137 \tTraining Acuuarcy 36.256% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 765 \tTraining Loss: 0.01262547 \tValidation Loss 0.01643846 \tTraining Acuuarcy 35.810% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 766 \tTraining Loss: 0.01260866 \tValidation Loss 0.01674558 \tTraining Acuuarcy 36.016% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 767 \tTraining Loss: 0.01253520 \tValidation Loss 0.01672167 \tTraining Acuuarcy 36.919% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 768 \tTraining Loss: 0.01261157 \tValidation Loss 0.01660856 \tTraining Acuuarcy 36.094% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 769 \tTraining Loss: 0.01262665 \tValidation Loss 0.01671787 \tTraining Acuuarcy 35.899% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 770 \tTraining Loss: 0.01256241 \tValidation Loss 0.01695755 \tTraining Acuuarcy 35.949% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 771 \tTraining Loss: 0.01257610 \tValidation Loss 0.01661271 \tTraining Acuuarcy 35.999% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 772 \tTraining Loss: 0.01256525 \tValidation Loss 0.01680022 \tTraining Acuuarcy 36.635% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 773 \tTraining Loss: 0.01262585 \tValidation Loss 0.01687345 \tTraining Acuuarcy 36.239% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 774 \tTraining Loss: 0.01258953 \tValidation Loss 0.01689061 \tTraining Acuuarcy 36.362% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 775 \tTraining Loss: 0.01262574 \tValidation Loss 0.01682383 \tTraining Acuuarcy 35.893% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 776 \tTraining Loss: 0.01259145 \tValidation Loss 0.01643498 \tTraining Acuuarcy 35.799% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 777 \tTraining Loss: 0.01261484 \tValidation Loss 0.01666461 \tTraining Acuuarcy 35.944% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 778 \tTraining Loss: 0.01258504 \tValidation Loss 0.01722687 \tTraining Acuuarcy 36.317% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 779 \tTraining Loss: 0.01258706 \tValidation Loss 0.01699154 \tTraining Acuuarcy 35.966% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 780 \tTraining Loss: 0.01255501 \tValidation Loss 0.01672665 \tTraining Acuuarcy 35.782% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 781 \tTraining Loss: 0.01262188 \tValidation Loss 0.01680768 \tTraining Acuuarcy 36.161% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 782 \tTraining Loss: 0.01258286 \tValidation Loss 0.01670760 \tTraining Acuuarcy 36.495% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 783 \tTraining Loss: 0.01257433 \tValidation Loss 0.01664304 \tTraining Acuuarcy 36.077% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 784 \tTraining Loss: 0.01262266 \tValidation Loss 0.01671334 \tTraining Acuuarcy 36.061% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 785 \tTraining Loss: 0.01256529 \tValidation Loss 0.01699394 \tTraining Acuuarcy 36.267% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 786 \tTraining Loss: 0.01259669 \tValidation Loss 0.01651727 \tTraining Acuuarcy 36.423% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 787 \tTraining Loss: 0.01255384 \tValidation Loss 0.01721041 \tTraining Acuuarcy 36.529% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 788 \tTraining Loss: 0.01256925 \tValidation Loss 0.01690568 \tTraining Acuuarcy 36.300% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 789 \tTraining Loss: 0.01256974 \tValidation Loss 0.01670828 \tTraining Acuuarcy 36.858% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 790 \tTraining Loss: 0.01259803 \tValidation Loss 0.01675832 \tTraining Acuuarcy 36.261% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 791 \tTraining Loss: 0.01258271 \tValidation Loss 0.01676942 \tTraining Acuuarcy 36.239% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 792 \tTraining Loss: 0.01264398 \tValidation Loss 0.01709704 \tTraining Acuuarcy 35.932% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 793 \tTraining Loss: 0.01250461 \tValidation Loss 0.01692730 \tTraining Acuuarcy 36.936% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 794 \tTraining Loss: 0.01265813 \tValidation Loss 0.01667194 \tTraining Acuuarcy 35.765% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 795 \tTraining Loss: 0.01257306 \tValidation Loss 0.01673528 \tTraining Acuuarcy 35.793% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 796 \tTraining Loss: 0.01259880 \tValidation Loss 0.01671723 \tTraining Acuuarcy 36.512% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 797 \tTraining Loss: 0.01255251 \tValidation Loss 0.01670396 \tTraining Acuuarcy 36.105% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 798 \tTraining Loss: 0.01252598 \tValidation Loss 0.01711299 \tTraining Acuuarcy 36.144% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 799 \tTraining Loss: 0.01260296 \tValidation Loss 0.01685407 \tTraining Acuuarcy 36.105% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 800 \tTraining Loss: 0.01255761 \tValidation Loss 0.01689121 \tTraining Acuuarcy 36.239% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 801 \tTraining Loss: 0.01259111 \tValidation Loss 0.01694070 \tTraining Acuuarcy 36.685% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 802 \tTraining Loss: 0.01260492 \tValidation Loss 0.01730281 \tTraining Acuuarcy 35.971% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 803 \tTraining Loss: 0.01254210 \tValidation Loss 0.01693041 \tTraining Acuuarcy 36.819% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 804 \tTraining Loss: 0.01254892 \tValidation Loss 0.01684291 \tTraining Acuuarcy 36.306% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 805 \tTraining Loss: 0.01257392 \tValidation Loss 0.01686015 \tTraining Acuuarcy 36.674% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 806 \tTraining Loss: 0.01256448 \tValidation Loss 0.01693762 \tTraining Acuuarcy 36.155% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 807 \tTraining Loss: 0.01261651 \tValidation Loss 0.01691173 \tTraining Acuuarcy 35.949% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 808 \tTraining Loss: 0.01258373 \tValidation Loss 0.01680345 \tTraining Acuuarcy 36.133% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 809 \tTraining Loss: 0.01260585 \tValidation Loss 0.01661594 \tTraining Acuuarcy 35.871% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 810 \tTraining Loss: 0.01250246 \tValidation Loss 0.01681553 \tTraining Acuuarcy 36.289% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 811 \tTraining Loss: 0.01255733 \tValidation Loss 0.01678679 \tTraining Acuuarcy 36.501% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 812 \tTraining Loss: 0.01253790 \tValidation Loss 0.01704698 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 813 \tTraining Loss: 0.01258104 \tValidation Loss 0.01666220 \tTraining Acuuarcy 36.089% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 814 \tTraining Loss: 0.01263833 \tValidation Loss 0.01706052 \tTraining Acuuarcy 36.217% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 815 \tTraining Loss: 0.01249631 \tValidation Loss 0.01716866 \tTraining Acuuarcy 36.741% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 816 \tTraining Loss: 0.01258601 \tValidation Loss 0.01685596 \tTraining Acuuarcy 36.022% \tValidation Acuuarcy 20.953%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 817 \tTraining Loss: 0.01256083 \tValidation Loss 0.01700939 \tTraining Acuuarcy 36.557% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 818 \tTraining Loss: 0.01253667 \tValidation Loss 0.01666049 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 819 \tTraining Loss: 0.01257841 \tValidation Loss 0.01704124 \tTraining Acuuarcy 36.228% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 820 \tTraining Loss: 0.01254634 \tValidation Loss 0.01684521 \tTraining Acuuarcy 36.523% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 821 \tTraining Loss: 0.01256335 \tValidation Loss 0.01686637 \tTraining Acuuarcy 36.707% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 822 \tTraining Loss: 0.01256933 \tValidation Loss 0.01740149 \tTraining Acuuarcy 36.100% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 823 \tTraining Loss: 0.01262147 \tValidation Loss 0.01695592 \tTraining Acuuarcy 35.765% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 824 \tTraining Loss: 0.01262109 \tValidation Loss 0.01677310 \tTraining Acuuarcy 35.676% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 825 \tTraining Loss: 0.01256140 \tValidation Loss 0.01677203 \tTraining Acuuarcy 36.389% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 826 \tTraining Loss: 0.01259250 \tValidation Loss 0.01671672 \tTraining Acuuarcy 36.144% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 827 \tTraining Loss: 0.01255116 \tValidation Loss 0.01682054 \tTraining Acuuarcy 36.395% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 828 \tTraining Loss: 0.01255306 \tValidation Loss 0.01657816 \tTraining Acuuarcy 36.624% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 829 \tTraining Loss: 0.01252382 \tValidation Loss 0.01711058 \tTraining Acuuarcy 36.612% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 830 \tTraining Loss: 0.01258126 \tValidation Loss 0.01687396 \tTraining Acuuarcy 35.810% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 831 \tTraining Loss: 0.01255617 \tValidation Loss 0.01688494 \tTraining Acuuarcy 36.484% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 832 \tTraining Loss: 0.01258790 \tValidation Loss 0.01681151 \tTraining Acuuarcy 35.966% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 833 \tTraining Loss: 0.01251242 \tValidation Loss 0.01688670 \tTraining Acuuarcy 36.490% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 834 \tTraining Loss: 0.01253493 \tValidation Loss 0.01704605 \tTraining Acuuarcy 36.362% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 835 \tTraining Loss: 0.01253799 \tValidation Loss 0.01666929 \tTraining Acuuarcy 36.250% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 836 \tTraining Loss: 0.01254488 \tValidation Loss 0.01664459 \tTraining Acuuarcy 35.726% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 837 \tTraining Loss: 0.01260700 \tValidation Loss 0.01692391 \tTraining Acuuarcy 35.866% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 838 \tTraining Loss: 0.01260641 \tValidation Loss 0.01672572 \tTraining Acuuarcy 35.849% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 839 \tTraining Loss: 0.01256981 \tValidation Loss 0.01698592 \tTraining Acuuarcy 35.905% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 840 \tTraining Loss: 0.01254411 \tValidation Loss 0.01716826 \tTraining Acuuarcy 36.044% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 841 \tTraining Loss: 0.01259969 \tValidation Loss 0.01701888 \tTraining Acuuarcy 35.871% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 842 \tTraining Loss: 0.01265579 \tValidation Loss 0.01686146 \tTraining Acuuarcy 35.670% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 843 \tTraining Loss: 0.01261038 \tValidation Loss 0.01674328 \tTraining Acuuarcy 36.289% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 844 \tTraining Loss: 0.01262364 \tValidation Loss 0.01687265 \tTraining Acuuarcy 35.704% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 845 \tTraining Loss: 0.01252242 \tValidation Loss 0.01739405 \tTraining Acuuarcy 36.490% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 846 \tTraining Loss: 0.01257016 \tValidation Loss 0.01677661 \tTraining Acuuarcy 36.300% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 847 \tTraining Loss: 0.01257361 \tValidation Loss 0.01754411 \tTraining Acuuarcy 36.189% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 848 \tTraining Loss: 0.01262774 \tValidation Loss 0.01699573 \tTraining Acuuarcy 35.832% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 849 \tTraining Loss: 0.01259777 \tValidation Loss 0.01701539 \tTraining Acuuarcy 36.133% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 850 \tTraining Loss: 0.01254747 \tValidation Loss 0.01678352 \tTraining Acuuarcy 36.501% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 851 \tTraining Loss: 0.01255496 \tValidation Loss 0.01691568 \tTraining Acuuarcy 36.189% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 852 \tTraining Loss: 0.01258226 \tValidation Loss 0.01672720 \tTraining Acuuarcy 36.328% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 853 \tTraining Loss: 0.01257576 \tValidation Loss 0.01727748 \tTraining Acuuarcy 36.495% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 854 \tTraining Loss: 0.01260630 \tValidation Loss 0.01695276 \tTraining Acuuarcy 36.122% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 855 \tTraining Loss: 0.01251633 \tValidation Loss 0.01719252 \tTraining Acuuarcy 36.462% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 856 \tTraining Loss: 0.01260669 \tValidation Loss 0.01693082 \tTraining Acuuarcy 36.267% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 857 \tTraining Loss: 0.01256655 \tValidation Loss 0.01690310 \tTraining Acuuarcy 36.306% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 858 \tTraining Loss: 0.01256780 \tValidation Loss 0.01698987 \tTraining Acuuarcy 36.462% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 859 \tTraining Loss: 0.01260675 \tValidation Loss 0.01674256 \tTraining Acuuarcy 36.289% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 860 \tTraining Loss: 0.01251435 \tValidation Loss 0.01735071 \tTraining Acuuarcy 36.702% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 861 \tTraining Loss: 0.01253803 \tValidation Loss 0.01675219 \tTraining Acuuarcy 36.518% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 862 \tTraining Loss: 0.01259320 \tValidation Loss 0.01673072 \tTraining Acuuarcy 36.590% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 863 \tTraining Loss: 0.01258664 \tValidation Loss 0.01722471 \tTraining Acuuarcy 36.172% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 864 \tTraining Loss: 0.01252606 \tValidation Loss 0.01691147 \tTraining Acuuarcy 36.534% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 865 \tTraining Loss: 0.01256146 \tValidation Loss 0.01662051 \tTraining Acuuarcy 36.573% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 866 \tTraining Loss: 0.01257798 \tValidation Loss 0.01677727 \tTraining Acuuarcy 36.066% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 867 \tTraining Loss: 0.01258831 \tValidation Loss 0.01686991 \tTraining Acuuarcy 36.161% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 868 \tTraining Loss: 0.01252705 \tValidation Loss 0.01678078 \tTraining Acuuarcy 36.663% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 869 \tTraining Loss: 0.01253897 \tValidation Loss 0.01664925 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 870 \tTraining Loss: 0.01255029 \tValidation Loss 0.01724782 \tTraining Acuuarcy 36.395% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 871 \tTraining Loss: 0.01257097 \tValidation Loss 0.01694397 \tTraining Acuuarcy 36.468% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 872 \tTraining Loss: 0.01252822 \tValidation Loss 0.01695312 \tTraining Acuuarcy 36.462% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 873 \tTraining Loss: 0.01252047 \tValidation Loss 0.01726136 \tTraining Acuuarcy 36.534% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 874 \tTraining Loss: 0.01260627 \tValidation Loss 0.01674642 \tTraining Acuuarcy 36.038% \tValidation Acuuarcy 21.928%\n",
      "Epoch: 875 \tTraining Loss: 0.01262144 \tValidation Loss 0.01679051 \tTraining Acuuarcy 35.760% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 876 \tTraining Loss: 0.01258725 \tValidation Loss 0.01672762 \tTraining Acuuarcy 36.222% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 877 \tTraining Loss: 0.01257917 \tValidation Loss 0.01682633 \tTraining Acuuarcy 36.072% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 878 \tTraining Loss: 0.01255485 \tValidation Loss 0.01691536 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 879 \tTraining Loss: 0.01257869 \tValidation Loss 0.01663752 \tTraining Acuuarcy 36.250% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 880 \tTraining Loss: 0.01249546 \tValidation Loss 0.01682484 \tTraining Acuuarcy 36.568% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 881 \tTraining Loss: 0.01259686 \tValidation Loss 0.01662023 \tTraining Acuuarcy 35.888% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 882 \tTraining Loss: 0.01258389 \tValidation Loss 0.01683395 \tTraining Acuuarcy 36.111% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 883 \tTraining Loss: 0.01257441 \tValidation Loss 0.01685206 \tTraining Acuuarcy 35.983% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 884 \tTraining Loss: 0.01256205 \tValidation Loss 0.01730591 \tTraining Acuuarcy 36.155% \tValidation Acuuarcy 19.114%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 885 \tTraining Loss: 0.01255761 \tValidation Loss 0.01700484 \tTraining Acuuarcy 36.428% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 886 \tTraining Loss: 0.01257497 \tValidation Loss 0.01700757 \tTraining Acuuarcy 36.529% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 887 \tTraining Loss: 0.01262248 \tValidation Loss 0.01655284 \tTraining Acuuarcy 35.849% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 888 \tTraining Loss: 0.01253150 \tValidation Loss 0.01711514 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 889 \tTraining Loss: 0.01257454 \tValidation Loss 0.01661653 \tTraining Acuuarcy 36.289% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 890 \tTraining Loss: 0.01255167 \tValidation Loss 0.01682854 \tTraining Acuuarcy 35.866% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 891 \tTraining Loss: 0.01256743 \tValidation Loss 0.01689147 \tTraining Acuuarcy 36.295% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 892 \tTraining Loss: 0.01253397 \tValidation Loss 0.01687213 \tTraining Acuuarcy 36.624% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 893 \tTraining Loss: 0.01256481 \tValidation Loss 0.01669603 \tTraining Acuuarcy 36.189% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 894 \tTraining Loss: 0.01252944 \tValidation Loss 0.01688363 \tTraining Acuuarcy 36.930% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 895 \tTraining Loss: 0.01253114 \tValidation Loss 0.01680326 \tTraining Acuuarcy 36.702% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 896 \tTraining Loss: 0.01251124 \tValidation Loss 0.01705795 \tTraining Acuuarcy 36.651% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 897 \tTraining Loss: 0.01259463 \tValidation Loss 0.01697179 \tTraining Acuuarcy 36.150% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 898 \tTraining Loss: 0.01251840 \tValidation Loss 0.01689072 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 899 \tTraining Loss: 0.01261298 \tValidation Loss 0.01677390 \tTraining Acuuarcy 36.144% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 900 \tTraining Loss: 0.01261168 \tValidation Loss 0.01661953 \tTraining Acuuarcy 36.005% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 901 \tTraining Loss: 0.01259312 \tValidation Loss 0.01709126 \tTraining Acuuarcy 36.105% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 902 \tTraining Loss: 0.01257177 \tValidation Loss 0.01675820 \tTraining Acuuarcy 36.635% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 903 \tTraining Loss: 0.01258741 \tValidation Loss 0.01671582 \tTraining Acuuarcy 36.406% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 904 \tTraining Loss: 0.01248874 \tValidation Loss 0.01685124 \tTraining Acuuarcy 36.624% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 905 \tTraining Loss: 0.01256501 \tValidation Loss 0.01656555 \tTraining Acuuarcy 36.858% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 906 \tTraining Loss: 0.01260832 \tValidation Loss 0.01668069 \tTraining Acuuarcy 36.055% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 907 \tTraining Loss: 0.01255894 \tValidation Loss 0.01701862 \tTraining Acuuarcy 35.860% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 908 \tTraining Loss: 0.01252411 \tValidation Loss 0.01659783 \tTraining Acuuarcy 36.490% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 909 \tTraining Loss: 0.01254847 \tValidation Loss 0.01704257 \tTraining Acuuarcy 36.311% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 910 \tTraining Loss: 0.01250285 \tValidation Loss 0.01702462 \tTraining Acuuarcy 36.690% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 911 \tTraining Loss: 0.01250522 \tValidation Loss 0.01661568 \tTraining Acuuarcy 36.852% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 912 \tTraining Loss: 0.01256673 \tValidation Loss 0.01705773 \tTraining Acuuarcy 36.317% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 913 \tTraining Loss: 0.01254973 \tValidation Loss 0.01669534 \tTraining Acuuarcy 36.284% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 914 \tTraining Loss: 0.01255346 \tValidation Loss 0.01686646 \tTraining Acuuarcy 36.546% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 915 \tTraining Loss: 0.01255166 \tValidation Loss 0.01678018 \tTraining Acuuarcy 36.395% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 916 \tTraining Loss: 0.01257234 \tValidation Loss 0.01673990 \tTraining Acuuarcy 36.523% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 917 \tTraining Loss: 0.01254756 \tValidation Loss 0.01668550 \tTraining Acuuarcy 36.507% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 918 \tTraining Loss: 0.01263200 \tValidation Loss 0.01700618 \tTraining Acuuarcy 35.492% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 919 \tTraining Loss: 0.01256269 \tValidation Loss 0.01670593 \tTraining Acuuarcy 36.507% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 920 \tTraining Loss: 0.01260301 \tValidation Loss 0.01712578 \tTraining Acuuarcy 36.155% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 921 \tTraining Loss: 0.01252432 \tValidation Loss 0.01688146 \tTraining Acuuarcy 36.713% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 922 \tTraining Loss: 0.01257725 \tValidation Loss 0.01713689 \tTraining Acuuarcy 36.089% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 923 \tTraining Loss: 0.01251181 \tValidation Loss 0.01718521 \tTraining Acuuarcy 36.451% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 924 \tTraining Loss: 0.01253300 \tValidation Loss 0.01719382 \tTraining Acuuarcy 36.819% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 925 \tTraining Loss: 0.01251883 \tValidation Loss 0.01677065 \tTraining Acuuarcy 36.651% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 926 \tTraining Loss: 0.01252430 \tValidation Loss 0.01701349 \tTraining Acuuarcy 36.780% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 927 \tTraining Loss: 0.01251766 \tValidation Loss 0.01703033 \tTraining Acuuarcy 36.646% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 928 \tTraining Loss: 0.01251573 \tValidation Loss 0.01714469 \tTraining Acuuarcy 36.468% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 929 \tTraining Loss: 0.01259303 \tValidation Loss 0.01689263 \tTraining Acuuarcy 36.373% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 930 \tTraining Loss: 0.01256240 \tValidation Loss 0.01709863 \tTraining Acuuarcy 36.317% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 931 \tTraining Loss: 0.01255614 \tValidation Loss 0.01682185 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 932 \tTraining Loss: 0.01257256 \tValidation Loss 0.01688344 \tTraining Acuuarcy 36.512% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 933 \tTraining Loss: 0.01255628 \tValidation Loss 0.01658461 \tTraining Acuuarcy 36.334% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 934 \tTraining Loss: 0.01256084 \tValidation Loss 0.01698055 \tTraining Acuuarcy 36.428% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 935 \tTraining Loss: 0.01254737 \tValidation Loss 0.01658867 \tTraining Acuuarcy 36.033% \tValidation Acuuarcy 21.566%\n",
      "Epoch: 936 \tTraining Loss: 0.01253804 \tValidation Loss 0.01665502 \tTraining Acuuarcy 36.217% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 937 \tTraining Loss: 0.01256856 \tValidation Loss 0.01661304 \tTraining Acuuarcy 36.501% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 938 \tTraining Loss: 0.01252547 \tValidation Loss 0.01693223 \tTraining Acuuarcy 36.507% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 939 \tTraining Loss: 0.01258273 \tValidation Loss 0.01700525 \tTraining Acuuarcy 36.200% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 940 \tTraining Loss: 0.01256778 \tValidation Loss 0.01707677 \tTraining Acuuarcy 36.874% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 941 \tTraining Loss: 0.01251115 \tValidation Loss 0.01673146 \tTraining Acuuarcy 36.663% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 942 \tTraining Loss: 0.01246251 \tValidation Loss 0.01690635 \tTraining Acuuarcy 37.108% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 943 \tTraining Loss: 0.01258631 \tValidation Loss 0.01679061 \tTraining Acuuarcy 36.005% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 944 \tTraining Loss: 0.01257150 \tValidation Loss 0.01679427 \tTraining Acuuarcy 36.128% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 945 \tTraining Loss: 0.01257735 \tValidation Loss 0.01663233 \tTraining Acuuarcy 35.938% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 946 \tTraining Loss: 0.01257029 \tValidation Loss 0.01668309 \tTraining Acuuarcy 36.791% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 947 \tTraining Loss: 0.01254986 \tValidation Loss 0.01656809 \tTraining Acuuarcy 36.289% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 948 \tTraining Loss: 0.01259780 \tValidation Loss 0.01672943 \tTraining Acuuarcy 36.089% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 949 \tTraining Loss: 0.01257066 \tValidation Loss 0.01680314 \tTraining Acuuarcy 36.495% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 950 \tTraining Loss: 0.01252285 \tValidation Loss 0.01699665 \tTraining Acuuarcy 36.908% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 951 \tTraining Loss: 0.01250265 \tValidation Loss 0.01698716 \tTraining Acuuarcy 36.757% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 952 \tTraining Loss: 0.01249863 \tValidation Loss 0.01698167 \tTraining Acuuarcy 36.897% \tValidation Acuuarcy 19.560%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 953 \tTraining Loss: 0.01255791 \tValidation Loss 0.01694956 \tTraining Acuuarcy 36.194% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 954 \tTraining Loss: 0.01252603 \tValidation Loss 0.01677791 \tTraining Acuuarcy 36.713% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 955 \tTraining Loss: 0.01247470 \tValidation Loss 0.01710316 \tTraining Acuuarcy 37.053% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 956 \tTraining Loss: 0.01255095 \tValidation Loss 0.01679934 \tTraining Acuuarcy 36.507% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 957 \tTraining Loss: 0.01256972 \tValidation Loss 0.01710838 \tTraining Acuuarcy 36.311% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 958 \tTraining Loss: 0.01251975 \tValidation Loss 0.01697362 \tTraining Acuuarcy 36.824% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 959 \tTraining Loss: 0.01254343 \tValidation Loss 0.01690658 \tTraining Acuuarcy 36.757% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 960 \tTraining Loss: 0.01255973 \tValidation Loss 0.01673784 \tTraining Acuuarcy 36.311% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 961 \tTraining Loss: 0.01256009 \tValidation Loss 0.01676822 \tTraining Acuuarcy 36.356% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 962 \tTraining Loss: 0.01253488 \tValidation Loss 0.01723017 \tTraining Acuuarcy 36.468% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 963 \tTraining Loss: 0.01248556 \tValidation Loss 0.01714867 \tTraining Acuuarcy 37.058% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 964 \tTraining Loss: 0.01251355 \tValidation Loss 0.01695592 \tTraining Acuuarcy 36.523% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 965 \tTraining Loss: 0.01258382 \tValidation Loss 0.01667881 \tTraining Acuuarcy 36.317% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 966 \tTraining Loss: 0.01250710 \tValidation Loss 0.01714241 \tTraining Acuuarcy 36.707% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 967 \tTraining Loss: 0.01253633 \tValidation Loss 0.01725416 \tTraining Acuuarcy 36.328% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 968 \tTraining Loss: 0.01255618 \tValidation Loss 0.01684647 \tTraining Acuuarcy 36.384% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 969 \tTraining Loss: 0.01257432 \tValidation Loss 0.01692171 \tTraining Acuuarcy 36.150% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 970 \tTraining Loss: 0.01254027 \tValidation Loss 0.01707113 \tTraining Acuuarcy 36.512% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 971 \tTraining Loss: 0.01260230 \tValidation Loss 0.01669631 \tTraining Acuuarcy 36.462% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 972 \tTraining Loss: 0.01254707 \tValidation Loss 0.01672523 \tTraining Acuuarcy 36.462% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 973 \tTraining Loss: 0.01254069 \tValidation Loss 0.01688498 \tTraining Acuuarcy 36.607% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 974 \tTraining Loss: 0.01248588 \tValidation Loss 0.01710301 \tTraining Acuuarcy 36.713% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 975 \tTraining Loss: 0.01259602 \tValidation Loss 0.01697250 \tTraining Acuuarcy 36.440% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 976 \tTraining Loss: 0.01264297 \tValidation Loss 0.01669699 \tTraining Acuuarcy 35.743% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 977 \tTraining Loss: 0.01247670 \tValidation Loss 0.01687574 \tTraining Acuuarcy 37.014% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 978 \tTraining Loss: 0.01252763 \tValidation Loss 0.01690685 \tTraining Acuuarcy 36.395% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 979 \tTraining Loss: 0.01254048 \tValidation Loss 0.01684908 \tTraining Acuuarcy 36.323% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 980 \tTraining Loss: 0.01252344 \tValidation Loss 0.01685209 \tTraining Acuuarcy 36.490% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 981 \tTraining Loss: 0.01253334 \tValidation Loss 0.01676665 \tTraining Acuuarcy 36.947% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 982 \tTraining Loss: 0.01257935 \tValidation Loss 0.01704218 \tTraining Acuuarcy 36.362% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 983 \tTraining Loss: 0.01252360 \tValidation Loss 0.01729367 \tTraining Acuuarcy 36.952% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 984 \tTraining Loss: 0.01252295 \tValidation Loss 0.01694310 \tTraining Acuuarcy 36.590% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 985 \tTraining Loss: 0.01252883 \tValidation Loss 0.01705000 \tTraining Acuuarcy 36.713% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 986 \tTraining Loss: 0.01249295 \tValidation Loss 0.01711200 \tTraining Acuuarcy 36.490% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 987 \tTraining Loss: 0.01259935 \tValidation Loss 0.01696531 \tTraining Acuuarcy 36.083% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 988 \tTraining Loss: 0.01258173 \tValidation Loss 0.01683083 \tTraining Acuuarcy 36.116% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 989 \tTraining Loss: 0.01256737 \tValidation Loss 0.01692380 \tTraining Acuuarcy 36.334% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 990 \tTraining Loss: 0.01250049 \tValidation Loss 0.01686325 \tTraining Acuuarcy 36.841% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 991 \tTraining Loss: 0.01251077 \tValidation Loss 0.01679610 \tTraining Acuuarcy 37.030% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 992 \tTraining Loss: 0.01252385 \tValidation Loss 0.01695285 \tTraining Acuuarcy 36.378% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 993 \tTraining Loss: 0.01253501 \tValidation Loss 0.01681306 \tTraining Acuuarcy 36.646% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 994 \tTraining Loss: 0.01248992 \tValidation Loss 0.01688647 \tTraining Acuuarcy 36.807% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 995 \tTraining Loss: 0.01258807 \tValidation Loss 0.01694410 \tTraining Acuuarcy 36.356% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 996 \tTraining Loss: 0.01256328 \tValidation Loss 0.01672215 \tTraining Acuuarcy 36.462% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 997 \tTraining Loss: 0.01253693 \tValidation Loss 0.01718724 \tTraining Acuuarcy 36.356% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 998 \tTraining Loss: 0.01248510 \tValidation Loss 0.01699983 \tTraining Acuuarcy 36.746% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 999 \tTraining Loss: 0.01250886 \tValidation Loss 0.01694888 \tTraining Acuuarcy 36.456% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1000 \tTraining Loss: 0.01250128 \tValidation Loss 0.01679460 \tTraining Acuuarcy 36.986% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1001 \tTraining Loss: 0.01246349 \tValidation Loss 0.01721434 \tTraining Acuuarcy 37.003% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1002 \tTraining Loss: 0.01256133 \tValidation Loss 0.01695331 \tTraining Acuuarcy 36.306% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1003 \tTraining Loss: 0.01257183 \tValidation Loss 0.01696479 \tTraining Acuuarcy 36.311% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1004 \tTraining Loss: 0.01255083 \tValidation Loss 0.01694044 \tTraining Acuuarcy 36.373% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1005 \tTraining Loss: 0.01251150 \tValidation Loss 0.01687693 \tTraining Acuuarcy 36.540% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1006 \tTraining Loss: 0.01246304 \tValidation Loss 0.01677501 \tTraining Acuuarcy 36.902% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1007 \tTraining Loss: 0.01247926 \tValidation Loss 0.01682000 \tTraining Acuuarcy 36.869% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1008 \tTraining Loss: 0.01254985 \tValidation Loss 0.01707211 \tTraining Acuuarcy 36.707% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1009 \tTraining Loss: 0.01252402 \tValidation Loss 0.01717725 \tTraining Acuuarcy 36.573% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1010 \tTraining Loss: 0.01254957 \tValidation Loss 0.01696380 \tTraining Acuuarcy 36.601% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1011 \tTraining Loss: 0.01251307 \tValidation Loss 0.01699511 \tTraining Acuuarcy 36.573% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1012 \tTraining Loss: 0.01258114 \tValidation Loss 0.01673481 \tTraining Acuuarcy 36.150% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1013 \tTraining Loss: 0.01247412 \tValidation Loss 0.01673677 \tTraining Acuuarcy 36.501% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1014 \tTraining Loss: 0.01257604 \tValidation Loss 0.01697882 \tTraining Acuuarcy 36.272% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1015 \tTraining Loss: 0.01251020 \tValidation Loss 0.01733168 \tTraining Acuuarcy 37.042% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1016 \tTraining Loss: 0.01255022 \tValidation Loss 0.01720000 \tTraining Acuuarcy 36.540% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1017 \tTraining Loss: 0.01249267 \tValidation Loss 0.01707855 \tTraining Acuuarcy 37.003% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1018 \tTraining Loss: 0.01257326 \tValidation Loss 0.01717396 \tTraining Acuuarcy 36.278% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1019 \tTraining Loss: 0.01250905 \tValidation Loss 0.01689751 \tTraining Acuuarcy 36.975% \tValidation Acuuarcy 20.006%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1020 \tTraining Loss: 0.01255754 \tValidation Loss 0.01676233 \tTraining Acuuarcy 36.635% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1021 \tTraining Loss: 0.01250294 \tValidation Loss 0.01703989 \tTraining Acuuarcy 36.696% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 1022 \tTraining Loss: 0.01255919 \tValidation Loss 0.01716717 \tTraining Acuuarcy 36.562% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 1023 \tTraining Loss: 0.01251681 \tValidation Loss 0.01690107 \tTraining Acuuarcy 36.768% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1024 \tTraining Loss: 0.01255627 \tValidation Loss 0.01716856 \tTraining Acuuarcy 36.378% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1025 \tTraining Loss: 0.01254065 \tValidation Loss 0.01686244 \tTraining Acuuarcy 36.211% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1026 \tTraining Loss: 0.01253676 \tValidation Loss 0.01685456 \tTraining Acuuarcy 36.222% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1027 \tTraining Loss: 0.01261877 \tValidation Loss 0.01685038 \tTraining Acuuarcy 36.334% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1028 \tTraining Loss: 0.01251126 \tValidation Loss 0.01691308 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1029 \tTraining Loss: 0.01249903 \tValidation Loss 0.01709251 \tTraining Acuuarcy 36.791% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 1030 \tTraining Loss: 0.01252162 \tValidation Loss 0.01704010 \tTraining Acuuarcy 36.874% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1031 \tTraining Loss: 0.01252616 \tValidation Loss 0.01679763 \tTraining Acuuarcy 36.423% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1032 \tTraining Loss: 0.01247509 \tValidation Loss 0.01670366 \tTraining Acuuarcy 36.913% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1033 \tTraining Loss: 0.01257182 \tValidation Loss 0.01674407 \tTraining Acuuarcy 36.194% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1034 \tTraining Loss: 0.01249557 \tValidation Loss 0.01703125 \tTraining Acuuarcy 36.590% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1035 \tTraining Loss: 0.01252985 \tValidation Loss 0.01675996 \tTraining Acuuarcy 36.785% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1036 \tTraining Loss: 0.01248404 \tValidation Loss 0.01705819 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1037 \tTraining Loss: 0.01260358 \tValidation Loss 0.01686535 \tTraining Acuuarcy 35.782% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1038 \tTraining Loss: 0.01249180 \tValidation Loss 0.01673825 \tTraining Acuuarcy 36.384% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1039 \tTraining Loss: 0.01252967 \tValidation Loss 0.01719838 \tTraining Acuuarcy 36.657% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1040 \tTraining Loss: 0.01248240 \tValidation Loss 0.01705970 \tTraining Acuuarcy 37.042% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1041 \tTraining Loss: 0.01259627 \tValidation Loss 0.01672633 \tTraining Acuuarcy 36.139% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1042 \tTraining Loss: 0.01249663 \tValidation Loss 0.01706433 \tTraining Acuuarcy 36.835% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1043 \tTraining Loss: 0.01251417 \tValidation Loss 0.01698806 \tTraining Acuuarcy 36.546% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1044 \tTraining Loss: 0.01250065 \tValidation Loss 0.01697162 \tTraining Acuuarcy 36.807% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1045 \tTraining Loss: 0.01248172 \tValidation Loss 0.01689470 \tTraining Acuuarcy 36.629% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1046 \tTraining Loss: 0.01255497 \tValidation Loss 0.01685522 \tTraining Acuuarcy 36.395% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1047 \tTraining Loss: 0.01254083 \tValidation Loss 0.01677937 \tTraining Acuuarcy 36.657% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1048 \tTraining Loss: 0.01248617 \tValidation Loss 0.01713421 \tTraining Acuuarcy 36.763% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1049 \tTraining Loss: 0.01254117 \tValidation Loss 0.01675591 \tTraining Acuuarcy 36.579% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1050 \tTraining Loss: 0.01249188 \tValidation Loss 0.01701989 \tTraining Acuuarcy 36.897% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1051 \tTraining Loss: 0.01246813 \tValidation Loss 0.01707706 \tTraining Acuuarcy 36.964% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1052 \tTraining Loss: 0.01255010 \tValidation Loss 0.01690966 \tTraining Acuuarcy 36.345% \tValidation Acuuarcy 17.665%\n",
      "Epoch: 1053 \tTraining Loss: 0.01249544 \tValidation Loss 0.01695194 \tTraining Acuuarcy 36.049% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1054 \tTraining Loss: 0.01254516 \tValidation Loss 0.01695238 \tTraining Acuuarcy 36.629% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1055 \tTraining Loss: 0.01245043 \tValidation Loss 0.01709676 \tTraining Acuuarcy 36.824% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1056 \tTraining Loss: 0.01256167 \tValidation Loss 0.01696084 \tTraining Acuuarcy 36.139% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1057 \tTraining Loss: 0.01249267 \tValidation Loss 0.01690369 \tTraining Acuuarcy 36.729% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1058 \tTraining Loss: 0.01259649 \tValidation Loss 0.01699633 \tTraining Acuuarcy 36.250% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1059 \tTraining Loss: 0.01259233 \tValidation Loss 0.01670313 \tTraining Acuuarcy 36.356% \tValidation Acuuarcy 20.841%\n",
      "Epoch: 1060 \tTraining Loss: 0.01254182 \tValidation Loss 0.01675847 \tTraining Acuuarcy 36.657% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1061 \tTraining Loss: 0.01247860 \tValidation Loss 0.01702413 \tTraining Acuuarcy 36.757% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1062 \tTraining Loss: 0.01251012 \tValidation Loss 0.01683199 \tTraining Acuuarcy 36.847% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1063 \tTraining Loss: 0.01253270 \tValidation Loss 0.01691626 \tTraining Acuuarcy 36.490% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1064 \tTraining Loss: 0.01253227 \tValidation Loss 0.01672983 \tTraining Acuuarcy 36.540% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1065 \tTraining Loss: 0.01252938 \tValidation Loss 0.01692299 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1066 \tTraining Loss: 0.01242227 \tValidation Loss 0.01705158 \tTraining Acuuarcy 37.014% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1067 \tTraining Loss: 0.01248243 \tValidation Loss 0.01698380 \tTraining Acuuarcy 37.053% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1068 \tTraining Loss: 0.01252258 \tValidation Loss 0.01668560 \tTraining Acuuarcy 36.663% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1069 \tTraining Loss: 0.01249858 \tValidation Loss 0.01705413 \tTraining Acuuarcy 36.445% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1070 \tTraining Loss: 0.01251623 \tValidation Loss 0.01682092 \tTraining Acuuarcy 36.462% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1071 \tTraining Loss: 0.01250302 \tValidation Loss 0.01724758 \tTraining Acuuarcy 37.114% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1072 \tTraining Loss: 0.01250369 \tValidation Loss 0.01683701 \tTraining Acuuarcy 36.886% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1073 \tTraining Loss: 0.01251693 \tValidation Loss 0.01704008 \tTraining Acuuarcy 36.925% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1074 \tTraining Loss: 0.01248968 \tValidation Loss 0.01692412 \tTraining Acuuarcy 36.991% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1075 \tTraining Loss: 0.01245329 \tValidation Loss 0.01684757 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1076 \tTraining Loss: 0.01252338 \tValidation Loss 0.01718710 \tTraining Acuuarcy 36.707% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1077 \tTraining Loss: 0.01247155 \tValidation Loss 0.01728702 \tTraining Acuuarcy 37.125% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1078 \tTraining Loss: 0.01250238 \tValidation Loss 0.01692212 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1079 \tTraining Loss: 0.01253978 \tValidation Loss 0.01693389 \tTraining Acuuarcy 36.495% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1080 \tTraining Loss: 0.01254788 \tValidation Loss 0.01650199 \tTraining Acuuarcy 36.389% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1081 \tTraining Loss: 0.01251448 \tValidation Loss 0.01672721 \tTraining Acuuarcy 36.579% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1082 \tTraining Loss: 0.01248532 \tValidation Loss 0.01708387 \tTraining Acuuarcy 36.713% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1083 \tTraining Loss: 0.01247525 \tValidation Loss 0.01675150 \tTraining Acuuarcy 36.757% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1084 \tTraining Loss: 0.01254563 \tValidation Loss 0.01700540 \tTraining Acuuarcy 36.150% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1085 \tTraining Loss: 0.01247657 \tValidation Loss 0.01724950 \tTraining Acuuarcy 37.198% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1086 \tTraining Loss: 0.01253961 \tValidation Loss 0.01690296 \tTraining Acuuarcy 36.590% \tValidation Acuuarcy 20.061%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1087 \tTraining Loss: 0.01256352 \tValidation Loss 0.01672587 \tTraining Acuuarcy 36.278% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1088 \tTraining Loss: 0.01252422 \tValidation Loss 0.01721718 \tTraining Acuuarcy 37.086% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1089 \tTraining Loss: 0.01256155 \tValidation Loss 0.01689808 \tTraining Acuuarcy 37.036% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1090 \tTraining Loss: 0.01251809 \tValidation Loss 0.01698654 \tTraining Acuuarcy 36.741% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1091 \tTraining Loss: 0.01246924 \tValidation Loss 0.01693967 \tTraining Acuuarcy 37.053% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1092 \tTraining Loss: 0.01254562 \tValidation Loss 0.01722539 \tTraining Acuuarcy 36.824% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1093 \tTraining Loss: 0.01252817 \tValidation Loss 0.01716557 \tTraining Acuuarcy 36.350% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1094 \tTraining Loss: 0.01249818 \tValidation Loss 0.01714982 \tTraining Acuuarcy 36.997% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1095 \tTraining Loss: 0.01254395 \tValidation Loss 0.01713081 \tTraining Acuuarcy 36.852% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1096 \tTraining Loss: 0.01246589 \tValidation Loss 0.01751486 \tTraining Acuuarcy 36.512% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1097 \tTraining Loss: 0.01256711 \tValidation Loss 0.01671329 \tTraining Acuuarcy 36.529% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1098 \tTraining Loss: 0.01247582 \tValidation Loss 0.01691629 \tTraining Acuuarcy 36.902% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1099 \tTraining Loss: 0.01254892 \tValidation Loss 0.01707476 \tTraining Acuuarcy 36.122% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1100 \tTraining Loss: 0.01251801 \tValidation Loss 0.01695632 \tTraining Acuuarcy 36.947% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1101 \tTraining Loss: 0.01248433 \tValidation Loss 0.01695515 \tTraining Acuuarcy 36.501% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1102 \tTraining Loss: 0.01245138 \tValidation Loss 0.01713788 \tTraining Acuuarcy 37.030% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1103 \tTraining Loss: 0.01255927 \tValidation Loss 0.01721466 \tTraining Acuuarcy 36.378% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1104 \tTraining Loss: 0.01247114 \tValidation Loss 0.01706035 \tTraining Acuuarcy 37.131% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1105 \tTraining Loss: 0.01253114 \tValidation Loss 0.01706881 \tTraining Acuuarcy 36.507% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1106 \tTraining Loss: 0.01256852 \tValidation Loss 0.01691624 \tTraining Acuuarcy 36.612% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1107 \tTraining Loss: 0.01252111 \tValidation Loss 0.01689627 \tTraining Acuuarcy 36.707% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1108 \tTraining Loss: 0.01261893 \tValidation Loss 0.01670003 \tTraining Acuuarcy 35.776% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1109 \tTraining Loss: 0.01252390 \tValidation Loss 0.01697333 \tTraining Acuuarcy 36.468% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1110 \tTraining Loss: 0.01250233 \tValidation Loss 0.01672351 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1111 \tTraining Loss: 0.01253846 \tValidation Loss 0.01682110 \tTraining Acuuarcy 36.674% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1112 \tTraining Loss: 0.01250936 \tValidation Loss 0.01713127 \tTraining Acuuarcy 36.746% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1113 \tTraining Loss: 0.01248939 \tValidation Loss 0.01704678 \tTraining Acuuarcy 36.657% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1114 \tTraining Loss: 0.01249435 \tValidation Loss 0.01699969 \tTraining Acuuarcy 36.830% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1115 \tTraining Loss: 0.01248242 \tValidation Loss 0.01698369 \tTraining Acuuarcy 36.668% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1116 \tTraining Loss: 0.01252043 \tValidation Loss 0.01712857 \tTraining Acuuarcy 36.735% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1117 \tTraining Loss: 0.01252075 \tValidation Loss 0.01673759 \tTraining Acuuarcy 36.707% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1118 \tTraining Loss: 0.01249156 \tValidation Loss 0.01677515 \tTraining Acuuarcy 36.824% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1119 \tTraining Loss: 0.01253958 \tValidation Loss 0.01736750 \tTraining Acuuarcy 36.802% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1120 \tTraining Loss: 0.01248674 \tValidation Loss 0.01713580 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1121 \tTraining Loss: 0.01244147 \tValidation Loss 0.01712468 \tTraining Acuuarcy 37.376% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1122 \tTraining Loss: 0.01254974 \tValidation Loss 0.01710485 \tTraining Acuuarcy 36.813% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1123 \tTraining Loss: 0.01250906 \tValidation Loss 0.01686110 \tTraining Acuuarcy 36.757% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1124 \tTraining Loss: 0.01251004 \tValidation Loss 0.01705172 \tTraining Acuuarcy 36.612% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1125 \tTraining Loss: 0.01257887 \tValidation Loss 0.01676539 \tTraining Acuuarcy 36.395% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1126 \tTraining Loss: 0.01246889 \tValidation Loss 0.01707189 \tTraining Acuuarcy 36.802% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1127 \tTraining Loss: 0.01250198 \tValidation Loss 0.01664263 \tTraining Acuuarcy 36.807% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1128 \tTraining Loss: 0.01246327 \tValidation Loss 0.01691714 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1129 \tTraining Loss: 0.01254875 \tValidation Loss 0.01713611 \tTraining Acuuarcy 36.802% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1130 \tTraining Loss: 0.01255096 \tValidation Loss 0.01716475 \tTraining Acuuarcy 36.796% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1131 \tTraining Loss: 0.01251661 \tValidation Loss 0.01709548 \tTraining Acuuarcy 36.579% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1132 \tTraining Loss: 0.01250363 \tValidation Loss 0.01685466 \tTraining Acuuarcy 36.373% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1133 \tTraining Loss: 0.01250370 \tValidation Loss 0.01688676 \tTraining Acuuarcy 36.456% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1134 \tTraining Loss: 0.01249707 \tValidation Loss 0.01666976 \tTraining Acuuarcy 36.768% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1135 \tTraining Loss: 0.01254201 \tValidation Loss 0.01705678 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1136 \tTraining Loss: 0.01250640 \tValidation Loss 0.01668868 \tTraining Acuuarcy 36.049% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1137 \tTraining Loss: 0.01240756 \tValidation Loss 0.01729693 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1138 \tTraining Loss: 0.01251080 \tValidation Loss 0.01688316 \tTraining Acuuarcy 36.635% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1139 \tTraining Loss: 0.01251595 \tValidation Loss 0.01707965 \tTraining Acuuarcy 36.523% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1140 \tTraining Loss: 0.01244963 \tValidation Loss 0.01694319 \tTraining Acuuarcy 37.103% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1141 \tTraining Loss: 0.01249445 \tValidation Loss 0.01715566 \tTraining Acuuarcy 36.869% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1142 \tTraining Loss: 0.01253286 \tValidation Loss 0.01702358 \tTraining Acuuarcy 36.807% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1143 \tTraining Loss: 0.01255730 \tValidation Loss 0.01678745 \tTraining Acuuarcy 36.713% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1144 \tTraining Loss: 0.01251232 \tValidation Loss 0.01708084 \tTraining Acuuarcy 36.791% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1145 \tTraining Loss: 0.01250676 \tValidation Loss 0.01690846 \tTraining Acuuarcy 37.415% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1146 \tTraining Loss: 0.01252615 \tValidation Loss 0.01700019 \tTraining Acuuarcy 36.718% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1147 \tTraining Loss: 0.01255813 \tValidation Loss 0.01711466 \tTraining Acuuarcy 36.133% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1148 \tTraining Loss: 0.01243800 \tValidation Loss 0.01695291 \tTraining Acuuarcy 36.902% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1149 \tTraining Loss: 0.01251939 \tValidation Loss 0.01720138 \tTraining Acuuarcy 36.300% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1150 \tTraining Loss: 0.01253160 \tValidation Loss 0.01720160 \tTraining Acuuarcy 36.612% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1151 \tTraining Loss: 0.01248887 \tValidation Loss 0.01716770 \tTraining Acuuarcy 36.401% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1152 \tTraining Loss: 0.01249957 \tValidation Loss 0.01641080 \tTraining Acuuarcy 36.618% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1153 \tTraining Loss: 0.01256317 \tValidation Loss 0.01672939 \tTraining Acuuarcy 36.568% \tValidation Acuuarcy 21.148%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1154 \tTraining Loss: 0.01253504 \tValidation Loss 0.01698299 \tTraining Acuuarcy 36.367% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1155 \tTraining Loss: 0.01248800 \tValidation Loss 0.01692688 \tTraining Acuuarcy 36.852% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1156 \tTraining Loss: 0.01251460 \tValidation Loss 0.01672752 \tTraining Acuuarcy 36.551% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1157 \tTraining Loss: 0.01248240 \tValidation Loss 0.01702323 \tTraining Acuuarcy 36.562% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1158 \tTraining Loss: 0.01247051 \tValidation Loss 0.01733967 \tTraining Acuuarcy 36.902% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1159 \tTraining Loss: 0.01254938 \tValidation Loss 0.01705683 \tTraining Acuuarcy 36.512% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1160 \tTraining Loss: 0.01251523 \tValidation Loss 0.01707985 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1161 \tTraining Loss: 0.01252648 \tValidation Loss 0.01695462 \tTraining Acuuarcy 36.729% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 1162 \tTraining Loss: 0.01251263 \tValidation Loss 0.01690249 \tTraining Acuuarcy 36.663% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1163 \tTraining Loss: 0.01247774 \tValidation Loss 0.01712867 \tTraining Acuuarcy 36.807% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 1164 \tTraining Loss: 0.01252129 \tValidation Loss 0.01707392 \tTraining Acuuarcy 37.008% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1165 \tTraining Loss: 0.01241026 \tValidation Loss 0.01722178 \tTraining Acuuarcy 37.337% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 1166 \tTraining Loss: 0.01249674 \tValidation Loss 0.01717710 \tTraining Acuuarcy 36.495% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1167 \tTraining Loss: 0.01251811 \tValidation Loss 0.01658249 \tTraining Acuuarcy 36.629% \tValidation Acuuarcy 21.399%\n",
      "Epoch: 1168 \tTraining Loss: 0.01242961 \tValidation Loss 0.01688633 \tTraining Acuuarcy 37.231% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1169 \tTraining Loss: 0.01258442 \tValidation Loss 0.01698700 \tTraining Acuuarcy 36.534% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1170 \tTraining Loss: 0.01247207 \tValidation Loss 0.01713265 \tTraining Acuuarcy 37.147% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1171 \tTraining Loss: 0.01251032 \tValidation Loss 0.01700130 \tTraining Acuuarcy 37.610% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1172 \tTraining Loss: 0.01250669 \tValidation Loss 0.01689877 \tTraining Acuuarcy 36.217% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1173 \tTraining Loss: 0.01250816 \tValidation Loss 0.01687644 \tTraining Acuuarcy 36.562% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1174 \tTraining Loss: 0.01249108 \tValidation Loss 0.01705538 \tTraining Acuuarcy 37.265% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1175 \tTraining Loss: 0.01244661 \tValidation Loss 0.01703931 \tTraining Acuuarcy 37.387% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1176 \tTraining Loss: 0.01254258 \tValidation Loss 0.01685326 \tTraining Acuuarcy 36.640% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1177 \tTraining Loss: 0.01249762 \tValidation Loss 0.01702437 \tTraining Acuuarcy 36.763% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1178 \tTraining Loss: 0.01254305 \tValidation Loss 0.01689694 \tTraining Acuuarcy 36.607% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1179 \tTraining Loss: 0.01255321 \tValidation Loss 0.01686248 \tTraining Acuuarcy 36.679% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1180 \tTraining Loss: 0.01246555 \tValidation Loss 0.01698725 \tTraining Acuuarcy 36.986% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1181 \tTraining Loss: 0.01244236 \tValidation Loss 0.01703416 \tTraining Acuuarcy 36.964% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1182 \tTraining Loss: 0.01254644 \tValidation Loss 0.01711703 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1183 \tTraining Loss: 0.01247458 \tValidation Loss 0.01716463 \tTraining Acuuarcy 36.908% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1184 \tTraining Loss: 0.01256895 \tValidation Loss 0.01725862 \tTraining Acuuarcy 36.456% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1185 \tTraining Loss: 0.01251641 \tValidation Loss 0.01688145 \tTraining Acuuarcy 36.718% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1186 \tTraining Loss: 0.01260217 \tValidation Loss 0.01707170 \tTraining Acuuarcy 36.194% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1187 \tTraining Loss: 0.01248466 \tValidation Loss 0.01691482 \tTraining Acuuarcy 36.880% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1188 \tTraining Loss: 0.01249439 \tValidation Loss 0.01702748 \tTraining Acuuarcy 36.891% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1189 \tTraining Loss: 0.01249884 \tValidation Loss 0.01686910 \tTraining Acuuarcy 36.802% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1190 \tTraining Loss: 0.01254013 \tValidation Loss 0.01692055 \tTraining Acuuarcy 36.813% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1191 \tTraining Loss: 0.01254643 \tValidation Loss 0.01671335 \tTraining Acuuarcy 36.579% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1192 \tTraining Loss: 0.01249988 \tValidation Loss 0.01663617 \tTraining Acuuarcy 36.964% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1193 \tTraining Loss: 0.01245486 \tValidation Loss 0.01694523 \tTraining Acuuarcy 37.281% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1194 \tTraining Loss: 0.01246917 \tValidation Loss 0.01703587 \tTraining Acuuarcy 37.281% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1195 \tTraining Loss: 0.01252372 \tValidation Loss 0.01695457 \tTraining Acuuarcy 36.618% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1196 \tTraining Loss: 0.01253372 \tValidation Loss 0.01709983 \tTraining Acuuarcy 36.791% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1197 \tTraining Loss: 0.01255354 \tValidation Loss 0.01710565 \tTraining Acuuarcy 36.066% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 1198 \tTraining Loss: 0.01252780 \tValidation Loss 0.01697576 \tTraining Acuuarcy 36.596% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1199 \tTraining Loss: 0.01251248 \tValidation Loss 0.01669057 \tTraining Acuuarcy 36.690% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1200 \tTraining Loss: 0.01250227 \tValidation Loss 0.01697424 \tTraining Acuuarcy 36.651% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1201 \tTraining Loss: 0.01242251 \tValidation Loss 0.01710250 \tTraining Acuuarcy 37.181% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1202 \tTraining Loss: 0.01251205 \tValidation Loss 0.01681663 \tTraining Acuuarcy 37.237% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1203 \tTraining Loss: 0.01254153 \tValidation Loss 0.01710497 \tTraining Acuuarcy 36.529% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1204 \tTraining Loss: 0.01244897 \tValidation Loss 0.01714916 \tTraining Acuuarcy 37.025% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1205 \tTraining Loss: 0.01241861 \tValidation Loss 0.01709934 \tTraining Acuuarcy 37.058% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1206 \tTraining Loss: 0.01255365 \tValidation Loss 0.01692867 \tTraining Acuuarcy 36.507% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1207 \tTraining Loss: 0.01247226 \tValidation Loss 0.01686017 \tTraining Acuuarcy 37.036% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1208 \tTraining Loss: 0.01248567 \tValidation Loss 0.01671815 \tTraining Acuuarcy 36.975% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1209 \tTraining Loss: 0.01248327 \tValidation Loss 0.01720307 \tTraining Acuuarcy 37.125% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 1210 \tTraining Loss: 0.01250372 \tValidation Loss 0.01668852 \tTraining Acuuarcy 36.651% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1211 \tTraining Loss: 0.01254468 \tValidation Loss 0.01693086 \tTraining Acuuarcy 36.685% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1212 \tTraining Loss: 0.01249776 \tValidation Loss 0.01690677 \tTraining Acuuarcy 36.624% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1213 \tTraining Loss: 0.01249316 \tValidation Loss 0.01709945 \tTraining Acuuarcy 36.785% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1214 \tTraining Loss: 0.01249055 \tValidation Loss 0.01687517 \tTraining Acuuarcy 36.919% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1215 \tTraining Loss: 0.01249211 \tValidation Loss 0.01713404 \tTraining Acuuarcy 37.092% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1216 \tTraining Loss: 0.01252833 \tValidation Loss 0.01712935 \tTraining Acuuarcy 36.573% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1217 \tTraining Loss: 0.01249236 \tValidation Loss 0.01697276 \tTraining Acuuarcy 36.908% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1218 \tTraining Loss: 0.01244304 \tValidation Loss 0.01696411 \tTraining Acuuarcy 36.757% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1219 \tTraining Loss: 0.01246652 \tValidation Loss 0.01670907 \tTraining Acuuarcy 36.930% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1220 \tTraining Loss: 0.01246966 \tValidation Loss 0.01705108 \tTraining Acuuarcy 36.941% \tValidation Acuuarcy 18.947%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1221 \tTraining Loss: 0.01257445 \tValidation Loss 0.01665199 \tTraining Acuuarcy 36.434% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1222 \tTraining Loss: 0.01256180 \tValidation Loss 0.01667279 \tTraining Acuuarcy 36.484% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1223 \tTraining Loss: 0.01250701 \tValidation Loss 0.01697191 \tTraining Acuuarcy 36.624% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1224 \tTraining Loss: 0.01252890 \tValidation Loss 0.01701784 \tTraining Acuuarcy 36.172% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1225 \tTraining Loss: 0.01244451 \tValidation Loss 0.01673473 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1226 \tTraining Loss: 0.01248457 \tValidation Loss 0.01682351 \tTraining Acuuarcy 36.646% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1227 \tTraining Loss: 0.01249075 \tValidation Loss 0.01712823 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1228 \tTraining Loss: 0.01250226 \tValidation Loss 0.01698796 \tTraining Acuuarcy 36.512% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1229 \tTraining Loss: 0.01250540 \tValidation Loss 0.01686024 \tTraining Acuuarcy 37.008% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1230 \tTraining Loss: 0.01248565 \tValidation Loss 0.01662633 \tTraining Acuuarcy 36.785% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1231 \tTraining Loss: 0.01251261 \tValidation Loss 0.01688748 \tTraining Acuuarcy 36.534% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1232 \tTraining Loss: 0.01246953 \tValidation Loss 0.01714108 \tTraining Acuuarcy 37.181% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1233 \tTraining Loss: 0.01247360 \tValidation Loss 0.01699848 \tTraining Acuuarcy 36.523% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1234 \tTraining Loss: 0.01249267 \tValidation Loss 0.01699002 \tTraining Acuuarcy 36.651% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1235 \tTraining Loss: 0.01245487 \tValidation Loss 0.01659988 \tTraining Acuuarcy 36.780% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1236 \tTraining Loss: 0.01246412 \tValidation Loss 0.01701795 \tTraining Acuuarcy 36.952% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1237 \tTraining Loss: 0.01247711 \tValidation Loss 0.01727259 \tTraining Acuuarcy 36.947% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1238 \tTraining Loss: 0.01246508 \tValidation Loss 0.01694655 \tTraining Acuuarcy 36.445% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1239 \tTraining Loss: 0.01252091 \tValidation Loss 0.01666602 \tTraining Acuuarcy 36.445% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1240 \tTraining Loss: 0.01246850 \tValidation Loss 0.01696962 \tTraining Acuuarcy 37.030% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1241 \tTraining Loss: 0.01254100 \tValidation Loss 0.01663342 \tTraining Acuuarcy 36.919% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1242 \tTraining Loss: 0.01242425 \tValidation Loss 0.01709884 \tTraining Acuuarcy 36.847% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1243 \tTraining Loss: 0.01247766 \tValidation Loss 0.01677391 \tTraining Acuuarcy 37.019% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1244 \tTraining Loss: 0.01252856 \tValidation Loss 0.01664141 \tTraining Acuuarcy 36.880% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1245 \tTraining Loss: 0.01249163 \tValidation Loss 0.01679011 \tTraining Acuuarcy 36.780% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1246 \tTraining Loss: 0.01252453 \tValidation Loss 0.01686354 \tTraining Acuuarcy 36.384% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1247 \tTraining Loss: 0.01253286 \tValidation Loss 0.01703081 \tTraining Acuuarcy 37.003% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1248 \tTraining Loss: 0.01248050 \tValidation Loss 0.01708076 \tTraining Acuuarcy 37.086% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1249 \tTraining Loss: 0.01248894 \tValidation Loss 0.01678063 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1250 \tTraining Loss: 0.01250707 \tValidation Loss 0.01693370 \tTraining Acuuarcy 36.886% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1251 \tTraining Loss: 0.01244259 \tValidation Loss 0.01689711 \tTraining Acuuarcy 37.588% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1252 \tTraining Loss: 0.01253489 \tValidation Loss 0.01696387 \tTraining Acuuarcy 36.484% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1253 \tTraining Loss: 0.01246525 \tValidation Loss 0.01679837 \tTraining Acuuarcy 36.919% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1254 \tTraining Loss: 0.01244617 \tValidation Loss 0.01689395 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1255 \tTraining Loss: 0.01243971 \tValidation Loss 0.01715430 \tTraining Acuuarcy 37.226% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1256 \tTraining Loss: 0.01247145 \tValidation Loss 0.01716957 \tTraining Acuuarcy 36.757% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1257 \tTraining Loss: 0.01246635 \tValidation Loss 0.01717727 \tTraining Acuuarcy 37.192% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1258 \tTraining Loss: 0.01247738 \tValidation Loss 0.01695792 \tTraining Acuuarcy 36.462% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1259 \tTraining Loss: 0.01250620 \tValidation Loss 0.01674245 \tTraining Acuuarcy 37.259% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1260 \tTraining Loss: 0.01244391 \tValidation Loss 0.01705362 \tTraining Acuuarcy 37.125% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 1261 \tTraining Loss: 0.01248584 \tValidation Loss 0.01701192 \tTraining Acuuarcy 36.891% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1262 \tTraining Loss: 0.01251014 \tValidation Loss 0.01719899 \tTraining Acuuarcy 36.568% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1263 \tTraining Loss: 0.01247981 \tValidation Loss 0.01715876 \tTraining Acuuarcy 37.315% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1264 \tTraining Loss: 0.01249142 \tValidation Loss 0.01682697 \tTraining Acuuarcy 36.752% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1265 \tTraining Loss: 0.01249127 \tValidation Loss 0.01756248 \tTraining Acuuarcy 36.741% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1266 \tTraining Loss: 0.01254491 \tValidation Loss 0.01657410 \tTraining Acuuarcy 36.546% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1267 \tTraining Loss: 0.01252892 \tValidation Loss 0.01679204 \tTraining Acuuarcy 36.389% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1268 \tTraining Loss: 0.01245529 \tValidation Loss 0.01742236 \tTraining Acuuarcy 37.114% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 1269 \tTraining Loss: 0.01246912 \tValidation Loss 0.01692154 \tTraining Acuuarcy 36.947% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1270 \tTraining Loss: 0.01252723 \tValidation Loss 0.01698244 \tTraining Acuuarcy 36.724% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1271 \tTraining Loss: 0.01246026 \tValidation Loss 0.01714855 \tTraining Acuuarcy 36.841% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1272 \tTraining Loss: 0.01246135 \tValidation Loss 0.01703068 \tTraining Acuuarcy 36.930% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1273 \tTraining Loss: 0.01250009 \tValidation Loss 0.01664466 \tTraining Acuuarcy 36.841% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1274 \tTraining Loss: 0.01250827 \tValidation Loss 0.01705663 \tTraining Acuuarcy 36.707% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1275 \tTraining Loss: 0.01242894 \tValidation Loss 0.01710942 \tTraining Acuuarcy 37.192% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1276 \tTraining Loss: 0.01248164 \tValidation Loss 0.01675478 \tTraining Acuuarcy 37.192% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1277 \tTraining Loss: 0.01244541 \tValidation Loss 0.01711934 \tTraining Acuuarcy 36.858% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1278 \tTraining Loss: 0.01251709 \tValidation Loss 0.01683800 \tTraining Acuuarcy 37.075% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 1279 \tTraining Loss: 0.01237744 \tValidation Loss 0.01717690 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1280 \tTraining Loss: 0.01254176 \tValidation Loss 0.01701603 \tTraining Acuuarcy 36.841% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1281 \tTraining Loss: 0.01248003 \tValidation Loss 0.01682812 \tTraining Acuuarcy 36.964% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1282 \tTraining Loss: 0.01248759 \tValidation Loss 0.01666687 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1283 \tTraining Loss: 0.01246387 \tValidation Loss 0.01690806 \tTraining Acuuarcy 37.192% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1284 \tTraining Loss: 0.01247555 \tValidation Loss 0.01719595 \tTraining Acuuarcy 36.852% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1285 \tTraining Loss: 0.01252819 \tValidation Loss 0.01693548 \tTraining Acuuarcy 36.785% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1286 \tTraining Loss: 0.01251302 \tValidation Loss 0.01714485 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1287 \tTraining Loss: 0.01244095 \tValidation Loss 0.01685278 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 19.504%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1288 \tTraining Loss: 0.01251722 \tValidation Loss 0.01692659 \tTraining Acuuarcy 36.663% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1289 \tTraining Loss: 0.01248527 \tValidation Loss 0.01689306 \tTraining Acuuarcy 36.785% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1290 \tTraining Loss: 0.01248365 \tValidation Loss 0.01683329 \tTraining Acuuarcy 36.902% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1291 \tTraining Loss: 0.01253776 \tValidation Loss 0.01669494 \tTraining Acuuarcy 36.512% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1292 \tTraining Loss: 0.01248384 \tValidation Loss 0.01688893 \tTraining Acuuarcy 36.941% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1293 \tTraining Loss: 0.01245611 \tValidation Loss 0.01683362 \tTraining Acuuarcy 37.393% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1294 \tTraining Loss: 0.01245109 \tValidation Loss 0.01718787 \tTraining Acuuarcy 36.986% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1295 \tTraining Loss: 0.01250635 \tValidation Loss 0.01675555 \tTraining Acuuarcy 36.412% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1296 \tTraining Loss: 0.01252276 \tValidation Loss 0.01712706 \tTraining Acuuarcy 36.651% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1297 \tTraining Loss: 0.01245217 \tValidation Loss 0.01735041 \tTraining Acuuarcy 36.936% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1298 \tTraining Loss: 0.01252865 \tValidation Loss 0.01699552 \tTraining Acuuarcy 36.796% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1299 \tTraining Loss: 0.01252589 \tValidation Loss 0.01684528 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1300 \tTraining Loss: 0.01249637 \tValidation Loss 0.01718649 \tTraining Acuuarcy 36.841% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1301 \tTraining Loss: 0.01248791 \tValidation Loss 0.01653503 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1302 \tTraining Loss: 0.01253036 \tValidation Loss 0.01676524 \tTraining Acuuarcy 36.328% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1303 \tTraining Loss: 0.01248850 \tValidation Loss 0.01700830 \tTraining Acuuarcy 36.746% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1304 \tTraining Loss: 0.01243800 \tValidation Loss 0.01710097 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1305 \tTraining Loss: 0.01248524 \tValidation Loss 0.01694905 \tTraining Acuuarcy 36.612% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1306 \tTraining Loss: 0.01243505 \tValidation Loss 0.01689135 \tTraining Acuuarcy 37.008% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1307 \tTraining Loss: 0.01247812 \tValidation Loss 0.01704417 \tTraining Acuuarcy 37.421% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1308 \tTraining Loss: 0.01240798 \tValidation Loss 0.01710986 \tTraining Acuuarcy 37.549% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1309 \tTraining Loss: 0.01246945 \tValidation Loss 0.01664181 \tTraining Acuuarcy 36.986% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1310 \tTraining Loss: 0.01243987 \tValidation Loss 0.01684503 \tTraining Acuuarcy 37.309% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1311 \tTraining Loss: 0.01245081 \tValidation Loss 0.01697776 \tTraining Acuuarcy 36.830% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1312 \tTraining Loss: 0.01246183 \tValidation Loss 0.01714138 \tTraining Acuuarcy 37.186% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1313 \tTraining Loss: 0.01241526 \tValidation Loss 0.01724331 \tTraining Acuuarcy 37.192% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1314 \tTraining Loss: 0.01250022 \tValidation Loss 0.01667380 \tTraining Acuuarcy 37.058% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1315 \tTraining Loss: 0.01248169 \tValidation Loss 0.01692190 \tTraining Acuuarcy 37.086% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1316 \tTraining Loss: 0.01248844 \tValidation Loss 0.01712883 \tTraining Acuuarcy 36.651% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1317 \tTraining Loss: 0.01242929 \tValidation Loss 0.01691198 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1318 \tTraining Loss: 0.01248787 \tValidation Loss 0.01678840 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1319 \tTraining Loss: 0.01243780 \tValidation Loss 0.01705534 \tTraining Acuuarcy 37.287% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1320 \tTraining Loss: 0.01246186 \tValidation Loss 0.01719479 \tTraining Acuuarcy 37.114% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1321 \tTraining Loss: 0.01247535 \tValidation Loss 0.01696140 \tTraining Acuuarcy 36.969% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1322 \tTraining Loss: 0.01255752 \tValidation Loss 0.01688573 \tTraining Acuuarcy 36.367% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1323 \tTraining Loss: 0.01242767 \tValidation Loss 0.01701335 \tTraining Acuuarcy 37.097% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 1324 \tTraining Loss: 0.01246332 \tValidation Loss 0.01687453 \tTraining Acuuarcy 36.763% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1325 \tTraining Loss: 0.01245322 \tValidation Loss 0.01731965 \tTraining Acuuarcy 37.209% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1326 \tTraining Loss: 0.01242144 \tValidation Loss 0.01726493 \tTraining Acuuarcy 37.198% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1327 \tTraining Loss: 0.01244812 \tValidation Loss 0.01734666 \tTraining Acuuarcy 37.253% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1328 \tTraining Loss: 0.01246792 \tValidation Loss 0.01692863 \tTraining Acuuarcy 37.621% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1329 \tTraining Loss: 0.01240915 \tValidation Loss 0.01737009 \tTraining Acuuarcy 37.069% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1330 \tTraining Loss: 0.01249713 \tValidation Loss 0.01720442 \tTraining Acuuarcy 37.175% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1331 \tTraining Loss: 0.01250886 \tValidation Loss 0.01713655 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1332 \tTraining Loss: 0.01247312 \tValidation Loss 0.01683502 \tTraining Acuuarcy 36.618% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1333 \tTraining Loss: 0.01247182 \tValidation Loss 0.01671367 \tTraining Acuuarcy 37.019% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1334 \tTraining Loss: 0.01240817 \tValidation Loss 0.01712195 \tTraining Acuuarcy 37.638% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1335 \tTraining Loss: 0.01240469 \tValidation Loss 0.01691354 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1336 \tTraining Loss: 0.01246134 \tValidation Loss 0.01695047 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1337 \tTraining Loss: 0.01242744 \tValidation Loss 0.01723414 \tTraining Acuuarcy 36.729% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1338 \tTraining Loss: 0.01251442 \tValidation Loss 0.01684845 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1339 \tTraining Loss: 0.01245811 \tValidation Loss 0.01680491 \tTraining Acuuarcy 37.209% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1340 \tTraining Loss: 0.01246725 \tValidation Loss 0.01705913 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1341 \tTraining Loss: 0.01255111 \tValidation Loss 0.01687622 \tTraining Acuuarcy 36.802% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1342 \tTraining Loss: 0.01252728 \tValidation Loss 0.01713225 \tTraining Acuuarcy 36.339% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1343 \tTraining Loss: 0.01252930 \tValidation Loss 0.01667115 \tTraining Acuuarcy 36.579% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1344 \tTraining Loss: 0.01248782 \tValidation Loss 0.01684491 \tTraining Acuuarcy 36.941% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1345 \tTraining Loss: 0.01244611 \tValidation Loss 0.01716501 \tTraining Acuuarcy 37.147% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1346 \tTraining Loss: 0.01246785 \tValidation Loss 0.01731081 \tTraining Acuuarcy 37.448% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1347 \tTraining Loss: 0.01247573 \tValidation Loss 0.01702048 \tTraining Acuuarcy 36.551% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1348 \tTraining Loss: 0.01248372 \tValidation Loss 0.01680644 \tTraining Acuuarcy 36.735% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1349 \tTraining Loss: 0.01242532 \tValidation Loss 0.01701794 \tTraining Acuuarcy 37.298% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1350 \tTraining Loss: 0.01253920 \tValidation Loss 0.01687155 \tTraining Acuuarcy 37.047% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1351 \tTraining Loss: 0.01243729 \tValidation Loss 0.01676819 \tTraining Acuuarcy 37.727% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1352 \tTraining Loss: 0.01248778 \tValidation Loss 0.01670240 \tTraining Acuuarcy 37.030% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1353 \tTraining Loss: 0.01246975 \tValidation Loss 0.01687286 \tTraining Acuuarcy 37.019% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1354 \tTraining Loss: 0.01243107 \tValidation Loss 0.01683576 \tTraining Acuuarcy 37.042% \tValidation Acuuarcy 19.811%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1355 \tTraining Loss: 0.01246596 \tValidation Loss 0.01674556 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1356 \tTraining Loss: 0.01249942 \tValidation Loss 0.01685887 \tTraining Acuuarcy 36.763% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1357 \tTraining Loss: 0.01244207 \tValidation Loss 0.01695343 \tTraining Acuuarcy 37.186% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1358 \tTraining Loss: 0.01248300 \tValidation Loss 0.01744182 \tTraining Acuuarcy 36.741% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1359 \tTraining Loss: 0.01245788 \tValidation Loss 0.01704212 \tTraining Acuuarcy 37.393% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1360 \tTraining Loss: 0.01243465 \tValidation Loss 0.01723418 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1361 \tTraining Loss: 0.01245877 \tValidation Loss 0.01668026 \tTraining Acuuarcy 37.147% \tValidation Acuuarcy 21.427%\n",
      "Epoch: 1362 \tTraining Loss: 0.01245345 \tValidation Loss 0.01708897 \tTraining Acuuarcy 37.019% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1363 \tTraining Loss: 0.01255723 \tValidation Loss 0.01700565 \tTraining Acuuarcy 36.562% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1364 \tTraining Loss: 0.01244309 \tValidation Loss 0.01695885 \tTraining Acuuarcy 37.365% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1365 \tTraining Loss: 0.01243326 \tValidation Loss 0.01692301 \tTraining Acuuarcy 37.343% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1366 \tTraining Loss: 0.01243341 \tValidation Loss 0.01728976 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1367 \tTraining Loss: 0.01249759 \tValidation Loss 0.01721398 \tTraining Acuuarcy 36.997% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1368 \tTraining Loss: 0.01250605 \tValidation Loss 0.01730277 \tTraining Acuuarcy 37.159% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1369 \tTraining Loss: 0.01248977 \tValidation Loss 0.01715430 \tTraining Acuuarcy 36.601% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1370 \tTraining Loss: 0.01241097 \tValidation Loss 0.01696014 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1371 \tTraining Loss: 0.01249905 \tValidation Loss 0.01693991 \tTraining Acuuarcy 36.919% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1372 \tTraining Loss: 0.01243819 \tValidation Loss 0.01660703 \tTraining Acuuarcy 37.025% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1373 \tTraining Loss: 0.01238975 \tValidation Loss 0.01666021 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1374 \tTraining Loss: 0.01244860 \tValidation Loss 0.01702705 \tTraining Acuuarcy 37.097% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1375 \tTraining Loss: 0.01244713 \tValidation Loss 0.01708372 \tTraining Acuuarcy 36.936% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1376 \tTraining Loss: 0.01245055 \tValidation Loss 0.01706681 \tTraining Acuuarcy 37.125% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1377 \tTraining Loss: 0.01238346 \tValidation Loss 0.01724966 \tTraining Acuuarcy 37.487% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1378 \tTraining Loss: 0.01250032 \tValidation Loss 0.01696877 \tTraining Acuuarcy 36.590% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1379 \tTraining Loss: 0.01245846 \tValidation Loss 0.01729275 \tTraining Acuuarcy 36.729% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 1380 \tTraining Loss: 0.01247865 \tValidation Loss 0.01664710 \tTraining Acuuarcy 36.785% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1381 \tTraining Loss: 0.01250494 \tValidation Loss 0.01689804 \tTraining Acuuarcy 36.551% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1382 \tTraining Loss: 0.01246821 \tValidation Loss 0.01727590 \tTraining Acuuarcy 37.036% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1383 \tTraining Loss: 0.01241041 \tValidation Loss 0.01715411 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1384 \tTraining Loss: 0.01242762 \tValidation Loss 0.01706138 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1385 \tTraining Loss: 0.01252914 \tValidation Loss 0.01672179 \tTraining Acuuarcy 36.969% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1386 \tTraining Loss: 0.01246439 \tValidation Loss 0.01724176 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1387 \tTraining Loss: 0.01243384 \tValidation Loss 0.01710137 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1388 \tTraining Loss: 0.01243479 \tValidation Loss 0.01760449 \tTraining Acuuarcy 37.627% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1389 \tTraining Loss: 0.01240619 \tValidation Loss 0.01691214 \tTraining Acuuarcy 37.265% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1390 \tTraining Loss: 0.01243655 \tValidation Loss 0.01702817 \tTraining Acuuarcy 37.181% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1391 \tTraining Loss: 0.01245247 \tValidation Loss 0.01688503 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1392 \tTraining Loss: 0.01243318 \tValidation Loss 0.01721888 \tTraining Acuuarcy 37.359% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 1393 \tTraining Loss: 0.01243496 \tValidation Loss 0.01673006 \tTraining Acuuarcy 37.075% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1394 \tTraining Loss: 0.01245624 \tValidation Loss 0.01709310 \tTraining Acuuarcy 36.819% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 1395 \tTraining Loss: 0.01245574 \tValidation Loss 0.01680001 \tTraining Acuuarcy 37.248% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1396 \tTraining Loss: 0.01246014 \tValidation Loss 0.01691718 \tTraining Acuuarcy 36.975% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 1397 \tTraining Loss: 0.01246120 \tValidation Loss 0.01712566 \tTraining Acuuarcy 37.259% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1398 \tTraining Loss: 0.01241243 \tValidation Loss 0.01721295 \tTraining Acuuarcy 37.175% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1399 \tTraining Loss: 0.01246907 \tValidation Loss 0.01734899 \tTraining Acuuarcy 37.053% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1400 \tTraining Loss: 0.01248814 \tValidation Loss 0.01693226 \tTraining Acuuarcy 37.053% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1401 \tTraining Loss: 0.01250810 \tValidation Loss 0.01696293 \tTraining Acuuarcy 36.690% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1402 \tTraining Loss: 0.01240349 \tValidation Loss 0.01722259 \tTraining Acuuarcy 37.883% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1403 \tTraining Loss: 0.01244394 \tValidation Loss 0.01708955 \tTraining Acuuarcy 37.493% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1404 \tTraining Loss: 0.01246183 \tValidation Loss 0.01692849 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1405 \tTraining Loss: 0.01251934 \tValidation Loss 0.01681676 \tTraining Acuuarcy 36.975% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1406 \tTraining Loss: 0.01243626 \tValidation Loss 0.01724538 \tTraining Acuuarcy 37.136% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1407 \tTraining Loss: 0.01251840 \tValidation Loss 0.01676881 \tTraining Acuuarcy 36.718% \tValidation Acuuarcy 20.758%\n",
      "Epoch: 1408 \tTraining Loss: 0.01241186 \tValidation Loss 0.01704937 \tTraining Acuuarcy 37.136% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1409 \tTraining Loss: 0.01250396 \tValidation Loss 0.01686816 \tTraining Acuuarcy 37.259% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1410 \tTraining Loss: 0.01247648 \tValidation Loss 0.01682986 \tTraining Acuuarcy 36.835% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1411 \tTraining Loss: 0.01242569 \tValidation Loss 0.01744334 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1412 \tTraining Loss: 0.01246679 \tValidation Loss 0.01701564 \tTraining Acuuarcy 37.025% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1413 \tTraining Loss: 0.01246904 \tValidation Loss 0.01702286 \tTraining Acuuarcy 37.120% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1414 \tTraining Loss: 0.01240218 \tValidation Loss 0.01712399 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1415 \tTraining Loss: 0.01243191 \tValidation Loss 0.01729858 \tTraining Acuuarcy 37.460% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1416 \tTraining Loss: 0.01246541 \tValidation Loss 0.01703049 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 17.804%\n",
      "Epoch: 1417 \tTraining Loss: 0.01241003 \tValidation Loss 0.01724841 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1418 \tTraining Loss: 0.01246185 \tValidation Loss 0.01779120 \tTraining Acuuarcy 37.287% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1419 \tTraining Loss: 0.01248060 \tValidation Loss 0.01699817 \tTraining Acuuarcy 37.103% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1420 \tTraining Loss: 0.01249948 \tValidation Loss 0.01705958 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1421 \tTraining Loss: 0.01248723 \tValidation Loss 0.01689250 \tTraining Acuuarcy 36.852% \tValidation Acuuarcy 18.947%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1422 \tTraining Loss: 0.01247731 \tValidation Loss 0.01684150 \tTraining Acuuarcy 37.092% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1423 \tTraining Loss: 0.01249017 \tValidation Loss 0.01701303 \tTraining Acuuarcy 37.421% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1424 \tTraining Loss: 0.01250177 \tValidation Loss 0.01692859 \tTraining Acuuarcy 36.757% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1425 \tTraining Loss: 0.01248025 \tValidation Loss 0.01712524 \tTraining Acuuarcy 36.785% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1426 \tTraining Loss: 0.01248185 \tValidation Loss 0.01671447 \tTraining Acuuarcy 37.359% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 1427 \tTraining Loss: 0.01246709 \tValidation Loss 0.01677901 \tTraining Acuuarcy 36.635% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1428 \tTraining Loss: 0.01243845 \tValidation Loss 0.01709089 \tTraining Acuuarcy 37.482% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1429 \tTraining Loss: 0.01245345 \tValidation Loss 0.01700764 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1430 \tTraining Loss: 0.01244020 \tValidation Loss 0.01711254 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1431 \tTraining Loss: 0.01241544 \tValidation Loss 0.01710125 \tTraining Acuuarcy 37.359% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1432 \tTraining Loss: 0.01241172 \tValidation Loss 0.01693161 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1433 \tTraining Loss: 0.01236350 \tValidation Loss 0.01721835 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1434 \tTraining Loss: 0.01246528 \tValidation Loss 0.01702174 \tTraining Acuuarcy 36.819% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1435 \tTraining Loss: 0.01240755 \tValidation Loss 0.01707064 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1436 \tTraining Loss: 0.01249856 \tValidation Loss 0.01703286 \tTraining Acuuarcy 36.880% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1437 \tTraining Loss: 0.01243808 \tValidation Loss 0.01660529 \tTraining Acuuarcy 37.309% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1438 \tTraining Loss: 0.01248746 \tValidation Loss 0.01714431 \tTraining Acuuarcy 36.841% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1439 \tTraining Loss: 0.01251809 \tValidation Loss 0.01693056 \tTraining Acuuarcy 36.573% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1440 \tTraining Loss: 0.01247304 \tValidation Loss 0.01715611 \tTraining Acuuarcy 36.919% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1441 \tTraining Loss: 0.01242531 \tValidation Loss 0.01699295 \tTraining Acuuarcy 37.198% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1442 \tTraining Loss: 0.01246321 \tValidation Loss 0.01687791 \tTraining Acuuarcy 37.131% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1443 \tTraining Loss: 0.01246279 \tValidation Loss 0.01732251 \tTraining Acuuarcy 37.019% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1444 \tTraining Loss: 0.01235108 \tValidation Loss 0.01726909 \tTraining Acuuarcy 38.056% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1445 \tTraining Loss: 0.01237454 \tValidation Loss 0.01723394 \tTraining Acuuarcy 37.304% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1446 \tTraining Loss: 0.01239050 \tValidation Loss 0.01713559 \tTraining Acuuarcy 37.181% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1447 \tTraining Loss: 0.01247488 \tValidation Loss 0.01722996 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1448 \tTraining Loss: 0.01249994 \tValidation Loss 0.01675682 \tTraining Acuuarcy 36.780% \tValidation Acuuarcy 20.897%\n",
      "Epoch: 1449 \tTraining Loss: 0.01249928 \tValidation Loss 0.01692645 \tTraining Acuuarcy 36.713% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1450 \tTraining Loss: 0.01241056 \tValidation Loss 0.01718822 \tTraining Acuuarcy 37.521% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1451 \tTraining Loss: 0.01241316 \tValidation Loss 0.01748503 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1452 \tTraining Loss: 0.01243677 \tValidation Loss 0.01673590 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1453 \tTraining Loss: 0.01246489 \tValidation Loss 0.01731175 \tTraining Acuuarcy 37.003% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1454 \tTraining Loss: 0.01244846 \tValidation Loss 0.01688237 \tTraining Acuuarcy 37.621% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1455 \tTraining Loss: 0.01244433 \tValidation Loss 0.01741314 \tTraining Acuuarcy 37.309% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1456 \tTraining Loss: 0.01247381 \tValidation Loss 0.01711770 \tTraining Acuuarcy 37.042% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1457 \tTraining Loss: 0.01244989 \tValidation Loss 0.01704582 \tTraining Acuuarcy 37.493% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 1458 \tTraining Loss: 0.01236171 \tValidation Loss 0.01687357 \tTraining Acuuarcy 37.493% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1459 \tTraining Loss: 0.01244426 \tValidation Loss 0.01764329 \tTraining Acuuarcy 36.925% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1460 \tTraining Loss: 0.01242717 \tValidation Loss 0.01709945 \tTraining Acuuarcy 37.231% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1461 \tTraining Loss: 0.01245298 \tValidation Loss 0.01721843 \tTraining Acuuarcy 37.304% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1462 \tTraining Loss: 0.01246661 \tValidation Loss 0.01703578 \tTraining Acuuarcy 37.131% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1463 \tTraining Loss: 0.01251077 \tValidation Loss 0.01681373 \tTraining Acuuarcy 36.412% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1464 \tTraining Loss: 0.01239016 \tValidation Loss 0.01696633 \tTraining Acuuarcy 37.387% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1465 \tTraining Loss: 0.01245450 \tValidation Loss 0.01693325 \tTraining Acuuarcy 37.270% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1466 \tTraining Loss: 0.01245330 \tValidation Loss 0.01701826 \tTraining Acuuarcy 37.064% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 1467 \tTraining Loss: 0.01241290 \tValidation Loss 0.01702672 \tTraining Acuuarcy 37.844% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1468 \tTraining Loss: 0.01242461 \tValidation Loss 0.01753460 \tTraining Acuuarcy 37.220% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1469 \tTraining Loss: 0.01243030 \tValidation Loss 0.01693898 \tTraining Acuuarcy 37.515% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1470 \tTraining Loss: 0.01248276 \tValidation Loss 0.01722022 \tTraining Acuuarcy 36.975% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1471 \tTraining Loss: 0.01248531 \tValidation Loss 0.01686136 \tTraining Acuuarcy 36.947% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1472 \tTraining Loss: 0.01244073 \tValidation Loss 0.01718074 \tTraining Acuuarcy 36.847% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1473 \tTraining Loss: 0.01244929 \tValidation Loss 0.01693176 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1474 \tTraining Loss: 0.01240557 \tValidation Loss 0.01715145 \tTraining Acuuarcy 37.421% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1475 \tTraining Loss: 0.01242921 \tValidation Loss 0.01709144 \tTraining Acuuarcy 37.844% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1476 \tTraining Loss: 0.01234982 \tValidation Loss 0.01707897 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 1477 \tTraining Loss: 0.01248406 \tValidation Loss 0.01720237 \tTraining Acuuarcy 36.980% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1478 \tTraining Loss: 0.01246609 \tValidation Loss 0.01701257 \tTraining Acuuarcy 36.830% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1479 \tTraining Loss: 0.01243476 \tValidation Loss 0.01687830 \tTraining Acuuarcy 36.858% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1480 \tTraining Loss: 0.01243859 \tValidation Loss 0.01707545 \tTraining Acuuarcy 37.393% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1481 \tTraining Loss: 0.01235924 \tValidation Loss 0.01743478 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1482 \tTraining Loss: 0.01249844 \tValidation Loss 0.01700125 \tTraining Acuuarcy 36.568% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1483 \tTraining Loss: 0.01246123 \tValidation Loss 0.01699731 \tTraining Acuuarcy 37.214% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1484 \tTraining Loss: 0.01247572 \tValidation Loss 0.01687697 \tTraining Acuuarcy 37.443% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1485 \tTraining Loss: 0.01249414 \tValidation Loss 0.01707885 \tTraining Acuuarcy 37.159% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1486 \tTraining Loss: 0.01240379 \tValidation Loss 0.01710887 \tTraining Acuuarcy 37.220% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1487 \tTraining Loss: 0.01240175 \tValidation Loss 0.01693810 \tTraining Acuuarcy 37.515% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1488 \tTraining Loss: 0.01241079 \tValidation Loss 0.01703592 \tTraining Acuuarcy 37.515% \tValidation Acuuarcy 19.699%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1489 \tTraining Loss: 0.01243534 \tValidation Loss 0.01710609 \tTraining Acuuarcy 37.387% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1490 \tTraining Loss: 0.01241529 \tValidation Loss 0.01705953 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1491 \tTraining Loss: 0.01246389 \tValidation Loss 0.01687575 \tTraining Acuuarcy 36.925% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1492 \tTraining Loss: 0.01240257 \tValidation Loss 0.01702992 \tTraining Acuuarcy 37.337% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1493 \tTraining Loss: 0.01243374 \tValidation Loss 0.01721426 \tTraining Acuuarcy 37.003% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1494 \tTraining Loss: 0.01245206 \tValidation Loss 0.01716586 \tTraining Acuuarcy 37.186% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1495 \tTraining Loss: 0.01244360 \tValidation Loss 0.01714068 \tTraining Acuuarcy 36.908% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1496 \tTraining Loss: 0.01251206 \tValidation Loss 0.01682072 \tTraining Acuuarcy 36.473% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1497 \tTraining Loss: 0.01234835 \tValidation Loss 0.01718214 \tTraining Acuuarcy 37.822% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1498 \tTraining Loss: 0.01245282 \tValidation Loss 0.01716246 \tTraining Acuuarcy 37.069% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1499 \tTraining Loss: 0.01250896 \tValidation Loss 0.01680777 \tTraining Acuuarcy 37.008% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1500 \tTraining Loss: 0.01248747 \tValidation Loss 0.01664517 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1501 \tTraining Loss: 0.01246101 \tValidation Loss 0.01693195 \tTraining Acuuarcy 36.930% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1502 \tTraining Loss: 0.01242915 \tValidation Loss 0.01686740 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1503 \tTraining Loss: 0.01255389 \tValidation Loss 0.01716475 \tTraining Acuuarcy 36.807% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1504 \tTraining Loss: 0.01241642 \tValidation Loss 0.01713919 \tTraining Acuuarcy 37.253% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1505 \tTraining Loss: 0.01242806 \tValidation Loss 0.01701575 \tTraining Acuuarcy 37.209% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1506 \tTraining Loss: 0.01246651 \tValidation Loss 0.01730665 \tTraining Acuuarcy 37.198% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1507 \tTraining Loss: 0.01253703 \tValidation Loss 0.01672426 \tTraining Acuuarcy 36.969% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1508 \tTraining Loss: 0.01243150 \tValidation Loss 0.01725111 \tTraining Acuuarcy 37.125% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1509 \tTraining Loss: 0.01246359 \tValidation Loss 0.01695579 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1510 \tTraining Loss: 0.01246212 \tValidation Loss 0.01713055 \tTraining Acuuarcy 36.913% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1511 \tTraining Loss: 0.01243038 \tValidation Loss 0.01699643 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1512 \tTraining Loss: 0.01247895 \tValidation Loss 0.01709510 \tTraining Acuuarcy 36.902% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1513 \tTraining Loss: 0.01250466 \tValidation Loss 0.01681294 \tTraining Acuuarcy 36.880% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1514 \tTraining Loss: 0.01249102 \tValidation Loss 0.01705767 \tTraining Acuuarcy 36.858% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1515 \tTraining Loss: 0.01244112 \tValidation Loss 0.01696189 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1516 \tTraining Loss: 0.01248028 \tValidation Loss 0.01698632 \tTraining Acuuarcy 36.880% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1517 \tTraining Loss: 0.01236664 \tValidation Loss 0.01699592 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1518 \tTraining Loss: 0.01240426 \tValidation Loss 0.01713968 \tTraining Acuuarcy 37.621% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1519 \tTraining Loss: 0.01243686 \tValidation Loss 0.01713365 \tTraining Acuuarcy 36.607% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1520 \tTraining Loss: 0.01239442 \tValidation Loss 0.01700826 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1521 \tTraining Loss: 0.01251152 \tValidation Loss 0.01704550 \tTraining Acuuarcy 36.908% \tValidation Acuuarcy 21.232%\n",
      "Epoch: 1522 \tTraining Loss: 0.01248704 \tValidation Loss 0.01738684 \tTraining Acuuarcy 36.250% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1523 \tTraining Loss: 0.01243950 \tValidation Loss 0.01699786 \tTraining Acuuarcy 37.287% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 1524 \tTraining Loss: 0.01241030 \tValidation Loss 0.01724190 \tTraining Acuuarcy 37.554% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1525 \tTraining Loss: 0.01243148 \tValidation Loss 0.01678538 \tTraining Acuuarcy 36.830% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 1526 \tTraining Loss: 0.01248518 \tValidation Loss 0.01725704 \tTraining Acuuarcy 37.298% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1527 \tTraining Loss: 0.01240903 \tValidation Loss 0.01702031 \tTraining Acuuarcy 36.774% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1528 \tTraining Loss: 0.01245452 \tValidation Loss 0.01674705 \tTraining Acuuarcy 37.192% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1529 \tTraining Loss: 0.01245672 \tValidation Loss 0.01693967 \tTraining Acuuarcy 36.952% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1530 \tTraining Loss: 0.01241733 \tValidation Loss 0.01702495 \tTraining Acuuarcy 37.042% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1531 \tTraining Loss: 0.01244147 \tValidation Loss 0.01698086 \tTraining Acuuarcy 36.802% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1532 \tTraining Loss: 0.01242046 \tValidation Loss 0.01686711 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1533 \tTraining Loss: 0.01253134 \tValidation Loss 0.01703428 \tTraining Acuuarcy 36.540% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1534 \tTraining Loss: 0.01240826 \tValidation Loss 0.01689258 \tTraining Acuuarcy 37.248% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1535 \tTraining Loss: 0.01246312 \tValidation Loss 0.01712705 \tTraining Acuuarcy 37.265% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1536 \tTraining Loss: 0.01248399 \tValidation Loss 0.01683016 \tTraining Acuuarcy 36.585% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1537 \tTraining Loss: 0.01242017 \tValidation Loss 0.01714847 \tTraining Acuuarcy 37.203% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1538 \tTraining Loss: 0.01243290 \tValidation Loss 0.01695842 \tTraining Acuuarcy 36.874% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1539 \tTraining Loss: 0.01246017 \tValidation Loss 0.01718947 \tTraining Acuuarcy 36.947% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1540 \tTraining Loss: 0.01247820 \tValidation Loss 0.01718855 \tTraining Acuuarcy 37.042% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1541 \tTraining Loss: 0.01250832 \tValidation Loss 0.01676722 \tTraining Acuuarcy 36.729% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1542 \tTraining Loss: 0.01245449 \tValidation Loss 0.01685581 \tTraining Acuuarcy 37.103% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1543 \tTraining Loss: 0.01239220 \tValidation Loss 0.01681344 \tTraining Acuuarcy 37.866% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1544 \tTraining Loss: 0.01247265 \tValidation Loss 0.01673653 \tTraining Acuuarcy 36.902% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 1545 \tTraining Loss: 0.01248548 \tValidation Loss 0.01681374 \tTraining Acuuarcy 36.635% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1546 \tTraining Loss: 0.01238211 \tValidation Loss 0.01693576 \tTraining Acuuarcy 37.811% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1547 \tTraining Loss: 0.01248422 \tValidation Loss 0.01700588 \tTraining Acuuarcy 36.897% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1548 \tTraining Loss: 0.01252635 \tValidation Loss 0.01695071 \tTraining Acuuarcy 36.573% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1549 \tTraining Loss: 0.01249950 \tValidation Loss 0.01687388 \tTraining Acuuarcy 37.214% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1550 \tTraining Loss: 0.01242327 \tValidation Loss 0.01685661 \tTraining Acuuarcy 37.131% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1551 \tTraining Loss: 0.01246510 \tValidation Loss 0.01724261 \tTraining Acuuarcy 36.869% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1552 \tTraining Loss: 0.01238166 \tValidation Loss 0.01756190 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1553 \tTraining Loss: 0.01241209 \tValidation Loss 0.01734289 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1554 \tTraining Loss: 0.01236453 \tValidation Loss 0.01729819 \tTraining Acuuarcy 37.103% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1555 \tTraining Loss: 0.01247500 \tValidation Loss 0.01684789 \tTraining Acuuarcy 37.209% \tValidation Acuuarcy 19.978%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1556 \tTraining Loss: 0.01239602 \tValidation Loss 0.01715216 \tTraining Acuuarcy 37.738% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1557 \tTraining Loss: 0.01246059 \tValidation Loss 0.01713170 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1558 \tTraining Loss: 0.01248606 \tValidation Loss 0.01705834 \tTraining Acuuarcy 36.980% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1559 \tTraining Loss: 0.01243855 \tValidation Loss 0.01687187 \tTraining Acuuarcy 37.393% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1560 \tTraining Loss: 0.01244836 \tValidation Loss 0.01696567 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1561 \tTraining Loss: 0.01244656 \tValidation Loss 0.01677403 \tTraining Acuuarcy 37.198% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1562 \tTraining Loss: 0.01246959 \tValidation Loss 0.01703570 \tTraining Acuuarcy 36.858% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1563 \tTraining Loss: 0.01244298 \tValidation Loss 0.01673932 \tTraining Acuuarcy 37.259% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1564 \tTraining Loss: 0.01246148 \tValidation Loss 0.01711107 \tTraining Acuuarcy 37.343% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1565 \tTraining Loss: 0.01239652 \tValidation Loss 0.01688372 \tTraining Acuuarcy 37.220% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1566 \tTraining Loss: 0.01243083 \tValidation Loss 0.01719116 \tTraining Acuuarcy 36.969% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1567 \tTraining Loss: 0.01244004 \tValidation Loss 0.01706820 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1568 \tTraining Loss: 0.01244304 \tValidation Loss 0.01708719 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1569 \tTraining Loss: 0.01244996 \tValidation Loss 0.01726282 \tTraining Acuuarcy 37.226% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1570 \tTraining Loss: 0.01243464 \tValidation Loss 0.01718310 \tTraining Acuuarcy 37.644% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1571 \tTraining Loss: 0.01241287 \tValidation Loss 0.01743502 \tTraining Acuuarcy 37.203% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1572 \tTraining Loss: 0.01242069 \tValidation Loss 0.01711964 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1573 \tTraining Loss: 0.01241253 \tValidation Loss 0.01714414 \tTraining Acuuarcy 37.621% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1574 \tTraining Loss: 0.01234851 \tValidation Loss 0.01699379 \tTraining Acuuarcy 37.978% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1575 \tTraining Loss: 0.01242544 \tValidation Loss 0.01699747 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1576 \tTraining Loss: 0.01251000 \tValidation Loss 0.01677738 \tTraining Acuuarcy 36.813% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1577 \tTraining Loss: 0.01238115 \tValidation Loss 0.01694316 \tTraining Acuuarcy 37.265% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1578 \tTraining Loss: 0.01240563 \tValidation Loss 0.01709665 \tTraining Acuuarcy 37.749% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1579 \tTraining Loss: 0.01240617 \tValidation Loss 0.01709151 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1580 \tTraining Loss: 0.01241809 \tValidation Loss 0.01678782 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1581 \tTraining Loss: 0.01243822 \tValidation Loss 0.01727262 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 1582 \tTraining Loss: 0.01243386 \tValidation Loss 0.01688290 \tTraining Acuuarcy 37.850% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1583 \tTraining Loss: 0.01247270 \tValidation Loss 0.01717689 \tTraining Acuuarcy 37.326% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 1584 \tTraining Loss: 0.01245792 \tValidation Loss 0.01674719 \tTraining Acuuarcy 36.863% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1585 \tTraining Loss: 0.01247117 \tValidation Loss 0.01699561 \tTraining Acuuarcy 37.170% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1586 \tTraining Loss: 0.01240950 \tValidation Loss 0.01715647 \tTraining Acuuarcy 37.147% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1587 \tTraining Loss: 0.01245952 \tValidation Loss 0.01738592 \tTraining Acuuarcy 36.774% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1588 \tTraining Loss: 0.01247527 \tValidation Loss 0.01708766 \tTraining Acuuarcy 36.852% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1589 \tTraining Loss: 0.01242451 \tValidation Loss 0.01703147 \tTraining Acuuarcy 37.560% \tValidation Acuuarcy 18.139%\n",
      "Epoch: 1590 \tTraining Loss: 0.01243860 \tValidation Loss 0.01694734 \tTraining Acuuarcy 37.086% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1591 \tTraining Loss: 0.01247046 \tValidation Loss 0.01706668 \tTraining Acuuarcy 36.824% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1592 \tTraining Loss: 0.01245084 \tValidation Loss 0.01683181 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1593 \tTraining Loss: 0.01242784 \tValidation Loss 0.01717488 \tTraining Acuuarcy 36.964% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1594 \tTraining Loss: 0.01242959 \tValidation Loss 0.01681265 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1595 \tTraining Loss: 0.01242392 \tValidation Loss 0.01699266 \tTraining Acuuarcy 37.448% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1596 \tTraining Loss: 0.01241702 \tValidation Loss 0.01704310 \tTraining Acuuarcy 37.337% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1597 \tTraining Loss: 0.01245609 \tValidation Loss 0.01713539 \tTraining Acuuarcy 37.108% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1598 \tTraining Loss: 0.01249118 \tValidation Loss 0.01703251 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1599 \tTraining Loss: 0.01237367 \tValidation Loss 0.01688206 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1600 \tTraining Loss: 0.01243285 \tValidation Loss 0.01685801 \tTraining Acuuarcy 37.120% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1601 \tTraining Loss: 0.01242496 \tValidation Loss 0.01699375 \tTraining Acuuarcy 37.526% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1602 \tTraining Loss: 0.01239375 \tValidation Loss 0.01721603 \tTraining Acuuarcy 37.181% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1603 \tTraining Loss: 0.01243370 \tValidation Loss 0.01714171 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1604 \tTraining Loss: 0.01240788 \tValidation Loss 0.01685907 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1605 \tTraining Loss: 0.01246355 \tValidation Loss 0.01700340 \tTraining Acuuarcy 36.824% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1606 \tTraining Loss: 0.01237366 \tValidation Loss 0.01726154 \tTraining Acuuarcy 37.811% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1607 \tTraining Loss: 0.01244730 \tValidation Loss 0.01699027 \tTraining Acuuarcy 37.270% \tValidation Acuuarcy 18.334%\n",
      "Epoch: 1608 \tTraining Loss: 0.01246523 \tValidation Loss 0.01739731 \tTraining Acuuarcy 37.058% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1609 \tTraining Loss: 0.01244101 \tValidation Loss 0.01676015 \tTraining Acuuarcy 37.220% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1610 \tTraining Loss: 0.01249097 \tValidation Loss 0.01674817 \tTraining Acuuarcy 36.930% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1611 \tTraining Loss: 0.01239362 \tValidation Loss 0.01713887 \tTraining Acuuarcy 37.287% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1612 \tTraining Loss: 0.01245878 \tValidation Loss 0.01709230 \tTraining Acuuarcy 37.404% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1613 \tTraining Loss: 0.01238583 \tValidation Loss 0.01707308 \tTraining Acuuarcy 37.298% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1614 \tTraining Loss: 0.01238802 \tValidation Loss 0.01735000 \tTraining Acuuarcy 37.833% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1615 \tTraining Loss: 0.01243308 \tValidation Loss 0.01709959 \tTraining Acuuarcy 37.415% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1616 \tTraining Loss: 0.01245948 \tValidation Loss 0.01706257 \tTraining Acuuarcy 36.746% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1617 \tTraining Loss: 0.01241279 \tValidation Loss 0.01687508 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1618 \tTraining Loss: 0.01246054 \tValidation Loss 0.01731586 \tTraining Acuuarcy 37.175% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1619 \tTraining Loss: 0.01246669 \tValidation Loss 0.01710782 \tTraining Acuuarcy 37.409% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1620 \tTraining Loss: 0.01241188 \tValidation Loss 0.01691732 \tTraining Acuuarcy 36.902% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1621 \tTraining Loss: 0.01240133 \tValidation Loss 0.01713645 \tTraining Acuuarcy 37.599% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1622 \tTraining Loss: 0.01236585 \tValidation Loss 0.01674924 \tTraining Acuuarcy 37.343% \tValidation Acuuarcy 19.420%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1623 \tTraining Loss: 0.01240725 \tValidation Loss 0.01715383 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1624 \tTraining Loss: 0.01250732 \tValidation Loss 0.01679671 \tTraining Acuuarcy 36.841% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1625 \tTraining Loss: 0.01236876 \tValidation Loss 0.01693083 \tTraining Acuuarcy 37.404% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1626 \tTraining Loss: 0.01239686 \tValidation Loss 0.01724964 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 1627 \tTraining Loss: 0.01243678 \tValidation Loss 0.01703109 \tTraining Acuuarcy 37.460% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1628 \tTraining Loss: 0.01240866 \tValidation Loss 0.01694171 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1629 \tTraining Loss: 0.01243568 \tValidation Loss 0.01719203 \tTraining Acuuarcy 37.359% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1630 \tTraining Loss: 0.01242279 \tValidation Loss 0.01716624 \tTraining Acuuarcy 37.761% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 1631 \tTraining Loss: 0.01241168 \tValidation Loss 0.01698151 \tTraining Acuuarcy 37.259% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1632 \tTraining Loss: 0.01238048 \tValidation Loss 0.01690313 \tTraining Acuuarcy 37.387% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1633 \tTraining Loss: 0.01240442 \tValidation Loss 0.01705269 \tTraining Acuuarcy 37.382% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1634 \tTraining Loss: 0.01240444 \tValidation Loss 0.01728567 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 1635 \tTraining Loss: 0.01236218 \tValidation Loss 0.01728642 \tTraining Acuuarcy 37.454% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1636 \tTraining Loss: 0.01235819 \tValidation Loss 0.01721805 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1637 \tTraining Loss: 0.01239743 \tValidation Loss 0.01706782 \tTraining Acuuarcy 37.621% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1638 \tTraining Loss: 0.01252524 \tValidation Loss 0.01698248 \tTraining Acuuarcy 36.618% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1639 \tTraining Loss: 0.01239514 \tValidation Loss 0.01693640 \tTraining Acuuarcy 38.128% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1640 \tTraining Loss: 0.01239882 \tValidation Loss 0.01716252 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1641 \tTraining Loss: 0.01236984 \tValidation Loss 0.01709811 \tTraining Acuuarcy 37.387% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1642 \tTraining Loss: 0.01241827 \tValidation Loss 0.01706963 \tTraining Acuuarcy 37.855% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1643 \tTraining Loss: 0.01242008 \tValidation Loss 0.01735754 \tTraining Acuuarcy 37.560% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1644 \tTraining Loss: 0.01242811 \tValidation Loss 0.01734316 \tTraining Acuuarcy 37.147% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1645 \tTraining Loss: 0.01244000 \tValidation Loss 0.01701690 \tTraining Acuuarcy 36.941% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 1646 \tTraining Loss: 0.01240359 \tValidation Loss 0.01715751 \tTraining Acuuarcy 37.699% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1647 \tTraining Loss: 0.01238174 \tValidation Loss 0.01757005 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 1648 \tTraining Loss: 0.01239461 \tValidation Loss 0.01738237 \tTraining Acuuarcy 37.482% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1649 \tTraining Loss: 0.01243701 \tValidation Loss 0.01691586 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1650 \tTraining Loss: 0.01243338 \tValidation Loss 0.01735672 \tTraining Acuuarcy 37.749% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1651 \tTraining Loss: 0.01239370 \tValidation Loss 0.01727881 \tTraining Acuuarcy 37.493% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1652 \tTraining Loss: 0.01237950 \tValidation Loss 0.01703358 \tTraining Acuuarcy 37.978% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 1653 \tTraining Loss: 0.01240439 \tValidation Loss 0.01741323 \tTraining Acuuarcy 37.616% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1654 \tTraining Loss: 0.01239092 \tValidation Loss 0.01699676 \tTraining Acuuarcy 37.432% \tValidation Acuuarcy 20.869%\n",
      "Epoch: 1655 \tTraining Loss: 0.01241607 \tValidation Loss 0.01689865 \tTraining Acuuarcy 37.710% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1656 \tTraining Loss: 0.01238912 \tValidation Loss 0.01686130 \tTraining Acuuarcy 37.577% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1657 \tTraining Loss: 0.01241721 \tValidation Loss 0.01708769 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1658 \tTraining Loss: 0.01243762 \tValidation Loss 0.01719675 \tTraining Acuuarcy 37.209% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1659 \tTraining Loss: 0.01245427 \tValidation Loss 0.01676230 \tTraining Acuuarcy 36.980% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1660 \tTraining Loss: 0.01241897 \tValidation Loss 0.01705030 \tTraining Acuuarcy 38.011% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1661 \tTraining Loss: 0.01243578 \tValidation Loss 0.01744998 \tTraining Acuuarcy 37.170% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1662 \tTraining Loss: 0.01245589 \tValidation Loss 0.01674566 \tTraining Acuuarcy 37.304% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1663 \tTraining Loss: 0.01244224 \tValidation Loss 0.01698446 \tTraining Acuuarcy 37.482% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1664 \tTraining Loss: 0.01238706 \tValidation Loss 0.01700047 \tTraining Acuuarcy 38.134% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1665 \tTraining Loss: 0.01241757 \tValidation Loss 0.01691974 \tTraining Acuuarcy 36.824% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1666 \tTraining Loss: 0.01235043 \tValidation Loss 0.01715154 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1667 \tTraining Loss: 0.01237097 \tValidation Loss 0.01750829 \tTraining Acuuarcy 37.855% \tValidation Acuuarcy 17.972%\n",
      "Epoch: 1668 \tTraining Loss: 0.01243222 \tValidation Loss 0.01695278 \tTraining Acuuarcy 37.270% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1669 \tTraining Loss: 0.01242347 \tValidation Loss 0.01706781 \tTraining Acuuarcy 37.549% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1670 \tTraining Loss: 0.01236404 \tValidation Loss 0.01715690 \tTraining Acuuarcy 37.610% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1671 \tTraining Loss: 0.01246050 \tValidation Loss 0.01694142 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1672 \tTraining Loss: 0.01237770 \tValidation Loss 0.01725910 \tTraining Acuuarcy 37.800% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1673 \tTraining Loss: 0.01245791 \tValidation Loss 0.01714211 \tTraining Acuuarcy 37.019% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1674 \tTraining Loss: 0.01242092 \tValidation Loss 0.01696194 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1675 \tTraining Loss: 0.01240866 \tValidation Loss 0.01719542 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1676 \tTraining Loss: 0.01244655 \tValidation Loss 0.01695733 \tTraining Acuuarcy 36.980% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1677 \tTraining Loss: 0.01241538 \tValidation Loss 0.01718914 \tTraining Acuuarcy 37.265% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1678 \tTraining Loss: 0.01244956 \tValidation Loss 0.01706547 \tTraining Acuuarcy 37.142% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1679 \tTraining Loss: 0.01238826 \tValidation Loss 0.01713060 \tTraining Acuuarcy 37.359% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1680 \tTraining Loss: 0.01241405 \tValidation Loss 0.01684660 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1681 \tTraining Loss: 0.01237112 \tValidation Loss 0.01723590 \tTraining Acuuarcy 37.911% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1682 \tTraining Loss: 0.01243062 \tValidation Loss 0.01695615 \tTraining Acuuarcy 36.869% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1683 \tTraining Loss: 0.01245070 \tValidation Loss 0.01714297 \tTraining Acuuarcy 37.454% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1684 \tTraining Loss: 0.01246717 \tValidation Loss 0.01684084 \tTraining Acuuarcy 36.813% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 1685 \tTraining Loss: 0.01233696 \tValidation Loss 0.01681839 \tTraining Acuuarcy 37.744% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1686 \tTraining Loss: 0.01244178 \tValidation Loss 0.01695213 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1687 \tTraining Loss: 0.01244468 \tValidation Loss 0.01699132 \tTraining Acuuarcy 36.958% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1688 \tTraining Loss: 0.01244304 \tValidation Loss 0.01704417 \tTraining Acuuarcy 37.281% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1689 \tTraining Loss: 0.01240542 \tValidation Loss 0.01713598 \tTraining Acuuarcy 37.265% \tValidation Acuuarcy 18.835%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1690 \tTraining Loss: 0.01236177 \tValidation Loss 0.01731892 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 1691 \tTraining Loss: 0.01243017 \tValidation Loss 0.01723111 \tTraining Acuuarcy 37.409% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1692 \tTraining Loss: 0.01240219 \tValidation Loss 0.01726860 \tTraining Acuuarcy 37.281% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1693 \tTraining Loss: 0.01235488 \tValidation Loss 0.01733705 \tTraining Acuuarcy 38.011% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 1694 \tTraining Loss: 0.01241387 \tValidation Loss 0.01695574 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1695 \tTraining Loss: 0.01247932 \tValidation Loss 0.01696505 \tTraining Acuuarcy 37.136% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1696 \tTraining Loss: 0.01245309 \tValidation Loss 0.01718405 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 1697 \tTraining Loss: 0.01241699 \tValidation Loss 0.01726602 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1698 \tTraining Loss: 0.01243968 \tValidation Loss 0.01699866 \tTraining Acuuarcy 37.231% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 1699 \tTraining Loss: 0.01246905 \tValidation Loss 0.01711485 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 20.786%\n",
      "Epoch: 1700 \tTraining Loss: 0.01246475 \tValidation Loss 0.01713219 \tTraining Acuuarcy 37.069% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1701 \tTraining Loss: 0.01246391 \tValidation Loss 0.01710175 \tTraining Acuuarcy 36.847% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1702 \tTraining Loss: 0.01243582 \tValidation Loss 0.01712950 \tTraining Acuuarcy 36.874% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1703 \tTraining Loss: 0.01238770 \tValidation Loss 0.01697245 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1704 \tTraining Loss: 0.01243538 \tValidation Loss 0.01757411 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1705 \tTraining Loss: 0.01239844 \tValidation Loss 0.01689071 \tTraining Acuuarcy 37.755% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1706 \tTraining Loss: 0.01247619 \tValidation Loss 0.01689222 \tTraining Acuuarcy 36.847% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1707 \tTraining Loss: 0.01241251 \tValidation Loss 0.01717283 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1708 \tTraining Loss: 0.01246067 \tValidation Loss 0.01722885 \tTraining Acuuarcy 36.830% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1709 \tTraining Loss: 0.01241506 \tValidation Loss 0.01724534 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1710 \tTraining Loss: 0.01241862 \tValidation Loss 0.01708188 \tTraining Acuuarcy 37.175% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1711 \tTraining Loss: 0.01239984 \tValidation Loss 0.01734959 \tTraining Acuuarcy 37.354% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1712 \tTraining Loss: 0.01236405 \tValidation Loss 0.01729865 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1713 \tTraining Loss: 0.01243530 \tValidation Loss 0.01715180 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1714 \tTraining Loss: 0.01238495 \tValidation Loss 0.01702496 \tTraining Acuuarcy 37.253% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1715 \tTraining Loss: 0.01243562 \tValidation Loss 0.01732590 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 1716 \tTraining Loss: 0.01246915 \tValidation Loss 0.01744275 \tTraining Acuuarcy 36.919% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1717 \tTraining Loss: 0.01243735 \tValidation Loss 0.01711987 \tTraining Acuuarcy 37.253% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 1718 \tTraining Loss: 0.01238407 \tValidation Loss 0.01714792 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1719 \tTraining Loss: 0.01242553 \tValidation Loss 0.01666009 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1720 \tTraining Loss: 0.01241831 \tValidation Loss 0.01714426 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1721 \tTraining Loss: 0.01245112 \tValidation Loss 0.01699091 \tTraining Acuuarcy 36.819% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1722 \tTraining Loss: 0.01240749 \tValidation Loss 0.01700496 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1723 \tTraining Loss: 0.01242311 \tValidation Loss 0.01705658 \tTraining Acuuarcy 37.415% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1724 \tTraining Loss: 0.01243172 \tValidation Loss 0.01682181 \tTraining Acuuarcy 37.226% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1725 \tTraining Loss: 0.01246755 \tValidation Loss 0.01687932 \tTraining Acuuarcy 36.780% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1726 \tTraining Loss: 0.01241245 \tValidation Loss 0.01698824 \tTraining Acuuarcy 37.326% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1727 \tTraining Loss: 0.01241106 \tValidation Loss 0.01758870 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1728 \tTraining Loss: 0.01236033 \tValidation Loss 0.01734586 \tTraining Acuuarcy 37.827% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1729 \tTraining Loss: 0.01246579 \tValidation Loss 0.01687287 \tTraining Acuuarcy 37.053% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1730 \tTraining Loss: 0.01242853 \tValidation Loss 0.01694899 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1731 \tTraining Loss: 0.01244916 \tValidation Loss 0.01717840 \tTraining Acuuarcy 37.058% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1732 \tTraining Loss: 0.01235435 \tValidation Loss 0.01767359 \tTraining Acuuarcy 38.006% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1733 \tTraining Loss: 0.01243536 \tValidation Loss 0.01698041 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1734 \tTraining Loss: 0.01242121 \tValidation Loss 0.01729814 \tTraining Acuuarcy 36.952% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 1735 \tTraining Loss: 0.01241841 \tValidation Loss 0.01710168 \tTraining Acuuarcy 37.733% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1736 \tTraining Loss: 0.01242038 \tValidation Loss 0.01731037 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1737 \tTraining Loss: 0.01239764 \tValidation Loss 0.01721191 \tTraining Acuuarcy 37.599% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1738 \tTraining Loss: 0.01239232 \tValidation Loss 0.01723127 \tTraining Acuuarcy 37.917% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1739 \tTraining Loss: 0.01237480 \tValidation Loss 0.01711231 \tTraining Acuuarcy 37.421% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1740 \tTraining Loss: 0.01243564 \tValidation Loss 0.01704633 \tTraining Acuuarcy 37.086% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1741 \tTraining Loss: 0.01237723 \tValidation Loss 0.01716807 \tTraining Acuuarcy 37.482% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1742 \tTraining Loss: 0.01245254 \tValidation Loss 0.01687325 \tTraining Acuuarcy 37.242% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 1743 \tTraining Loss: 0.01240433 \tValidation Loss 0.01730671 \tTraining Acuuarcy 37.627% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1744 \tTraining Loss: 0.01240661 \tValidation Loss 0.01702119 \tTraining Acuuarcy 37.855% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1745 \tTraining Loss: 0.01240056 \tValidation Loss 0.01683088 \tTraining Acuuarcy 37.343% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1746 \tTraining Loss: 0.01240307 \tValidation Loss 0.01700677 \tTraining Acuuarcy 37.521% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1747 \tTraining Loss: 0.01242225 \tValidation Loss 0.01773674 \tTraining Acuuarcy 37.722% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1748 \tTraining Loss: 0.01247998 \tValidation Loss 0.01706364 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 1749 \tTraining Loss: 0.01240350 \tValidation Loss 0.01702053 \tTraining Acuuarcy 37.393% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1750 \tTraining Loss: 0.01244007 \tValidation Loss 0.01711481 \tTraining Acuuarcy 37.354% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1751 \tTraining Loss: 0.01247784 \tValidation Loss 0.01701941 \tTraining Acuuarcy 36.724% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1752 \tTraining Loss: 0.01235908 \tValidation Loss 0.01720432 \tTraining Acuuarcy 37.621% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1753 \tTraining Loss: 0.01247823 \tValidation Loss 0.01666061 \tTraining Acuuarcy 36.724% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1754 \tTraining Loss: 0.01240973 \tValidation Loss 0.01704478 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1755 \tTraining Loss: 0.01250221 \tValidation Loss 0.01677158 \tTraining Acuuarcy 36.796% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1756 \tTraining Loss: 0.01235638 \tValidation Loss 0.01719714 \tTraining Acuuarcy 37.850% \tValidation Acuuarcy 20.061%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1757 \tTraining Loss: 0.01237494 \tValidation Loss 0.01726814 \tTraining Acuuarcy 37.788% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1758 \tTraining Loss: 0.01249360 \tValidation Loss 0.01703758 \tTraining Acuuarcy 37.237% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1759 \tTraining Loss: 0.01238187 \tValidation Loss 0.01714819 \tTraining Acuuarcy 37.298% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1760 \tTraining Loss: 0.01240121 \tValidation Loss 0.01717920 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1761 \tTraining Loss: 0.01238068 \tValidation Loss 0.01708579 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1762 \tTraining Loss: 0.01239351 \tValidation Loss 0.01716007 \tTraining Acuuarcy 37.170% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1763 \tTraining Loss: 0.01243911 \tValidation Loss 0.01708759 \tTraining Acuuarcy 37.019% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1764 \tTraining Loss: 0.01235201 \tValidation Loss 0.01726646 \tTraining Acuuarcy 37.777% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1765 \tTraining Loss: 0.01239674 \tValidation Loss 0.01699481 \tTraining Acuuarcy 37.811% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1766 \tTraining Loss: 0.01243408 \tValidation Loss 0.01699305 \tTraining Acuuarcy 37.170% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1767 \tTraining Loss: 0.01242619 \tValidation Loss 0.01712274 \tTraining Acuuarcy 37.147% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1768 \tTraining Loss: 0.01240482 \tValidation Loss 0.01696537 \tTraining Acuuarcy 37.304% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 1769 \tTraining Loss: 0.01238746 \tValidation Loss 0.01702713 \tTraining Acuuarcy 37.315% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1770 \tTraining Loss: 0.01237683 \tValidation Loss 0.01728603 \tTraining Acuuarcy 37.432% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1771 \tTraining Loss: 0.01250834 \tValidation Loss 0.01708535 \tTraining Acuuarcy 37.064% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1772 \tTraining Loss: 0.01238315 \tValidation Loss 0.01702842 \tTraining Acuuarcy 37.749% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1773 \tTraining Loss: 0.01236758 \tValidation Loss 0.01724065 \tTraining Acuuarcy 37.783% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 1774 \tTraining Loss: 0.01239029 \tValidation Loss 0.01697666 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1775 \tTraining Loss: 0.01242924 \tValidation Loss 0.01709397 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 1776 \tTraining Loss: 0.01245742 \tValidation Loss 0.01710763 \tTraining Acuuarcy 36.969% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1777 \tTraining Loss: 0.01247395 \tValidation Loss 0.01694190 \tTraining Acuuarcy 37.120% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1778 \tTraining Loss: 0.01235911 \tValidation Loss 0.01726196 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1779 \tTraining Loss: 0.01241074 \tValidation Loss 0.01705346 \tTraining Acuuarcy 37.175% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1780 \tTraining Loss: 0.01238832 \tValidation Loss 0.01744625 \tTraining Acuuarcy 37.175% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 1781 \tTraining Loss: 0.01241023 \tValidation Loss 0.01698416 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1782 \tTraining Loss: 0.01240501 \tValidation Loss 0.01704241 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1783 \tTraining Loss: 0.01235350 \tValidation Loss 0.01717065 \tTraining Acuuarcy 37.749% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1784 \tTraining Loss: 0.01240176 \tValidation Loss 0.01709038 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1785 \tTraining Loss: 0.01239149 \tValidation Loss 0.01691984 \tTraining Acuuarcy 37.359% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1786 \tTraining Loss: 0.01243846 \tValidation Loss 0.01685133 \tTraining Acuuarcy 37.120% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1787 \tTraining Loss: 0.01241558 \tValidation Loss 0.01694873 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1788 \tTraining Loss: 0.01236810 \tValidation Loss 0.01718815 \tTraining Acuuarcy 37.716% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1789 \tTraining Loss: 0.01244110 \tValidation Loss 0.01738248 \tTraining Acuuarcy 37.359% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1790 \tTraining Loss: 0.01238412 \tValidation Loss 0.01725166 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1791 \tTraining Loss: 0.01240713 \tValidation Loss 0.01704254 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 1792 \tTraining Loss: 0.01243736 \tValidation Loss 0.01700254 \tTraining Acuuarcy 37.315% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1793 \tTraining Loss: 0.01240629 \tValidation Loss 0.01724873 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1794 \tTraining Loss: 0.01246323 \tValidation Loss 0.01702990 \tTraining Acuuarcy 36.674% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1795 \tTraining Loss: 0.01237277 \tValidation Loss 0.01715539 \tTraining Acuuarcy 37.504% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1796 \tTraining Loss: 0.01243073 \tValidation Loss 0.01724031 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1797 \tTraining Loss: 0.01241593 \tValidation Loss 0.01743004 \tTraining Acuuarcy 36.952% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1798 \tTraining Loss: 0.01250155 \tValidation Loss 0.01699294 \tTraining Acuuarcy 36.975% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1799 \tTraining Loss: 0.01240335 \tValidation Loss 0.01717771 \tTraining Acuuarcy 37.593% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1800 \tTraining Loss: 0.01242629 \tValidation Loss 0.01702935 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1801 \tTraining Loss: 0.01237664 \tValidation Loss 0.01729000 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1802 \tTraining Loss: 0.01241859 \tValidation Loss 0.01700595 \tTraining Acuuarcy 37.627% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1803 \tTraining Loss: 0.01242103 \tValidation Loss 0.01724337 \tTraining Acuuarcy 37.588% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 1804 \tTraining Loss: 0.01243521 \tValidation Loss 0.01691682 \tTraining Acuuarcy 36.941% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1805 \tTraining Loss: 0.01247731 \tValidation Loss 0.01725955 \tTraining Acuuarcy 37.069% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1806 \tTraining Loss: 0.01242412 \tValidation Loss 0.01721743 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1807 \tTraining Loss: 0.01233898 \tValidation Loss 0.01722935 \tTraining Acuuarcy 37.939% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1808 \tTraining Loss: 0.01238825 \tValidation Loss 0.01734971 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1809 \tTraining Loss: 0.01238009 \tValidation Loss 0.01683845 \tTraining Acuuarcy 37.632% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1810 \tTraining Loss: 0.01238033 \tValidation Loss 0.01742071 \tTraining Acuuarcy 37.404% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 1811 \tTraining Loss: 0.01243409 \tValidation Loss 0.01735127 \tTraining Acuuarcy 37.153% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 1812 \tTraining Loss: 0.01247171 \tValidation Loss 0.01726082 \tTraining Acuuarcy 36.635% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1813 \tTraining Loss: 0.01234634 \tValidation Loss 0.01730702 \tTraining Acuuarcy 37.409% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1814 \tTraining Loss: 0.01236781 \tValidation Loss 0.01737157 \tTraining Acuuarcy 37.699% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1815 \tTraining Loss: 0.01247047 \tValidation Loss 0.01718641 \tTraining Acuuarcy 37.203% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1816 \tTraining Loss: 0.01245734 \tValidation Loss 0.01702532 \tTraining Acuuarcy 37.460% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1817 \tTraining Loss: 0.01236798 \tValidation Loss 0.01697170 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1818 \tTraining Loss: 0.01246073 \tValidation Loss 0.01687041 \tTraining Acuuarcy 37.058% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1819 \tTraining Loss: 0.01239000 \tValidation Loss 0.01737190 \tTraining Acuuarcy 37.487% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1820 \tTraining Loss: 0.01236617 \tValidation Loss 0.01727581 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1821 \tTraining Loss: 0.01244885 \tValidation Loss 0.01702630 \tTraining Acuuarcy 37.036% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1822 \tTraining Loss: 0.01243722 \tValidation Loss 0.01714065 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1823 \tTraining Loss: 0.01241183 \tValidation Loss 0.01762741 \tTraining Acuuarcy 37.265% \tValidation Acuuarcy 19.978%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1824 \tTraining Loss: 0.01237470 \tValidation Loss 0.01751091 \tTraining Acuuarcy 37.939% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1825 \tTraining Loss: 0.01242785 \tValidation Loss 0.01712486 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1826 \tTraining Loss: 0.01234492 \tValidation Loss 0.01699698 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1827 \tTraining Loss: 0.01243537 \tValidation Loss 0.01734387 \tTraining Acuuarcy 36.919% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1828 \tTraining Loss: 0.01241059 \tValidation Loss 0.01689948 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1829 \tTraining Loss: 0.01238489 \tValidation Loss 0.01720561 \tTraining Acuuarcy 37.526% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1830 \tTraining Loss: 0.01247283 \tValidation Loss 0.01667408 \tTraining Acuuarcy 37.181% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1831 \tTraining Loss: 0.01245220 \tValidation Loss 0.01681296 \tTraining Acuuarcy 37.231% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 1832 \tTraining Loss: 0.01241384 \tValidation Loss 0.01710822 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1833 \tTraining Loss: 0.01238401 \tValidation Loss 0.01723713 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1834 \tTraining Loss: 0.01248481 \tValidation Loss 0.01691865 \tTraining Acuuarcy 37.114% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1835 \tTraining Loss: 0.01238873 \tValidation Loss 0.01726230 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1836 \tTraining Loss: 0.01247809 \tValidation Loss 0.01716896 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 1837 \tTraining Loss: 0.01240911 \tValidation Loss 0.01767609 \tTraining Acuuarcy 37.460% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1838 \tTraining Loss: 0.01242946 \tValidation Loss 0.01693850 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 1839 \tTraining Loss: 0.01238609 \tValidation Loss 0.01690102 \tTraining Acuuarcy 37.315% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1840 \tTraining Loss: 0.01239066 \tValidation Loss 0.01711423 \tTraining Acuuarcy 37.749% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1841 \tTraining Loss: 0.01240361 \tValidation Loss 0.01680638 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1842 \tTraining Loss: 0.01236294 \tValidation Loss 0.01724083 \tTraining Acuuarcy 37.549% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1843 \tTraining Loss: 0.01245345 \tValidation Loss 0.01706162 \tTraining Acuuarcy 36.964% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1844 \tTraining Loss: 0.01240810 \tValidation Loss 0.01717819 \tTraining Acuuarcy 37.805% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1845 \tTraining Loss: 0.01243869 \tValidation Loss 0.01733537 \tTraining Acuuarcy 37.125% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1846 \tTraining Loss: 0.01243615 \tValidation Loss 0.01729655 \tTraining Acuuarcy 37.432% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1847 \tTraining Loss: 0.01243573 \tValidation Loss 0.01732485 \tTraining Acuuarcy 37.131% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1848 \tTraining Loss: 0.01238287 \tValidation Loss 0.01705881 \tTraining Acuuarcy 37.504% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 1849 \tTraining Loss: 0.01234070 \tValidation Loss 0.01728685 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1850 \tTraining Loss: 0.01231990 \tValidation Loss 0.01728721 \tTraining Acuuarcy 37.627% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1851 \tTraining Loss: 0.01241403 \tValidation Loss 0.01695894 \tTraining Acuuarcy 36.980% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1852 \tTraining Loss: 0.01238278 \tValidation Loss 0.01710735 \tTraining Acuuarcy 37.304% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1853 \tTraining Loss: 0.01242798 \tValidation Loss 0.01716703 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1854 \tTraining Loss: 0.01240620 \tValidation Loss 0.01722370 \tTraining Acuuarcy 37.203% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1855 \tTraining Loss: 0.01240940 \tValidation Loss 0.01691858 \tTraining Acuuarcy 37.582% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1856 \tTraining Loss: 0.01237973 \tValidation Loss 0.01730700 \tTraining Acuuarcy 37.800% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1857 \tTraining Loss: 0.01240891 \tValidation Loss 0.01726160 \tTraining Acuuarcy 37.482% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1858 \tTraining Loss: 0.01243530 \tValidation Loss 0.01744388 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1859 \tTraining Loss: 0.01236329 \tValidation Loss 0.01707642 \tTraining Acuuarcy 37.554% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1860 \tTraining Loss: 0.01237931 \tValidation Loss 0.01688028 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1861 \tTraining Loss: 0.01236441 \tValidation Loss 0.01735039 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1862 \tTraining Loss: 0.01241090 \tValidation Loss 0.01751913 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 1863 \tTraining Loss: 0.01239598 \tValidation Loss 0.01739694 \tTraining Acuuarcy 37.644% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1864 \tTraining Loss: 0.01241050 \tValidation Loss 0.01767056 \tTraining Acuuarcy 37.376% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 1865 \tTraining Loss: 0.01245266 \tValidation Loss 0.01705126 \tTraining Acuuarcy 36.941% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 1866 \tTraining Loss: 0.01242891 \tValidation Loss 0.01689307 \tTraining Acuuarcy 37.203% \tValidation Acuuarcy 20.953%\n",
      "Epoch: 1867 \tTraining Loss: 0.01244746 \tValidation Loss 0.01736647 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1868 \tTraining Loss: 0.01244757 \tValidation Loss 0.01708617 \tTraining Acuuarcy 37.365% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 1869 \tTraining Loss: 0.01242876 \tValidation Loss 0.01693842 \tTraining Acuuarcy 37.181% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1870 \tTraining Loss: 0.01239699 \tValidation Loss 0.01710104 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1871 \tTraining Loss: 0.01236716 \tValidation Loss 0.01716184 \tTraining Acuuarcy 37.749% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1872 \tTraining Loss: 0.01244772 \tValidation Loss 0.01703561 \tTraining Acuuarcy 37.978% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1873 \tTraining Loss: 0.01239131 \tValidation Loss 0.01732029 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1874 \tTraining Loss: 0.01238597 \tValidation Loss 0.01702825 \tTraining Acuuarcy 37.855% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1875 \tTraining Loss: 0.01241293 \tValidation Loss 0.01692213 \tTraining Acuuarcy 37.515% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1876 \tTraining Loss: 0.01244624 \tValidation Loss 0.01702989 \tTraining Acuuarcy 37.471% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1877 \tTraining Loss: 0.01238635 \tValidation Loss 0.01726808 \tTraining Acuuarcy 36.980% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1878 \tTraining Loss: 0.01237531 \tValidation Loss 0.01720823 \tTraining Acuuarcy 37.794% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1879 \tTraining Loss: 0.01239872 \tValidation Loss 0.01684726 \tTraining Acuuarcy 37.844% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1880 \tTraining Loss: 0.01244311 \tValidation Loss 0.01667494 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1881 \tTraining Loss: 0.01244749 \tValidation Loss 0.01701659 \tTraining Acuuarcy 37.343% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1882 \tTraining Loss: 0.01244921 \tValidation Loss 0.01708407 \tTraining Acuuarcy 37.421% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 1883 \tTraining Loss: 0.01236352 \tValidation Loss 0.01760319 \tTraining Acuuarcy 37.554% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 1884 \tTraining Loss: 0.01235053 \tValidation Loss 0.01705549 \tTraining Acuuarcy 37.593% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1885 \tTraining Loss: 0.01239549 \tValidation Loss 0.01700494 \tTraining Acuuarcy 37.749% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 1886 \tTraining Loss: 0.01239435 \tValidation Loss 0.01750219 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1887 \tTraining Loss: 0.01242741 \tValidation Loss 0.01719319 \tTraining Acuuarcy 37.487% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1888 \tTraining Loss: 0.01245790 \tValidation Loss 0.01711203 \tTraining Acuuarcy 37.343% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1889 \tTraining Loss: 0.01236870 \tValidation Loss 0.01695714 \tTraining Acuuarcy 37.749% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 1890 \tTraining Loss: 0.01243430 \tValidation Loss 0.01749653 \tTraining Acuuarcy 37.487% \tValidation Acuuarcy 18.278%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1891 \tTraining Loss: 0.01243008 \tValidation Loss 0.01699655 \tTraining Acuuarcy 37.265% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1892 \tTraining Loss: 0.01237052 \tValidation Loss 0.01714380 \tTraining Acuuarcy 37.699% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1893 \tTraining Loss: 0.01244917 \tValidation Loss 0.01676683 \tTraining Acuuarcy 37.504% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1894 \tTraining Loss: 0.01241545 \tValidation Loss 0.01707588 \tTraining Acuuarcy 37.014% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 1895 \tTraining Loss: 0.01241861 \tValidation Loss 0.01739301 \tTraining Acuuarcy 37.421% \tValidation Acuuarcy 18.250%\n",
      "Epoch: 1896 \tTraining Loss: 0.01237655 \tValidation Loss 0.01699932 \tTraining Acuuarcy 37.788% \tValidation Acuuarcy 21.287%\n",
      "Epoch: 1897 \tTraining Loss: 0.01236433 \tValidation Loss 0.01692377 \tTraining Acuuarcy 37.677% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 1898 \tTraining Loss: 0.01239445 \tValidation Loss 0.01684037 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1899 \tTraining Loss: 0.01233611 \tValidation Loss 0.01704088 \tTraining Acuuarcy 37.827% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1900 \tTraining Loss: 0.01235757 \tValidation Loss 0.01717827 \tTraining Acuuarcy 37.515% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 1901 \tTraining Loss: 0.01233123 \tValidation Loss 0.01704593 \tTraining Acuuarcy 37.738% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 1902 \tTraining Loss: 0.01242355 \tValidation Loss 0.01731218 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 18.194%\n",
      "Epoch: 1903 \tTraining Loss: 0.01236277 \tValidation Loss 0.01724259 \tTraining Acuuarcy 37.683% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 1904 \tTraining Loss: 0.01238671 \tValidation Loss 0.01713835 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1905 \tTraining Loss: 0.01236353 \tValidation Loss 0.01706557 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 1906 \tTraining Loss: 0.01243833 \tValidation Loss 0.01721862 \tTraining Acuuarcy 37.599% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 1907 \tTraining Loss: 0.01250577 \tValidation Loss 0.01710707 \tTraining Acuuarcy 36.941% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1908 \tTraining Loss: 0.01245242 \tValidation Loss 0.01718076 \tTraining Acuuarcy 37.181% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1909 \tTraining Loss: 0.01243049 \tValidation Loss 0.01681860 \tTraining Acuuarcy 37.164% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 1910 \tTraining Loss: 0.01237713 \tValidation Loss 0.01735969 \tTraining Acuuarcy 37.599% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 1911 \tTraining Loss: 0.01248502 \tValidation Loss 0.01698189 \tTraining Acuuarcy 37.053% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1912 \tTraining Loss: 0.01233380 \tValidation Loss 0.01723064 \tTraining Acuuarcy 38.023% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1913 \tTraining Loss: 0.01241233 \tValidation Loss 0.01715852 \tTraining Acuuarcy 37.515% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1914 \tTraining Loss: 0.01240008 \tValidation Loss 0.01704258 \tTraining Acuuarcy 37.125% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1915 \tTraining Loss: 0.01234642 \tValidation Loss 0.01699071 \tTraining Acuuarcy 37.788% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1916 \tTraining Loss: 0.01242828 \tValidation Loss 0.01731677 \tTraining Acuuarcy 37.287% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1917 \tTraining Loss: 0.01239960 \tValidation Loss 0.01701627 \tTraining Acuuarcy 37.192% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1918 \tTraining Loss: 0.01238617 \tValidation Loss 0.01702655 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1919 \tTraining Loss: 0.01242464 \tValidation Loss 0.01672327 \tTraining Acuuarcy 37.800% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1920 \tTraining Loss: 0.01244552 \tValidation Loss 0.01699392 \tTraining Acuuarcy 37.159% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1921 \tTraining Loss: 0.01235427 \tValidation Loss 0.01712401 \tTraining Acuuarcy 37.995% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1922 \tTraining Loss: 0.01244480 \tValidation Loss 0.01699219 \tTraining Acuuarcy 37.415% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1923 \tTraining Loss: 0.01237223 \tValidation Loss 0.01726459 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1924 \tTraining Loss: 0.01234638 \tValidation Loss 0.01703342 \tTraining Acuuarcy 38.050% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 1925 \tTraining Loss: 0.01247311 \tValidation Loss 0.01721267 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 1926 \tTraining Loss: 0.01238454 \tValidation Loss 0.01681268 \tTraining Acuuarcy 38.262% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1927 \tTraining Loss: 0.01238247 \tValidation Loss 0.01710980 \tTraining Acuuarcy 37.393% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1928 \tTraining Loss: 0.01241560 \tValidation Loss 0.01702323 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 1929 \tTraining Loss: 0.01239723 \tValidation Loss 0.01734993 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 1930 \tTraining Loss: 0.01240342 \tValidation Loss 0.01690597 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1931 \tTraining Loss: 0.01239828 \tValidation Loss 0.01720567 \tTraining Acuuarcy 37.454% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 1932 \tTraining Loss: 0.01235054 \tValidation Loss 0.01734118 \tTraining Acuuarcy 37.710% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1933 \tTraining Loss: 0.01236523 \tValidation Loss 0.01714787 \tTraining Acuuarcy 37.632% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1934 \tTraining Loss: 0.01235607 \tValidation Loss 0.01729601 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1935 \tTraining Loss: 0.01238069 \tValidation Loss 0.01689550 \tTraining Acuuarcy 37.186% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1936 \tTraining Loss: 0.01238694 \tValidation Loss 0.01697717 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1937 \tTraining Loss: 0.01248496 \tValidation Loss 0.01675440 \tTraining Acuuarcy 37.242% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1938 \tTraining Loss: 0.01243216 \tValidation Loss 0.01710816 \tTraining Acuuarcy 37.370% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1939 \tTraining Loss: 0.01234182 \tValidation Loss 0.01712275 \tTraining Acuuarcy 37.772% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1940 \tTraining Loss: 0.01234510 \tValidation Loss 0.01727250 \tTraining Acuuarcy 37.950% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1941 \tTraining Loss: 0.01236774 \tValidation Loss 0.01735750 \tTraining Acuuarcy 37.209% \tValidation Acuuarcy 17.944%\n",
      "Epoch: 1942 \tTraining Loss: 0.01247219 \tValidation Loss 0.01650347 \tTraining Acuuarcy 37.025% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 1943 \tTraining Loss: 0.01243719 \tValidation Loss 0.01716030 \tTraining Acuuarcy 37.326% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1944 \tTraining Loss: 0.01239796 \tValidation Loss 0.01699615 \tTraining Acuuarcy 36.908% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 1945 \tTraining Loss: 0.01243747 \tValidation Loss 0.01694292 \tTraining Acuuarcy 36.969% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 1946 \tTraining Loss: 0.01242222 \tValidation Loss 0.01710992 \tTraining Acuuarcy 37.281% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1947 \tTraining Loss: 0.01231131 \tValidation Loss 0.01744948 \tTraining Acuuarcy 37.878% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 1948 \tTraining Loss: 0.01242161 \tValidation Loss 0.01726155 \tTraining Acuuarcy 37.632% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 1949 \tTraining Loss: 0.01247653 \tValidation Loss 0.01705369 \tTraining Acuuarcy 36.925% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 1950 \tTraining Loss: 0.01239941 \tValidation Loss 0.01699675 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 1951 \tTraining Loss: 0.01242023 \tValidation Loss 0.01706378 \tTraining Acuuarcy 37.114% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 1952 \tTraining Loss: 0.01232508 \tValidation Loss 0.01705781 \tTraining Acuuarcy 37.448% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 1953 \tTraining Loss: 0.01244453 \tValidation Loss 0.01685633 \tTraining Acuuarcy 37.069% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 1954 \tTraining Loss: 0.01237938 \tValidation Loss 0.01716059 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1955 \tTraining Loss: 0.01233460 \tValidation Loss 0.01733468 \tTraining Acuuarcy 38.112% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 1956 \tTraining Loss: 0.01235450 \tValidation Loss 0.01715327 \tTraining Acuuarcy 37.917% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1957 \tTraining Loss: 0.01242213 \tValidation Loss 0.01736216 \tTraining Acuuarcy 37.186% \tValidation Acuuarcy 19.003%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1958 \tTraining Loss: 0.01240327 \tValidation Loss 0.01702454 \tTraining Acuuarcy 37.471% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1959 \tTraining Loss: 0.01235940 \tValidation Loss 0.01702903 \tTraining Acuuarcy 37.839% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 1960 \tTraining Loss: 0.01239850 \tValidation Loss 0.01733270 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 1961 \tTraining Loss: 0.01237725 \tValidation Loss 0.01709313 \tTraining Acuuarcy 37.421% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 1962 \tTraining Loss: 0.01237709 \tValidation Loss 0.01707758 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1963 \tTraining Loss: 0.01246189 \tValidation Loss 0.01680415 \tTraining Acuuarcy 37.042% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1964 \tTraining Loss: 0.01236710 \tValidation Loss 0.01724922 \tTraining Acuuarcy 37.738% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 1965 \tTraining Loss: 0.01243927 \tValidation Loss 0.01690675 \tTraining Acuuarcy 37.231% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1966 \tTraining Loss: 0.01236107 \tValidation Loss 0.01674267 \tTraining Acuuarcy 37.710% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 1967 \tTraining Loss: 0.01239028 \tValidation Loss 0.01730996 \tTraining Acuuarcy 37.326% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 1968 \tTraining Loss: 0.01235689 \tValidation Loss 0.01718955 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 1969 \tTraining Loss: 0.01238283 \tValidation Loss 0.01688708 \tTraining Acuuarcy 37.827% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1970 \tTraining Loss: 0.01240946 \tValidation Loss 0.01708422 \tTraining Acuuarcy 37.237% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 1971 \tTraining Loss: 0.01235611 \tValidation Loss 0.01712876 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 1972 \tTraining Loss: 0.01237092 \tValidation Loss 0.01716954 \tTraining Acuuarcy 37.582% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 1973 \tTraining Loss: 0.01240972 \tValidation Loss 0.01706777 \tTraining Acuuarcy 37.086% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 1974 \tTraining Loss: 0.01240438 \tValidation Loss 0.01726951 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 20.507%\n",
      "Epoch: 1975 \tTraining Loss: 0.01246941 \tValidation Loss 0.01717901 \tTraining Acuuarcy 37.019% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1976 \tTraining Loss: 0.01238595 \tValidation Loss 0.01712848 \tTraining Acuuarcy 37.984% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 1977 \tTraining Loss: 0.01252295 \tValidation Loss 0.01711825 \tTraining Acuuarcy 37.259% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 1978 \tTraining Loss: 0.01233654 \tValidation Loss 0.01696288 \tTraining Acuuarcy 37.844% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1979 \tTraining Loss: 0.01238879 \tValidation Loss 0.01702358 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 1980 \tTraining Loss: 0.01241226 \tValidation Loss 0.01720678 \tTraining Acuuarcy 37.722% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 1981 \tTraining Loss: 0.01243637 \tValidation Loss 0.01694820 \tTraining Acuuarcy 37.142% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 1982 \tTraining Loss: 0.01238672 \tValidation Loss 0.01737150 \tTraining Acuuarcy 37.794% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 1983 \tTraining Loss: 0.01242636 \tValidation Loss 0.01698296 \tTraining Acuuarcy 37.103% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 1984 \tTraining Loss: 0.01240106 \tValidation Loss 0.01715140 \tTraining Acuuarcy 37.577% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 1985 \tTraining Loss: 0.01232543 \tValidation Loss 0.01719354 \tTraining Acuuarcy 38.240% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 1986 \tTraining Loss: 0.01246698 \tValidation Loss 0.01710500 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 1987 \tTraining Loss: 0.01232143 \tValidation Loss 0.01762350 \tTraining Acuuarcy 38.073% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1988 \tTraining Loss: 0.01235669 \tValidation Loss 0.01743979 \tTraining Acuuarcy 37.872% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 1989 \tTraining Loss: 0.01242955 \tValidation Loss 0.01706815 \tTraining Acuuarcy 37.220% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 1990 \tTraining Loss: 0.01234520 \tValidation Loss 0.01714879 \tTraining Acuuarcy 37.644% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 1991 \tTraining Loss: 0.01239134 \tValidation Loss 0.01723626 \tTraining Acuuarcy 37.794% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 1992 \tTraining Loss: 0.01238643 \tValidation Loss 0.01712613 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 1993 \tTraining Loss: 0.01230844 \tValidation Loss 0.01688692 \tTraining Acuuarcy 38.519% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 1994 \tTraining Loss: 0.01233297 \tValidation Loss 0.01766648 \tTraining Acuuarcy 37.627% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 1995 \tTraining Loss: 0.01242367 \tValidation Loss 0.01703876 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 1996 \tTraining Loss: 0.01237288 \tValidation Loss 0.01687531 \tTraining Acuuarcy 37.722% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1997 \tTraining Loss: 0.01233195 \tValidation Loss 0.01705600 \tTraining Acuuarcy 38.201% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 1998 \tTraining Loss: 0.01237948 \tValidation Loss 0.01740027 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 1999 \tTraining Loss: 0.01236379 \tValidation Loss 0.01690855 \tTraining Acuuarcy 37.889% \tValidation Acuuarcy 21.009%\n",
      "Epoch: 2000 \tTraining Loss: 0.01238736 \tValidation Loss 0.01687198 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2001 \tTraining Loss: 0.01243956 \tValidation Loss 0.01713580 \tTraining Acuuarcy 37.008% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2002 \tTraining Loss: 0.01241215 \tValidation Loss 0.01706512 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2003 \tTraining Loss: 0.01239568 \tValidation Loss 0.01713807 \tTraining Acuuarcy 37.248% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2004 \tTraining Loss: 0.01233640 \tValidation Loss 0.01742571 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 17.526%\n",
      "Epoch: 2005 \tTraining Loss: 0.01234012 \tValidation Loss 0.01693771 \tTraining Acuuarcy 37.928% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2006 \tTraining Loss: 0.01237784 \tValidation Loss 0.01723396 \tTraining Acuuarcy 37.788% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2007 \tTraining Loss: 0.01234084 \tValidation Loss 0.01704949 \tTraining Acuuarcy 37.794% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2008 \tTraining Loss: 0.01241622 \tValidation Loss 0.01697097 \tTraining Acuuarcy 37.549% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 2009 \tTraining Loss: 0.01242558 \tValidation Loss 0.01725997 \tTraining Acuuarcy 36.869% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2010 \tTraining Loss: 0.01242413 \tValidation Loss 0.01733617 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2011 \tTraining Loss: 0.01232761 \tValidation Loss 0.01732652 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2012 \tTraining Loss: 0.01241682 \tValidation Loss 0.01685853 \tTraining Acuuarcy 37.788% \tValidation Acuuarcy 20.312%\n",
      "Epoch: 2013 \tTraining Loss: 0.01240924 \tValidation Loss 0.01716105 \tTraining Acuuarcy 37.493% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2014 \tTraining Loss: 0.01238591 \tValidation Loss 0.01701576 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2015 \tTraining Loss: 0.01242771 \tValidation Loss 0.01701897 \tTraining Acuuarcy 37.008% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2016 \tTraining Loss: 0.01241016 \tValidation Loss 0.01736446 \tTraining Acuuarcy 37.097% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2017 \tTraining Loss: 0.01235093 \tValidation Loss 0.01706208 \tTraining Acuuarcy 37.878% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 2018 \tTraining Loss: 0.01235224 \tValidation Loss 0.01756176 \tTraining Acuuarcy 37.454% \tValidation Acuuarcy 18.111%\n",
      "Epoch: 2019 \tTraining Loss: 0.01239574 \tValidation Loss 0.01731824 \tTraining Acuuarcy 37.231% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 2020 \tTraining Loss: 0.01235290 \tValidation Loss 0.01709452 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2021 \tTraining Loss: 0.01233480 \tValidation Loss 0.01711956 \tTraining Acuuarcy 37.900% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2022 \tTraining Loss: 0.01238033 \tValidation Loss 0.01709959 \tTraining Acuuarcy 37.309% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2023 \tTraining Loss: 0.01237398 \tValidation Loss 0.01721462 \tTraining Acuuarcy 37.861% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2024 \tTraining Loss: 0.01232180 \tValidation Loss 0.01696759 \tTraining Acuuarcy 37.883% \tValidation Acuuarcy 19.476%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2025 \tTraining Loss: 0.01243436 \tValidation Loss 0.01685550 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2026 \tTraining Loss: 0.01234577 \tValidation Loss 0.01712539 \tTraining Acuuarcy 37.705% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2027 \tTraining Loss: 0.01246595 \tValidation Loss 0.01727119 \tTraining Acuuarcy 36.975% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 2028 \tTraining Loss: 0.01230666 \tValidation Loss 0.01731175 \tTraining Acuuarcy 38.067% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2029 \tTraining Loss: 0.01235742 \tValidation Loss 0.01711297 \tTraining Acuuarcy 37.577% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2030 \tTraining Loss: 0.01241897 \tValidation Loss 0.01741279 \tTraining Acuuarcy 37.315% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2031 \tTraining Loss: 0.01239554 \tValidation Loss 0.01696902 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2032 \tTraining Loss: 0.01242314 \tValidation Loss 0.01693772 \tTraining Acuuarcy 37.632% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 2033 \tTraining Loss: 0.01237996 \tValidation Loss 0.01671811 \tTraining Acuuarcy 37.058% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2034 \tTraining Loss: 0.01244588 \tValidation Loss 0.01690934 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2035 \tTraining Loss: 0.01237714 \tValidation Loss 0.01713247 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2036 \tTraining Loss: 0.01232743 \tValidation Loss 0.01688077 \tTraining Acuuarcy 37.270% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2037 \tTraining Loss: 0.01242603 \tValidation Loss 0.01698586 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2038 \tTraining Loss: 0.01237746 \tValidation Loss 0.01709020 \tTraining Acuuarcy 38.006% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2039 \tTraining Loss: 0.01241116 \tValidation Loss 0.01737300 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2040 \tTraining Loss: 0.01241093 \tValidation Loss 0.01730721 \tTraining Acuuarcy 37.710% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2041 \tTraining Loss: 0.01240743 \tValidation Loss 0.01707110 \tTraining Acuuarcy 37.905% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2042 \tTraining Loss: 0.01241636 \tValidation Loss 0.01697851 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2043 \tTraining Loss: 0.01238831 \tValidation Loss 0.01711631 \tTraining Acuuarcy 37.348% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2044 \tTraining Loss: 0.01239327 \tValidation Loss 0.01747057 \tTraining Acuuarcy 37.844% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2045 \tTraining Loss: 0.01238625 \tValidation Loss 0.01717728 \tTraining Acuuarcy 37.894% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2046 \tTraining Loss: 0.01230801 \tValidation Loss 0.01721236 \tTraining Acuuarcy 37.827% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2047 \tTraining Loss: 0.01241020 \tValidation Loss 0.01712637 \tTraining Acuuarcy 37.304% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2048 \tTraining Loss: 0.01239976 \tValidation Loss 0.01740016 \tTraining Acuuarcy 37.827% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2049 \tTraining Loss: 0.01241799 \tValidation Loss 0.01709126 \tTraining Acuuarcy 37.677% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2050 \tTraining Loss: 0.01237347 \tValidation Loss 0.01715412 \tTraining Acuuarcy 37.393% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2051 \tTraining Loss: 0.01235959 \tValidation Loss 0.01706051 \tTraining Acuuarcy 37.861% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2052 \tTraining Loss: 0.01237967 \tValidation Loss 0.01720677 \tTraining Acuuarcy 37.839% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2053 \tTraining Loss: 0.01239384 \tValidation Loss 0.01718135 \tTraining Acuuarcy 37.833% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2054 \tTraining Loss: 0.01243271 \tValidation Loss 0.01686645 \tTraining Acuuarcy 36.936% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2055 \tTraining Loss: 0.01238706 \tValidation Loss 0.01695708 \tTraining Acuuarcy 37.866% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2056 \tTraining Loss: 0.01235994 \tValidation Loss 0.01746730 \tTraining Acuuarcy 37.989% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2057 \tTraining Loss: 0.01236169 \tValidation Loss 0.01708035 \tTraining Acuuarcy 37.582% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2058 \tTraining Loss: 0.01240091 \tValidation Loss 0.01731454 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2059 \tTraining Loss: 0.01235810 \tValidation Loss 0.01709181 \tTraining Acuuarcy 37.287% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2060 \tTraining Loss: 0.01235521 \tValidation Loss 0.01699968 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 2061 \tTraining Loss: 0.01237475 \tValidation Loss 0.01703707 \tTraining Acuuarcy 37.671% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2062 \tTraining Loss: 0.01236670 \tValidation Loss 0.01685554 \tTraining Acuuarcy 37.432% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2063 \tTraining Loss: 0.01242411 \tValidation Loss 0.01708062 \tTraining Acuuarcy 37.298% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 2064 \tTraining Loss: 0.01239096 \tValidation Loss 0.01720023 \tTraining Acuuarcy 37.382% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2065 \tTraining Loss: 0.01233203 \tValidation Loss 0.01726945 \tTraining Acuuarcy 37.922% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2066 \tTraining Loss: 0.01235961 \tValidation Loss 0.01717422 \tTraining Acuuarcy 37.950% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2067 \tTraining Loss: 0.01236530 \tValidation Loss 0.01681830 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2068 \tTraining Loss: 0.01239536 \tValidation Loss 0.01693334 \tTraining Acuuarcy 37.833% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2069 \tTraining Loss: 0.01234225 \tValidation Loss 0.01695655 \tTraining Acuuarcy 37.683% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2070 \tTraining Loss: 0.01236084 \tValidation Loss 0.01705137 \tTraining Acuuarcy 37.922% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2071 \tTraining Loss: 0.01236494 \tValidation Loss 0.01711758 \tTraining Acuuarcy 37.978% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 2072 \tTraining Loss: 0.01235613 \tValidation Loss 0.01691019 \tTraining Acuuarcy 37.766% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2073 \tTraining Loss: 0.01238678 \tValidation Loss 0.01702822 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2074 \tTraining Loss: 0.01245522 \tValidation Loss 0.01730320 \tTraining Acuuarcy 37.354% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2075 \tTraining Loss: 0.01239271 \tValidation Loss 0.01683830 \tTraining Acuuarcy 37.460% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2076 \tTraining Loss: 0.01237671 \tValidation Loss 0.01725254 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2077 \tTraining Loss: 0.01239276 \tValidation Loss 0.01692672 \tTraining Acuuarcy 37.805% \tValidation Acuuarcy 20.674%\n",
      "Epoch: 2078 \tTraining Loss: 0.01240478 \tValidation Loss 0.01715662 \tTraining Acuuarcy 37.365% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 2079 \tTraining Loss: 0.01234859 \tValidation Loss 0.01748456 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2080 \tTraining Loss: 0.01237400 \tValidation Loss 0.01680766 \tTraining Acuuarcy 37.588% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2081 \tTraining Loss: 0.01235823 \tValidation Loss 0.01709939 \tTraining Acuuarcy 37.822% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2082 \tTraining Loss: 0.01231842 \tValidation Loss 0.01738723 \tTraining Acuuarcy 37.995% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2083 \tTraining Loss: 0.01238121 \tValidation Loss 0.01701202 \tTraining Acuuarcy 37.515% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2084 \tTraining Loss: 0.01233159 \tValidation Loss 0.01714379 \tTraining Acuuarcy 37.978% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2085 \tTraining Loss: 0.01240707 \tValidation Loss 0.01706100 \tTraining Acuuarcy 37.454% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2086 \tTraining Loss: 0.01242232 \tValidation Loss 0.01725164 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2087 \tTraining Loss: 0.01242139 \tValidation Loss 0.01731414 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2088 \tTraining Loss: 0.01243114 \tValidation Loss 0.01711598 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2089 \tTraining Loss: 0.01232133 \tValidation Loss 0.01735443 \tTraining Acuuarcy 37.894% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2090 \tTraining Loss: 0.01239782 \tValidation Loss 0.01701687 \tTraining Acuuarcy 37.632% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2091 \tTraining Loss: 0.01239467 \tValidation Loss 0.01721864 \tTraining Acuuarcy 37.627% \tValidation Acuuarcy 19.337%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2092 \tTraining Loss: 0.01235209 \tValidation Loss 0.01710559 \tTraining Acuuarcy 37.956% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2093 \tTraining Loss: 0.01239908 \tValidation Loss 0.01694475 \tTraining Acuuarcy 37.504% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2094 \tTraining Loss: 0.01234206 \tValidation Loss 0.01707277 \tTraining Acuuarcy 37.917% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 2095 \tTraining Loss: 0.01242556 \tValidation Loss 0.01752413 \tTraining Acuuarcy 37.220% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2096 \tTraining Loss: 0.01238103 \tValidation Loss 0.01726371 \tTraining Acuuarcy 37.214% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2097 \tTraining Loss: 0.01239852 \tValidation Loss 0.01716029 \tTraining Acuuarcy 37.616% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2098 \tTraining Loss: 0.01239093 \tValidation Loss 0.01729983 \tTraining Acuuarcy 37.621% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2099 \tTraining Loss: 0.01235150 \tValidation Loss 0.01708139 \tTraining Acuuarcy 37.744% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2100 \tTraining Loss: 0.01240835 \tValidation Loss 0.01711722 \tTraining Acuuarcy 37.081% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 2101 \tTraining Loss: 0.01240360 \tValidation Loss 0.01697640 \tTraining Acuuarcy 37.114% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2102 \tTraining Loss: 0.01243686 \tValidation Loss 0.01696431 \tTraining Acuuarcy 37.103% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2103 \tTraining Loss: 0.01241804 \tValidation Loss 0.01730634 \tTraining Acuuarcy 37.276% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 2104 \tTraining Loss: 0.01240954 \tValidation Loss 0.01699192 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2105 \tTraining Loss: 0.01236591 \tValidation Loss 0.01693579 \tTraining Acuuarcy 37.833% \tValidation Acuuarcy 18.390%\n",
      "Epoch: 2106 \tTraining Loss: 0.01230096 \tValidation Loss 0.01724743 \tTraining Acuuarcy 37.872% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 2107 \tTraining Loss: 0.01236922 \tValidation Loss 0.01715420 \tTraining Acuuarcy 37.827% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2108 \tTraining Loss: 0.01237189 \tValidation Loss 0.01710157 \tTraining Acuuarcy 37.298% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2109 \tTraining Loss: 0.01241401 \tValidation Loss 0.01686288 \tTraining Acuuarcy 38.028% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2110 \tTraining Loss: 0.01236932 \tValidation Loss 0.01744802 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 2111 \tTraining Loss: 0.01239062 \tValidation Loss 0.01721236 \tTraining Acuuarcy 37.644% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2112 \tTraining Loss: 0.01240398 \tValidation Loss 0.01680934 \tTraining Acuuarcy 37.655% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2113 \tTraining Loss: 0.01229219 \tValidation Loss 0.01702933 \tTraining Acuuarcy 38.073% \tValidation Acuuarcy 20.925%\n",
      "Epoch: 2114 \tTraining Loss: 0.01242178 \tValidation Loss 0.01719945 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2115 \tTraining Loss: 0.01235144 \tValidation Loss 0.01737931 \tTraining Acuuarcy 38.023% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2116 \tTraining Loss: 0.01234715 \tValidation Loss 0.01720412 \tTraining Acuuarcy 37.844% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2117 \tTraining Loss: 0.01235197 \tValidation Loss 0.01734973 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2118 \tTraining Loss: 0.01244871 \tValidation Loss 0.01729606 \tTraining Acuuarcy 36.986% \tValidation Acuuarcy 17.888%\n",
      "Epoch: 2119 \tTraining Loss: 0.01232361 \tValidation Loss 0.01709700 \tTraining Acuuarcy 37.415% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2120 \tTraining Loss: 0.01231870 \tValidation Loss 0.01688072 \tTraining Acuuarcy 38.284% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2121 \tTraining Loss: 0.01233762 \tValidation Loss 0.01712292 \tTraining Acuuarcy 37.426% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2122 \tTraining Loss: 0.01238693 \tValidation Loss 0.01703545 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2123 \tTraining Loss: 0.01236759 \tValidation Loss 0.01743171 \tTraining Acuuarcy 38.056% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2124 \tTraining Loss: 0.01240544 \tValidation Loss 0.01693098 \tTraining Acuuarcy 37.727% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2125 \tTraining Loss: 0.01233339 \tValidation Loss 0.01750693 \tTraining Acuuarcy 37.989% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2126 \tTraining Loss: 0.01240184 \tValidation Loss 0.01726932 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2127 \tTraining Loss: 0.01233015 \tValidation Loss 0.01708712 \tTraining Acuuarcy 38.017% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2128 \tTraining Loss: 0.01237371 \tValidation Loss 0.01701938 \tTraining Acuuarcy 37.460% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2129 \tTraining Loss: 0.01238513 \tValidation Loss 0.01811344 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2130 \tTraining Loss: 0.01244128 \tValidation Loss 0.01692866 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2131 \tTraining Loss: 0.01236133 \tValidation Loss 0.01746299 \tTraining Acuuarcy 37.504% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2132 \tTraining Loss: 0.01238806 \tValidation Loss 0.01724554 \tTraining Acuuarcy 37.610% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2133 \tTraining Loss: 0.01228770 \tValidation Loss 0.01721056 \tTraining Acuuarcy 38.418% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 2134 \tTraining Loss: 0.01233646 \tValidation Loss 0.01707843 \tTraining Acuuarcy 38.424% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2135 \tTraining Loss: 0.01237757 \tValidation Loss 0.01719551 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 2136 \tTraining Loss: 0.01238339 \tValidation Loss 0.01704879 \tTraining Acuuarcy 37.710% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2137 \tTraining Loss: 0.01233611 \tValidation Loss 0.01718728 \tTraining Acuuarcy 37.894% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 2138 \tTraining Loss: 0.01238290 \tValidation Loss 0.01713789 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2139 \tTraining Loss: 0.01231568 \tValidation Loss 0.01711999 \tTraining Acuuarcy 38.357% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2140 \tTraining Loss: 0.01239163 \tValidation Loss 0.01710209 \tTraining Acuuarcy 37.287% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2141 \tTraining Loss: 0.01237521 \tValidation Loss 0.01734059 \tTraining Acuuarcy 37.984% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2142 \tTraining Loss: 0.01234612 \tValidation Loss 0.01740360 \tTraining Acuuarcy 38.346% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2143 \tTraining Loss: 0.01233057 \tValidation Loss 0.01726665 \tTraining Acuuarcy 38.045% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2144 \tTraining Loss: 0.01236513 \tValidation Loss 0.01731265 \tTraining Acuuarcy 37.599% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 2145 \tTraining Loss: 0.01240039 \tValidation Loss 0.01741641 \tTraining Acuuarcy 37.593% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2146 \tTraining Loss: 0.01236011 \tValidation Loss 0.01709767 \tTraining Acuuarcy 37.749% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2147 \tTraining Loss: 0.01233402 \tValidation Loss 0.01752600 \tTraining Acuuarcy 37.805% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2148 \tTraining Loss: 0.01236974 \tValidation Loss 0.01700571 \tTraining Acuuarcy 38.011% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2149 \tTraining Loss: 0.01234690 \tValidation Loss 0.01757654 \tTraining Acuuarcy 38.067% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 2150 \tTraining Loss: 0.01238345 \tValidation Loss 0.01720120 \tTraining Acuuarcy 37.487% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2151 \tTraining Loss: 0.01241913 \tValidation Loss 0.01700383 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2152 \tTraining Loss: 0.01234851 \tValidation Loss 0.01703384 \tTraining Acuuarcy 38.179% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2153 \tTraining Loss: 0.01233286 \tValidation Loss 0.01728260 \tTraining Acuuarcy 38.452% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 2154 \tTraining Loss: 0.01238787 \tValidation Loss 0.01671592 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2155 \tTraining Loss: 0.01244436 \tValidation Loss 0.01704536 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2156 \tTraining Loss: 0.01233688 \tValidation Loss 0.01707987 \tTraining Acuuarcy 37.861% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2157 \tTraining Loss: 0.01237307 \tValidation Loss 0.01720234 \tTraining Acuuarcy 37.933% \tValidation Acuuarcy 18.362%\n",
      "Epoch: 2158 \tTraining Loss: 0.01239092 \tValidation Loss 0.01721006 \tTraining Acuuarcy 37.588% \tValidation Acuuarcy 19.783%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2159 \tTraining Loss: 0.01241606 \tValidation Loss 0.01694470 \tTraining Acuuarcy 37.616% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2160 \tTraining Loss: 0.01239812 \tValidation Loss 0.01709834 \tTraining Acuuarcy 38.151% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2161 \tTraining Loss: 0.01227776 \tValidation Loss 0.01711872 \tTraining Acuuarcy 38.307% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2162 \tTraining Loss: 0.01234075 \tValidation Loss 0.01729109 \tTraining Acuuarcy 37.961% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2163 \tTraining Loss: 0.01238303 \tValidation Loss 0.01693357 \tTraining Acuuarcy 37.727% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 2164 \tTraining Loss: 0.01242425 \tValidation Loss 0.01687091 \tTraining Acuuarcy 37.777% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2165 \tTraining Loss: 0.01240537 \tValidation Loss 0.01749006 \tTraining Acuuarcy 37.409% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2166 \tTraining Loss: 0.01233188 \tValidation Loss 0.01713039 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2167 \tTraining Loss: 0.01244519 \tValidation Loss 0.01668501 \tTraining Acuuarcy 37.281% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2168 \tTraining Loss: 0.01240576 \tValidation Loss 0.01700147 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2169 \tTraining Loss: 0.01234968 \tValidation Loss 0.01703925 \tTraining Acuuarcy 37.309% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2170 \tTraining Loss: 0.01241051 \tValidation Loss 0.01708863 \tTraining Acuuarcy 37.839% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2171 \tTraining Loss: 0.01248203 \tValidation Loss 0.01746487 \tTraining Acuuarcy 37.186% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2172 \tTraining Loss: 0.01237726 \tValidation Loss 0.01724495 \tTraining Acuuarcy 37.822% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2173 \tTraining Loss: 0.01245110 \tValidation Loss 0.01694435 \tTraining Acuuarcy 37.214% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2174 \tTraining Loss: 0.01243126 \tValidation Loss 0.01701327 \tTraining Acuuarcy 36.975% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2175 \tTraining Loss: 0.01239437 \tValidation Loss 0.01717681 \tTraining Acuuarcy 37.632% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2176 \tTraining Loss: 0.01240950 \tValidation Loss 0.01720128 \tTraining Acuuarcy 37.560% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2177 \tTraining Loss: 0.01233499 \tValidation Loss 0.01720555 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2178 \tTraining Loss: 0.01229897 \tValidation Loss 0.01742542 \tTraining Acuuarcy 38.212% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2179 \tTraining Loss: 0.01233819 \tValidation Loss 0.01710216 \tTraining Acuuarcy 37.738% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 2180 \tTraining Loss: 0.01241123 \tValidation Loss 0.01719177 \tTraining Acuuarcy 37.097% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2181 \tTraining Loss: 0.01235662 \tValidation Loss 0.01722302 \tTraining Acuuarcy 37.839% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2182 \tTraining Loss: 0.01239370 \tValidation Loss 0.01711532 \tTraining Acuuarcy 37.922% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2183 \tTraining Loss: 0.01238288 \tValidation Loss 0.01703116 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2184 \tTraining Loss: 0.01237233 \tValidation Loss 0.01715060 \tTraining Acuuarcy 37.504% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2185 \tTraining Loss: 0.01237192 \tValidation Loss 0.01729109 \tTraining Acuuarcy 37.638% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 2186 \tTraining Loss: 0.01235566 \tValidation Loss 0.01765866 \tTraining Acuuarcy 37.443% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2187 \tTraining Loss: 0.01243983 \tValidation Loss 0.01706580 \tTraining Acuuarcy 37.443% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2188 \tTraining Loss: 0.01235064 \tValidation Loss 0.01790823 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2189 \tTraining Loss: 0.01234934 \tValidation Loss 0.01738226 \tTraining Acuuarcy 38.028% \tValidation Acuuarcy 18.557%\n",
      "Epoch: 2190 \tTraining Loss: 0.01230064 \tValidation Loss 0.01731733 \tTraining Acuuarcy 38.184% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2191 \tTraining Loss: 0.01239720 \tValidation Loss 0.01698734 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2192 \tTraining Loss: 0.01232083 \tValidation Loss 0.01707322 \tTraining Acuuarcy 38.195% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2193 \tTraining Loss: 0.01239252 \tValidation Loss 0.01728795 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2194 \tTraining Loss: 0.01227610 \tValidation Loss 0.01709274 \tTraining Acuuarcy 38.084% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2195 \tTraining Loss: 0.01236933 \tValidation Loss 0.01712837 \tTraining Acuuarcy 37.560% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2196 \tTraining Loss: 0.01238648 \tValidation Loss 0.01738315 \tTraining Acuuarcy 37.331% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 2197 \tTraining Loss: 0.01235427 \tValidation Loss 0.01752066 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2198 \tTraining Loss: 0.01234462 \tValidation Loss 0.01743820 \tTraining Acuuarcy 37.593% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2199 \tTraining Loss: 0.01236680 \tValidation Loss 0.01724329 \tTraining Acuuarcy 37.800% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2200 \tTraining Loss: 0.01234295 \tValidation Loss 0.01711828 \tTraining Acuuarcy 37.822% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2201 \tTraining Loss: 0.01233814 \tValidation Loss 0.01732533 \tTraining Acuuarcy 37.526% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2202 \tTraining Loss: 0.01243413 \tValidation Loss 0.01704269 \tTraining Acuuarcy 37.432% \tValidation Acuuarcy 18.278%\n",
      "Epoch: 2203 \tTraining Loss: 0.01235736 \tValidation Loss 0.01709807 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2204 \tTraining Loss: 0.01236792 \tValidation Loss 0.01741767 \tTraining Acuuarcy 37.744% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2205 \tTraining Loss: 0.01239089 \tValidation Loss 0.01724318 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2206 \tTraining Loss: 0.01229059 \tValidation Loss 0.01733708 \tTraining Acuuarcy 38.480% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2207 \tTraining Loss: 0.01240046 \tValidation Loss 0.01697787 \tTraining Acuuarcy 37.359% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2208 \tTraining Loss: 0.01236390 \tValidation Loss 0.01716518 \tTraining Acuuarcy 37.421% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2209 \tTraining Loss: 0.01238875 \tValidation Loss 0.01731200 \tTraining Acuuarcy 37.521% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2210 \tTraining Loss: 0.01235572 \tValidation Loss 0.01708686 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2211 \tTraining Loss: 0.01232763 \tValidation Loss 0.01762894 \tTraining Acuuarcy 38.067% \tValidation Acuuarcy 19.978%\n",
      "Epoch: 2212 \tTraining Loss: 0.01240562 \tValidation Loss 0.01717215 \tTraining Acuuarcy 37.794% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2213 \tTraining Loss: 0.01236625 \tValidation Loss 0.01697670 \tTraining Acuuarcy 38.179% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2214 \tTraining Loss: 0.01241357 \tValidation Loss 0.01705771 \tTraining Acuuarcy 37.231% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2215 \tTraining Loss: 0.01233887 \tValidation Loss 0.01716954 \tTraining Acuuarcy 38.513% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2216 \tTraining Loss: 0.01240516 \tValidation Loss 0.01715472 \tTraining Acuuarcy 37.382% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2217 \tTraining Loss: 0.01237949 \tValidation Loss 0.01677466 \tTraining Acuuarcy 37.755% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2218 \tTraining Loss: 0.01235621 \tValidation Loss 0.01699782 \tTraining Acuuarcy 37.922% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2219 \tTraining Loss: 0.01233619 \tValidation Loss 0.01693853 \tTraining Acuuarcy 38.050% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2220 \tTraining Loss: 0.01231816 \tValidation Loss 0.01692683 \tTraining Acuuarcy 38.480% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2221 \tTraining Loss: 0.01235179 \tValidation Loss 0.01698102 \tTraining Acuuarcy 37.744% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2222 \tTraining Loss: 0.01235629 \tValidation Loss 0.01736542 \tTraining Acuuarcy 37.933% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2223 \tTraining Loss: 0.01238999 \tValidation Loss 0.01722529 \tTraining Acuuarcy 37.805% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2224 \tTraining Loss: 0.01239196 \tValidation Loss 0.01704312 \tTraining Acuuarcy 37.298% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2225 \tTraining Loss: 0.01230315 \tValidation Loss 0.01723939 \tTraining Acuuarcy 37.944% \tValidation Acuuarcy 19.699%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2226 \tTraining Loss: 0.01241253 \tValidation Loss 0.01723724 \tTraining Acuuarcy 37.482% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 2227 \tTraining Loss: 0.01242719 \tValidation Loss 0.01698621 \tTraining Acuuarcy 37.755% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2228 \tTraining Loss: 0.01239055 \tValidation Loss 0.01710857 \tTraining Acuuarcy 37.409% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2229 \tTraining Loss: 0.01233507 \tValidation Loss 0.01670957 \tTraining Acuuarcy 37.655% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2230 \tTraining Loss: 0.01233307 \tValidation Loss 0.01817262 \tTraining Acuuarcy 37.510% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2231 \tTraining Loss: 0.01236159 \tValidation Loss 0.01719964 \tTraining Acuuarcy 37.766% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2232 \tTraining Loss: 0.01235884 \tValidation Loss 0.01745974 \tTraining Acuuarcy 38.134% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2233 \tTraining Loss: 0.01234849 \tValidation Loss 0.01753490 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 2234 \tTraining Loss: 0.01243857 \tValidation Loss 0.01701524 \tTraining Acuuarcy 37.504% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2235 \tTraining Loss: 0.01234245 \tValidation Loss 0.01708808 \tTraining Acuuarcy 37.833% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2236 \tTraining Loss: 0.01240663 \tValidation Loss 0.01726839 \tTraining Acuuarcy 37.905% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2237 \tTraining Loss: 0.01234383 \tValidation Loss 0.01705581 \tTraining Acuuarcy 38.329% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2238 \tTraining Loss: 0.01230775 \tValidation Loss 0.01733281 \tTraining Acuuarcy 37.995% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 2239 \tTraining Loss: 0.01235391 \tValidation Loss 0.01709280 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2240 \tTraining Loss: 0.01228983 \tValidation Loss 0.01773799 \tTraining Acuuarcy 38.480% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2241 \tTraining Loss: 0.01238762 \tValidation Loss 0.01720744 \tTraining Acuuarcy 37.744% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 2242 \tTraining Loss: 0.01235263 \tValidation Loss 0.01713593 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 2243 \tTraining Loss: 0.01239140 \tValidation Loss 0.01702135 \tTraining Acuuarcy 37.905% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2244 \tTraining Loss: 0.01239276 \tValidation Loss 0.01727749 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2245 \tTraining Loss: 0.01238706 \tValidation Loss 0.01702788 \tTraining Acuuarcy 37.627% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2246 \tTraining Loss: 0.01237630 \tValidation Loss 0.01708480 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2247 \tTraining Loss: 0.01236463 \tValidation Loss 0.01731353 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2248 \tTraining Loss: 0.01230619 \tValidation Loss 0.01729175 \tTraining Acuuarcy 38.073% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2249 \tTraining Loss: 0.01239375 \tValidation Loss 0.01775113 \tTraining Acuuarcy 37.560% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 2250 \tTraining Loss: 0.01239960 \tValidation Loss 0.01711579 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2251 \tTraining Loss: 0.01241689 \tValidation Loss 0.01718630 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2252 \tTraining Loss: 0.01241232 \tValidation Loss 0.01727594 \tTraining Acuuarcy 37.409% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2253 \tTraining Loss: 0.01232510 \tValidation Loss 0.01737930 \tTraining Acuuarcy 38.296% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2254 \tTraining Loss: 0.01233036 \tValidation Loss 0.01711680 \tTraining Acuuarcy 38.418% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2255 \tTraining Loss: 0.01237528 \tValidation Loss 0.01691281 \tTraining Acuuarcy 37.683% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2256 \tTraining Loss: 0.01236714 \tValidation Loss 0.01691300 \tTraining Acuuarcy 37.409% \tValidation Acuuarcy 20.256%\n",
      "Epoch: 2257 \tTraining Loss: 0.01243087 \tValidation Loss 0.01680538 \tTraining Acuuarcy 37.616% \tValidation Acuuarcy 20.814%\n",
      "Epoch: 2258 \tTraining Loss: 0.01238851 \tValidation Loss 0.01701092 \tTraining Acuuarcy 37.554% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 2259 \tTraining Loss: 0.01241865 \tValidation Loss 0.01691255 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2260 \tTraining Loss: 0.01239722 \tValidation Loss 0.01707862 \tTraining Acuuarcy 37.777% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2261 \tTraining Loss: 0.01228964 \tValidation Loss 0.01733476 \tTraining Acuuarcy 38.502% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 2262 \tTraining Loss: 0.01238977 \tValidation Loss 0.01707029 \tTraining Acuuarcy 37.454% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2263 \tTraining Loss: 0.01239611 \tValidation Loss 0.01729162 \tTraining Acuuarcy 37.515% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2264 \tTraining Loss: 0.01236598 \tValidation Loss 0.01733327 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2265 \tTraining Loss: 0.01235935 \tValidation Loss 0.01691418 \tTraining Acuuarcy 37.699% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2266 \tTraining Loss: 0.01242528 \tValidation Loss 0.01694675 \tTraining Acuuarcy 37.415% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2267 \tTraining Loss: 0.01233205 \tValidation Loss 0.01720127 \tTraining Acuuarcy 37.905% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2268 \tTraining Loss: 0.01243951 \tValidation Loss 0.01705627 \tTraining Acuuarcy 37.376% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2269 \tTraining Loss: 0.01235579 \tValidation Loss 0.01743686 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2270 \tTraining Loss: 0.01231207 \tValidation Loss 0.01695981 \tTraining Acuuarcy 37.755% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2271 \tTraining Loss: 0.01233844 \tValidation Loss 0.01704528 \tTraining Acuuarcy 37.415% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2272 \tTraining Loss: 0.01233222 \tValidation Loss 0.01766182 \tTraining Acuuarcy 38.017% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2273 \tTraining Loss: 0.01231476 \tValidation Loss 0.01717226 \tTraining Acuuarcy 37.978% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2274 \tTraining Loss: 0.01231001 \tValidation Loss 0.01726872 \tTraining Acuuarcy 37.855% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 2275 \tTraining Loss: 0.01232484 \tValidation Loss 0.01727894 \tTraining Acuuarcy 38.279% \tValidation Acuuarcy 20.089%\n",
      "Epoch: 2276 \tTraining Loss: 0.01232423 \tValidation Loss 0.01740643 \tTraining Acuuarcy 38.474% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2277 \tTraining Loss: 0.01240116 \tValidation Loss 0.01718113 \tTraining Acuuarcy 37.476% \tValidation Acuuarcy 18.752%\n",
      "Epoch: 2278 \tTraining Loss: 0.01239140 \tValidation Loss 0.01679672 \tTraining Acuuarcy 37.248% \tValidation Acuuarcy 19.309%\n",
      "Epoch: 2279 \tTraining Loss: 0.01234412 \tValidation Loss 0.01686070 \tTraining Acuuarcy 37.655% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2280 \tTraining Loss: 0.01248963 \tValidation Loss 0.01698309 \tTraining Acuuarcy 37.036% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2281 \tTraining Loss: 0.01229326 \tValidation Loss 0.01712955 \tTraining Acuuarcy 38.268% \tValidation Acuuarcy 19.142%\n",
      "Epoch: 2282 \tTraining Loss: 0.01230907 \tValidation Loss 0.01741023 \tTraining Acuuarcy 38.162% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 2283 \tTraining Loss: 0.01238158 \tValidation Loss 0.01721882 \tTraining Acuuarcy 37.811% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2284 \tTraining Loss: 0.01234159 \tValidation Loss 0.01701042 \tTraining Acuuarcy 37.889% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2285 \tTraining Loss: 0.01242032 \tValidation Loss 0.01721945 \tTraining Acuuarcy 37.337% \tValidation Acuuarcy 19.866%\n",
      "Epoch: 2286 \tTraining Loss: 0.01246846 \tValidation Loss 0.01689534 \tTraining Acuuarcy 37.042% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2287 \tTraining Loss: 0.01239058 \tValidation Loss 0.01716788 \tTraining Acuuarcy 37.855% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2288 \tTraining Loss: 0.01232319 \tValidation Loss 0.01706471 \tTraining Acuuarcy 37.922% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 2289 \tTraining Loss: 0.01241606 \tValidation Loss 0.01714178 \tTraining Acuuarcy 37.487% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2290 \tTraining Loss: 0.01244041 \tValidation Loss 0.01697581 \tTraining Acuuarcy 36.824% \tValidation Acuuarcy 20.228%\n",
      "Epoch: 2291 \tTraining Loss: 0.01245421 \tValidation Loss 0.01724060 \tTraining Acuuarcy 36.807% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2292 \tTraining Loss: 0.01230596 \tValidation Loss 0.01707765 \tTraining Acuuarcy 37.683% \tValidation Acuuarcy 19.699%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2293 \tTraining Loss: 0.01239847 \tValidation Loss 0.01705818 \tTraining Acuuarcy 37.075% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2294 \tTraining Loss: 0.01231918 \tValidation Loss 0.01734576 \tTraining Acuuarcy 38.167% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2295 \tTraining Loss: 0.01238589 \tValidation Loss 0.01741588 \tTraining Acuuarcy 37.526% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2296 \tTraining Loss: 0.01249643 \tValidation Loss 0.01710829 \tTraining Acuuarcy 36.534% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2297 \tTraining Loss: 0.01235588 \tValidation Loss 0.01728805 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 18.027%\n",
      "Epoch: 2298 \tTraining Loss: 0.01232742 \tValidation Loss 0.01726236 \tTraining Acuuarcy 38.441% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2299 \tTraining Loss: 0.01234335 \tValidation Loss 0.01698115 \tTraining Acuuarcy 37.716% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2300 \tTraining Loss: 0.01227808 \tValidation Loss 0.01709406 \tTraining Acuuarcy 38.095% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2301 \tTraining Loss: 0.01236172 \tValidation Loss 0.01690449 \tTraining Acuuarcy 37.649% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2302 \tTraining Loss: 0.01238770 \tValidation Loss 0.01734396 \tTraining Acuuarcy 37.460% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2303 \tTraining Loss: 0.01237311 \tValidation Loss 0.01708479 \tTraining Acuuarcy 37.755% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2304 \tTraining Loss: 0.01238823 \tValidation Loss 0.01695951 \tTraining Acuuarcy 37.727% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2305 \tTraining Loss: 0.01242268 \tValidation Loss 0.01721474 \tTraining Acuuarcy 37.175% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2306 \tTraining Loss: 0.01232400 \tValidation Loss 0.01707460 \tTraining Acuuarcy 37.839% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2307 \tTraining Loss: 0.01238728 \tValidation Loss 0.01724018 \tTraining Acuuarcy 37.582% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2308 \tTraining Loss: 0.01237124 \tValidation Loss 0.01701474 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2309 \tTraining Loss: 0.01236367 \tValidation Loss 0.01736029 \tTraining Acuuarcy 37.777% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2310 \tTraining Loss: 0.01240071 \tValidation Loss 0.01707479 \tTraining Acuuarcy 37.588% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2311 \tTraining Loss: 0.01240479 \tValidation Loss 0.01722018 \tTraining Acuuarcy 37.560% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2312 \tTraining Loss: 0.01235943 \tValidation Loss 0.01732347 \tTraining Acuuarcy 38.106% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2313 \tTraining Loss: 0.01238378 \tValidation Loss 0.01717861 \tTraining Acuuarcy 37.777% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2314 \tTraining Loss: 0.01226336 \tValidation Loss 0.01745421 \tTraining Acuuarcy 38.569% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 2315 \tTraining Loss: 0.01234372 \tValidation Loss 0.01742665 \tTraining Acuuarcy 37.978% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 2316 \tTraining Loss: 0.01238684 \tValidation Loss 0.01711684 \tTraining Acuuarcy 37.487% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2317 \tTraining Loss: 0.01232659 \tValidation Loss 0.01743214 \tTraining Acuuarcy 37.889% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2318 \tTraining Loss: 0.01237892 \tValidation Loss 0.01722661 \tTraining Acuuarcy 37.716% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2319 \tTraining Loss: 0.01239863 \tValidation Loss 0.01696683 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2320 \tTraining Loss: 0.01230911 \tValidation Loss 0.01721790 \tTraining Acuuarcy 38.112% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2321 \tTraining Loss: 0.01235460 \tValidation Loss 0.01726342 \tTraining Acuuarcy 37.900% \tValidation Acuuarcy 18.529%\n",
      "Epoch: 2322 \tTraining Loss: 0.01234219 \tValidation Loss 0.01722432 \tTraining Acuuarcy 38.011% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2323 \tTraining Loss: 0.01238663 \tValidation Loss 0.01722723 \tTraining Acuuarcy 37.560% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2324 \tTraining Loss: 0.01242176 \tValidation Loss 0.01728393 \tTraining Acuuarcy 37.582% \tValidation Acuuarcy 20.535%\n",
      "Epoch: 2325 \tTraining Loss: 0.01240914 \tValidation Loss 0.01698463 \tTraining Acuuarcy 37.253% \tValidation Acuuarcy 20.702%\n",
      "Epoch: 2326 \tTraining Loss: 0.01238376 \tValidation Loss 0.01724672 \tTraining Acuuarcy 37.437% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2327 \tTraining Loss: 0.01238215 \tValidation Loss 0.01735255 \tTraining Acuuarcy 37.738% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2328 \tTraining Loss: 0.01242109 \tValidation Loss 0.01703170 \tTraining Acuuarcy 37.398% \tValidation Acuuarcy 18.612%\n",
      "Epoch: 2329 \tTraining Loss: 0.01244394 \tValidation Loss 0.01731615 \tTraining Acuuarcy 37.454% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2330 \tTraining Loss: 0.01240442 \tValidation Loss 0.01733941 \tTraining Acuuarcy 37.744% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2331 \tTraining Loss: 0.01235286 \tValidation Loss 0.01717591 \tTraining Acuuarcy 37.727% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2332 \tTraining Loss: 0.01239035 \tValidation Loss 0.01712020 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2333 \tTraining Loss: 0.01234432 \tValidation Loss 0.01732783 \tTraining Acuuarcy 37.978% \tValidation Acuuarcy 19.365%\n",
      "Epoch: 2334 \tTraining Loss: 0.01235973 \tValidation Loss 0.01708355 \tTraining Acuuarcy 38.006% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2335 \tTraining Loss: 0.01239511 \tValidation Loss 0.01748195 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 2336 \tTraining Loss: 0.01237718 \tValidation Loss 0.01736993 \tTraining Acuuarcy 37.900% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2337 \tTraining Loss: 0.01236366 \tValidation Loss 0.01722673 \tTraining Acuuarcy 38.134% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2338 \tTraining Loss: 0.01239642 \tValidation Loss 0.01702881 \tTraining Acuuarcy 37.404% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2339 \tTraining Loss: 0.01223783 \tValidation Loss 0.01713056 \tTraining Acuuarcy 38.134% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2340 \tTraining Loss: 0.01240938 \tValidation Loss 0.01702025 \tTraining Acuuarcy 37.554% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2341 \tTraining Loss: 0.01231437 \tValidation Loss 0.01702750 \tTraining Acuuarcy 38.034% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2342 \tTraining Loss: 0.01231664 \tValidation Loss 0.01736842 \tTraining Acuuarcy 38.050% \tValidation Acuuarcy 18.501%\n",
      "Epoch: 2343 \tTraining Loss: 0.01241858 \tValidation Loss 0.01704288 \tTraining Acuuarcy 37.605% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2344 \tTraining Loss: 0.01229237 \tValidation Loss 0.01728510 \tTraining Acuuarcy 38.034% \tValidation Acuuarcy 18.222%\n",
      "Epoch: 2345 \tTraining Loss: 0.01239259 \tValidation Loss 0.01703748 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 2346 \tTraining Loss: 0.01237925 \tValidation Loss 0.01703727 \tTraining Acuuarcy 37.610% \tValidation Acuuarcy 19.922%\n",
      "Epoch: 2347 \tTraining Loss: 0.01244004 \tValidation Loss 0.01726965 \tTraining Acuuarcy 37.192% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2348 \tTraining Loss: 0.01238126 \tValidation Loss 0.01701395 \tTraining Acuuarcy 37.627% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2349 \tTraining Loss: 0.01234003 \tValidation Loss 0.01715907 \tTraining Acuuarcy 37.811% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2350 \tTraining Loss: 0.01237877 \tValidation Loss 0.01714176 \tTraining Acuuarcy 37.549% \tValidation Acuuarcy 18.780%\n",
      "Epoch: 2351 \tTraining Loss: 0.01237697 \tValidation Loss 0.01731648 \tTraining Acuuarcy 37.833% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 2352 \tTraining Loss: 0.01231217 \tValidation Loss 0.01729789 \tTraining Acuuarcy 38.262% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2353 \tTraining Loss: 0.01237188 \tValidation Loss 0.01724267 \tTraining Acuuarcy 37.103% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2354 \tTraining Loss: 0.01235363 \tValidation Loss 0.01722277 \tTraining Acuuarcy 37.800% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2355 \tTraining Loss: 0.01235323 \tValidation Loss 0.01703097 \tTraining Acuuarcy 37.359% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2356 \tTraining Loss: 0.01238049 \tValidation Loss 0.01702011 \tTraining Acuuarcy 37.666% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2357 \tTraining Loss: 0.01241477 \tValidation Loss 0.01676276 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2358 \tTraining Loss: 0.01243384 \tValidation Loss 0.01685718 \tTraining Acuuarcy 37.170% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 2359 \tTraining Loss: 0.01237366 \tValidation Loss 0.01727983 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 19.588%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2360 \tTraining Loss: 0.01236159 \tValidation Loss 0.01690257 \tTraining Acuuarcy 37.744% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2361 \tTraining Loss: 0.01243246 \tValidation Loss 0.01709581 \tTraining Acuuarcy 37.181% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2362 \tTraining Loss: 0.01245433 \tValidation Loss 0.01670365 \tTraining Acuuarcy 37.755% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2363 \tTraining Loss: 0.01232790 \tValidation Loss 0.01721191 \tTraining Acuuarcy 37.727% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2364 \tTraining Loss: 0.01233017 \tValidation Loss 0.01710288 \tTraining Acuuarcy 38.006% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2365 \tTraining Loss: 0.01238383 \tValidation Loss 0.01749634 \tTraining Acuuarcy 37.855% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2366 \tTraining Loss: 0.01236494 \tValidation Loss 0.01705670 \tTraining Acuuarcy 37.788% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2367 \tTraining Loss: 0.01233038 \tValidation Loss 0.01730403 \tTraining Acuuarcy 37.727% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2368 \tTraining Loss: 0.01238042 \tValidation Loss 0.01708270 \tTraining Acuuarcy 37.705% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2369 \tTraining Loss: 0.01233172 \tValidation Loss 0.01723178 \tTraining Acuuarcy 38.056% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2370 \tTraining Loss: 0.01237405 \tValidation Loss 0.01713188 \tTraining Acuuarcy 37.883% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2371 \tTraining Loss: 0.01233733 \tValidation Loss 0.01723781 \tTraining Acuuarcy 37.800% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2372 \tTraining Loss: 0.01238735 \tValidation Loss 0.01721118 \tTraining Acuuarcy 37.354% \tValidation Acuuarcy 20.424%\n",
      "Epoch: 2373 \tTraining Loss: 0.01238437 \tValidation Loss 0.01734185 \tTraining Acuuarcy 38.000% \tValidation Acuuarcy 21.120%\n",
      "Epoch: 2374 \tTraining Loss: 0.01240908 \tValidation Loss 0.01752286 \tTraining Acuuarcy 37.554% \tValidation Acuuarcy 18.306%\n",
      "Epoch: 2375 \tTraining Loss: 0.01235374 \tValidation Loss 0.01714021 \tTraining Acuuarcy 37.944% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2376 \tTraining Loss: 0.01235286 \tValidation Loss 0.01725047 \tTraining Acuuarcy 38.151% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2377 \tTraining Loss: 0.01235826 \tValidation Loss 0.01715329 \tTraining Acuuarcy 37.376% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2378 \tTraining Loss: 0.01237000 \tValidation Loss 0.01726938 \tTraining Acuuarcy 38.023% \tValidation Acuuarcy 19.393%\n",
      "Epoch: 2379 \tTraining Loss: 0.01237552 \tValidation Loss 0.01702261 \tTraining Acuuarcy 37.521% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2380 \tTraining Loss: 0.01234715 \tValidation Loss 0.01710104 \tTraining Acuuarcy 38.206% \tValidation Acuuarcy 20.591%\n",
      "Epoch: 2381 \tTraining Loss: 0.01237736 \tValidation Loss 0.01737850 \tTraining Acuuarcy 37.337% \tValidation Acuuarcy 19.198%\n",
      "Epoch: 2382 \tTraining Loss: 0.01232830 \tValidation Loss 0.01718418 \tTraining Acuuarcy 37.972% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2383 \tTraining Loss: 0.01238945 \tValidation Loss 0.01687177 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2384 \tTraining Loss: 0.01229309 \tValidation Loss 0.01729259 \tTraining Acuuarcy 38.039% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2385 \tTraining Loss: 0.01236332 \tValidation Loss 0.01700409 \tTraining Acuuarcy 37.844% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2386 \tTraining Loss: 0.01233102 \tValidation Loss 0.01710720 \tTraining Acuuarcy 37.811% \tValidation Acuuarcy 18.585%\n",
      "Epoch: 2387 \tTraining Loss: 0.01247701 \tValidation Loss 0.01693120 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2388 \tTraining Loss: 0.01235739 \tValidation Loss 0.01698241 \tTraining Acuuarcy 37.755% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 2389 \tTraining Loss: 0.01242287 \tValidation Loss 0.01720476 \tTraining Acuuarcy 37.515% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2390 \tTraining Loss: 0.01233868 \tValidation Loss 0.01714740 \tTraining Acuuarcy 37.917% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2391 \tTraining Loss: 0.01242089 \tValidation Loss 0.01691417 \tTraining Acuuarcy 37.030% \tValidation Acuuarcy 18.919%\n",
      "Epoch: 2392 \tTraining Loss: 0.01238314 \tValidation Loss 0.01710933 \tTraining Acuuarcy 37.644% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2393 \tTraining Loss: 0.01235407 \tValidation Loss 0.01714265 \tTraining Acuuarcy 37.866% \tValidation Acuuarcy 20.368%\n",
      "Epoch: 2394 \tTraining Loss: 0.01233058 \tValidation Loss 0.01712895 \tTraining Acuuarcy 38.034% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2395 \tTraining Loss: 0.01236917 \tValidation Loss 0.01699904 \tTraining Acuuarcy 37.705% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 2396 \tTraining Loss: 0.01236375 \tValidation Loss 0.01697128 \tTraining Acuuarcy 38.101% \tValidation Acuuarcy 20.340%\n",
      "Epoch: 2397 \tTraining Loss: 0.01242361 \tValidation Loss 0.01696318 \tTraining Acuuarcy 37.443% \tValidation Acuuarcy 19.950%\n",
      "Epoch: 2398 \tTraining Loss: 0.01227968 \tValidation Loss 0.01702283 \tTraining Acuuarcy 38.062% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2399 \tTraining Loss: 0.01237060 \tValidation Loss 0.01704976 \tTraining Acuuarcy 38.011% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2400 \tTraining Loss: 0.01241685 \tValidation Loss 0.01746525 \tTraining Acuuarcy 37.337% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2401 \tTraining Loss: 0.01234903 \tValidation Loss 0.01724803 \tTraining Acuuarcy 37.961% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 2402 \tTraining Loss: 0.01240875 \tValidation Loss 0.01687532 \tTraining Acuuarcy 37.482% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 2403 \tTraining Loss: 0.01234624 \tValidation Loss 0.01716911 \tTraining Acuuarcy 38.073% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2404 \tTraining Loss: 0.01235048 \tValidation Loss 0.01723997 \tTraining Acuuarcy 37.610% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2405 \tTraining Loss: 0.01230809 \tValidation Loss 0.01693178 \tTraining Acuuarcy 38.017% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2406 \tTraining Loss: 0.01234237 \tValidation Loss 0.01743856 \tTraining Acuuarcy 37.565% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2407 \tTraining Loss: 0.01227769 \tValidation Loss 0.01718503 \tTraining Acuuarcy 37.772% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2408 \tTraining Loss: 0.01242265 \tValidation Loss 0.01740450 \tTraining Acuuarcy 36.874% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2409 \tTraining Loss: 0.01243490 \tValidation Loss 0.01716888 \tTraining Acuuarcy 37.365% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2410 \tTraining Loss: 0.01236652 \tValidation Loss 0.01707390 \tTraining Acuuarcy 38.128% \tValidation Acuuarcy 18.640%\n",
      "Epoch: 2411 \tTraining Loss: 0.01242719 \tValidation Loss 0.01724205 \tTraining Acuuarcy 37.214% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2412 \tTraining Loss: 0.01234447 \tValidation Loss 0.01734627 \tTraining Acuuarcy 37.905% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2413 \tTraining Loss: 0.01230078 \tValidation Loss 0.01724579 \tTraining Acuuarcy 38.156% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2414 \tTraining Loss: 0.01232299 \tValidation Loss 0.01747580 \tTraining Acuuarcy 38.368% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2415 \tTraining Loss: 0.01245462 \tValidation Loss 0.01695892 \tTraining Acuuarcy 37.198% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2416 \tTraining Loss: 0.01234340 \tValidation Loss 0.01709578 \tTraining Acuuarcy 38.101% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2417 \tTraining Loss: 0.01235749 \tValidation Loss 0.01692592 \tTraining Acuuarcy 37.827% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2418 \tTraining Loss: 0.01247473 \tValidation Loss 0.01707470 \tTraining Acuuarcy 37.108% \tValidation Acuuarcy 18.417%\n",
      "Epoch: 2419 \tTraining Loss: 0.01233033 \tValidation Loss 0.01719424 \tTraining Acuuarcy 37.772% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2420 \tTraining Loss: 0.01232118 \tValidation Loss 0.01733669 \tTraining Acuuarcy 38.184% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2421 \tTraining Loss: 0.01231997 \tValidation Loss 0.01746977 \tTraining Acuuarcy 37.710% \tValidation Acuuarcy 20.201%\n",
      "Epoch: 2422 \tTraining Loss: 0.01238743 \tValidation Loss 0.01770623 \tTraining Acuuarcy 37.393% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2423 \tTraining Loss: 0.01236186 \tValidation Loss 0.01693768 \tTraining Acuuarcy 38.023% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2424 \tTraining Loss: 0.01233765 \tValidation Loss 0.01706231 \tTraining Acuuarcy 37.688% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2425 \tTraining Loss: 0.01235960 \tValidation Loss 0.01706509 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 20.284%\n",
      "Epoch: 2426 \tTraining Loss: 0.01231855 \tValidation Loss 0.01717710 \tTraining Acuuarcy 38.201% \tValidation Acuuarcy 20.312%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2427 \tTraining Loss: 0.01232223 \tValidation Loss 0.01754592 \tTraining Acuuarcy 37.800% \tValidation Acuuarcy 18.947%\n",
      "Epoch: 2428 \tTraining Loss: 0.01232070 \tValidation Loss 0.01710081 \tTraining Acuuarcy 38.117% \tValidation Acuuarcy 21.092%\n",
      "Epoch: 2429 \tTraining Loss: 0.01234293 \tValidation Loss 0.01776106 \tTraining Acuuarcy 37.855% \tValidation Acuuarcy 19.588%\n",
      "Epoch: 2430 \tTraining Loss: 0.01241362 \tValidation Loss 0.01675409 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2431 \tTraining Loss: 0.01237307 \tValidation Loss 0.01703863 \tTraining Acuuarcy 37.939% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2432 \tTraining Loss: 0.01233576 \tValidation Loss 0.01703464 \tTraining Acuuarcy 38.184% \tValidation Acuuarcy 19.560%\n",
      "Epoch: 2433 \tTraining Loss: 0.01243553 \tValidation Loss 0.01696648 \tTraining Acuuarcy 37.354% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2434 \tTraining Loss: 0.01232927 \tValidation Loss 0.01700751 \tTraining Acuuarcy 38.084% \tValidation Acuuarcy 19.337%\n",
      "Epoch: 2435 \tTraining Loss: 0.01239552 \tValidation Loss 0.01683860 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2436 \tTraining Loss: 0.01230034 \tValidation Loss 0.01710588 \tTraining Acuuarcy 38.413% \tValidation Acuuarcy 20.451%\n",
      "Epoch: 2437 \tTraining Loss: 0.01235970 \tValidation Loss 0.01706021 \tTraining Acuuarcy 37.526% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2438 \tTraining Loss: 0.01235529 \tValidation Loss 0.01720814 \tTraining Acuuarcy 37.811% \tValidation Acuuarcy 19.420%\n",
      "Epoch: 2439 \tTraining Loss: 0.01240046 \tValidation Loss 0.01715316 \tTraining Acuuarcy 37.917% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2440 \tTraining Loss: 0.01241406 \tValidation Loss 0.01719661 \tTraining Acuuarcy 37.036% \tValidation Acuuarcy 18.696%\n",
      "Epoch: 2441 \tTraining Loss: 0.01231170 \tValidation Loss 0.01759457 \tTraining Acuuarcy 38.374% \tValidation Acuuarcy 19.170%\n",
      "Epoch: 2442 \tTraining Loss: 0.01232415 \tValidation Loss 0.01726729 \tTraining Acuuarcy 37.922% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2443 \tTraining Loss: 0.01242753 \tValidation Loss 0.01726898 \tTraining Acuuarcy 37.621% \tValidation Acuuarcy 18.167%\n",
      "Epoch: 2444 \tTraining Loss: 0.01238323 \tValidation Loss 0.01738172 \tTraining Acuuarcy 38.190% \tValidation Acuuarcy 19.225%\n",
      "Epoch: 2445 \tTraining Loss: 0.01237186 \tValidation Loss 0.01702614 \tTraining Acuuarcy 37.292% \tValidation Acuuarcy 20.145%\n",
      "Epoch: 2446 \tTraining Loss: 0.01242394 \tValidation Loss 0.01716081 \tTraining Acuuarcy 37.409% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2447 \tTraining Loss: 0.01229062 \tValidation Loss 0.01731708 \tTraining Acuuarcy 37.827% \tValidation Acuuarcy 18.445%\n",
      "Epoch: 2448 \tTraining Loss: 0.01237836 \tValidation Loss 0.01699240 \tTraining Acuuarcy 37.465% \tValidation Acuuarcy 19.783%\n",
      "Epoch: 2449 \tTraining Loss: 0.01231628 \tValidation Loss 0.01708257 \tTraining Acuuarcy 38.535% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2450 \tTraining Loss: 0.01228972 \tValidation Loss 0.01742566 \tTraining Acuuarcy 38.145% \tValidation Acuuarcy 19.476%\n",
      "Epoch: 2451 \tTraining Loss: 0.01229874 \tValidation Loss 0.01721999 \tTraining Acuuarcy 37.900% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2452 \tTraining Loss: 0.01235883 \tValidation Loss 0.01709809 \tTraining Acuuarcy 37.995% \tValidation Acuuarcy 19.504%\n",
      "Epoch: 2453 \tTraining Loss: 0.01238884 \tValidation Loss 0.01701554 \tTraining Acuuarcy 37.220% \tValidation Acuuarcy 19.671%\n",
      "Epoch: 2454 \tTraining Loss: 0.01238950 \tValidation Loss 0.01738832 \tTraining Acuuarcy 37.365% \tValidation Acuuarcy 19.281%\n",
      "Epoch: 2455 \tTraining Loss: 0.01230680 \tValidation Loss 0.01705702 \tTraining Acuuarcy 38.078% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2456 \tTraining Loss: 0.01234741 \tValidation Loss 0.01727512 \tTraining Acuuarcy 37.805% \tValidation Acuuarcy 18.724%\n",
      "Epoch: 2457 \tTraining Loss: 0.01228510 \tValidation Loss 0.01726167 \tTraining Acuuarcy 38.167% \tValidation Acuuarcy 18.473%\n",
      "Epoch: 2458 \tTraining Loss: 0.01242719 \tValidation Loss 0.01725713 \tTraining Acuuarcy 37.159% \tValidation Acuuarcy 18.668%\n",
      "Epoch: 2459 \tTraining Loss: 0.01231573 \tValidation Loss 0.01744888 \tTraining Acuuarcy 37.900% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2460 \tTraining Loss: 0.01235336 \tValidation Loss 0.01738110 \tTraining Acuuarcy 37.933% \tValidation Acuuarcy 19.003%\n",
      "Epoch: 2461 \tTraining Loss: 0.01231659 \tValidation Loss 0.01738114 \tTraining Acuuarcy 37.705% \tValidation Acuuarcy 20.033%\n",
      "Epoch: 2462 \tTraining Loss: 0.01238771 \tValidation Loss 0.01698126 \tTraining Acuuarcy 37.660% \tValidation Acuuarcy 18.083%\n",
      "Epoch: 2463 \tTraining Loss: 0.01231161 \tValidation Loss 0.01727016 \tTraining Acuuarcy 38.017% \tValidation Acuuarcy 20.563%\n",
      "Epoch: 2464 \tTraining Loss: 0.01239603 \tValidation Loss 0.01695118 \tTraining Acuuarcy 37.599% \tValidation Acuuarcy 19.755%\n",
      "Epoch: 2465 \tTraining Loss: 0.01240396 \tValidation Loss 0.01707302 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2466 \tTraining Loss: 0.01233634 \tValidation Loss 0.01680099 \tTraining Acuuarcy 37.833% \tValidation Acuuarcy 20.396%\n",
      "Epoch: 2467 \tTraining Loss: 0.01242227 \tValidation Loss 0.01721012 \tTraining Acuuarcy 37.432% \tValidation Acuuarcy 20.619%\n",
      "Epoch: 2468 \tTraining Loss: 0.01238698 \tValidation Loss 0.01686896 \tTraining Acuuarcy 37.599% \tValidation Acuuarcy 18.835%\n",
      "Epoch: 2469 \tTraining Loss: 0.01236763 \tValidation Loss 0.01688968 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 19.643%\n",
      "Epoch: 2470 \tTraining Loss: 0.01239304 \tValidation Loss 0.01708659 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 19.838%\n",
      "Epoch: 2471 \tTraining Loss: 0.01238977 \tValidation Loss 0.01719062 \tTraining Acuuarcy 37.538% \tValidation Acuuarcy 19.448%\n",
      "Epoch: 2472 \tTraining Loss: 0.01241494 \tValidation Loss 0.01709493 \tTraining Acuuarcy 37.761% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2473 \tTraining Loss: 0.01237617 \tValidation Loss 0.01740575 \tTraining Acuuarcy 37.571% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2474 \tTraining Loss: 0.01242635 \tValidation Loss 0.01709168 \tTraining Acuuarcy 37.487% \tValidation Acuuarcy 20.006%\n",
      "Epoch: 2475 \tTraining Loss: 0.01232709 \tValidation Loss 0.01733796 \tTraining Acuuarcy 37.894% \tValidation Acuuarcy 20.646%\n",
      "Epoch: 2476 \tTraining Loss: 0.01230486 \tValidation Loss 0.01800043 \tTraining Acuuarcy 38.006% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2477 \tTraining Loss: 0.01239488 \tValidation Loss 0.01738095 \tTraining Acuuarcy 37.404% \tValidation Acuuarcy 19.086%\n",
      "Epoch: 2478 \tTraining Loss: 0.01236381 \tValidation Loss 0.01732853 \tTraining Acuuarcy 37.409% \tValidation Acuuarcy 18.807%\n",
      "Epoch: 2479 \tTraining Loss: 0.01230860 \tValidation Loss 0.01707767 \tTraining Acuuarcy 38.675% \tValidation Acuuarcy 18.891%\n",
      "Epoch: 2480 \tTraining Loss: 0.01230286 \tValidation Loss 0.01731221 \tTraining Acuuarcy 37.922% \tValidation Acuuarcy 19.699%\n",
      "Epoch: 2481 \tTraining Loss: 0.01235007 \tValidation Loss 0.01677597 \tTraining Acuuarcy 38.028% \tValidation Acuuarcy 20.730%\n",
      "Epoch: 2482 \tTraining Loss: 0.01242476 \tValidation Loss 0.01703899 \tTraining Acuuarcy 37.270% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2483 \tTraining Loss: 0.01237509 \tValidation Loss 0.01710619 \tTraining Acuuarcy 37.816% \tValidation Acuuarcy 20.479%\n",
      "Epoch: 2484 \tTraining Loss: 0.01229980 \tValidation Loss 0.01682486 \tTraining Acuuarcy 38.346% \tValidation Acuuarcy 19.058%\n",
      "Epoch: 2485 \tTraining Loss: 0.01236535 \tValidation Loss 0.01732445 \tTraining Acuuarcy 37.850% \tValidation Acuuarcy 19.253%\n",
      "Epoch: 2486 \tTraining Loss: 0.01239754 \tValidation Loss 0.01717024 \tTraining Acuuarcy 36.991% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2487 \tTraining Loss: 0.01237955 \tValidation Loss 0.01716650 \tTraining Acuuarcy 37.543% \tValidation Acuuarcy 20.173%\n",
      "Epoch: 2488 \tTraining Loss: 0.01238696 \tValidation Loss 0.01696438 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 19.615%\n",
      "Epoch: 2489 \tTraining Loss: 0.01236425 \tValidation Loss 0.01688507 \tTraining Acuuarcy 37.866% \tValidation Acuuarcy 19.727%\n",
      "Epoch: 2490 \tTraining Loss: 0.01235451 \tValidation Loss 0.01704345 \tTraining Acuuarcy 37.532% \tValidation Acuuarcy 18.975%\n",
      "Epoch: 2491 \tTraining Loss: 0.01240487 \tValidation Loss 0.01709404 \tTraining Acuuarcy 37.833% \tValidation Acuuarcy 20.061%\n",
      "Epoch: 2492 \tTraining Loss: 0.01233870 \tValidation Loss 0.01722454 \tTraining Acuuarcy 37.850% \tValidation Acuuarcy 19.030%\n",
      "Epoch: 2493 \tTraining Loss: 0.01234137 \tValidation Loss 0.01690367 \tTraining Acuuarcy 37.722% \tValidation Acuuarcy 20.758%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2494 \tTraining Loss: 0.01230448 \tValidation Loss 0.01731439 \tTraining Acuuarcy 38.206% \tValidation Acuuarcy 19.811%\n",
      "Epoch: 2495 \tTraining Loss: 0.01241805 \tValidation Loss 0.01698382 \tTraining Acuuarcy 37.872% \tValidation Acuuarcy 19.894%\n",
      "Epoch: 2496 \tTraining Loss: 0.01233994 \tValidation Loss 0.01730118 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 19.532%\n",
      "Epoch: 2497 \tTraining Loss: 0.01235463 \tValidation Loss 0.01719306 \tTraining Acuuarcy 37.499% \tValidation Acuuarcy 20.117%\n",
      "Epoch: 2498 \tTraining Loss: 0.01239685 \tValidation Loss 0.01716368 \tTraining Acuuarcy 37.794% \tValidation Acuuarcy 18.863%\n",
      "Epoch: 2499 \tTraining Loss: 0.01240296 \tValidation Loss 0.01749012 \tTraining Acuuarcy 37.694% \tValidation Acuuarcy 19.114%\n",
      "Epoch: 2500 \tTraining Loss: 0.01236963 \tValidation Loss 0.01687615 \tTraining Acuuarcy 37.320% \tValidation Acuuarcy 19.811%\n",
      "===================================Training Finished===================================\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"CUDA is not available. Make sure you have a CUDA-enabled GPU.\")\n",
    "\n",
    "def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        # Train the model  #\n",
    "        net.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optmizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        #validate the model#\n",
    "        net.eval()\n",
    "        for data,labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            val_outputs = net(data)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs,1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss =  validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
    "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
    "\n",
    "    torch.save(net.state_dict(),'deep_emotion-{}-{}-{}.pt'.format(epochs,batchsize,lr))\n",
    "    print(\"===================================Training Finished===================================\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"Configuration of setup and training process\")\n",
    "#     parser.add_argument('-s', '--setup', type=bool, help='setup the dataset for the first time')\n",
    "#     parser.add_argument('-d', '--data', type=str,required= True,\n",
    "#                                help='data folder that contains data files that downloaded from kaggle (train.csv and test.csv)')\n",
    "#     parser.add_argument('-hparams', '--hyperparams', type=bool,\n",
    "#                                help='True when changing the hyperparameters e.g (batch size, LR, num. of epochs)')\n",
    "#     parser.add_argument('-e', '--epochs', type= int, help= 'number of epochs')\n",
    "#     parser.add_argument('-lr', '--learning_rate', type= float, help= 'value of learning rate')\n",
    "#     parser.add_argument('-bs', '--batch_size', type= int, help= 'training/validation batch size')\n",
    "#     parser.add_argument('-t', '--train', type=bool, help='True when training')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if args.setup :\n",
    "#         generate_dataset = Generate_data(args.data)\n",
    "#         generate_dataset.split_test()\n",
    "#         generate_dataset.save_images('train')\n",
    "#         generate_dataset.save_images('test')\n",
    "#         generate_dataset.save_images('val')\n",
    "\n",
    "#     if args.hyperparams:\n",
    "#         epochs = args.epochs\n",
    "#         lr = args.learning_rate\n",
    "#         batchsize = args.batch_size\n",
    "#     else :\n",
    "epochs = 2500\n",
    "lr = 0.005\n",
    "batchsize = 128\n",
    "\n",
    "#     if args.train:\n",
    "net = Deep_Emotion()\n",
    "net.to(device)\n",
    "print(\"Model archticture: \", net)\n",
    "traincsv_file = 'data'+'/'+'train.csv'\n",
    "validationcsv_file = 'data'+'/'+'val.csv'\n",
    "train_img_dir = 'data'+'/'+'train/'\n",
    "validation_img_dir = 'data'+'/'+'val/'\n",
    "\n",
    "transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
    "validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
    "train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optmizer= optim.Adam(net.parameters(),lr= lr)\n",
    "Train(epochs, train_loader, val_loader, criterion, optmizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cfe635",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'Speaktrum_by_SOVA_latest.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b387263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep_Emotion(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
       "  (localization): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_loc): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net= Deep_Emotion()\n",
    "net.load_state_dict(torch.load('Speaktrum_by_SOVA.pt'))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed2214e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "Face not detected\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "Face not detected\n",
      "Face not detected\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(1, device='cuda:0')\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m faceCascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(cv2\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mhaarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 26\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mfaceCascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, w, h \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     29\u001b[0m     roi_gray \u001b[38;5;241m=\u001b[39m gray[y:y\u001b[38;5;241m+\u001b[39mh, x:x\u001b[38;5;241m+\u001b[39mw]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Load the pre-trained model (assuming you have defined and loaded the 'net' model)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "                # Add a delay of 1 second\n",
    "                time.sleep(3)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "                                # Add a delay of 1 second\n",
    "                time.sleep(1)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "                                # Add a delay of 1 second\n",
    "                time.sleep(1)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "                                # Add a delay of 1 second\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e00068c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m delay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Delay time between predictions (in seconds)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     current_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     28\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m current_time \u001b[38;5;241m-\u001b[39m last_time\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "last_time = time.time()\n",
    "delay = 0.5  # Delay time between predictions (in seconds)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - last_time\n",
    "\n",
    "    if elapsed_time < delay:\n",
    "        continue\n",
    "\n",
    "    last_time = current_time\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42cee95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import threading\n",
    "import time\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "last_time = time.time()\n",
    "delay = 0.5  # Delay time between predictions (in seconds)\n",
    "\n",
    "def process_frame(frame):\n",
    "    global last_time\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - last_time\n",
    "\n",
    "    if elapsed_time < delay:\n",
    "        time.sleep(delay - elapsed_time)\n",
    "\n",
    "    last_time = time.time()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "\n",
    "    cv2.imshow('Face', frame)\n",
    "\n",
    "def video_thread():\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        process_frame(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a45debf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after class definition on line 26 (552671800.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 30\u001b[1;36m\u001b[0m\n\u001b[1;33m    net = YourModel()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after class definition on line 26\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a83d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import threading\n",
    "import time\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "last_time = time.time()\n",
    "delay = 0.5  # Delay time between predictions (in seconds)\n",
    "\n",
    "def process_frame(frame):\n",
    "    global last_time\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - last_time\n",
    "\n",
    "    if elapsed_time < delay:\n",
    "        time.sleep(delay - elapsed_time)\n",
    "\n",
    "    last_time = time.time()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "\n",
    "    cv2.imshow('Face', frame)\n",
    "\n",
    "def video_thread():\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        process_frame(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57823a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m faceCascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(cv2\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mhaarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 38\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mfaceCascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, w, h \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     41\u001b[0m     roi_gray \u001b[38;5;241m=\u001b[39m gray[y:y\u001b[38;5;241m+\u001b[39mh, x:x\u001b[38;5;241m+\u001b[39mw]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "last_time = time.time()\n",
    "delay = 0.5  # Delay time between predictions (in seconds)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - last_time\n",
    "\n",
    "    if elapsed_time < delay:\n",
    "        continue\n",
    "\n",
    "    last_time = current_time\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83cca630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (process_frames_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"D:\\Anaconda\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\xcite\\AppData\\Local\\Temp\\ipykernel_15984\\3921651129.py\", line 107, in process_frames_thread\n",
      "  File \"C:\\Users\\xcite\\AppData\\Local\\Temp\\ipykernel_15984\\3921651129.py\", line 80, in process_frame\n",
      "UnboundLocalError: local variable 'font' referenced before assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 118\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Main thread continues to capture frames without waiting for predictions\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import threading\n",
    "import time\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "last_time = time.time()\n",
    "delay = 0.5  # Delay time between predictions (in seconds)\n",
    "\n",
    "def process_frame(frame):\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "def process_frames_thread():\n",
    "    global last_time\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - last_time\n",
    "\n",
    "        if elapsed_time < delay:\n",
    "            continue\n",
    "\n",
    "        last_time = current_time\n",
    "\n",
    "        process_frame(frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Create a separate thread for processing frames\n",
    "processing_thread = threading.Thread(target=process_frames_thread)\n",
    "processing_thread.start()\n",
    "\n",
    "# Main thread continues to capture frames without waiting for predictions\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72c1bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7 (process_frames_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"D:\\Anaconda\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\xcite\\AppData\\Local\\Temp\\ipykernel_15984\\3648131566.py\", line 109, in process_frames_thread\n",
      "  File \"C:\\Users\\xcite\\AppData\\Local\\Temp\\ipykernel_15984\\3648131566.py\", line 87, in process_frame\n",
      "UnboundLocalError: local variable 'status' referenced before assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not detected\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Main thread continues to capture frames without waiting for predictions\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import threading\n",
    "import time\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "last_time = time.time()\n",
    "delay = 0.5  # Delay time between predictions (in seconds)\n",
    "\n",
    "def process_frame(frame):\n",
    "    global font\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame,\n",
    "                status,\n",
    "                (50, 50),\n",
    "                font, 0,\n",
    "                color,\n",
    "                2,\n",
    "                cv2.LINE_4)\n",
    "    cv2.imshow('Face', frame)\n",
    "\n",
    "def process_frames_thread():\n",
    "    global last_time\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - last_time\n",
    "\n",
    "        if elapsed_time < delay:\n",
    "            continue\n",
    "\n",
    "        last_time = current_time\n",
    "\n",
    "        process_frame(frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Create a separate thread for processing frames\n",
    "processing_thread = threading.Thread(target=process_frames_thread)\n",
    "processing_thread.start()\n",
    "\n",
    "# Main thread continues to capture frames without waiting for predictions\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32e0f2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (process_frames_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"D:\\Anaconda\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\xcite\\AppData\\Local\\Temp\\ipykernel_15984\\3648131566.py\", line 109, in process_frames_thread\n",
      "  File \"C:\\Users\\xcite\\AppData\\Local\\Temp\\ipykernel_15984\\3648131566.py\", line 87, in process_frame\n",
      "UnboundLocalError: local variable 'status' referenced before assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not detected\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Main thread continues to capture frames without waiting for predictions\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import threading\n",
    "import time\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "last_time = time.time()\n",
    "delay = 0.5  # Delay time between predictions (in seconds)\n",
    "\n",
    "def process_frame(frame):\n",
    "    global font\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame,\n",
    "                status,\n",
    "                (50, 50),\n",
    "                font, 0,\n",
    "                color,\n",
    "                2,\n",
    "                cv2.LINE_4)\n",
    "    cv2.imshow('Face', frame)\n",
    "\n",
    "def process_frames_thread():\n",
    "    global last_time\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - last_time\n",
    "\n",
    "        if elapsed_time < delay:\n",
    "            continue\n",
    "\n",
    "        last_time = current_time\n",
    "\n",
    "        process_frame(frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Create a separate thread for processing frames\n",
    "processing_thread = threading.Thread(target=process_frames_thread)\n",
    "processing_thread.start()\n",
    "\n",
    "# Main thread continues to capture frames without waiting for predictions\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3fe0970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not detected\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "Face not detected\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot open webcam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     faceCascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(cv2\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mhaarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m     gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5847b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture: Deep_Emotion(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "===================================Start Training===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4277: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4215: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.01436080 \tValidation Loss 0.01473029 \tTraining Accuracy 23.710% \tValidation Accuracy 24.882%\n",
      "Epoch: 2 \tTraining Loss: 0.01426829 \tValidation Loss 0.01470124 \tTraining Accuracy 25.025% \tValidation Accuracy 25.132%\n",
      "Epoch: 3 \tTraining Loss: 0.01425268 \tValidation Loss 0.01475179 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n",
      "Epoch: 4 \tTraining Loss: 0.01424120 \tValidation Loss 0.01475170 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n",
      "Epoch: 5 \tTraining Loss: 0.01423309 \tValidation Loss 0.01471210 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n",
      "Epoch: 6 \tTraining Loss: 0.01422150 \tValidation Loss 0.01471593 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n",
      "Epoch: 7 \tTraining Loss: 0.01422524 \tValidation Loss 0.01471605 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n",
      "Epoch: 8 \tTraining Loss: 0.01422129 \tValidation Loss 0.01468835 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n",
      "Epoch: 9 \tTraining Loss: 0.01420936 \tValidation Loss 0.01473240 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n",
      "Epoch: 10 \tTraining Loss: 0.01421916 \tValidation Loss 0.01470196 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n",
      "Epoch: 11 \tTraining Loss: 0.01420790 \tValidation Loss 0.01480003 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n",
      "Epoch: 12 \tTraining Loss: 0.01420669 \tValidation Loss 0.01471475 \tTraining Accuracy 25.047% \tValidation Accuracy 25.132%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_loaders import Plain_Dataset\n",
    "from deep_emotion import Deep_Emotion\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def Train(epochs, train_loader, val_loader, criterion, optimizer, device):\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        net.train()  # Set the model to training mode\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                val_outputs = net(data)\n",
    "                val_loss = criterion(val_outputs, labels)\n",
    "                validation_loss += val_loss.item()\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss / len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss = validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Accuracy {:.3f}% \\tValidation Accuracy {:.3f}%'\n",
    "              .format(e+1, train_loss, validation_loss, train_acc * 100, val_acc * 100))\n",
    "\n",
    "    torch.save(net.state_dict(), 'deep_emotion-{}-{}-{}.pt'.format(epochs, batchsize, lr))\n",
    "    print(\"===================================Training Finished===================================\")\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.005\n",
    "batchsize = 128\n",
    "\n",
    "net = Deep_Emotion().to(device)\n",
    "print(\"Model architecture:\", net)\n",
    "\n",
    "traincsv_file = 'data/train.csv'\n",
    "validationcsv_file = 'data/val.csv'\n",
    "train_img_dir = 'data/train/'\n",
    "validation_img_dir = 'data/val/'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(size=48),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = Plain_Dataset(csv_file=traincsv_file, img_dir=train_img_dir, datatype='train', transform=train_transform)\n",
    "validation_dataset = Plain_Dataset(csv_file=validationcsv_file, img_dir=validation_img_dir, datatype='val', transform=train_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "Train(epochs, train_loader, val_loader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589b6bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeaktrum_by_SOVA.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.load('Speaktrum_by_SOVA.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee2b40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy  as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb1cbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[-1.3373,  0.0349,  4.8073],\n",
       "                        [-2.7566,  2.5705,  5.8771],\n",
       "                        [ 1.9646,  2.8807,  0.9151]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2351,  4.1658,  2.0277],\n",
       "                        [-3.7566,  1.9988,  3.8267],\n",
       "                        [-5.9522, -1.6863,  2.3646]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4110, -2.5929, -0.3401],\n",
       "                        [-2.3929, -1.0409, -2.4871],\n",
       "                        [ 0.5789,  1.4128, -3.7534]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.3382,  3.4311,  0.3542],\n",
       "                        [ 2.2669,  2.2597, -0.8006],\n",
       "                        [ 2.6262,  2.3853,  1.1712]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8800, -0.3192,  4.3025],\n",
       "                        [-7.1857,  0.5128,  0.6028],\n",
       "                        [-2.4710,  0.1690, -3.3596]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.9596,  1.4075, -6.8317],\n",
       "                        [-1.9682,  3.3239,  1.6794],\n",
       "                        [ 4.2800, -1.4093,  4.4525]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.0663, -3.2258, -1.7136],\n",
       "                        [-0.3996, -3.0985,  0.9061],\n",
       "                        [-0.7429, -2.4813,  1.6325]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.0229, -3.9140,  5.0645],\n",
       "                        [-1.9072, -2.6228, -0.2695],\n",
       "                        [ 7.4093,  1.3833,  2.4225]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4410, -3.1752, -6.3015],\n",
       "                        [-0.4588,  2.6888,  1.1538],\n",
       "                        [-2.3273, -4.8125,  7.6302]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.1076,  2.3843,  8.2610],\n",
       "                        [ 1.1655, -6.1219, -0.2558],\n",
       "                        [-3.4120, -4.9167,  1.8566]]]], device='cuda:0')),\n",
       "             ('conv1.bias',\n",
       "              tensor([-1.7185, -1.7422,  2.4595, -0.2717, -1.6484, -0.9967,  2.3490, -0.5572,\n",
       "                      -0.5983, -4.9344], device='cuda:0')),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[-5.5408e-02, -1.0129e-01, -6.0681e-02],\n",
       "                        [ 1.3404e-02, -9.8178e-02,  2.1980e-02],\n",
       "                        [-6.9553e-02, -9.8364e-02, -3.4914e-02]],\n",
       "              \n",
       "                       [[-5.0724e-02, -6.0382e-02, -8.2591e-02],\n",
       "                        [-1.0560e-01, -8.4673e-02, -7.9187e-02],\n",
       "                        [-8.9515e-02, -5.7180e-02, -4.4412e-03]],\n",
       "              \n",
       "                       [[ 5.8708e-02, -1.2875e-01,  2.8819e-02],\n",
       "                        [-4.1707e-02,  3.9986e-02,  3.1636e-02],\n",
       "                        [-3.9496e-02, -1.4872e-01, -1.3046e-01]],\n",
       "              \n",
       "                       [[-2.8684e-02, -7.6279e-02, -4.6735e-02],\n",
       "                        [ 7.4986e-02, -8.9151e-02, -3.7861e-02],\n",
       "                        [ 2.3358e-02, -5.7634e-02,  4.8911e-02]],\n",
       "              \n",
       "                       [[-6.2199e-02, -3.4652e-02, -6.9747e-02],\n",
       "                        [-5.2185e-02, -7.5984e-02, -1.5562e-02],\n",
       "                        [-9.0587e-02, -3.1335e-03,  6.5442e-02]],\n",
       "              \n",
       "                       [[-2.8121e-02,  1.5991e-02, -1.0737e-01],\n",
       "                        [-1.1728e-01,  6.2679e-02, -2.1521e-02],\n",
       "                        [-1.0391e-01, -5.3923e-02, -1.1054e-01]],\n",
       "              \n",
       "                       [[-1.0259e-01, -9.6973e-02,  1.1885e-02],\n",
       "                        [-9.6461e-02,  4.8096e-02,  9.3046e-03],\n",
       "                        [-8.0510e-02,  2.6634e-02, -7.1122e-02]],\n",
       "              \n",
       "                       [[ 7.8813e-03, -5.8381e-03,  6.0308e-02],\n",
       "                        [-5.3676e-02, -6.0166e-02, -8.4891e-02],\n",
       "                        [-4.1147e-02, -7.3141e-02, -1.3567e-01]],\n",
       "              \n",
       "                       [[-8.0968e-02, -7.2857e-02,  4.2400e-02],\n",
       "                        [ 4.8521e-02, -1.3599e-01, -2.0919e-02],\n",
       "                        [ 6.8080e-02, -1.1188e-01, -5.7191e-03]],\n",
       "              \n",
       "                       [[-4.1166e-02, -1.1944e-01, -8.2765e-02],\n",
       "                        [ 2.8260e-02, -7.6243e-02, -1.6505e-03],\n",
       "                        [ 6.9834e-02, -1.8384e-02,  9.8738e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8551e-01, -1.0187e-01, -2.1455e-01],\n",
       "                        [-1.9231e-01, -2.3598e-01, -1.2018e-01],\n",
       "                        [-4.3827e-01, -3.0992e-01, -3.9034e-01]],\n",
       "              \n",
       "                       [[-4.4050e-01, -1.6876e-01, -1.2331e-01],\n",
       "                        [-3.4996e-01, -2.9290e-01, -2.6886e-01],\n",
       "                        [-4.3317e-01, -3.8107e-01, -4.9085e-01]],\n",
       "              \n",
       "                       [[-4.4002e-01, -6.8790e-01, -8.4232e-01],\n",
       "                        [-4.6477e-01, -5.8579e-01, -4.1754e-01],\n",
       "                        [-3.5522e-01, -3.2543e-01, -2.1748e-01]],\n",
       "              \n",
       "                       [[-3.6545e-01, -1.9652e-01, -1.8112e-01],\n",
       "                        [-2.8037e-01, -1.8678e-01, -1.1282e-01],\n",
       "                        [-5.5509e-01, -3.4448e-01, -5.2458e-01]],\n",
       "              \n",
       "                       [[-6.5595e-02,  2.1980e-01, -4.9137e-01],\n",
       "                        [-1.7810e-01,  1.3892e-01, -1.9379e-01],\n",
       "                        [-2.9804e-01, -1.9738e-01, -5.7362e-01]],\n",
       "              \n",
       "                       [[-7.3550e-02, -7.4631e-02, -2.4291e-01],\n",
       "                        [-1.8218e-01, -2.8896e-01, -3.0619e-01],\n",
       "                        [-3.8444e-01, -3.7241e-01, -6.1472e-01]],\n",
       "              \n",
       "                       [[-3.4406e-01, -3.2776e-01, -5.5446e-01],\n",
       "                        [-4.2553e-01, -4.0361e-01, -3.4127e-01],\n",
       "                        [-2.9243e-01, -6.0645e-01, -5.2502e-01]],\n",
       "              \n",
       "                       [[ 5.4304e-02,  1.5604e-01,  2.3880e-02],\n",
       "                        [ 2.2443e-01, -1.6546e-02, -5.7578e-02],\n",
       "                        [-3.3751e-01,  3.5582e-02, -1.3600e-01]],\n",
       "              \n",
       "                       [[ 1.1636e-01,  2.2014e-02, -6.2977e-02],\n",
       "                        [ 1.8884e-02, -3.0474e-01, -2.9216e-01],\n",
       "                        [-1.4102e-03,  6.0022e-02, -1.8826e-01]],\n",
       "              \n",
       "                       [[-1.9247e-01,  2.7982e-01,  6.7626e-02],\n",
       "                        [-3.4497e-01, -3.9612e-01, -3.0593e-01],\n",
       "                        [-4.1162e-01, -6.5882e-01, -3.5279e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0050e-01, -1.1757e-01, -1.6165e-01],\n",
       "                        [-2.0980e-01, -1.3934e-01, -3.6144e-02],\n",
       "                        [-2.1685e-01, -1.2228e-01, -1.6214e-01]],\n",
       "              \n",
       "                       [[-5.7164e-02, -1.7697e-01, -1.3703e-01],\n",
       "                        [-3.0876e-02, -1.6617e-02, -1.3199e-01],\n",
       "                        [-7.3582e-02, -2.9931e-02, -7.4828e-02]],\n",
       "              \n",
       "                       [[-1.1008e-01,  7.9903e-02, -2.8695e-02],\n",
       "                        [-2.4054e-02,  2.7778e-02, -2.2865e-02],\n",
       "                        [-1.5300e-01, -1.2212e-01, -2.0932e-01]],\n",
       "              \n",
       "                       [[ 1.0024e-01, -1.3639e-01, -1.2566e-01],\n",
       "                        [-1.2991e-01, -1.0205e-01, -1.3462e-02],\n",
       "                        [-2.8115e-01, -2.1081e-01, -2.0901e-01]],\n",
       "              \n",
       "                       [[-6.4237e-02,  1.4119e-02,  4.1786e-02],\n",
       "                        [-7.4729e-02, -9.4435e-02,  1.8129e-02],\n",
       "                        [-5.2642e-02, -4.9021e-03, -1.1442e-01]],\n",
       "              \n",
       "                       [[ 3.5524e-02, -1.8238e-01, -3.1621e-02],\n",
       "                        [-1.5420e-01,  2.0388e-02, -1.6471e-02],\n",
       "                        [-1.4519e-01, -1.3882e-01, -5.3727e-02]],\n",
       "              \n",
       "                       [[-1.7411e-01, -1.0733e-01, -1.2917e-02],\n",
       "                        [-1.2310e-01, -2.8563e-02, -1.1450e-01],\n",
       "                        [-1.9384e-01, -2.0473e-01, -2.9815e-01]],\n",
       "              \n",
       "                       [[-2.3442e-02,  3.3497e-02, -1.4457e-02],\n",
       "                        [-1.3345e-01, -8.8857e-02,  2.4496e-02],\n",
       "                        [-1.3666e-01, -6.9111e-02,  1.0666e-02]],\n",
       "              \n",
       "                       [[-9.5361e-02,  1.6160e-02, -7.2789e-02],\n",
       "                        [-7.6460e-02,  7.3131e-02,  7.6012e-03],\n",
       "                        [ 4.3772e-02, -1.3976e-01, -1.8882e-01]],\n",
       "              \n",
       "                       [[-5.3744e-02,  4.9045e-03,  8.3149e-02],\n",
       "                        [ 5.0777e-02, -2.6012e-02,  5.8529e-02],\n",
       "                        [-2.5295e-02, -2.7053e-02, -8.4585e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.7057e+00,  2.8945e+00, -1.4202e+00],\n",
       "                        [-4.1907e+00,  1.7654e+00, -2.2771e+00],\n",
       "                        [-1.6082e+00,  1.6439e+00, -4.4254e+00]],\n",
       "              \n",
       "                       [[ 3.2145e+00,  2.6312e+00, -4.2149e+00],\n",
       "                        [ 8.9252e-01,  5.0150e+00, -5.5071e+00],\n",
       "                        [ 5.5081e+00,  2.0255e+00, -5.8957e-02]],\n",
       "              \n",
       "                       [[ 1.2100e+00,  2.1093e+00,  1.6429e+00],\n",
       "                        [ 2.9869e-01,  7.0776e-01,  1.3605e+00],\n",
       "                        [-2.0756e+00, -6.3685e-01, -1.4124e-01]],\n",
       "              \n",
       "                       [[-2.2900e+00, -9.3689e-01,  5.4947e+00],\n",
       "                        [-2.0640e-01, -1.2621e+00,  1.7022e+00],\n",
       "                        [-5.9639e-02,  3.5679e+00,  1.4989e+00]],\n",
       "              \n",
       "                       [[ 1.8758e+00,  1.0529e-01,  4.6130e+00],\n",
       "                        [-1.0497e+00,  2.8867e+00,  3.3858e+00],\n",
       "                        [-5.8019e+00, -2.4342e+00, -2.1989e+00]],\n",
       "              \n",
       "                       [[-7.6675e-01, -9.6455e-01, -5.0569e+00],\n",
       "                        [-5.4636e+00,  5.7283e-01, -2.2358e+00],\n",
       "                        [-1.3053e+00,  7.0704e+00, -3.0156e+00]],\n",
       "              \n",
       "                       [[ 8.8013e-01,  6.4805e-01,  9.1793e-01],\n",
       "                        [ 1.5547e+00,  1.4746e+00,  6.1737e-01],\n",
       "                        [-3.4277e+00, -1.7889e+00, -1.0439e+00]],\n",
       "              \n",
       "                       [[-3.9829e+00, -9.3641e-01,  6.0938e+00],\n",
       "                        [-3.6925e+00,  2.6114e+00, -9.2975e-01],\n",
       "                        [-6.1815e+00,  2.9350e+00,  6.0206e-01]],\n",
       "              \n",
       "                       [[-3.3047e+00,  3.0747e+00, -7.1644e-02],\n",
       "                        [ 2.5568e+00,  1.8560e+00,  6.9198e+00],\n",
       "                        [ 1.4768e+00, -1.9047e+00,  3.5444e+00]],\n",
       "              \n",
       "                       [[ 2.2374e+00,  1.2146e-01, -5.2589e+00],\n",
       "                        [ 1.1759e+00, -3.8857e+00,  4.4544e+00],\n",
       "                        [-1.0141e+00,  2.6831e+00,  7.7451e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8834e-02, -2.0454e-01, -1.1068e-01],\n",
       "                        [-5.0174e-02, -7.1287e-02, -1.2168e-01],\n",
       "                        [-7.8787e-02, -1.2480e-01, -5.3256e-02]],\n",
       "              \n",
       "                       [[ 8.0772e-02, -1.4323e-01, -1.5202e-01],\n",
       "                        [-1.5594e-01, -2.3927e-02, -3.3425e-02],\n",
       "                        [ 9.2086e-02,  6.2042e-02, -4.9323e-02]],\n",
       "              \n",
       "                       [[-1.2652e-01, -1.7859e-02, -1.0334e-01],\n",
       "                        [-1.9898e-01, -2.6439e-01,  3.8819e-02],\n",
       "                        [-2.0368e-01, -5.8142e-02,  8.9110e-02]],\n",
       "              \n",
       "                       [[-1.8011e-01, -8.6046e-02, -1.4822e-01],\n",
       "                        [-2.0213e-01, -1.5194e-02, -1.8263e-01],\n",
       "                        [-1.6515e-01, -1.7702e-01,  5.2333e-02]],\n",
       "              \n",
       "                       [[-1.9536e-01, -1.2734e-01, -1.5322e-01],\n",
       "                        [-1.8252e-01, -9.8517e-02, -1.2890e-01],\n",
       "                        [-5.1843e-02, -1.0173e-01, -1.9923e-01]],\n",
       "              \n",
       "                       [[ 2.5103e-02, -1.3860e-01, -1.1540e-01],\n",
       "                        [ 6.7596e-02,  7.1194e-02, -1.5138e-01],\n",
       "                        [-7.4797e-02, -1.1977e-01,  6.3986e-02]],\n",
       "              \n",
       "                       [[-6.8585e-02, -1.3580e-01, -2.0215e-01],\n",
       "                        [-1.3368e-01, -2.1438e-01, -1.8021e-01],\n",
       "                        [-2.0370e-01, -1.4782e-01, -1.2800e-02]],\n",
       "              \n",
       "                       [[-1.2111e-01, -1.1050e-01, -6.7484e-02],\n",
       "                        [-6.3329e-02, -1.7004e-01, -2.1915e-02],\n",
       "                        [-1.1411e-02, -1.5292e-01, -1.0435e-01]],\n",
       "              \n",
       "                       [[-4.6589e-02, -1.2326e-01, -1.1009e-02],\n",
       "                        [ 6.1149e-02, -2.8772e-02, -1.7488e-01],\n",
       "                        [-1.5768e-01, -5.9618e-02, -1.2527e-01]],\n",
       "              \n",
       "                       [[-2.0684e-02, -1.1828e-01, -1.2206e-01],\n",
       "                        [-4.5647e-02,  2.1205e-02, -1.0967e-01],\n",
       "                        [ 5.3446e-02, -7.7945e-03, -1.7343e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.9363e-01, -2.2732e+00,  2.5857e+00],\n",
       "                        [ 1.3681e+00,  4.3032e-01, -1.0975e+00],\n",
       "                        [ 5.2927e-01, -1.8709e+00,  2.4787e+00]],\n",
       "              \n",
       "                       [[ 5.6274e-01,  1.5148e+00, -1.6032e-01],\n",
       "                        [-1.2698e+00,  5.5457e-01,  4.2548e+00],\n",
       "                        [-1.1232e-01,  2.9019e+00,  1.5504e+00]],\n",
       "              \n",
       "                       [[-1.3274e+00, -1.8659e+00, -2.2731e+00],\n",
       "                        [-4.0287e+00, -1.5248e+00,  9.8438e-02],\n",
       "                        [-2.0896e+00, -1.9320e-01,  1.7014e+00]],\n",
       "              \n",
       "                       [[-1.0563e-01,  2.4905e+00,  7.7796e-01],\n",
       "                        [ 2.9798e+00,  1.1528e+00,  1.0905e+00],\n",
       "                        [-1.0575e+00,  2.0347e+00,  3.8443e+00]],\n",
       "              \n",
       "                       [[-7.1601e-01,  8.7181e-01, -3.6750e+00],\n",
       "                        [-5.3317e+00, -2.0656e+00, -6.0330e-01],\n",
       "                        [-1.0119e+00,  2.2940e+00, -1.4850e+00]],\n",
       "              \n",
       "                       [[ 3.1931e+00,  2.3031e+00,  2.0163e+00],\n",
       "                        [-1.8810e+00,  3.5731e+00,  3.5120e+00],\n",
       "                        [-5.3207e+00, -1.2147e+00,  8.3031e-01]],\n",
       "              \n",
       "                       [[ 7.8815e-01, -5.2019e+00, -4.2990e+00],\n",
       "                        [-4.0346e-01, -4.2700e+00, -2.6297e+00],\n",
       "                        [ 4.9135e+00, -3.8451e-01, -1.5952e+00]],\n",
       "              \n",
       "                       [[-2.0435e-01, -3.9586e-01,  2.0861e+00],\n",
       "                        [ 3.1248e-01,  3.8551e+00,  2.0564e-01],\n",
       "                        [-3.1909e+00, -7.1842e+00, -5.2502e+00]],\n",
       "              \n",
       "                       [[-8.4361e+00, -4.0337e+00,  2.3336e+00],\n",
       "                        [ 2.0861e+00, -8.1133e+00, -1.4209e+01],\n",
       "                        [ 2.6617e+00, -1.0398e+00,  2.3610e+00]],\n",
       "              \n",
       "                       [[-3.2483e+00,  3.3503e-01,  6.2268e-01],\n",
       "                        [ 3.8157e+00,  2.6496e+00, -4.3824e+00],\n",
       "                        [-3.1876e+00,  5.9994e+00, -4.3939e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2090e+00,  1.7558e-01, -7.9589e-01],\n",
       "                        [ 1.5184e-02,  4.6671e-01,  3.2614e+00],\n",
       "                        [-1.9185e+00,  6.0297e-01,  1.8578e+00]],\n",
       "              \n",
       "                       [[ 2.1363e+00, -1.8363e-01, -4.3603e+00],\n",
       "                        [ 2.7081e+00, -2.5037e+00,  3.7107e+00],\n",
       "                        [-3.3490e+00, -2.3756e+00, -1.9722e+00]],\n",
       "              \n",
       "                       [[-4.1980e+00,  1.8264e+00,  3.4605e+00],\n",
       "                        [-5.8965e+00, -2.2813e+00,  2.6833e+00],\n",
       "                        [ 2.2788e+00, -1.0770e+00, -4.0192e+00]],\n",
       "              \n",
       "                       [[ 3.3504e+00,  1.6093e+00,  1.1608e+00],\n",
       "                        [ 3.0448e+00,  1.6933e+00,  2.1205e+00],\n",
       "                        [ 3.3544e+00, -1.4531e-01,  1.5374e+00]],\n",
       "              \n",
       "                       [[ 6.6903e+00,  5.1711e+00, -5.9203e+00],\n",
       "                        [ 5.2727e-01, -2.3228e+00,  7.4469e+00],\n",
       "                        [ 3.8348e+00, -3.3330e+00, -1.8787e+00]],\n",
       "              \n",
       "                       [[ 8.0997e-01, -2.2334e+00,  1.4601e+00],\n",
       "                        [ 2.4234e-01, -5.3667e-01, -2.4533e+00],\n",
       "                        [-4.1712e+00,  1.6794e+00,  7.7255e-01]],\n",
       "              \n",
       "                       [[-4.0453e+00,  7.8055e-01, -5.8319e+00],\n",
       "                        [-7.1897e+00, -5.4781e-02, -3.1050e+00],\n",
       "                        [-4.1098e+00, -9.8669e-01, -7.1550e+00]],\n",
       "              \n",
       "                       [[-8.2510e-01,  3.2691e+00,  2.7551e-01],\n",
       "                        [ 2.1135e+00,  6.1226e-01, -1.7952e+00],\n",
       "                        [ 6.9847e+00,  2.7877e-01, -1.4494e+00]],\n",
       "              \n",
       "                       [[ 2.1903e+00, -8.2751e-01, -4.6479e+00],\n",
       "                        [ 7.6978e+00,  2.8801e+00, -1.2882e+01],\n",
       "                        [ 6.0772e+00, -1.2202e+00, -7.4844e-01]],\n",
       "              \n",
       "                       [[-2.9189e+00, -2.5517e+00,  7.6005e+00],\n",
       "                        [ 4.1716e+00, -2.6002e+00,  5.7384e+00],\n",
       "                        [-4.7316e-01,  4.5977e+00, -1.3347e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.7363e+00,  3.9077e+00, -4.8938e+00],\n",
       "                        [ 2.7511e+00,  5.0464e-01, -4.8946e+00],\n",
       "                        [ 5.2745e+00, -1.6886e+00, -4.1665e+00]],\n",
       "              \n",
       "                       [[ 3.7126e+00,  3.0057e-01, -1.5973e+00],\n",
       "                        [ 2.3924e+00, -3.5333e+00, -8.7657e-01],\n",
       "                        [ 1.8958e+00, -2.3268e+00, -5.3708e+00]],\n",
       "              \n",
       "                       [[-5.0931e+00,  1.0480e+00, -4.3228e-01],\n",
       "                        [-1.9767e+00, -6.7711e-01,  2.4380e+00],\n",
       "                        [-1.4638e-01, -5.7114e-01,  7.6194e-01]],\n",
       "              \n",
       "                       [[ 3.1002e+00,  1.4210e+00,  3.1859e+00],\n",
       "                        [ 2.3670e+00,  1.6585e+00, -2.2598e+00],\n",
       "                        [ 3.0466e+00, -2.2989e-01, -5.2267e+00]],\n",
       "              \n",
       "                       [[ 4.1561e+00,  1.9846e+00,  6.3535e+00],\n",
       "                        [-3.8886e+00, -3.0982e+00,  4.8562e+00],\n",
       "                        [ 2.8413e-01,  2.5495e+00, -2.6686e+00]],\n",
       "              \n",
       "                       [[ 2.1803e+00, -1.0969e+00, -2.9921e+00],\n",
       "                        [ 6.9135e+00, -1.2183e+00, -3.9588e+00],\n",
       "                        [ 4.2137e+00,  4.0990e+00, -4.5900e+00]],\n",
       "              \n",
       "                       [[-1.2537e+00, -6.7462e+00, -2.5142e+00],\n",
       "                        [ 2.5004e+00, -2.9934e+00,  2.0532e+00],\n",
       "                        [ 4.9912e-02, -7.2690e-01,  2.5619e+00]],\n",
       "              \n",
       "                       [[ 2.9132e+00, -5.1142e+00, -5.9166e-01],\n",
       "                        [-6.9951e+00, -2.4514e+00, -6.1223e-01],\n",
       "                        [-1.0234e+00,  3.0210e-01,  4.7375e-01]],\n",
       "              \n",
       "                       [[ 6.9870e-01,  4.8482e+00, -4.2937e-01],\n",
       "                        [-3.6377e+00,  6.6815e-01, -1.7751e+00],\n",
       "                        [-4.6257e+00,  3.4213e+00, -3.9291e-01]],\n",
       "              \n",
       "                       [[-2.0876e+00, -7.3944e+00,  1.6659e+00],\n",
       "                        [ 4.5861e+00, -1.2329e+01, -3.2489e+00],\n",
       "                        [ 2.4689e-01,  7.0995e+00, -9.5585e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3698e+00,  5.1054e+00, -2.9892e+00],\n",
       "                        [-8.6313e+00, -1.0038e+01, -3.9807e-01],\n",
       "                        [ 2.1012e+00, -4.0617e+00, -5.3502e-01]],\n",
       "              \n",
       "                       [[-8.6257e-01, -5.0090e-01, -6.1339e+00],\n",
       "                        [ 3.0924e+00,  7.4534e+00, -1.4763e+00],\n",
       "                        [-3.2052e+00, -1.2420e+01, -8.0248e-01]],\n",
       "              \n",
       "                       [[ 1.1297e+00,  1.4538e+00, -5.7051e-01],\n",
       "                        [ 1.9025e+00,  2.0455e+00,  2.4424e+00],\n",
       "                        [ 3.6347e-01,  5.1502e-01,  2.5370e+00]],\n",
       "              \n",
       "                       [[-4.5816e+00, -2.4683e+00, -1.0136e+00],\n",
       "                        [-9.3384e-01, -8.2643e-01, -3.5272e+00],\n",
       "                        [-6.2195e-01,  5.8292e+00,  4.1587e-01]],\n",
       "              \n",
       "                       [[ 8.8325e-01,  2.0254e+00,  2.6753e+00],\n",
       "                        [-7.9676e-02,  7.1460e-01,  7.9385e-01],\n",
       "                        [-5.1852e+00,  1.1344e-02, -1.5947e+00]],\n",
       "              \n",
       "                       [[-6.9482e+00,  8.0091e-01,  5.1382e+00],\n",
       "                        [ 8.7904e-01, -1.7414e+00,  1.6156e+00],\n",
       "                        [ 5.5323e+00,  8.4790e-02, -2.0697e+00]],\n",
       "              \n",
       "                       [[ 6.5002e-01,  1.1520e-01,  6.0551e-01],\n",
       "                        [ 1.2650e+00,  1.2979e+00,  6.0201e-01],\n",
       "                        [-1.0851e+00,  3.3102e+00,  1.9830e+00]],\n",
       "              \n",
       "                       [[-6.0289e+00, -5.5604e+00,  1.8550e+00],\n",
       "                        [-6.2532e+00,  2.6186e+00,  1.7949e+00],\n",
       "                        [ 3.0399e+00, -6.4857e+00,  3.3102e+00]],\n",
       "              \n",
       "                       [[ 4.2045e-01, -1.6243e+00, -5.2094e+00],\n",
       "                        [ 5.7112e-02, -6.8005e-02, -4.8309e-02],\n",
       "                        [-2.1876e+00, -1.0482e+00,  4.0162e+00]],\n",
       "              \n",
       "                       [[-4.9958e+00, -2.4800e+00, -1.3654e+00],\n",
       "                        [-7.7713e+00,  5.7932e+00,  2.6183e+00],\n",
       "                        [-3.5635e+00, -7.5444e+00,  3.3806e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.5435e+00,  2.7182e+00,  4.7491e+00],\n",
       "                        [ 1.5050e+00, -9.1677e-01,  1.0351e-01],\n",
       "                        [-2.0805e+00, -1.1283e+00, -3.4594e-01]],\n",
       "              \n",
       "                       [[-1.1032e+00,  2.8677e+00,  6.2140e+00],\n",
       "                        [ 4.2453e+00, -2.0488e+00, -1.1662e-01],\n",
       "                        [-9.4228e-01, -1.1629e+00, -6.4177e-01]],\n",
       "              \n",
       "                       [[-1.0114e+01,  3.1972e+00, -1.2522e+01],\n",
       "                        [-7.5999e+00, -7.3731e+00, -3.5434e+00],\n",
       "                        [ 5.3938e-01, -3.2642e+00,  8.2919e+00]],\n",
       "              \n",
       "                       [[ 2.4780e+00,  4.3505e+00,  3.5569e+00],\n",
       "                        [-2.0343e+00,  1.0754e+00,  7.1602e-01],\n",
       "                        [ 1.9783e+00, -1.2948e+00, -2.8765e+00]],\n",
       "              \n",
       "                       [[-1.1847e+00,  7.6780e+00,  8.7811e+00],\n",
       "                        [ 2.4487e+00,  3.0280e+00, -2.3970e+00],\n",
       "                        [ 8.4598e-01, -1.5212e+00, -1.3914e+00]],\n",
       "              \n",
       "                       [[-2.9331e+00, -1.6787e+00,  5.2281e-02],\n",
       "                        [-2.2678e+00, -4.1296e+00,  4.9691e-01],\n",
       "                        [ 4.1483e+00,  8.3017e-01, -2.7726e+00]],\n",
       "              \n",
       "                       [[-1.2325e+00, -3.1450e+00,  2.9016e+00],\n",
       "                        [-6.8191e+00, -2.6741e+00, -7.0497e-01],\n",
       "                        [ 5.6058e-01,  1.3425e+00, -2.2840e+00]],\n",
       "              \n",
       "                       [[-2.2420e-01, -1.5253e+00,  1.4183e+00],\n",
       "                        [ 7.1568e+00,  8.7313e-01, -2.2568e+00],\n",
       "                        [ 2.5304e+00,  8.4964e+00, -2.1727e+00]],\n",
       "              \n",
       "                       [[-3.0088e+00,  7.6105e+00,  1.9060e+00],\n",
       "                        [ 5.9263e-01,  3.3560e+00,  3.0976e+00],\n",
       "                        [-8.5107e+00, -3.2526e+00, -4.7110e+00]],\n",
       "              \n",
       "                       [[-1.1170e+00,  1.0763e+01,  6.2219e+00],\n",
       "                        [-1.4444e+00,  1.2047e+00, -4.6828e+00],\n",
       "                        [-6.9292e+00, -6.6751e+00, -7.3032e-01]]]], device='cuda:0')),\n",
       "             ('conv2.bias',\n",
       "              tensor([-4.2557e-02, -3.0514e-01, -2.0360e-01, -2.1445e-01,  2.6700e-03,\n",
       "                       3.1667e+00,  3.9605e+00, -3.9731e+00,  3.4765e+00, -1.2382e-03],\n",
       "                     device='cuda:0')),\n",
       "             ('conv3.weight',\n",
       "              tensor([[[[-2.2625e-02,  5.9877e-03, -9.4217e-04],\n",
       "                        [-9.5516e-03, -9.9398e-02, -9.4853e-03],\n",
       "                        [ 4.3575e-02, -3.6334e-02, -2.7380e-02]],\n",
       "              \n",
       "                       [[ 5.2378e-02, -1.0700e-01, -5.1264e-02],\n",
       "                        [ 2.4847e-01,  2.9843e-01,  3.1439e-01],\n",
       "                        [-1.7135e-01,  1.0430e-01,  7.7543e-02]],\n",
       "              \n",
       "                       [[-6.5844e-02,  6.7981e-02, -1.7692e-01],\n",
       "                        [ 8.4533e-02,  3.4239e-01,  1.0026e-01],\n",
       "                        [ 1.3853e-02, -1.0649e-01, -1.5913e-01]],\n",
       "              \n",
       "                       [[ 8.8423e-01, -1.5456e+00,  1.5911e+00],\n",
       "                        [ 2.4134e+00, -3.8633e+00,  6.7915e-01],\n",
       "                        [-1.7666e+00,  1.9662e+00,  2.7472e-01]],\n",
       "              \n",
       "                       [[-3.6231e-02,  8.8554e-02, -5.7565e-02],\n",
       "                        [-3.3314e-02,  1.5658e-01, -1.5017e-02],\n",
       "                        [ 8.8354e-02,  1.0818e-02, -1.6345e-01]],\n",
       "              \n",
       "                       [[-4.7089e-01,  1.7492e+00, -2.0717e+00],\n",
       "                        [ 4.2269e+00,  9.9729e-01, -3.4523e-01],\n",
       "                        [-4.2658e-01,  5.7591e-01, -2.8218e+00]],\n",
       "              \n",
       "                       [[ 2.4717e+00,  1.4076e+00, -1.4478e+00],\n",
       "                        [ 3.2965e+00,  2.2344e+00,  1.9144e+00],\n",
       "                        [-3.8100e+00,  1.5336e+00,  2.0286e-01]],\n",
       "              \n",
       "                       [[-1.7842e+00, -3.3057e+00,  2.3894e+00],\n",
       "                        [-2.2721e+00,  3.4470e+00,  3.7603e+00],\n",
       "                        [ 1.6075e+00,  5.3610e+00, -1.0782e-01]],\n",
       "              \n",
       "                       [[-2.2738e+00, -1.5761e+00,  8.7239e-01],\n",
       "                        [-1.0794e+00, -8.4322e-01, -2.5125e+00],\n",
       "                        [ 1.8002e+00, -2.9997e+00,  9.8277e-01]],\n",
       "              \n",
       "                       [[ 4.2133e+00, -2.9752e+00, -4.2122e-01],\n",
       "                        [-1.5737e+00,  3.0851e+00,  2.6583e+00],\n",
       "                        [ 1.7443e+00, -4.4866e-02, -3.6190e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0661e-01,  1.0527e-01,  3.7044e-02],\n",
       "                        [ 1.0962e-01, -8.6024e-03,  2.7279e-02],\n",
       "                        [-1.5666e-02,  1.0289e-02,  1.1878e-01]],\n",
       "              \n",
       "                       [[-1.3338e-01,  2.9829e-02, -8.2338e-02],\n",
       "                        [-1.3676e-01,  5.3482e-02,  1.5606e-02],\n",
       "                        [-1.0104e-01, -1.3150e-01, -1.7532e-02]],\n",
       "              \n",
       "                       [[ 1.2031e-03,  2.3225e-02,  2.8233e-02],\n",
       "                        [-4.0933e-02,  4.9934e-03, -6.8051e-02],\n",
       "                        [-5.6424e-02, -1.1559e-01, -9.0246e-02]],\n",
       "              \n",
       "                       [[ 1.9941e-02, -1.0912e-02, -8.9608e-02],\n",
       "                        [-1.7135e-01, -8.2165e-02, -1.4968e-01],\n",
       "                        [-2.6112e-01, -2.1450e-01, -9.8790e-02]],\n",
       "              \n",
       "                       [[ 5.0907e-02, -5.6576e-02,  7.2441e-02],\n",
       "                        [ 7.4221e-02, -1.0500e-01,  1.4480e-02],\n",
       "                        [ 5.1961e-02, -8.8097e-02, -1.1276e-01]],\n",
       "              \n",
       "                       [[-2.0113e-01, -1.4975e-01, -1.0902e-02],\n",
       "                        [-2.5954e-02, -7.3836e-02, -5.1570e-02],\n",
       "                        [ 1.6059e-02,  2.5981e-02, -5.9994e-02]],\n",
       "              \n",
       "                       [[-5.5158e-02,  1.0638e-02, -5.3654e-02],\n",
       "                        [-1.1043e-01, -6.3950e-02,  1.5358e-02],\n",
       "                        [-1.9314e-02, -3.0931e-02,  2.5895e-02]],\n",
       "              \n",
       "                       [[-6.4319e-02, -3.5051e-01, -2.5224e-02],\n",
       "                        [-1.2517e-01, -5.5155e-02,  9.8464e-02],\n",
       "                        [ 2.3459e-02,  4.4975e-02,  7.1124e-02]],\n",
       "              \n",
       "                       [[-6.9052e-02,  3.0349e-02, -6.3591e-02],\n",
       "                        [-7.0903e-02, -7.0277e-02, -9.0210e-02],\n",
       "                        [-1.6840e-01, -2.6379e-01, -5.2576e-02]],\n",
       "              \n",
       "                       [[-1.0701e-01, -3.0054e-03,  7.0625e-02],\n",
       "                        [-8.0808e-03, -1.2992e-01, -1.2090e-01],\n",
       "                        [-2.4976e-02,  3.4587e-02, -1.2265e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5958e-02, -7.5787e-02, -4.9834e-02],\n",
       "                        [ 6.0426e-02,  6.2501e-02, -8.4918e-02],\n",
       "                        [-6.3011e-02,  6.8544e-02,  5.2117e-02]],\n",
       "              \n",
       "                       [[-1.3212e-02,  9.8336e-02,  3.4865e-02],\n",
       "                        [ 5.5423e-01,  2.3795e-01,  2.8476e-02],\n",
       "                        [-2.0589e-01, -1.7974e-01,  2.1520e-01]],\n",
       "              \n",
       "                       [[-7.0364e-02, -5.5346e-02,  1.7594e-03],\n",
       "                        [ 7.7627e-02,  2.0711e-02,  6.6303e-02],\n",
       "                        [-1.5322e-02, -5.9619e-03,  1.2564e-01]],\n",
       "              \n",
       "                       [[ 1.1200e+00,  2.8756e+00, -4.2075e+00],\n",
       "                        [ 2.6072e+00, -8.2816e+00, -9.0657e-01],\n",
       "                        [ 7.1918e-01, -6.1572e+00, -9.6396e-01]],\n",
       "              \n",
       "                       [[-2.1148e-01,  2.7906e-01,  2.2946e-01],\n",
       "                        [-3.5357e-02, -1.1430e-01, -1.1261e-01],\n",
       "                        [-3.6060e-02, -1.6170e-01, -4.0754e-02]],\n",
       "              \n",
       "                       [[ 9.4105e-01,  2.1434e-01, -9.6937e-03],\n",
       "                        [ 3.1302e+00, -3.1391e+00,  2.8826e+00],\n",
       "                        [-3.2955e+00,  2.5716e+00,  7.3341e-01]],\n",
       "              \n",
       "                       [[ 9.5172e-01,  3.6532e+00,  5.2292e+00],\n",
       "                        [-1.3268e+00, -7.0657e-01,  2.9501e+00],\n",
       "                        [-4.1964e+00, -1.8863e+00,  6.3932e-01]],\n",
       "              \n",
       "                       [[-4.3759e+00,  6.8761e+00,  1.5206e+00],\n",
       "                        [ 4.9307e-01,  7.5503e-01, -8.2755e-01],\n",
       "                        [-1.7432e+00,  1.8190e+00,  1.8835e+00]],\n",
       "              \n",
       "                       [[ 1.0233e+00, -2.0136e+00,  2.4534e+00],\n",
       "                        [ 3.7063e+00, -7.3578e+00,  8.1364e-01],\n",
       "                        [-2.1428e+00, -2.6183e-02, -8.2114e+00]],\n",
       "              \n",
       "                       [[-2.2269e+00, -4.0148e+00, -2.0587e+00],\n",
       "                        [-1.0642e+00,  2.5080e-01,  7.3177e+00],\n",
       "                        [-8.0727e-01,  3.6863e+00, -3.4042e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.3032e-02, -9.0961e-02, -2.1652e-02],\n",
       "                        [-1.9337e-04, -8.2655e-02,  1.0159e-01],\n",
       "                        [-1.6203e-02, -1.0183e-01, -2.1277e-02]],\n",
       "              \n",
       "                       [[ 5.1424e-02, -2.2231e-01,  1.0556e-02],\n",
       "                        [-4.0613e-02, -1.2017e-02,  1.4768e-01],\n",
       "                        [ 1.2056e-02, -4.9903e-02,  1.3576e-02]],\n",
       "              \n",
       "                       [[-1.1787e-01, -1.3126e-01, -9.8357e-03],\n",
       "                        [-1.6101e-01, -1.6601e-01, -1.0398e-01],\n",
       "                        [ 9.6549e-04, -1.2748e-01, -4.9598e-02]],\n",
       "              \n",
       "                       [[-4.4289e-01, -2.4144e-01, -3.5572e-01],\n",
       "                        [-4.0386e-01, -2.9605e-01, -2.6697e-01],\n",
       "                        [ 3.7580e-02, -1.5859e-01, -1.6598e-01]],\n",
       "              \n",
       "                       [[ 1.1814e-01, -1.1226e-02, -5.7375e-02],\n",
       "                        [-4.9328e-02, -3.5780e-02,  1.1423e-01],\n",
       "                        [ 9.8059e-02, -5.6672e-02,  4.0960e-02]],\n",
       "              \n",
       "                       [[-1.2504e-01, -3.7797e-01, -2.7542e-01],\n",
       "                        [-3.1961e-01, -2.9898e-01, -3.9998e-01],\n",
       "                        [-5.9220e-01, -3.3883e-01, -1.6246e-01]],\n",
       "              \n",
       "                       [[-1.6142e-01, -1.9613e-01, -4.3333e-01],\n",
       "                        [-2.2469e-01, -3.2460e-01, -4.5820e-01],\n",
       "                        [-5.1066e-01, -4.0905e-01, -3.4136e-01]],\n",
       "              \n",
       "                       [[-2.7548e-01, -1.6706e-01, -3.5892e-01],\n",
       "                        [-2.2787e-01, -3.0984e-01, -1.7240e-01],\n",
       "                        [-4.0698e-01, -2.5776e-01, -2.7767e-01]],\n",
       "              \n",
       "                       [[-2.6461e-01, -2.5385e-01, -1.7475e-01],\n",
       "                        [-3.7287e-01, -4.3618e-01, -2.8225e-01],\n",
       "                        [-1.9762e-02, -1.8311e-01, -1.3130e-01]],\n",
       "              \n",
       "                       [[-3.3326e-01, -3.1185e-01, -2.2673e-01],\n",
       "                        [-1.0777e-01, -2.7057e-01, -2.1405e-01],\n",
       "                        [-3.3622e-01, -2.3237e-01, -1.7128e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7690e-02,  2.8424e-02, -8.9569e-02],\n",
       "                        [-1.7506e-02,  1.4692e-02,  5.2292e-02],\n",
       "                        [ 3.5240e-02, -6.1195e-02, -1.1597e-02]],\n",
       "              \n",
       "                       [[ 1.0336e-01, -9.7091e-02,  4.0030e-02],\n",
       "                        [ 7.0278e-02, -1.7035e-02, -6.9257e-02],\n",
       "                        [ 1.8493e-02,  3.1721e-02, -2.8292e-02]],\n",
       "              \n",
       "                       [[-4.7902e-03,  2.5896e-02, -1.1883e-01],\n",
       "                        [ 6.0512e-02, -3.6067e-02,  5.5340e-02],\n",
       "                        [ 1.2051e-02,  1.2563e-03, -1.0588e-01]],\n",
       "              \n",
       "                       [[-1.2508e-01, -1.3487e-01, -2.0921e-01],\n",
       "                        [-2.1884e-01, -4.6064e-02, -2.1562e-01],\n",
       "                        [-1.9746e-01, -7.5461e-02, -1.6809e-01]],\n",
       "              \n",
       "                       [[-2.1145e-02,  1.1226e-02,  5.3982e-02],\n",
       "                        [ 3.0652e-02,  1.3154e-01,  7.0734e-02],\n",
       "                        [-1.1061e-01,  6.9352e-02, -3.9214e-02]],\n",
       "              \n",
       "                       [[-2.3564e-01, -1.0636e-01, -1.4617e-01],\n",
       "                        [-1.6568e-01, -7.9919e-02, -2.2433e-01],\n",
       "                        [-8.3228e-02, -8.1204e-02, -1.9607e-01]],\n",
       "              \n",
       "                       [[-1.1566e-01, -1.3097e-01, -2.0960e-01],\n",
       "                        [-9.4426e-02, -2.0605e-01, -2.1279e-01],\n",
       "                        [-2.1205e-01, -1.3554e-01, -1.6238e-01]],\n",
       "              \n",
       "                       [[-8.5166e-02, -5.0106e-02, -9.1304e-02],\n",
       "                        [-2.7279e-01, -8.5567e-02, -5.9275e-02],\n",
       "                        [-1.5551e-01, -1.2041e-01, -2.7247e-01]],\n",
       "              \n",
       "                       [[-6.7138e-02, -1.9165e-02, -1.9825e-01],\n",
       "                        [-5.8453e-03,  1.2040e-02, -1.4218e-01],\n",
       "                        [-8.9195e-02, -5.9888e-02, -7.4082e-02]],\n",
       "              \n",
       "                       [[-2.8464e-01, -2.7178e-01, -8.5030e-02],\n",
       "                        [-1.5636e-01, -1.2291e-01, -5.5182e-02],\n",
       "                        [-1.7166e-01, -2.3361e-01, -8.9966e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.7257e-02, -5.2191e-03, -2.2356e-02],\n",
       "                        [ 2.4942e-02,  4.0955e-02, -4.4799e-02],\n",
       "                        [ 1.5941e-03,  2.7274e-02, -6.3584e-02]],\n",
       "              \n",
       "                       [[ 3.8838e-01, -6.9353e-02, -3.6042e-02],\n",
       "                        [ 6.5691e-02,  1.1692e-01, -2.0284e-01],\n",
       "                        [-2.9589e-01,  1.0171e-01, -4.9378e-01]],\n",
       "              \n",
       "                       [[-6.7927e-02,  5.0961e-02, -1.3754e-01],\n",
       "                        [-1.4197e-01, -8.4174e-02, -6.2380e-02],\n",
       "                        [-8.1731e-02,  2.9674e-02, -1.3691e-01]],\n",
       "              \n",
       "                       [[ 7.1640e+00, -1.5310e+00,  1.5046e+00],\n",
       "                        [-2.2388e-01,  6.8641e-01,  2.7315e+00],\n",
       "                        [-6.7140e-01,  5.0713e-01,  1.4608e+00]],\n",
       "              \n",
       "                       [[-2.9321e-01, -5.6311e-02, -2.5960e-02],\n",
       "                        [ 1.9504e-01,  1.3169e-01, -1.0097e-01],\n",
       "                        [ 7.0491e-02,  2.3456e-01, -1.7599e-01]],\n",
       "              \n",
       "                       [[-2.0131e+00, -1.4605e+01, -5.1564e+00],\n",
       "                        [-3.3226e+00,  2.2762e+00, -1.4788e+00],\n",
       "                        [-9.6272e-01, -1.3674e+00, -1.4183e+00]],\n",
       "              \n",
       "                       [[ 2.6309e+00, -6.1889e+00, -5.9593e+00],\n",
       "                        [-2.0279e+00, -1.5427e+00,  7.8914e-01],\n",
       "                        [-3.6647e+00, -5.8361e+00,  5.8406e+00]],\n",
       "              \n",
       "                       [[-1.9303e+00, -9.3576e+00,  3.9957e-01],\n",
       "                        [ 4.8956e-01, -6.5600e+00, -7.0842e+00],\n",
       "                        [-1.8877e+00, -4.8300e+00,  4.2781e-01]],\n",
       "              \n",
       "                       [[ 1.2783e+00,  1.0182e+00, -1.2970e-01],\n",
       "                        [ 1.9752e+00,  3.8832e-01,  2.1390e+00],\n",
       "                        [-1.3637e+00,  3.4296e+00, -1.0120e+00]],\n",
       "              \n",
       "                       [[ 3.1915e+00,  5.1868e-02,  4.3424e+00],\n",
       "                        [ 6.5686e+00, -1.4922e+01,  4.0338e+00],\n",
       "                        [-1.0594e+01,  3.4265e-01, -1.2914e+01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.9709e-02,  3.3890e-03, -6.1094e-02],\n",
       "                        [-4.5270e-02,  5.9810e-02,  4.9939e-02],\n",
       "                        [-5.0239e-03,  8.8257e-02, -5.9590e-02]],\n",
       "              \n",
       "                       [[-3.8088e-02,  4.3601e-02, -9.9002e-02],\n",
       "                        [-5.3805e-02, -1.1165e-01, -7.7766e-02],\n",
       "                        [-3.5005e-02,  3.1230e-02, -9.1242e-02]],\n",
       "              \n",
       "                       [[-6.8229e-02,  5.6371e-02,  5.3042e-02],\n",
       "                        [-4.6589e-02,  2.3301e-02,  1.2373e-02],\n",
       "                        [-1.4544e-01, -4.5209e-02, -1.6806e-02]],\n",
       "              \n",
       "                       [[-1.5309e-01, -2.4204e-01, -1.2733e-01],\n",
       "                        [-2.0460e-01, -2.0962e-01, -1.7298e-01],\n",
       "                        [-1.8367e-01, -2.6455e-01, -1.3720e-01]],\n",
       "              \n",
       "                       [[-2.8941e-02, -6.9512e-02,  3.1043e-03],\n",
       "                        [ 3.7530e-02,  9.3764e-03, -9.6195e-02],\n",
       "                        [-5.5389e-02,  1.5085e-02, -1.1607e-01]],\n",
       "              \n",
       "                       [[-9.1810e-02, -6.6595e-02, -8.1825e-02],\n",
       "                        [-4.8426e-02, -6.7805e-02, -1.9155e-01],\n",
       "                        [-1.4693e-01, -1.3075e-01, -2.0321e-01]],\n",
       "              \n",
       "                       [[-1.4589e-01, -8.4184e-03, -6.7536e-02],\n",
       "                        [-1.6571e-01, -1.6927e-01, -2.5008e-01],\n",
       "                        [-1.6788e-01, -2.9812e-01, -1.9032e-01]],\n",
       "              \n",
       "                       [[-1.4502e-01, -8.3585e-02, -1.6415e-01],\n",
       "                        [-2.1300e-01, -6.9276e-02, -2.1241e-01],\n",
       "                        [-5.7522e-02, -2.0500e-01, -2.5707e-01]],\n",
       "              \n",
       "                       [[-1.7108e-01, -1.6012e-01, -7.9484e-02],\n",
       "                        [-8.6268e-02, -1.6826e-01, -1.8500e-01],\n",
       "                        [-7.1418e-02, -5.7684e-03, -1.8525e-01]],\n",
       "              \n",
       "                       [[-1.1715e-01, -6.8322e-02, -6.7393e-02],\n",
       "                        [-2.6193e-02,  7.6630e-03, -7.3390e-02],\n",
       "                        [-1.1809e-01, -1.3271e-02, -3.9426e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.7864e-02,  9.4850e-02, -4.8220e-03],\n",
       "                        [-1.7649e-02,  3.8555e-02, -3.5424e-02],\n",
       "                        [ 4.1039e-05,  3.5775e-02,  6.2609e-02]],\n",
       "              \n",
       "                       [[-1.2724e-01,  1.2463e-01,  2.3496e-01],\n",
       "                        [ 4.6672e-02,  5.3241e-01,  1.1014e-01],\n",
       "                        [ 3.6752e-01,  1.3560e-01,  2.0989e-01]],\n",
       "              \n",
       "                       [[ 3.4817e-02,  1.0778e-01,  9.8412e-02],\n",
       "                        [-5.4823e-02,  1.6254e-01,  1.0453e-01],\n",
       "                        [-1.0492e-01, -4.5328e-02,  4.4006e-02]],\n",
       "              \n",
       "                       [[-5.1369e-01,  3.0387e+00, -7.0035e+00],\n",
       "                        [ 3.9089e+00, -6.3299e-01, -7.1972e+00],\n",
       "                        [ 2.1576e+00,  1.4987e+00,  1.5995e+00]],\n",
       "              \n",
       "                       [[-1.1007e-01, -1.4748e-02, -2.2363e-01],\n",
       "                        [-1.3043e-02,  6.3424e-02, -1.0209e-01],\n",
       "                        [ 1.5738e-01,  4.1187e-02,  1.3341e-02]],\n",
       "              \n",
       "                       [[-5.8264e-01, -1.5797e+00, -2.8286e+00],\n",
       "                        [-1.5770e+00,  3.4748e+00,  3.2059e+00],\n",
       "                        [-3.9977e+00,  1.1217e+00, -4.0059e+00]],\n",
       "              \n",
       "                       [[ 5.0580e+00,  2.8413e+00,  3.1727e-01],\n",
       "                        [ 2.0835e+00, -8.4606e-02,  2.9518e+00],\n",
       "                        [-7.7976e+00, -2.5159e+00, -4.5433e+00]],\n",
       "              \n",
       "                       [[-4.7850e+00, -4.0202e+00,  2.9725e+00],\n",
       "                        [-9.1854e+00,  8.1211e+00, -9.4905e-03],\n",
       "                        [-4.1447e+00, -2.0534e+00, -1.8509e+00]],\n",
       "              \n",
       "                       [[ 1.2584e+00, -4.2214e+00, -1.2569e+01],\n",
       "                        [-4.5937e-01, -6.3622e+00, -5.7071e+00],\n",
       "                        [ 1.5888e+00,  4.3027e+00,  4.5275e+00]],\n",
       "              \n",
       "                       [[-5.2121e-01, -9.5253e+00, -1.3851e+00],\n",
       "                        [ 2.1259e+00,  6.5420e-01,  3.8904e+00],\n",
       "                        [ 2.5830e-02,  3.0066e+00,  3.4850e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.6200e-02,  9.0342e-02,  6.7779e-02],\n",
       "                        [ 5.4452e-02,  7.6332e-02, -2.4882e-02],\n",
       "                        [-1.1161e-01,  5.2500e-02, -9.4210e-02]],\n",
       "              \n",
       "                       [[-5.8258e-02, -1.2117e-02, -1.3048e-01],\n",
       "                        [-2.3426e-02, -1.0362e-01,  5.7756e-02],\n",
       "                        [-6.9597e-02,  5.5562e-02, -6.7781e-02]],\n",
       "              \n",
       "                       [[-7.6553e-02, -5.7904e-02, -6.5358e-02],\n",
       "                        [ 6.8109e-02, -1.1917e-01, -1.1411e-01],\n",
       "                        [-1.2389e-02, -1.2131e-01,  7.0586e-02]],\n",
       "              \n",
       "                       [[-2.7071e-01, -1.9435e-01, -1.4570e-01],\n",
       "                        [-2.5514e-01, -1.7925e-01, -2.7128e-01],\n",
       "                        [-1.2059e-01, -2.3411e-01, -2.8812e-01]],\n",
       "              \n",
       "                       [[ 1.7630e-02,  5.2094e-02,  6.6687e-02],\n",
       "                        [ 4.8699e-02, -1.0916e-01, -6.5404e-02],\n",
       "                        [-5.9404e-03,  3.3943e-02,  7.5758e-02]],\n",
       "              \n",
       "                       [[-1.0061e-01,  4.9870e-02, -2.2715e-01],\n",
       "                        [-3.3752e-01, -2.2567e-01, -9.0885e-02],\n",
       "                        [-2.9562e-01, -9.1290e-02, -2.7046e-01]],\n",
       "              \n",
       "                       [[-3.4217e-01, -2.0596e-01, -2.0829e-01],\n",
       "                        [-3.3734e-01, -2.7506e-01, -2.8175e-01],\n",
       "                        [-1.9997e-01, -1.5841e-01, -1.7630e-01]],\n",
       "              \n",
       "                       [[-4.3217e-01, -2.7462e-01, -1.1318e-01],\n",
       "                        [-3.4913e-01, -4.5565e-01, -2.4753e-01],\n",
       "                        [-4.6041e-01, -3.6346e-01, -3.4601e-01]],\n",
       "              \n",
       "                       [[-1.0592e-01, -1.5712e-01, -1.5922e-02],\n",
       "                        [-6.4312e-02, -2.1752e-01, -2.5296e-01],\n",
       "                        [-1.0286e-01, -6.8118e-02, -1.7942e-01]],\n",
       "              \n",
       "                       [[-2.8198e-01, -3.0452e-01, -3.1542e-01],\n",
       "                        [-3.0852e-01, -1.6929e-01, -1.9607e-01],\n",
       "                        [-1.4089e-01, -2.9766e-01, -3.9959e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.2910e-03, -2.8118e-03,  2.8554e-02],\n",
       "                        [ 9.2164e-02,  1.0954e-02,  9.7118e-02],\n",
       "                        [ 7.8248e-02,  1.0680e-02,  1.8351e-02]],\n",
       "              \n",
       "                       [[ 2.7027e-01,  1.4542e-01, -7.7049e-02],\n",
       "                        [ 3.3120e-01,  3.0493e-01, -8.1582e-02],\n",
       "                        [-1.8933e-02,  2.4063e-02, -3.2398e-01]],\n",
       "              \n",
       "                       [[ 1.0747e-01,  1.6669e-01,  8.5669e-03],\n",
       "                        [ 2.5334e-02, -1.7684e-02, -9.1534e-02],\n",
       "                        [-2.0782e-01, -2.4979e-01, -1.8717e-01]],\n",
       "              \n",
       "                       [[-2.3100e+00, -1.5107e+00, -1.3226e+00],\n",
       "                        [ 1.7582e+00,  1.9978e+00, -3.7109e+00],\n",
       "                        [ 5.6239e+00,  2.2740e+00, -5.5235e-01]],\n",
       "              \n",
       "                       [[ 5.6107e-02, -2.7581e-02, -2.5517e-02],\n",
       "                        [-1.6826e-01, -1.4505e-01, -6.6834e-02],\n",
       "                        [-5.7580e-02, -2.0767e-03,  8.4012e-02]],\n",
       "              \n",
       "                       [[ 6.9625e-01,  2.3458e+00, -3.6856e+00],\n",
       "                        [-1.8309e+00, -1.4438e+00,  1.1778e+00],\n",
       "                        [ 3.4291e+00,  1.7204e+00, -9.1374e+00]],\n",
       "              \n",
       "                       [[ 3.2218e+00,  4.7562e+00,  1.3603e+00],\n",
       "                        [-7.6459e-01, -1.6622e+00, -7.9921e-01],\n",
       "                        [ 1.1386e-01, -2.4881e+00, -8.5283e-01]],\n",
       "              \n",
       "                       [[ 3.6064e-01,  2.9837e+00,  7.5913e-01],\n",
       "                        [ 2.4323e+00,  9.2056e-01, -3.0308e+00],\n",
       "                        [ 8.6602e-01,  6.9552e-01, -6.7018e+00]],\n",
       "              \n",
       "                       [[ 1.7503e+00, -2.6129e+00,  1.2161e+00],\n",
       "                        [ 2.6642e+00, -9.5580e-01,  1.3002e+00],\n",
       "                        [ 5.7775e+00,  1.7737e+00, -2.6368e+00]],\n",
       "              \n",
       "                       [[-5.5175e-02,  2.1470e-01, -4.2561e-02],\n",
       "                        [-8.9884e-01,  1.3564e-01,  8.4457e-01],\n",
       "                        [-1.0118e+00, -1.5161e+00, -1.2342e+00]]]], device='cuda:0')),\n",
       "             ('conv3.bias',\n",
       "              tensor([-2.6667e+00, -1.0182e-01,  1.1039e+01, -3.0394e-01, -4.8691e-02,\n",
       "                       7.6264e+00,  6.9555e-03, -1.3640e+01, -2.5753e-01,  2.5182e+01],\n",
       "                     device='cuda:0')),\n",
       "             ('conv4.weight',\n",
       "              tensor([[[[-1.5423e-01, -3.2181e-01, -3.5281e-01],\n",
       "                        [-4.0824e-01, -4.3651e-01, -1.8002e-01],\n",
       "                        [-5.3648e-01, -4.1031e-01, -8.4450e-02]],\n",
       "              \n",
       "                       [[ 1.3251e-01, -5.3618e-02,  2.4045e-01],\n",
       "                        [-1.1481e-02,  9.2658e-02,  1.3403e-02],\n",
       "                        [ 5.1580e-02, -1.9624e-02,  1.9453e-01]],\n",
       "              \n",
       "                       [[-3.0384e-01, -4.6699e-01, -3.7390e-01],\n",
       "                        [-4.8735e-01, -5.3203e-01, -2.7357e-01],\n",
       "                        [-4.3914e-01, -3.4929e-01, -2.7561e-01]],\n",
       "              \n",
       "                       [[ 5.3011e-02,  3.9156e-02, -5.5584e-03],\n",
       "                        [-1.6571e-02,  8.9578e-02, -1.1105e-01],\n",
       "                        [ 6.7434e-02, -3.3618e-02,  5.0217e-03]],\n",
       "              \n",
       "                       [[-6.4061e-02, -5.6833e-02, -5.4302e-02],\n",
       "                        [ 9.6959e-02,  7.0487e-02,  4.7441e-02],\n",
       "                        [ 3.9651e-02, -3.7034e-03,  6.4446e-02]],\n",
       "              \n",
       "                       [[ 1.7390e-01, -4.5277e-02,  1.2478e-01],\n",
       "                        [ 2.5705e-02, -7.3550e-03,  4.5050e-02],\n",
       "                        [-4.9154e-02,  1.9494e-02,  1.0780e-01]],\n",
       "              \n",
       "                       [[-6.3421e-02,  1.0742e-01,  6.1490e-02],\n",
       "                        [ 3.7245e-02, -4.3683e-03,  6.1358e-02],\n",
       "                        [ 2.3184e-02,  3.7595e-02,  6.0875e-04]],\n",
       "              \n",
       "                       [[-5.4603e-02, -1.3446e-01, -7.7740e-02],\n",
       "                        [-2.1363e-01, -1.1977e-01,  8.3359e-03],\n",
       "                        [-6.1760e-02, -1.5121e-01,  9.4993e-02]],\n",
       "              \n",
       "                       [[ 8.9643e-02, -3.9124e-02,  3.3153e-02],\n",
       "                        [ 6.1020e-02,  1.1683e-01,  3.2880e-02],\n",
       "                        [ 3.4935e-02,  1.6271e-02,  9.5461e-02]],\n",
       "              \n",
       "                       [[ 6.8574e-02, -9.2674e-02, -1.9194e-01],\n",
       "                        [ 2.7767e-01,  1.4729e-01, -6.7719e-02],\n",
       "                        [ 4.4245e-02,  6.9976e-02,  7.6342e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4106e-03, -4.6205e-01, -4.8941e-01],\n",
       "                        [-3.4688e-01, -1.5068e+00, -6.1827e-01],\n",
       "                        [-3.0339e+00, -3.3207e+00,  1.8196e+00]],\n",
       "              \n",
       "                       [[ 1.7455e-02, -1.0948e-01, -7.7452e-02],\n",
       "                        [ 7.5866e-02, -7.3913e-02, -1.6039e-01],\n",
       "                        [ 7.1785e-02,  4.0688e-02,  8.9701e-02]],\n",
       "              \n",
       "                       [[-4.8459e+00, -1.6629e+00, -4.9721e-01],\n",
       "                        [ 3.7968e+00, -4.2743e+00,  1.1413e+00],\n",
       "                        [-4.0255e+00,  3.1503e+00,  1.7534e+00]],\n",
       "              \n",
       "                       [[-2.3728e-01, -1.7134e-01,  1.1610e-01],\n",
       "                        [-1.2515e-01, -2.9594e-01, -7.9076e-02],\n",
       "                        [-1.2269e-01, -1.2604e-01, -1.0681e-01]],\n",
       "              \n",
       "                       [[ 6.9894e-02, -9.0916e-03,  2.3885e-02],\n",
       "                        [ 1.1661e-01, -1.3755e-02, -3.4619e-02],\n",
       "                        [ 1.0404e-02, -5.5267e-02,  4.7257e-02]],\n",
       "              \n",
       "                       [[-3.8199e+00, -5.9800e+00,  1.0549e+00],\n",
       "                        [-1.9353e+00, -1.8949e+00, -3.3664e+00],\n",
       "                        [ 1.8973e+00, -4.0945e+00, -1.3657e+00]],\n",
       "              \n",
       "                       [[ 1.4745e-01, -5.9029e-02, -1.0307e-01],\n",
       "                        [ 6.2311e-02, -1.1257e-01,  6.9891e-02],\n",
       "                        [-2.0556e-02, -5.5127e-02, -1.2946e-01]],\n",
       "              \n",
       "                       [[-7.4591e+00,  1.3066e+00, -6.6050e-01],\n",
       "                        [-2.4794e+00, -5.0612e+00,  4.8851e+00],\n",
       "                        [-3.4580e+00, -7.2976e+00,  7.1865e+00]],\n",
       "              \n",
       "                       [[-2.9661e-01, -2.1036e-01,  1.2042e-01],\n",
       "                        [-2.0306e-01, -8.6761e-02,  1.2416e-01],\n",
       "                        [-2.1246e-01, -1.3898e-01, -2.9509e-02]],\n",
       "              \n",
       "                       [[-5.6002e-01,  2.2138e+00,  5.7260e-01],\n",
       "                        [ 2.8323e+00, -3.5904e+00, -5.4762e+00],\n",
       "                        [-2.3087e+00,  3.0373e+00, -2.3805e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.5808e+00,  6.5333e-01, -1.5215e+00],\n",
       "                        [-5.6037e-01, -5.1525e-01,  6.9196e-02],\n",
       "                        [-2.6153e-01, -2.1544e+00,  1.2761e-01]],\n",
       "              \n",
       "                       [[-1.8071e-01, -4.0246e-01, -2.8188e-01],\n",
       "                        [-1.7801e-02, -8.4977e-02, -4.2231e-01],\n",
       "                        [ 6.5794e-03,  7.9623e-02,  1.1460e-01]],\n",
       "              \n",
       "                       [[-2.7412e+00, -8.3125e-01,  1.0439e+00],\n",
       "                        [ 5.0274e-02,  1.7276e+00, -4.5244e+00],\n",
       "                        [-8.4419e-01, -1.5518e+00,  1.6296e+00]],\n",
       "              \n",
       "                       [[-3.2383e-01, -1.6302e-01, -8.2372e-02],\n",
       "                        [ 4.1675e-02,  3.4921e-01,  2.2727e-02],\n",
       "                        [ 3.6764e-01,  3.4961e-01,  2.8946e-01]],\n",
       "              \n",
       "                       [[-4.2681e-02,  1.5286e-01, -7.6101e-02],\n",
       "                        [ 4.8490e-02,  8.6755e-02, -7.2432e-02],\n",
       "                        [ 1.2307e-01, -9.4785e-02,  3.4194e-02]],\n",
       "              \n",
       "                       [[-3.6798e+00, -6.2568e+00, -1.0615e+00],\n",
       "                        [ 3.0567e+00, -1.3528e+00,  3.0368e+00],\n",
       "                        [-5.6154e+00,  3.2130e+00,  3.2121e-01]],\n",
       "              \n",
       "                       [[ 1.1473e-02,  9.1575e-02, -7.4830e-02],\n",
       "                        [ 9.0886e-02,  3.7783e-02, -3.1093e-02],\n",
       "                        [ 9.7589e-02,  1.2043e-01,  9.2663e-02]],\n",
       "              \n",
       "                       [[ 8.1211e-01, -3.6505e+00, -2.7642e+00],\n",
       "                        [ 6.0020e-01, -3.0702e+00,  3.7531e+00],\n",
       "                        [-5.1008e+00,  4.7495e-01, -5.7339e+00]],\n",
       "              \n",
       "                       [[ 7.0752e-02, -3.1521e-02, -1.0840e-01],\n",
       "                        [-1.8163e-02, -8.3682e-02,  6.5622e-02],\n",
       "                        [-1.0611e-01, -2.4289e-01, -2.3605e-01]],\n",
       "              \n",
       "                       [[-1.4207e+00, -4.3550e+00,  3.2241e+00],\n",
       "                        [-3.4295e+00,  2.4224e+00, -1.5369e+00],\n",
       "                        [ 1.5730e-01,  2.2378e+00,  2.9738e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.6233e+00, -4.4373e-01, -1.4773e+00],\n",
       "                        [ 5.1984e-01, -8.3099e-01,  8.0207e-01],\n",
       "                        [ 2.7403e+00,  9.6229e-01,  2.4802e-01]],\n",
       "              \n",
       "                       [[-4.9341e-02,  1.3148e-02, -6.4809e-02],\n",
       "                        [-1.3970e-02,  6.8173e-02,  5.0947e-02],\n",
       "                        [-2.0466e-01, -1.3829e-01, -2.3888e-01]],\n",
       "              \n",
       "                       [[ 1.2766e+00,  3.7808e-01, -2.9545e+00],\n",
       "                        [-4.4042e-02, -9.8244e-01, -2.8666e+00],\n",
       "                        [ 5.2381e+00,  7.3296e-01, -2.6261e+00]],\n",
       "              \n",
       "                       [[-3.5157e-01,  1.5435e-01, -2.2761e-01],\n",
       "                        [-3.2661e-01, -1.0248e-01, -3.2393e-03],\n",
       "                        [-1.2210e-01,  7.8673e-02, -1.7380e-02]],\n",
       "              \n",
       "                       [[ 4.8405e-02, -1.0549e-02, -6.8249e-03],\n",
       "                        [ 2.5729e-02,  3.4305e-02, -1.5097e-01],\n",
       "                        [-2.1257e-02, -4.0384e-02,  2.6988e-02]],\n",
       "              \n",
       "                       [[ 1.5994e+00, -2.1110e+00, -2.5022e-01],\n",
       "                        [ 3.9208e+00, -1.7511e+00, -7.2208e+00],\n",
       "                        [ 1.0671e+00,  5.3764e+00, -1.1654e-01]],\n",
       "              \n",
       "                       [[-8.2352e-02, -8.9093e-02,  3.6898e-02],\n",
       "                        [-1.5785e-01, -7.3543e-02,  9.7861e-02],\n",
       "                        [-1.0865e-01, -3.4795e-02,  2.0391e-02]],\n",
       "              \n",
       "                       [[-1.7637e+00, -5.5529e+00, -4.9389e+00],\n",
       "                        [ 1.8232e+00, -5.7700e+00,  3.7386e+00],\n",
       "                        [ 4.0214e+00,  1.6234e+00,  1.8807e+00]],\n",
       "              \n",
       "                       [[-2.9813e-01, -1.2016e-01, -1.1705e-01],\n",
       "                        [-1.2417e-01, -1.7548e-02,  3.5450e-02],\n",
       "                        [ 2.6146e-02,  1.3901e-01,  2.2926e-01]],\n",
       "              \n",
       "                       [[ 1.3254e+00,  3.0008e+00, -1.9820e+00],\n",
       "                        [ 1.0254e-01,  6.1536e+00,  4.4905e+00],\n",
       "                        [ 9.6895e-01, -4.1367e-01, -3.5406e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.8451e+00,  1.6367e+00, -1.5494e+00],\n",
       "                        [ 4.1168e+00, -4.8088e+00, -5.1129e+00],\n",
       "                        [-1.0228e+00, -5.8070e+00, -6.4905e-01]],\n",
       "              \n",
       "                       [[ 1.2194e-01,  3.3961e-02,  6.7911e-02],\n",
       "                        [-1.1239e-01, -4.7405e-02,  1.4080e-02],\n",
       "                        [-1.7234e-01, -5.4967e-02,  1.0815e-02]],\n",
       "              \n",
       "                       [[ 3.1296e+00,  3.3552e+00,  4.1731e-01],\n",
       "                        [-1.8119e+00, -2.4450e+00, -3.8505e+00],\n",
       "                        [-1.7940e+00, -1.2807e+00, -5.8621e+00]],\n",
       "              \n",
       "                       [[-2.4131e-01, -2.6104e-01, -1.5089e-01],\n",
       "                        [-4.6577e-01, -4.4031e-01, -3.7485e-01],\n",
       "                        [-3.8462e-01, -2.6351e-01, -9.6581e-02]],\n",
       "              \n",
       "                       [[-4.0400e-02,  1.5367e-01,  3.0361e-02],\n",
       "                        [-3.1809e-02,  1.6838e-01,  2.8543e-02],\n",
       "                        [-3.1816e-02,  3.5970e-02,  2.8866e-02]],\n",
       "              \n",
       "                       [[ 9.2100e-02,  2.6065e+00, -2.5210e-01],\n",
       "                        [ 1.0439e+00,  3.5676e-01, -9.9862e-02],\n",
       "                        [-1.2586e+00, -2.6144e+00, -7.7240e-01]],\n",
       "              \n",
       "                       [[-4.2292e-02, -1.3614e-01, -3.2880e-01],\n",
       "                        [-6.6506e-02, -1.0700e-01, -1.9567e-01],\n",
       "                        [-2.3243e-01, -1.0824e-01, -7.0422e-02]],\n",
       "              \n",
       "                       [[ 4.4707e+00, -9.2717e-01, -6.6345e+00],\n",
       "                        [ 2.7387e-01, -1.7490e+00,  4.9817e+00],\n",
       "                        [ 2.3223e+00, -6.6915e+00, -2.2209e+00]],\n",
       "              \n",
       "                       [[ 1.0014e-01, -2.5172e-02, -1.1837e-01],\n",
       "                        [-3.5448e-02,  2.3301e-02, -1.7630e-01],\n",
       "                        [ 9.4007e-02,  1.2507e-01, -3.3161e-02]],\n",
       "              \n",
       "                       [[-1.3638e+00,  2.0978e+00, -2.8791e+00],\n",
       "                        [ 3.2901e+00, -3.6544e+00,  3.9995e+00],\n",
       "                        [-1.3092e+00,  4.0377e+00, -1.5006e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1269e+00, -4.6577e+00, -1.3213e+00],\n",
       "                        [ 1.1904e+00, -1.3706e+00,  3.0830e-01],\n",
       "                        [ 1.8109e+00,  1.4893e+00, -3.5316e+00]],\n",
       "              \n",
       "                       [[ 1.1397e-01, -2.3897e-02, -2.0889e-01],\n",
       "                        [ 1.9576e-03,  3.0704e-01,  1.2277e-01],\n",
       "                        [ 1.9551e-01, -9.0126e-02, -8.5377e-02]],\n",
       "              \n",
       "                       [[-1.3034e+00, -4.1172e+00, -4.8462e-01],\n",
       "                        [ 1.3215e+00, -1.1406e+01,  3.8111e+00],\n",
       "                        [ 1.5655e+00,  5.1890e-01, -1.9109e-01]],\n",
       "              \n",
       "                       [[-8.0929e-02,  7.6676e-02,  1.1090e-01],\n",
       "                        [-3.0170e-01, -3.0930e-02,  1.2089e-01],\n",
       "                        [-4.8714e-01, -3.4429e-01, -8.9038e-02]],\n",
       "              \n",
       "                       [[-3.1942e-01, -9.5784e-03,  9.8148e-02],\n",
       "                        [ 7.9301e-02,  2.3265e-01,  1.8172e-02],\n",
       "                        [ 1.9609e-01,  1.1176e-03,  3.7885e-02]],\n",
       "              \n",
       "                       [[-9.8227e-01, -6.6309e-01, -5.0833e+00],\n",
       "                        [-1.8696e+00, -4.0950e-01,  1.0495e+00],\n",
       "                        [-1.2342e+00, -3.2489e-01,  3.0525e+00]],\n",
       "              \n",
       "                       [[ 8.9194e-02, -1.9165e-01,  3.9517e-02],\n",
       "                        [ 2.6197e-02, -1.2898e-02,  4.5829e-02],\n",
       "                        [-3.8602e-02,  3.1496e-02,  2.1212e-03]],\n",
       "              \n",
       "                       [[ 2.1659e+00,  2.0396e+00,  2.1657e+00],\n",
       "                        [ 5.9310e+00, -6.2078e+00, -6.3291e+00],\n",
       "                        [-5.6503e+00, -1.1038e-01,  2.0374e+00]],\n",
       "              \n",
       "                       [[ 1.0541e-01,  1.1591e-01,  7.4734e-02],\n",
       "                        [-5.0315e-02, -2.0407e-01,  7.4909e-02],\n",
       "                        [-2.3834e-01, -1.2880e-01,  1.3905e-02]],\n",
       "              \n",
       "                       [[-3.1699e+00, -4.8888e+00, -5.3374e+00],\n",
       "                        [-5.6659e+00, -1.8936e+00,  5.8542e-01],\n",
       "                        [-6.7959e+00, -1.5265e+00, -1.5104e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3068e-01,  1.7600e+00, -2.6117e+00],\n",
       "                        [-8.7980e-01, -6.8680e+00,  2.9169e+00],\n",
       "                        [-6.3179e-01, -1.9542e+00, -3.8261e+00]],\n",
       "              \n",
       "                       [[-3.8572e-01, -7.8859e-02, -6.3662e-02],\n",
       "                        [-2.2766e-02,  4.8798e-02,  4.0965e-03],\n",
       "                        [ 2.8449e-01,  2.3727e-01,  1.5293e-01]],\n",
       "              \n",
       "                       [[-2.7030e-01, -4.1344e+00,  4.1503e-01],\n",
       "                        [ 3.9932e+00, -5.5532e-01, -5.1593e+00],\n",
       "                        [ 1.9708e+00, -2.6156e-01, -2.7178e+00]],\n",
       "              \n",
       "                       [[ 1.0737e-01, -1.7104e-01, -1.2264e-02],\n",
       "                        [-2.9184e-01, -1.9411e-01, -2.1036e-01],\n",
       "                        [-2.2390e-03,  1.4381e-01, -1.2203e-01]],\n",
       "              \n",
       "                       [[-8.3626e-02, -2.4466e-02, -6.4401e-03],\n",
       "                        [ 8.2545e-02,  9.1756e-02,  1.9791e-03],\n",
       "                        [ 7.5122e-02, -2.9952e-02,  2.4279e-02]],\n",
       "              \n",
       "                       [[-2.7596e+00,  1.6129e+00, -1.2571e-01],\n",
       "                        [-4.1513e+00, -3.5408e+00,  2.6796e+00],\n",
       "                        [-1.0566e+00, -5.1206e+00, -2.0231e+00]],\n",
       "              \n",
       "                       [[ 1.1709e-01,  1.8741e-03, -6.9902e-02],\n",
       "                        [ 1.6803e-01, -2.5030e-03, -1.3875e-01],\n",
       "                        [ 2.1693e-01, -8.5157e-02,  1.6371e-02]],\n",
       "              \n",
       "                       [[-3.6286e+00,  6.9189e+00, -3.7375e+00],\n",
       "                        [-5.6839e+00, -9.6108e+00, -2.3625e+00],\n",
       "                        [-9.7065e+00, -3.1723e-01,  2.9543e+00]],\n",
       "              \n",
       "                       [[-1.0748e-01, -1.1137e-01,  7.4151e-03],\n",
       "                        [-2.6434e-02, -8.6931e-02, -4.1286e-02],\n",
       "                        [-1.1599e-01, -1.5820e-01, -1.3898e-01]],\n",
       "              \n",
       "                       [[-1.8997e+00, -2.6723e+00,  2.3396e+00],\n",
       "                        [ 2.7897e+00, -3.8282e-01, -7.3041e+00],\n",
       "                        [ 1.7392e+00,  8.0420e-01, -5.3496e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.2470e-01, -1.6816e+00,  6.3188e-02],\n",
       "                        [-1.2937e+00,  1.0148e+00, -5.2070e-01],\n",
       "                        [-7.6559e+00,  1.4590e+00, -2.4553e+00]],\n",
       "              \n",
       "                       [[-8.1466e-02, -8.0638e-03,  6.2091e-02],\n",
       "                        [-4.7322e-02,  1.0312e-01,  9.8767e-02],\n",
       "                        [ 1.0588e-01,  9.2323e-02,  1.3136e-01]],\n",
       "              \n",
       "                       [[-4.6441e+00, -1.0186e+00,  3.6159e+00],\n",
       "                        [ 2.5299e+00, -2.0480e+00, -7.4086e+00],\n",
       "                        [-1.4216e+00,  3.1169e+00, -4.7132e+00]],\n",
       "              \n",
       "                       [[ 2.0318e-01, -6.9424e-04, -1.1493e-01],\n",
       "                        [ 8.8826e-02,  3.3323e-01, -2.7536e-01],\n",
       "                        [-2.7571e-01, -2.0811e-01, -3.6397e-01]],\n",
       "              \n",
       "                       [[-2.6182e-01, -6.2448e-02, -4.8124e-02],\n",
       "                        [-3.2727e-02,  5.0499e-02,  4.0209e-02],\n",
       "                        [-5.3291e-02,  1.3872e-01,  8.7422e-02]],\n",
       "              \n",
       "                       [[-9.7272e-01, -3.9474e+00, -9.8688e-02],\n",
       "                        [ 7.8494e-01, -3.4066e+00, -4.7346e+00],\n",
       "                        [-1.0350e+00, -6.5749e+00, -8.2991e+00]],\n",
       "              \n",
       "                       [[-4.5719e-02, -2.1836e-02, -9.8323e-02],\n",
       "                        [ 5.2226e-03, -5.3581e-02,  6.3170e-02],\n",
       "                        [ 1.3258e-01, -8.5090e-02, -4.3873e-02]],\n",
       "              \n",
       "                       [[-4.7619e+00,  2.1498e+00,  1.3888e+00],\n",
       "                        [ 4.2214e+00, -4.5030e+00, -1.6210e+00],\n",
       "                        [-1.4551e+00,  1.7152e+00, -8.1506e+00]],\n",
       "              \n",
       "                       [[-2.1017e-01, -4.7041e-02,  1.7313e-01],\n",
       "                        [-9.7517e-02, -2.2582e-02,  3.2258e-01],\n",
       "                        [ 5.2284e-02,  1.5809e-01,  2.9110e-01]],\n",
       "              \n",
       "                       [[-7.3098e-01,  7.4865e-01,  1.3834e+00],\n",
       "                        [ 1.3405e+00,  3.2805e+00,  1.2519e-01],\n",
       "                        [ 1.5368e+00, -4.4935e+00, -2.1123e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.6699e+00, -5.6776e+00,  4.2284e-01],\n",
       "                        [-1.3297e+00, -5.3064e+00,  2.2633e+00],\n",
       "                        [-3.2749e+00,  9.0780e-01, -2.7361e+00]],\n",
       "              \n",
       "                       [[-1.8375e-01, -1.7729e-01, -9.9258e-02],\n",
       "                        [-2.3830e-02, -2.7338e-01, -2.0722e-01],\n",
       "                        [ 4.2799e-01,  1.6100e-01, -6.9057e-02]],\n",
       "              \n",
       "                       [[ 1.7250e-01, -2.1843e+00,  1.4058e-01],\n",
       "                        [ 4.9392e-01, -1.8029e-01, -3.6788e+00],\n",
       "                        [ 3.8315e+00,  5.9989e-01, -1.2717e+00]],\n",
       "              \n",
       "                       [[-1.6411e-01, -3.1439e-01, -1.5663e-02],\n",
       "                        [-9.4589e-03,  3.2328e-02, -4.0427e-02],\n",
       "                        [ 1.6203e-01,  1.2751e-01,  3.2140e-02]],\n",
       "              \n",
       "                       [[-2.4004e-01, -1.2196e-01,  1.5556e-01],\n",
       "                        [ 7.0901e-02,  4.4019e-02,  4.2496e-02],\n",
       "                        [ 3.8602e-02, -4.8884e-02, -2.3132e-03]],\n",
       "              \n",
       "                       [[-3.6145e+00, -3.8270e+00, -6.9850e-01],\n",
       "                        [-3.6396e-01, -3.7163e+00, -9.3431e+00],\n",
       "                        [ 1.5103e+00,  3.8295e+00, -4.2224e+00]],\n",
       "              \n",
       "                       [[-4.4791e-02,  1.2690e-03, -8.1467e-02],\n",
       "                        [ 6.0372e-02, -2.1475e-01, -1.8261e-01],\n",
       "                        [-9.0537e-02, -1.7185e-02, -7.3671e-02]],\n",
       "              \n",
       "                       [[-3.5349e+00,  2.7534e+00, -2.6282e+00],\n",
       "                        [ 1.2977e+00, -2.0266e-01, -3.0228e+00],\n",
       "                        [-3.0849e+00, -4.7334e+00,  3.0033e+00]],\n",
       "              \n",
       "                       [[-1.1753e-01, -3.1610e-01, -1.8220e-01],\n",
       "                        [-1.1531e-01, -1.7571e-01, -3.6135e-01],\n",
       "                        [-1.3105e-01, -9.5583e-03, -1.4355e-01]],\n",
       "              \n",
       "                       [[-2.3592e+00, -5.0388e+00,  2.6018e-01],\n",
       "                        [ 2.2073e+00,  3.8259e+00, -1.7317e-01],\n",
       "                        [-1.3391e-01,  2.3451e+00,  1.3781e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0028e+00, -3.3814e+00, -8.6086e-01],\n",
       "                        [ 1.2456e+00,  1.9565e+00,  2.5329e+00],\n",
       "                        [-8.8663e-01,  2.4815e+00,  1.9808e+00]],\n",
       "              \n",
       "                       [[ 1.7868e-01,  4.2610e-01,  2.1492e-01],\n",
       "                        [-1.0725e-02,  2.5564e-01,  1.9082e-01],\n",
       "                        [-1.5390e-01, -1.4932e-01,  3.4224e-01]],\n",
       "              \n",
       "                       [[-4.3650e+00, -2.5276e+00, -5.2706e+00],\n",
       "                        [ 3.4679e+00,  2.6579e+00, -2.5100e+00],\n",
       "                        [ 3.2803e+00,  6.1516e+00,  4.1767e+00]],\n",
       "              \n",
       "                       [[ 6.2176e-02,  3.8002e-02, -2.1790e-02],\n",
       "                        [ 9.7246e-04,  1.4045e-01,  8.5185e-03],\n",
       "                        [ 1.6338e-01,  9.0811e-02,  1.0812e-01]],\n",
       "              \n",
       "                       [[-5.0969e-02,  3.2923e-02,  2.4048e-02],\n",
       "                        [ 5.5141e-02,  4.8197e-02, -4.4414e-02],\n",
       "                        [ 1.0211e-01,  1.9596e-02, -2.5898e-01]],\n",
       "              \n",
       "                       [[-2.8608e+00,  1.9321e+00,  3.5080e+00],\n",
       "                        [ 1.0148e+00,  1.1149e+00,  2.6164e+00],\n",
       "                        [ 1.4871e-01,  8.3555e-01,  1.9995e+00]],\n",
       "              \n",
       "                       [[ 1.1826e-02,  7.7117e-02, -7.6100e-02],\n",
       "                        [-5.4457e-02,  1.2814e-01,  7.4098e-02],\n",
       "                        [-1.5676e-01, -1.6031e-03,  8.6223e-02]],\n",
       "              \n",
       "                       [[-6.3137e-01,  6.0210e-01,  5.6196e+00],\n",
       "                        [-1.8275e+00,  5.5335e+00,  2.0317e+00],\n",
       "                        [-8.7960e+00, -6.8457e+00, -6.0407e+00]],\n",
       "              \n",
       "                       [[-4.1271e-02, -8.0734e-02,  1.1251e-01],\n",
       "                        [-1.4927e-01, -5.7492e-02,  5.1230e-02],\n",
       "                        [ 4.0014e-02, -3.9468e-02,  8.3000e-04]],\n",
       "              \n",
       "                       [[ 7.4176e-01,  1.4423e+00, -3.2549e+00],\n",
       "                        [-3.2516e+00,  4.0104e+00,  3.5868e+00],\n",
       "                        [-3.2456e-01, -3.0547e+00, -2.0433e+00]]]], device='cuda:0')),\n",
       "             ('conv4.bias',\n",
       "              tensor([-0.0878, -0.0094,  0.0287,  0.0731,  0.0241,  0.0210, -0.0341,  0.0182,\n",
       "                      -0.0240,  0.0007], device='cuda:0')),\n",
       "             ('norm.weight',\n",
       "              tensor([0.3287, 0.3809, 0.5608, 0.3314, 0.3987, 0.4448, 0.4614, 0.5830, 0.4465,\n",
       "                      0.3165], device='cuda:0')),\n",
       "             ('norm.bias',\n",
       "              tensor([-0.6303, -0.6433, -0.7251, -0.6415, -0.5503, -0.7294, -0.6651, -0.9235,\n",
       "                      -0.7347, -0.6770], device='cuda:0')),\n",
       "             ('norm.running_mean',\n",
       "              tensor([ -5998.9580, -33554.0508, -28389.9844,   2288.7214, -20119.7441,\n",
       "                      -45089.0508, -40705.6445, -42992.5039, -30183.1250,  13635.0342],\n",
       "                     device='cuda:0')),\n",
       "             ('norm.running_var',\n",
       "              tensor([5.8566e+07, 4.1889e+08, 3.2818e+08, 2.6067e+08, 9.1264e+08, 5.5974e+08,\n",
       "                      4.7965e+08, 5.6435e+08, 3.5574e+08, 3.4808e+08], device='cuda:0')),\n",
       "             ('norm.num_batches_tracked', tensor(352500, device='cuda:0')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 2.0842e-02,  1.3905e-01, -2.0399e-01,  ..., -3.9997e-01,\n",
       "                        1.7362e+00, -4.2040e+00],\n",
       "                      [-2.2512e-01, -8.6565e-03, -1.9865e-02,  ...,  2.7069e+00,\n",
       "                        1.4183e-01, -7.5017e-01],\n",
       "                      [ 8.1348e-02,  7.8729e-02, -1.5432e-01,  ..., -2.0594e+00,\n",
       "                        9.5511e-01, -1.5343e+00],\n",
       "                      ...,\n",
       "                      [-1.3761e-01, -2.3141e-01, -1.5746e-04,  ...,  1.1639e+00,\n",
       "                        9.1536e-02,  7.4373e-01],\n",
       "                      [ 2.4964e-01,  2.1971e-01,  2.4342e-01,  ..., -2.0007e+00,\n",
       "                       -1.3558e+00,  1.4305e+00],\n",
       "                      [ 1.2691e-02, -1.3892e-02, -6.4551e-02,  ..., -3.4558e-02,\n",
       "                        1.8687e-02,  5.3377e-02]], device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.1166,  0.7025, -0.6845, -0.1947,  1.0243,  1.6210, -0.4670, -0.1557,\n",
       "                      -0.3602,  1.5287,  1.2117, -0.2503, -0.2239, -0.2404,  0.9140,  0.6077,\n",
       "                       0.5877,  0.4240, -0.2891, -0.2004, -0.2815, -0.2486,  0.9756, -0.0622,\n",
       "                       0.0229, -0.6077, -0.2837, -0.2584, -0.1913,  0.8062,  0.9224,  0.5421,\n",
       "                      -0.5673,  0.9993, -0.0249, -0.1547,  0.7140, -0.1499,  1.2998, -0.2765,\n",
       "                       0.2246, -0.1920, -0.0630, -0.1558,  0.0510,  0.3561, -0.3104,  0.5170,\n",
       "                       2.1133, -0.3524], device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-2.2111e-01, -9.4716e-01,  6.5428e-02, -1.0568e-01, -4.2276e-01,\n",
       "                        1.2321e-01, -5.7135e-01, -1.0809e-01, -1.2386e-01, -3.3699e-01,\n",
       "                        2.8656e-01, -8.1185e-02, -1.0670e-01, -4.4815e-02,  9.1222e-02,\n",
       "                       -9.8504e-01,  2.4307e-01, -3.6185e-01,  5.3307e-02,  7.6829e-02,\n",
       "                       -2.5628e-01, -4.4300e-01, -3.2663e-01,  5.2079e-02, -1.9951e-01,\n",
       "                       -1.4951e-01, -4.4953e-01, -4.7210e-04, -5.7934e-02, -1.6752e-01,\n",
       "                       -4.3891e-01, -2.1149e-01, -2.6528e-01, -3.7226e-01, -4.5419e-01,\n",
       "                        1.4778e-01, -2.6671e-01, -1.6066e-04,  2.3058e-01, -4.7981e-02,\n",
       "                       -2.1908e-01, -4.3612e-02, -6.8773e-02, -9.7873e-02, -3.2612e-01,\n",
       "                        1.2454e-01, -2.0635e-01, -4.7633e-02, -1.6364e-01, -1.1931e-01],\n",
       "                      [-5.5496e-01, -9.3653e-01, -6.0386e-02,  9.2964e-02, -3.4173e-01,\n",
       "                       -8.5673e-02, -4.8356e-01,  9.7071e-02, -1.6069e-02, -3.3270e-01,\n",
       "                       -1.9253e-01, -8.7706e-02,  7.6608e-03,  1.0807e-01, -7.2870e-01,\n",
       "                       -9.8459e-01, -3.6437e-01,  6.0227e-01, -1.3970e-01, -6.0850e-02,\n",
       "                        7.8354e-03, -8.2190e-02,  5.0052e-01, -1.5922e-01, -2.1750e-01,\n",
       "                       -3.4506e-01, -6.1429e-01,  7.9752e-02,  8.3804e-02, -6.7627e-01,\n",
       "                        3.3183e-01,  3.8991e-01,  1.0744e-01,  4.0929e-01, -7.2063e-01,\n",
       "                       -5.7338e-01, -6.5206e-01, -1.5862e-01, -3.0821e-01, -1.1176e-01,\n",
       "                       -2.2713e-01, -8.9100e-02,  1.0202e-01,  5.5276e-02, -2.6414e-01,\n",
       "                       -2.8720e-01, -2.1973e-01, -6.4602e-01, -3.1637e-01, -1.6658e-01],\n",
       "                      [ 1.0098e-01, -9.0467e-01,  1.0866e-01, -1.2955e-01, -2.9166e-01,\n",
       "                       -1.4305e-01, -6.3242e-01,  2.0524e-03,  9.0699e-03, -2.8985e-01,\n",
       "                       -2.9733e-01,  2.6549e-03, -7.1297e-02,  9.6525e-02, -1.3172e-01,\n",
       "                       -8.9572e-01, -2.4898e-02,  1.1610e-01,  2.8156e-02, -9.7514e-03,\n",
       "                       -2.2315e-01, -9.3711e-02, -2.1184e-01, -1.2709e-01, -3.0880e-01,\n",
       "                       -1.7759e-01, -3.9254e-01, -1.5856e-01, -4.3806e-02, -4.6153e-01,\n",
       "                       -1.6563e-01, -1.6830e-01,  1.9352e-01, -3.4019e-01, -3.4361e-01,\n",
       "                       -1.1466e-01, -4.4376e-01, -7.2368e-03, -2.6342e-01,  8.0430e-02,\n",
       "                        6.4664e-02, -1.6289e-01, -2.9299e-02,  8.6007e-02, -3.5540e-01,\n",
       "                       -2.2156e-02, -4.3758e-01, -3.3934e-02,  2.7914e-01, -5.6683e-02],\n",
       "                      [-1.2521e-01, -8.4533e-01,  4.1728e-03,  5.5014e-02,  1.4323e-01,\n",
       "                       -3.5063e-01, -1.8417e-01, -2.2114e-03, -1.6575e-01, -3.9472e-01,\n",
       "                        3.4013e-02, -1.3940e-01, -1.3386e-01,  2.1077e-02, -7.1768e-02,\n",
       "                       -6.5176e-01, -1.0893e-01, -1.8322e-01, -8.1442e-02, -3.5589e-02,\n",
       "                       -1.1752e-01, -3.6273e-02, -1.7884e-01,  8.6535e-02, -1.5539e-01,\n",
       "                        2.1879e-01, -4.0873e-01, -2.3025e-01,  9.5692e-02, -4.0672e-01,\n",
       "                        3.1127e-02, -2.9272e-01, -2.3329e-01, -5.4528e-01, -4.3177e-01,\n",
       "                       -3.8547e-01,  5.8531e-02, -3.9509e-02, -3.1781e-01,  1.2259e-01,\n",
       "                        8.4061e-02, -1.9275e-01, -5.1718e-02,  8.4296e-02, -1.8248e-01,\n",
       "                       -1.7593e-01, -8.6024e-02, -8.1586e-02, -1.2566e-01,  4.6376e-02],\n",
       "                      [-2.0203e-01, -2.9621e-01, -1.9244e-01, -5.7614e-02, -2.3008e-01,\n",
       "                       -1.4925e-01, -6.8874e-01,  6.8042e-03,  1.0423e-01, -1.0862e-01,\n",
       "                       -2.9788e-01, -2.5502e-02, -5.3397e-02, -2.3816e-02,  1.0757e-01,\n",
       "                       -8.0455e-01, -2.2604e-01, -2.6115e-02, -1.8957e-02,  1.7808e-02,\n",
       "                       -4.6636e-02, -2.5080e-01, -1.9977e-01,  6.5564e-02,  2.3520e-01,\n",
       "                       -1.4538e-01, -2.7583e-01, -1.9632e-01, -1.6522e-02, -4.3247e-01,\n",
       "                       -3.5891e-01,  1.6131e-01, -2.5525e-01,  3.7005e-02, -3.5188e-01,\n",
       "                       -2.0402e-01, -3.2298e-01, -7.0559e-02, -2.8443e-01, -1.3141e-02,\n",
       "                        1.0252e-01, -1.7491e-01, -9.3723e-02,  8.1113e-02, -2.0361e-01,\n",
       "                        1.7588e-01, -2.2158e-03, -3.4231e-02, -1.9555e-01,  1.0517e-02],\n",
       "                      [-6.6424e-02, -5.5755e-02, -2.7480e-01, -8.8182e-02, -3.1166e-01,\n",
       "                       -3.5263e-01, -6.8473e-01, -1.5868e-01, -7.4615e-02, -3.2334e-01,\n",
       "                       -9.9847e-02, -9.0529e-02, -1.1105e-01,  1.6504e-02,  9.8953e-02,\n",
       "                       -7.7820e-01, -1.5058e-01, -2.1584e-01, -3.4413e-02,  4.5080e-02,\n",
       "                       -1.7072e-01, -2.7108e-01,  1.8127e-01, -9.2482e-02, -1.9781e-01,\n",
       "                       -2.6009e-01, -3.8206e-01,  1.1724e-02, -8.6366e-02, -5.2478e-01,\n",
       "                       -7.7229e-02, -5.0447e-01,  1.7944e-01, -4.5358e-01, -4.9849e-01,\n",
       "                        1.1929e-01, -3.4297e-01, -4.3075e-02, -2.8299e-01, -4.8018e-02,\n",
       "                       -2.5123e-01,  9.6683e-02, -1.1978e-01, -6.5936e-02,  3.0011e-01,\n",
       "                        1.4405e-02, -2.5876e-01, -1.2241e-01, -8.6403e-02,  9.4028e-02],\n",
       "                      [ 1.5794e-01, -8.1090e-01, -2.4900e-01,  1.1727e-01, -3.0235e-01,\n",
       "                       -3.6331e-01, -6.8774e-01,  9.9962e-02, -2.2783e-01,  1.7991e-01,\n",
       "                       -1.2196e-01,  4.0930e-02, -2.0366e-01, -6.5891e-02, -1.8369e-01,\n",
       "                        6.6777e-02, -2.3038e-01, -2.1930e-01,  7.5600e-02,  1.2371e-01,\n",
       "                       -1.9409e-02, -5.1517e-02, -2.6039e-01, -1.0780e-01, -1.3709e-01,\n",
       "                       -9.4529e-02, -8.4650e-03, -4.7877e-02, -9.3177e-02,  2.1286e-01,\n",
       "                       -3.7288e-01, -2.8189e-01, -2.5217e-01, -5.0847e-01,  1.4284e-01,\n",
       "                       -2.5050e-01, -3.4299e-01, -9.9257e-02, -2.9633e-01, -8.3439e-02,\n",
       "                       -8.9645e-02, -1.6821e-01, -1.0721e-01,  1.0480e-01, -2.3607e-01,\n",
       "                       -2.2315e-01, -1.7703e-01,  2.1465e-01, -7.7760e-02,  8.8890e-02]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.2289, -1.5394, -0.0967,  0.7841, -0.0796, -0.1289,  0.1781],\n",
       "                     device='cuda:0')),\n",
       "             ('localization.0.weight',\n",
       "              tensor([[[[-0.0538, -0.0505,  0.0888,  0.0398, -0.0170,  0.0382, -0.1440],\n",
       "                        [-0.0687, -0.0154,  0.0942, -0.0915, -0.0387,  0.0658,  0.0150],\n",
       "                        [ 0.1412,  0.0105, -0.1105,  0.0071, -0.0355, -0.0541, -0.0110],\n",
       "                        [ 0.0353,  0.1114,  0.0166, -0.1172, -0.0348,  0.1255, -0.0647],\n",
       "                        [ 0.1015,  0.0167, -0.0482, -0.0373, -0.1108,  0.1021,  0.1209],\n",
       "                        [-0.0251,  0.1005, -0.0711,  0.1129, -0.0995,  0.0167,  0.1198],\n",
       "                        [ 0.0929, -0.1082,  0.0076, -0.0552,  0.0787, -0.0823, -0.0159]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0472,  0.0981,  0.0841, -0.0841, -0.0199,  0.0688,  0.0712],\n",
       "                        [-0.0134, -0.0123,  0.1161,  0.0057,  0.0893, -0.1261, -0.0993],\n",
       "                        [ 0.0583, -0.0974,  0.1003, -0.0514,  0.0085,  0.0041,  0.0929],\n",
       "                        [-0.1316, -0.1020, -0.1067, -0.1148, -0.1020, -0.1335,  0.0185],\n",
       "                        [-0.0782, -0.0417,  0.0816,  0.1297,  0.0411,  0.1099,  0.0747],\n",
       "                        [-0.0826,  0.0098, -0.0661, -0.0743, -0.1154, -0.1195, -0.1666],\n",
       "                        [-0.1214, -0.0665, -0.1326, -0.1322,  0.0021, -0.1041, -0.0902]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0386,  0.0109, -0.1118,  0.1184, -0.1319, -0.0967, -0.1127],\n",
       "                        [-0.0388,  0.0141, -0.0640, -0.0186, -0.0201,  0.0667, -0.0058],\n",
       "                        [ 0.1320,  0.0098, -0.1306,  0.1240,  0.0223, -0.1188, -0.0809],\n",
       "                        [ 0.1399, -0.0493,  0.0275,  0.0916,  0.0113, -0.0594,  0.0832],\n",
       "                        [-0.0232, -0.0549,  0.0264, -0.1432,  0.0834,  0.0733, -0.1055],\n",
       "                        [-0.0530,  0.0132, -0.1350,  0.0756, -0.1352, -0.0495,  0.0890],\n",
       "                        [ 0.1329,  0.0882, -0.0790, -0.0485,  0.0749,  0.0438, -0.0883]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1135, -0.0553, -0.0563,  0.0658,  0.0315, -0.0155, -0.0784],\n",
       "                        [-0.0269,  0.0885,  0.1226, -0.0729, -0.1222,  0.0324, -0.1013],\n",
       "                        [ 0.0244,  0.1256,  0.1271, -0.0990, -0.1198,  0.1192,  0.1017],\n",
       "                        [-0.1152,  0.1314, -0.0363, -0.0680,  0.0372, -0.0920,  0.0773],\n",
       "                        [ 0.0468, -0.0691, -0.0696,  0.0014, -0.0283, -0.0719,  0.0285],\n",
       "                        [ 0.0673,  0.0449, -0.0120, -0.1191, -0.0536,  0.0128, -0.0920],\n",
       "                        [ 0.1195, -0.0348, -0.0724,  0.0371, -0.0887,  0.0787,  0.0392]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1039, -0.1092,  0.0481,  0.1222,  0.1339, -0.0709, -0.0552],\n",
       "                        [-0.0064, -0.0499,  0.1196,  0.0696,  0.0331, -0.0723, -0.0024],\n",
       "                        [ 0.1187,  0.1239,  0.1086, -0.1187, -0.0982,  0.1068, -0.0334],\n",
       "                        [ 0.0340,  0.1096, -0.0240,  0.0607,  0.0035, -0.1097, -0.0195],\n",
       "                        [-0.0322,  0.1441,  0.0947, -0.0471, -0.1238,  0.1257, -0.0077],\n",
       "                        [-0.0397, -0.1178, -0.0273, -0.1201, -0.0942, -0.0790,  0.1515],\n",
       "                        [-0.1289,  0.0387,  0.0034,  0.0342,  0.0786, -0.0471, -0.0120]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0704, -0.0203,  0.1176, -0.0711,  0.0778, -0.1112,  0.0401],\n",
       "                        [ 0.1136, -0.0391,  0.0264,  0.0369, -0.0629, -0.0349,  0.0388],\n",
       "                        [-0.0494, -0.0760, -0.0526, -0.0989, -0.1234, -0.0681, -0.0561],\n",
       "                        [-0.0814, -0.0821, -0.0087,  0.0987, -0.0185, -0.1338, -0.0445],\n",
       "                        [ 0.0960,  0.1005,  0.0869,  0.1538,  0.0137,  0.0875,  0.0117],\n",
       "                        [-0.0408,  0.0573, -0.1054,  0.1088, -0.0368,  0.1454,  0.0648],\n",
       "                        [-0.0221,  0.0944,  0.1048, -0.1272, -0.0390, -0.1260, -0.1043]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0811, -0.0210,  0.1192,  0.0506, -0.0132, -0.0910, -0.0364],\n",
       "                        [ 0.1278, -0.0443,  0.0458,  0.1257, -0.1202,  0.0647, -0.0485],\n",
       "                        [ 0.1108,  0.0907, -0.0796, -0.0219, -0.1056,  0.0971, -0.0862],\n",
       "                        [-0.0686,  0.0316, -0.0981, -0.0822, -0.0479,  0.0974, -0.0518],\n",
       "                        [ 0.0985, -0.0234,  0.1683, -0.0415, -0.0841, -0.0509,  0.0323],\n",
       "                        [-0.0084, -0.0353,  0.0057, -0.0030, -0.1016,  0.1279,  0.0837],\n",
       "                        [ 0.1138,  0.0720, -0.0363,  0.1249,  0.0129, -0.0036, -0.0912]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0622,  0.0514, -0.0609,  0.0979,  0.0956,  0.0688, -0.0980],\n",
       "                        [ 0.0792,  0.1129, -0.1161, -0.0569, -0.0844, -0.0120,  0.0981],\n",
       "                        [-0.0008,  0.1185,  0.0602,  0.0054, -0.1304,  0.0943, -0.1537],\n",
       "                        [ 0.0792, -0.0099, -0.1201,  0.0988, -0.1355, -0.1361,  0.0384],\n",
       "                        [-0.1477,  0.0753,  0.0313,  0.0239,  0.0448,  0.0754, -0.1355],\n",
       "                        [ 0.0755,  0.0463,  0.0202, -0.1064, -0.0892,  0.0097, -0.1127],\n",
       "                        [ 0.0803, -0.1143,  0.0984,  0.0541, -0.0151, -0.0209, -0.1041]]]],\n",
       "                     device='cuda:0')),\n",
       "             ('localization.0.bias',\n",
       "              tensor([-0.1543, -0.0950, -0.0554,  0.0312, -0.1412, -0.1106, -0.1162, -0.0298],\n",
       "                     device='cuda:0')),\n",
       "             ('localization.3.weight',\n",
       "              tensor([[[[-0.0814, -0.0161, -0.0928, -0.0760,  0.0437],\n",
       "                        [-0.0655,  0.0031,  0.0273, -0.0650, -0.0706],\n",
       "                        [-0.0114, -0.0961, -0.0671,  0.0277,  0.0352],\n",
       "                        [-0.0252,  0.0282, -0.0444,  0.0473, -0.0242],\n",
       "                        [-0.0241, -0.0815,  0.0239,  0.0020, -0.0117]],\n",
       "              \n",
       "                       [[-0.0705, -0.0542, -0.0006, -0.0556,  0.0100],\n",
       "                        [-0.0294, -0.0969, -0.0015, -0.0568, -0.0681],\n",
       "                        [-0.0837, -0.0609, -0.0978,  0.0428,  0.0004],\n",
       "                        [-0.0596, -0.0649, -0.0754, -0.0904, -0.0509],\n",
       "                        [ 0.0274, -0.0682, -0.0846,  0.0021,  0.0337]],\n",
       "              \n",
       "                       [[ 0.0394, -0.0732, -0.0859, -0.0572, -0.0417],\n",
       "                        [-0.0119, -0.0009, -0.0032,  0.0311, -0.0814],\n",
       "                        [-0.0466, -0.0107, -0.0756, -0.0695, -0.0317],\n",
       "                        [-0.0711,  0.0488, -0.0352,  0.0283, -0.0177],\n",
       "                        [-0.0113, -0.0708, -0.0133, -0.0186, -0.0721]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0445,  0.0193, -0.0877, -0.0753, -0.0745],\n",
       "                        [-0.0496, -0.0712, -0.0134, -0.0015, -0.0465],\n",
       "                        [-0.0062, -0.0960,  0.0143, -0.0778, -0.0362],\n",
       "                        [-0.0878, -0.0398, -0.0237, -0.0177,  0.0384],\n",
       "                        [-0.0433, -0.0501,  0.0130,  0.0468, -0.0004]],\n",
       "              \n",
       "                       [[-0.0690, -0.0942, -0.0630, -0.0018,  0.0307],\n",
       "                        [-0.0871,  0.0439, -0.0313,  0.0470,  0.0499],\n",
       "                        [-0.0706, -0.0081, -0.0330,  0.0007, -0.0329],\n",
       "                        [-0.0012,  0.0189, -0.0519, -0.0283,  0.0557],\n",
       "                        [-0.0051, -0.0951, -0.0794, -0.0012,  0.0561]],\n",
       "              \n",
       "                       [[-0.0650,  0.0486,  0.0138,  0.0277, -0.0639],\n",
       "                        [ 0.0508, -0.0095, -0.0090, -0.0570,  0.0236],\n",
       "                        [ 0.0457,  0.0369,  0.0573, -0.0247,  0.0206],\n",
       "                        [ 0.0234,  0.0413, -0.0661,  0.0133, -0.0510],\n",
       "                        [-0.0371, -0.0753, -0.0092, -0.0445, -0.0715]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0123, -0.0002,  0.0050,  0.0339, -0.0495],\n",
       "                        [-0.0907, -0.0695,  0.0174,  0.0425, -0.0652],\n",
       "                        [-0.0068,  0.0324, -0.0374, -0.0101, -0.0370],\n",
       "                        [ 0.0473, -0.0274, -0.0655, -0.0275, -0.0284],\n",
       "                        [-0.0227,  0.0301, -0.0937, -0.0035,  0.0303]],\n",
       "              \n",
       "                       [[-0.0599, -0.0319, -0.0003,  0.0336, -0.0626],\n",
       "                        [-0.0497, -0.0033, -0.0672,  0.0240,  0.0549],\n",
       "                        [ 0.0243, -0.0704,  0.0188, -0.0161,  0.0517],\n",
       "                        [ 0.0087,  0.0421, -0.0825,  0.0576, -0.0420],\n",
       "                        [-0.0815, -0.0281, -0.0249,  0.0410, -0.0061]],\n",
       "              \n",
       "                       [[-0.0410, -0.0230,  0.0618,  0.0556,  0.0380],\n",
       "                        [-0.0141,  0.0042, -0.0118, -0.0734,  0.0080],\n",
       "                        [-0.0191, -0.0803, -0.0374, -0.0643, -0.0832],\n",
       "                        [-0.0829, -0.0601,  0.0300, -0.0363, -0.0950],\n",
       "                        [ 0.0382,  0.0072,  0.0210, -0.0433,  0.0546]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0604,  0.0098, -0.0462,  0.0032,  0.0197],\n",
       "                        [ 0.0226, -0.0881,  0.0052, -0.0066,  0.0092],\n",
       "                        [ 0.0458,  0.0216, -0.0023,  0.0388, -0.0208],\n",
       "                        [ 0.0078, -0.0460, -0.0275, -0.0113, -0.0106],\n",
       "                        [ 0.0502, -0.0262, -0.0570,  0.0205, -0.0453]],\n",
       "              \n",
       "                       [[-0.0798, -0.0528,  0.0262, -0.0619,  0.0132],\n",
       "                        [ 0.0327, -0.0537, -0.0891, -0.0517, -0.0054],\n",
       "                        [-0.0083, -0.0210,  0.0479, -0.0632, -0.0403],\n",
       "                        [-0.0206, -0.0545, -0.0299, -0.0748, -0.0644],\n",
       "                        [ 0.0418, -0.0570, -0.0259, -0.0345, -0.0685]],\n",
       "              \n",
       "                       [[ 0.0221, -0.0316,  0.0410, -0.0575,  0.0054],\n",
       "                        [-0.0469, -0.0452, -0.0742, -0.0585,  0.0298],\n",
       "                        [ 0.0500, -0.0568, -0.0307,  0.0038, -0.0761],\n",
       "                        [-0.0607, -0.0086,  0.0044, -0.0485,  0.0449],\n",
       "                        [ 0.0220,  0.0141, -0.0247,  0.0442, -0.0226]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0394, -0.0172,  0.0048, -0.0399, -0.0013],\n",
       "                        [ 0.0010, -0.0139,  0.0094, -0.0013,  0.0300],\n",
       "                        [-0.0133, -0.0874, -0.0090, -0.0868, -0.0584],\n",
       "                        [-0.0245, -0.0972, -0.0003, -0.0401,  0.0150],\n",
       "                        [ 0.0053, -0.0021, -0.0710,  0.0313, -0.0337]],\n",
       "              \n",
       "                       [[ 0.0303, -0.0389,  0.0008,  0.0178, -0.0116],\n",
       "                        [-0.0067, -0.0191,  0.0042, -0.0696, -0.1039],\n",
       "                        [-0.0429, -0.0363, -0.1407, -0.1093, -0.0368],\n",
       "                        [-0.0252, -0.1426, -0.0608, -0.0535, -0.0247],\n",
       "                        [-0.1061, -0.0877, -0.0350, -0.1519, -0.0191]],\n",
       "              \n",
       "                       [[-0.0855, -0.0420, -0.0498, -0.0328, -0.0951],\n",
       "                        [ 0.0221, -0.0700, -0.0752,  0.0133, -0.0670],\n",
       "                        [-0.0778, -0.0305, -0.0619, -0.0778, -0.1015],\n",
       "                        [ 0.0463, -0.0856, -0.0350, -0.0728, -0.0353],\n",
       "                        [ 0.0406, -0.0898,  0.0305, -0.0326,  0.0379]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0134,  0.0031, -0.0813, -0.0416, -0.0157],\n",
       "                        [ 0.0304, -0.0237, -0.0622,  0.0228,  0.0269],\n",
       "                        [-0.0065,  0.0346,  0.0010, -0.0627, -0.0567],\n",
       "                        [ 0.0067,  0.0279,  0.0160, -0.0533,  0.0262],\n",
       "                        [-0.0876, -0.0240, -0.0646, -0.0765,  0.0062]],\n",
       "              \n",
       "                       [[ 0.0330, -0.0537, -0.0468,  0.0230, -0.0146],\n",
       "                        [-0.0368, -0.0284,  0.0235,  0.0382, -0.0655],\n",
       "                        [ 0.0205, -0.0381, -0.0622,  0.0275,  0.0488],\n",
       "                        [-0.0489, -0.0252, -0.0576, -0.0137, -0.0542],\n",
       "                        [-0.0670, -0.0534, -0.0547,  0.0282, -0.0586]],\n",
       "              \n",
       "                       [[ 0.0318,  0.0186,  0.0486, -0.0569, -0.0071],\n",
       "                        [ 0.0483, -0.0237, -0.0027, -0.0947,  0.0451],\n",
       "                        [-0.0396,  0.0299,  0.0085, -0.0448,  0.0025],\n",
       "                        [ 0.0382, -0.0074, -0.0362, -0.0351, -0.0470],\n",
       "                        [-0.0154, -0.0791, -0.0752,  0.0086, -0.0886]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0181, -0.0058, -0.0284,  0.0117, -0.0471],\n",
       "                        [-0.0744, -0.0877, -0.0548, -0.0922, -0.0170],\n",
       "                        [-0.0502, -0.0621, -0.0004, -0.0596, -0.0784],\n",
       "                        [-0.0376, -0.0767,  0.0016, -0.0756, -0.0775],\n",
       "                        [-0.0623,  0.0378, -0.0383, -0.0361, -0.0296]],\n",
       "              \n",
       "                       [[-0.0047, -0.0928, -0.0110, -0.0881, -0.0077],\n",
       "                        [-0.0825, -0.0250, -0.0544, -0.0078, -0.0195],\n",
       "                        [ 0.0196, -0.0233,  0.0368,  0.0122, -0.0888],\n",
       "                        [ 0.0395, -0.0017, -0.0613,  0.0099, -0.0043],\n",
       "                        [-0.0012, -0.0320, -0.0206, -0.0044,  0.0339]],\n",
       "              \n",
       "                       [[ 0.0143, -0.0400,  0.0339, -0.0847, -0.0779],\n",
       "                        [ 0.0105, -0.0252,  0.0126, -0.0217,  0.0104],\n",
       "                        [-0.0873, -0.0630, -0.0153,  0.0060, -0.0316],\n",
       "                        [-0.0539, -0.0530, -0.0598, -0.0741, -0.0901],\n",
       "                        [ 0.0178, -0.0652, -0.0462, -0.0582, -0.1028]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0583, -0.0057, -0.0240, -0.0640, -0.0165],\n",
       "                        [ 0.0482, -0.0960,  0.0050, -0.0241, -0.0819],\n",
       "                        [ 0.0405,  0.0110,  0.0058, -0.0179, -0.0933],\n",
       "                        [-0.0640, -0.0745, -0.0430, -0.0945, -0.0128],\n",
       "                        [ 0.0239, -0.0163, -0.0183, -0.0876, -0.0307]],\n",
       "              \n",
       "                       [[-0.0903,  0.0332, -0.0914, -0.0590, -0.0775],\n",
       "                        [ 0.0352,  0.0205, -0.0635, -0.0092, -0.0329],\n",
       "                        [-0.0735,  0.0378, -0.0288, -0.0705,  0.0296],\n",
       "                        [-0.0343, -0.0209, -0.0226, -0.0429, -0.0052],\n",
       "                        [ 0.0324,  0.0122,  0.0204, -0.0197, -0.0493]],\n",
       "              \n",
       "                       [[-0.0009,  0.0359, -0.0844, -0.0835,  0.0255],\n",
       "                        [-0.0261,  0.0417,  0.0005,  0.0190, -0.0721],\n",
       "                        [ 0.0408, -0.0903, -0.0460,  0.0213, -0.0664],\n",
       "                        [-0.0828, -0.0773, -0.0690, -0.0429, -0.0356],\n",
       "                        [ 0.0118, -0.0798, -0.0786,  0.0409, -0.0800]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0200,  0.0197,  0.0361,  0.0485, -0.0687],\n",
       "                        [-0.0054, -0.0455,  0.0183, -0.0524, -0.0377],\n",
       "                        [-0.0244,  0.0122, -0.1047,  0.0293, -0.0332],\n",
       "                        [-0.0728, -0.0016, -0.0005, -0.0722, -0.0829],\n",
       "                        [-0.0563, -0.0074,  0.0115,  0.0040,  0.0470]],\n",
       "              \n",
       "                       [[-0.0657, -0.0655, -0.0285,  0.0198, -0.0322],\n",
       "                        [-0.0427,  0.0094, -0.0787, -0.0191,  0.0097],\n",
       "                        [-0.0330, -0.0211, -0.0296, -0.0586,  0.0408],\n",
       "                        [-0.0036,  0.0116,  0.0180, -0.0529,  0.0103],\n",
       "                        [-0.0757,  0.0358,  0.0663,  0.0685,  0.0697]],\n",
       "              \n",
       "                       [[-0.0430, -0.0779, -0.0596,  0.0421, -0.0596],\n",
       "                        [-0.0389, -0.0818,  0.0580, -0.0499,  0.0506],\n",
       "                        [-0.0488, -0.0338, -0.0137, -0.0176, -0.0375],\n",
       "                        [ 0.0088, -0.0622, -0.0570, -0.0233,  0.0231],\n",
       "                        [ 0.0272, -0.0399,  0.0093, -0.0683,  0.0117]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0379,  0.0237, -0.0783, -0.0502,  0.0196],\n",
       "                        [ 0.0454,  0.0187,  0.0121, -0.0034,  0.0481],\n",
       "                        [ 0.0172,  0.0391,  0.0132,  0.0260, -0.0402],\n",
       "                        [ 0.0563, -0.0759,  0.0041, -0.0492,  0.0069],\n",
       "                        [-0.0077,  0.0566,  0.0433, -0.0041, -0.0231]],\n",
       "              \n",
       "                       [[-0.0020, -0.0112, -0.0018, -0.0376, -0.0515],\n",
       "                        [-0.0334, -0.0553, -0.0371,  0.0375,  0.0104],\n",
       "                        [ 0.0016, -0.0687, -0.0801, -0.0693,  0.0416],\n",
       "                        [ 0.0148, -0.0291,  0.0546, -0.0670,  0.0482],\n",
       "                        [ 0.0595,  0.0372,  0.0346, -0.0133, -0.0190]],\n",
       "              \n",
       "                       [[ 0.0007,  0.0225, -0.0051,  0.0096,  0.0586],\n",
       "                        [-0.0158,  0.0470, -0.0014,  0.0493, -0.0740],\n",
       "                        [-0.0049, -0.0092,  0.0532,  0.0640,  0.0127],\n",
       "                        [-0.0078,  0.0644, -0.0651,  0.0255, -0.0277],\n",
       "                        [-0.0066, -0.0270,  0.0602,  0.0562,  0.0559]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0599, -0.0168, -0.0406, -0.0609,  0.0463],\n",
       "                        [-0.0099, -0.0578, -0.0726, -0.0502, -0.0750],\n",
       "                        [ 0.0025,  0.0063, -0.0264, -0.0162, -0.0297],\n",
       "                        [-0.0355, -0.0526, -0.0942, -0.0015, -0.0210],\n",
       "                        [-0.0206, -0.0812,  0.0361,  0.0086, -0.0819]],\n",
       "              \n",
       "                       [[-0.0868,  0.0434, -0.0570, -0.0329, -0.0042],\n",
       "                        [-0.0261, -0.0921,  0.0343,  0.0342,  0.0224],\n",
       "                        [ 0.0189,  0.0002,  0.0015, -0.0301,  0.0277],\n",
       "                        [ 0.0225, -0.0359, -0.0935, -0.0163, -0.0450],\n",
       "                        [-0.0882, -0.0437, -0.1041,  0.0174, -0.1047]],\n",
       "              \n",
       "                       [[-0.0555, -0.0600,  0.0235, -0.0258,  0.0124],\n",
       "                        [ 0.0221, -0.0527, -0.0407,  0.0023, -0.0076],\n",
       "                        [-0.0891, -0.0553, -0.0769,  0.0306, -0.0737],\n",
       "                        [-0.0277,  0.0134, -0.0246, -0.0477, -0.0620],\n",
       "                        [ 0.0325,  0.0477,  0.0453,  0.0049, -0.0903]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0319, -0.0164, -0.0794, -0.0378,  0.0317],\n",
       "                        [-0.0550, -0.0276, -0.0578, -0.0626, -0.0650],\n",
       "                        [-0.0296, -0.0891, -0.0439, -0.0697, -0.0709],\n",
       "                        [ 0.0022,  0.0359,  0.0389, -0.0068,  0.0211],\n",
       "                        [ 0.0347,  0.0011, -0.0322,  0.0279, -0.0316]],\n",
       "              \n",
       "                       [[-0.0404, -0.0402, -0.0946, -0.0582,  0.0234],\n",
       "                        [-0.0554,  0.0031, -0.0601, -0.0884,  0.0176],\n",
       "                        [-0.0880, -0.0274, -0.0327,  0.0371, -0.0969],\n",
       "                        [-0.0912, -0.0120,  0.0178, -0.0628,  0.0127],\n",
       "                        [-0.0077,  0.0372, -0.0625, -0.0245,  0.0300]],\n",
       "              \n",
       "                       [[ 0.0938, -0.0771, -0.0505, -0.0146, -0.0629],\n",
       "                        [ 0.0379, -0.0098, -0.0516, -0.0126, -0.0196],\n",
       "                        [ 0.0712, -0.0176, -0.0057,  0.0336, -0.1127],\n",
       "                        [ 0.0870, -0.0707, -0.0661, -0.0769, -0.1031],\n",
       "                        [ 0.0516, -0.0353, -0.0916, -0.0306, -0.0587]]]], device='cuda:0')),\n",
       "             ('localization.3.bias',\n",
       "              tensor([-0.0747, -0.0809, -0.0987,  0.0149,  0.0796,  0.0167,  0.1116, -0.0037,\n",
       "                      -0.0399, -0.0598], device='cuda:0')),\n",
       "             ('fc_loc.0.weight',\n",
       "              tensor([[-0.0838, -0.0187, -0.0091,  ..., -0.0166, -0.0051, -0.0095],\n",
       "                      [ 0.0277, -0.0226,  0.0187,  ...,  0.0018,  0.0360,  0.0469],\n",
       "                      [ 0.0494, -0.0269, -0.0295,  ...,  0.0070, -0.0216,  0.0586],\n",
       "                      ...,\n",
       "                      [ 0.0138, -0.0348,  0.0099,  ..., -0.0238,  0.0569,  0.0745],\n",
       "                      [-0.0160, -0.0380, -0.0206,  ..., -0.0051,  0.0501,  0.0744],\n",
       "                      [ 0.0242,  0.0017, -0.0222,  ...,  0.0192,  0.0269,  0.0683]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_loc.0.bias',\n",
       "              tensor([-0.0261, -0.0178, -0.0212, -0.0151, -0.0527,  0.0580, -0.0388,  0.0200,\n",
       "                      -0.1403,  0.0202, -0.0608, -0.0097, -0.0133, -0.0038,  0.0623, -0.0382,\n",
       "                      -0.0377, -0.0392,  0.0743, -0.0157, -0.0662, -0.0181,  0.0385, -0.0434,\n",
       "                      -0.0133, -0.0139, -0.0035, -0.0587, -0.0376, -0.0268, -0.0154,  0.0271],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_loc.2.weight',\n",
       "              tensor([[ 0.0313,  0.0195,  0.0396,  0.0270,  0.0375,  0.0511,  0.0392,  0.0338,\n",
       "                        0.0172,  0.0393, -0.0057,  0.0360,  0.0404,  0.0389,  0.0423,  0.0430,\n",
       "                       -0.0178,  0.0378,  0.0394,  0.0395,  0.0395,  0.0337,  0.0372,  0.0224,\n",
       "                        0.0367,  0.0330,  0.0369,  0.0342,  0.0334,  0.0358,  0.0378,  0.0301],\n",
       "                      [ 0.0400,  0.0377,  0.0319,  0.0289,  0.0489,  0.0524,  0.0484,  0.0452,\n",
       "                       -0.0342,  0.0388, -0.0403,  0.0461,  0.0360,  0.0467,  0.0546,  0.0476,\n",
       "                       -0.0086,  0.0497,  0.0511,  0.0418, -0.0152,  0.0364,  0.0365, -0.0407,\n",
       "                        0.0243,  0.0393,  0.0385,  0.0136,  0.0487,  0.0400,  0.0266,  0.0425],\n",
       "                      [ 0.0044, -0.0014,  0.0026,  0.0125,  0.0151,  0.0264,  0.0123,  0.0095,\n",
       "                        0.0364,  0.0103, -0.0112,  0.0135, -0.0009,  0.0063,  0.0266,  0.0100,\n",
       "                       -0.0386,  0.0143,  0.0200,  0.0058, -0.0119,  0.0008,  0.0160, -0.0363,\n",
       "                        0.0033,  0.0009,  0.0028, -0.0283,  0.0095,  0.0052,  0.0078,  0.0114],\n",
       "                      [-0.0203, -0.0219, -0.0232, -0.0170, -0.0140,  0.0146, -0.0009, -0.0022,\n",
       "                       -0.0369, -0.0134, -0.0365, -0.0100, -0.0275, -0.0063,  0.0169, -0.0082,\n",
       "                        0.0143, -0.0142, -0.0060, -0.0200,  0.0407, -0.0226,  0.0109, -0.0424,\n",
       "                       -0.0183, -0.0177, -0.0190, -0.0085, -0.0054, -0.0066, -0.0070,  0.0057],\n",
       "                      [ 0.0177,  0.0305,  0.0220, -0.0048,  0.0186,  0.0155,  0.0265,  0.0227,\n",
       "                        0.1426,  0.0260, -0.0352,  0.0187,  0.0104,  0.0191,  0.0410,  0.0101,\n",
       "                        0.0330,  0.0159,  0.0391,  0.0188,  0.0390,  0.0131,  0.0079, -0.0036,\n",
       "                        0.0158,  0.0256,  0.0249, -0.0214,  0.0172,  0.0199, -0.0025,  0.0092],\n",
       "                      [ 0.0302,  0.0194,  0.0120,  0.0046,  0.0203,  0.0367,  0.0207,  0.0216,\n",
       "                       -0.0368,  0.0169, -0.0196,  0.0111, -0.0017,  0.0240,  0.0396,  0.0206,\n",
       "                        0.0073,  0.0064,  0.0348,  0.0124, -0.0422,  0.0295,  0.0366, -0.0025,\n",
       "                        0.0040,  0.0275,  0.0116,  0.0138,  0.0052,  0.0247,  0.0241,  0.0242]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_loc.2.bias',\n",
       "              tensor([ 1.1307,  0.0292, -0.0490, -0.0142,  1.0922, -0.0295], device='cuda:0'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('Speaktrum_by_SOVA.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c1e02f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[0;32m     50\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m(data)\n\u001b[0;32m     53\u001b[0m pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e78ab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Deep_Emotion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m net\u001b[38;5;241m=\u001b[39m \u001b[43mDeep_Emotion\u001b[49m()\n\u001b[0;32m      2\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeaktrum_by_SOVA.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m net\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Deep_Emotion' is not defined"
     ]
    }
   ],
   "source": [
    "net= Deep_Emotion()\n",
    "net.load_state_dict(torch.load('Speaktrum_by_SOVA.pt'))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc36f7c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[0;32m     50\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m(data)\n\u001b[0;32m     53\u001b[0m pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "outputs = net(data)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "        \n",
    "        pred = F.softmax(outputs, dim=1)\n",
    "        prediction = torch.argmax(pred)\n",
    "\n",
    "        print(prediction)\n",
    "\n",
    "        if (prediction == 0):\n",
    "            status = \"Angry, take a deep breath\"\n",
    "            color = (0, 0, 255)\n",
    "        elif (prediction == 2):\n",
    "            status = \"Fear, calm down\"\n",
    "            color = (0, 0, 255)\n",
    "        elif (prediction == 3):\n",
    "            status = \"Happy, you are good\"\n",
    "            color = (0, 0, 255)\n",
    "        elif (prediction == 4):\n",
    "            status = \"Sad, relax and meditate\"\n",
    "            color = (0, 0, 255)\n",
    "        else:\n",
    "            status = \"\"\n",
    "            color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 175, 75\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 3, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (50, 50),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d4104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
