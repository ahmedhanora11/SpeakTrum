{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1849c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archticture:  Deep_Emotion(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "===================================Start Training===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4277: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:4215: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.01433015 \tValidation Loss 0.01474911 \tTraining Acuuarcy 24.278% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 2 \tTraining Loss: 0.01425290 \tValidation Loss 0.01472071 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 3 \tTraining Loss: 0.01424087 \tValidation Loss 0.01479749 \tTraining Acuuarcy 25.042% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 4 \tTraining Loss: 0.01423380 \tValidation Loss 0.01478658 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 5 \tTraining Loss: 0.01422237 \tValidation Loss 0.01472465 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 6 \tTraining Loss: 0.01421965 \tValidation Loss 0.01475685 \tTraining Acuuarcy 25.025% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 7 \tTraining Loss: 0.01422615 \tValidation Loss 0.01469110 \tTraining Acuuarcy 25.047% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 8 \tTraining Loss: 0.01421345 \tValidation Loss 0.01473570 \tTraining Acuuarcy 25.075% \tValidation Acuuarcy 25.188%\n",
      "Epoch: 9 \tTraining Loss: 0.01421489 \tValidation Loss 0.01476555 \tTraining Acuuarcy 24.992% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 10 \tTraining Loss: 0.01421032 \tValidation Loss 0.01471467 \tTraining Acuuarcy 25.020% \tValidation Acuuarcy 25.188%\n",
      "Epoch: 11 \tTraining Loss: 0.01421359 \tValidation Loss 0.01468415 \tTraining Acuuarcy 25.053% \tValidation Acuuarcy 25.383%\n",
      "Epoch: 12 \tTraining Loss: 0.01420150 \tValidation Loss 0.01471585 \tTraining Acuuarcy 25.059% \tValidation Acuuarcy 25.327%\n",
      "Epoch: 13 \tTraining Loss: 0.01420795 \tValidation Loss 0.01468832 \tTraining Acuuarcy 25.120% \tValidation Acuuarcy 25.383%\n",
      "Epoch: 14 \tTraining Loss: 0.01421100 \tValidation Loss 0.01467033 \tTraining Acuuarcy 24.897% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 15 \tTraining Loss: 0.01420113 \tValidation Loss 0.01473661 \tTraining Acuuarcy 25.059% \tValidation Acuuarcy 25.104%\n",
      "Epoch: 16 \tTraining Loss: 0.01420528 \tValidation Loss 0.01471146 \tTraining Acuuarcy 25.036% \tValidation Acuuarcy 25.216%\n",
      "Epoch: 17 \tTraining Loss: 0.01419956 \tValidation Loss 0.01465478 \tTraining Acuuarcy 25.008% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 18 \tTraining Loss: 0.01420353 \tValidation Loss 0.01483925 \tTraining Acuuarcy 25.008% \tValidation Acuuarcy 25.104%\n",
      "Epoch: 19 \tTraining Loss: 0.01419781 \tValidation Loss 0.01486380 \tTraining Acuuarcy 25.198% \tValidation Acuuarcy 25.132%\n",
      "Epoch: 20 \tTraining Loss: 0.01420365 \tValidation Loss 0.01474460 \tTraining Acuuarcy 25.020% \tValidation Acuuarcy 25.104%\n",
      "Epoch: 21 \tTraining Loss: 0.01419582 \tValidation Loss 0.01469700 \tTraining Acuuarcy 25.042% \tValidation Acuuarcy 25.272%\n",
      "Epoch: 22 \tTraining Loss: 0.01419803 \tValidation Loss 0.01474484 \tTraining Acuuarcy 24.969% \tValidation Acuuarcy 25.244%\n",
      "Epoch: 23 \tTraining Loss: 0.01419834 \tValidation Loss 0.01474794 \tTraining Acuuarcy 25.098% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 24 \tTraining Loss: 0.01418851 \tValidation Loss 0.01480083 \tTraining Acuuarcy 25.031% \tValidation Acuuarcy 25.188%\n",
      "Epoch: 25 \tTraining Loss: 0.01418817 \tValidation Loss 0.01489726 \tTraining Acuuarcy 25.003% \tValidation Acuuarcy 25.077%\n",
      "Epoch: 26 \tTraining Loss: 0.01418308 \tValidation Loss 0.01491023 \tTraining Acuuarcy 24.964% \tValidation Acuuarcy 25.049%\n",
      "Epoch: 27 \tTraining Loss: 0.01419885 \tValidation Loss 0.01470756 \tTraining Acuuarcy 25.170% \tValidation Acuuarcy 24.909%\n",
      "Epoch: 28 \tTraining Loss: 0.01418083 \tValidation Loss 0.01479020 \tTraining Acuuarcy 25.086% \tValidation Acuuarcy 24.854%\n",
      "Epoch: 29 \tTraining Loss: 0.01417856 \tValidation Loss 0.01478192 \tTraining Acuuarcy 25.064% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 30 \tTraining Loss: 0.01417897 \tValidation Loss 0.01473482 \tTraining Acuuarcy 25.059% \tValidation Acuuarcy 25.160%\n",
      "Epoch: 31 \tTraining Loss: 0.01418064 \tValidation Loss 0.01477020 \tTraining Acuuarcy 25.103% \tValidation Acuuarcy 24.993%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"CUDA is not available. Make sure you have a CUDA-enabled GPU.\")\n",
    "\n",
    "def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        # Train the model  #\n",
    "        net.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optmizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        #validate the model#\n",
    "        net.eval()\n",
    "        for data,labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            val_outputs = net(data)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs,1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss =  validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
    "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
    "\n",
    "    torch.save(net.state_dict(),'deep_emotion-{}-{}-{}.pt'.format(epochs,batchsize,lr))\n",
    "    print(\"===================================Training Finished===================================\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"Configuration of setup and training process\")\n",
    "#     parser.add_argument('-s', '--setup', type=bool, help='setup the dataset for the first time')\n",
    "#     parser.add_argument('-d', '--data', type=str,required= True,\n",
    "#                                help='data folder that contains data files that downloaded from kaggle (train.csv and test.csv)')\n",
    "#     parser.add_argument('-hparams', '--hyperparams', type=bool,\n",
    "#                                help='True when changing the hyperparameters e.g (batch size, LR, num. of epochs)')\n",
    "#     parser.add_argument('-e', '--epochs', type= int, help= 'number of epochs')\n",
    "#     parser.add_argument('-lr', '--learning_rate', type= float, help= 'value of learning rate')\n",
    "#     parser.add_argument('-bs', '--batch_size', type= int, help= 'training/validation batch size')\n",
    "#     parser.add_argument('-t', '--train', type=bool, help='True when training')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if args.setup :\n",
    "#         generate_dataset = Generate_data(args.data)\n",
    "#         generate_dataset.split_test()\n",
    "#         generate_dataset.save_images('train')\n",
    "#         generate_dataset.save_images('test')\n",
    "#         generate_dataset.save_images('val')\n",
    "\n",
    "#     if args.hyperparams:\n",
    "#         epochs = args.epochs\n",
    "#         lr = args.learning_rate\n",
    "#         batchsize = args.batch_size\n",
    "#     else :\n",
    "epochs = 3500\n",
    "lr = 0.005\n",
    "batchsize = 128\n",
    "\n",
    "#     if args.train:\n",
    "net = Deep_Emotion()\n",
    "net.to(device)\n",
    "print(\"Model archticture: \", net)\n",
    "traincsv_file = 'data'+'/'+'train.csv'\n",
    "validationcsv_file = 'data'+'/'+'val.csv'\n",
    "train_img_dir = 'data'+'/'+'train/'\n",
    "validation_img_dir = 'data'+'/'+'val/'\n",
    "\n",
    "transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
    "validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
    "train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optmizer= optim.Adam(net.parameters(),lr= lr)\n",
    "Train(epochs, train_loader, val_loader, criterion, optmizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfc823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
