{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba8df2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep_Emotion(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
       "  (localization): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_loc): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deep_emotion import Deep_Emotion  # Assuming `Deep_Emotion` is defined in a separate module\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Choose the appropriate device\n",
    "\n",
    "net = Deep_Emotion()  # Instantiate the Deep_Emotion model\n",
    "net.load_state_dict(torch.load('deep_emotion-1500-128-0.001.pt'))  # Load the saved state dictionary\n",
    "net.to(device)  # Move the model to the specified device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063858df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp, (48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if (prediction == 0):\n",
    "                status = \"Angry, take a deep breath\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 2):\n",
    "                status = \"Fear, calm down\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 3):\n",
    "                status = \"Happy, you are good\"\n",
    "                color = (0, 0, 255)\n",
    "            elif (prediction == 4):\n",
    "                status = \"Sad, relax and meditate\"\n",
    "                color = (0, 0, 255)\n",
    "            else:\n",
    "                status = \"\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 400, 40\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, status, (100, 150), font, 0, color, 2, cv2.LINE_4)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,\n",
    "                    status,\n",
    "                    (20, 20),\n",
    "                    font, 0,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_4)\n",
    "        cv2.imshow('Face', frame)\n",
    "\n",
    "        if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1deb723d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[0;32m     63\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 65\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(pred)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'ellipsis' object is not callable"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the pre-trained model\n",
    "net = ...  # Replace with your model initialization\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale = 1\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    # Check if the webcam is opened correctly\n",
    "    cap = cv2.VideoCapture(2)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Define emotion labels and corresponding colors\n",
    "emotion_labels = {\n",
    "    0: (\"Angry\", (0, 0, 255)),\n",
    "    2: (\"Fear\", (0, 0, 255)),\n",
    "    3: (\"Happy\", (0, 255, 0)),\n",
    "    4: (\"Sad\", (255, 0, 0))\n",
    "}\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    face_count = len(faces)  # Number of detected faces\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        if len(facess) == 0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex+ew]  # cropping the face\n",
    "\n",
    "            graytemp = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            final_image = cv2.resize(graytemp,(48, 48))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add third dimension\n",
    "            final_image = np.expand_dims(final_image, axis=0)  # Add fourth dimension\n",
    "            final_image = final_image / 255.0  # Normalization\n",
    "\n",
    "            data = torch.from_numpy(final_image)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            data = data.to(device)\n",
    "\n",
    "            outputs = net(data)\n",
    "            pred = F.softmax(outputs, dim=1)\n",
    "            prediction = torch.argmax(pred)\n",
    "\n",
    "            print(prediction)\n",
    "\n",
    "            if prediction.item() in emotion_labels:\n",
    "                emotion, color = emotion_labels[prediction.item()]\n",
    "                status = f\"{emotion}, take appropriate action\"\n",
    "            else:\n",
    "                status = \"Unknown emotion\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "            x1, y1, w1, h1 = 0, 0, 400, 40\n",
    "            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, status, (x1 + int(w1 / 10), y1 + int(h1 / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color)\n",
    "\n",
    "    # Display the face count\n",
    "    cv2.putText(frame, f\"Face Count: {face_count}\", (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d6f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
